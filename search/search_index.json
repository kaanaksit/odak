{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#getting-started","title":"Getting started","text":"<p> Informative</p> <p>Odak (pronounced \"O-dac\") is the fundamental library for scientific computing in optical sciences, computer graphics, and visual perception. We designed this page to help first-time users, new contributors, and existing users understand where to go within this documentation when they need help with certain aspects of Odak. If you think you need a refresher or are a beginner willing to learn more about light and computation, we created an entire course named <code>Computational Light</code> for you to get to pace with the computational aspects of light.</p>"},{"location":"#absolute-beginners","title":"Absolute Beginners","text":"<p> Informative \u00b7  Practical</p> <p>Computational Light Course: Learn Odak and Physics of Light</p>"},{"location":"#new-users","title":"New Users","text":"<p> Informative</p> <ul> <li><code>What is Odak?</code></li> <li><code>Installation</code></li> </ul>"},{"location":"#use-cases","title":"Use cases","text":"<p> Informative</p> <ul> <li><code>Computer-generated holography</code></li> <li><code>General toolkit</code></li> <li><code>Optical Raytracing</code></li> <li><code>Machine Learning</code></li> <li><code>Visual Perception</code></li> <li><code>Lensless Cameras</code></li> </ul>"},{"location":"#new-contributors","title":"New contributors","text":"<p> Informative</p> <ul> <li><code>Contributing to Odak</code></li> </ul>"},{"location":"#additional-information","title":"Additional information","text":"<p> Informative</p> <ul> <li><code>Citing Odak in a scientific publication using Zenodo</code></li> <li><code>License of Odak</code></li> <li><code>Reporting bugs or requesting a feature</code></li> </ul> <p>Reminder</p> <p>We host a Slack group with more than 300 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link. Readers can get in-touch with the wider community using this public group.</p>"},{"location":"beginning/","title":"What is Odak?","text":""},{"location":"beginning/#what-is-odak","title":"What is Odak?","text":"<p>Odak (pronounced \"O-dac\") is the fundamental library for scientific computing in optical sciences, computer graphics and visual perception.</p>"},{"location":"beginning/#why-does-it-exist","title":"Why does it exist?","text":"<p>This question has two answers.  One of them is related to the history of <code>Odak</code>, which is partially answered in the next section. The other answer lies in what kind of submodules <code>Odak</code> has in it. Depending on a need of a scientist at all levels or a professional from the industry, these submodules can help the design processes in optics and visual perception.</p> <p>Odak includes modules for geometric 3D raytracing, Jones calculus, wave optics, and a set of tools to ease pain in measurement, exporting/importing CAD, and visualization during a design process. We have generated a set of recipes that go well with machine learning approaches compatible with the PyTorch learning framework as provided here. We have created many test scripts to inspire how you use Odak and helping your design process. Finally, we have created a distribution system to process tasks in parallel across multiple computing resources within the same network. Odak can either run using CPUs or automatically switch to NVIDIA GPUs.</p>"},{"location":"beginning/#history","title":"History","text":"<p>In the summer of 2011, I, Kaan Ak\u015fit, was a PhD student. At the time, I had some understanding of the Python programming language, and I created my first Python based computer game using <code>pygame</code>, a fantastic library, over a weekend in 2009. I was actively using Python to deploy packages for the Linux distribution that I supported at the time, Pardus. Meantime, that summer, I didn't have any internship or any vital task that I had to complete. I was super curious about the internals of the optical design software that I used at the time, <code>ZEMAX</code>. All of this lead to an exciting never-ending excursion that I still enjoy to this day, which I named Odak. <code>Odak</code> means focus in Turkish, and pronounced as <code>O-dac</code>.</p> <p>The very first paper I read to build the pieces of Odak was <code>General Ray tracing procedure\" from G.H. Spencer and M.V.R.K Murty</code>, an article on routines for raytracing, published at the Journal of the Optical Society of America, Issue 6, Volume 52, Page 672. It helped to add reflection and refraction functions required in a raytracing routine. I continuously add to Odak over my entire professional life. That little raytracing program I wrote in 2011 is now a vital library for my research, and much more than a raytracer.</p> <p>I can write pages and pages about what happened next. You can accurately estimate what happened next by checking my website and my cv. But I think the most critical part is always the beginning as it can inspire many other people to follow their thoughts and build their own thing! I used Odak in my all published papers. When I look back, I can only say that I am thankful to 2011 me spending a part of his summer in front of a computer to code a raytracer for optical design. Odak is now more than a raytracer, expanding on many other aspects of light, including vision science, polarization optics, computer-generated holography or machine learning routines for light sciences. Odak keeps on growing thanks to a body of people that contributed over time. I will keep it growing in the future and will continually transform into the tool that I need to innovate. All of it is free as in free-free, and all is sharable as I believe in people.</p>"},{"location":"cgh/","title":"Getting Started","text":""},{"location":"cgh/#computer-generated-holography","title":"Computer-Generated Holography","text":"<p>Odak contains essential ingredients for research and development targeting Computer-Generated Holography. We consult the beginners in this matter to <code>Goodman's Introduction to Fourier Optics</code> book (ISBN-13:  978-0974707723) and <code>Principles of optics: electromagnetic theory of propagation, interference and diffraction of light</code> from Max Born and Emil Wolf (ISBN 0-08-26482-4). In the rest of this document, you will find engineering notes and relevant functions in Odak that helps you describing complex nature of light on a computer. Note that, the creators of this documentation are from <code>Computational Displays</code> domain, however the provided submodules can potentially aid other lines of research as well, such as <code>Computational Imaging</code> or <code>Computational Microscopy</code>.</p>"},{"location":"cgh/#engineering-notes","title":"Engineering notes","text":"Note Description <code>Holographic light transport</code> This engineering note will give you an idea about how coherent light propagates in free space. <code>Optimizing phase-only single plane holograms using Odak</code> This engineering note will give you an idea about how to calculate phase-only holograms using Odak. <code>Learning the model of a holographic display</code> This link navigates to a project website that provides a codebase that can learn the model of a holographic display using a single complex kernel. <code>Optimizing three-dimensional multiplane holograms using Odak</code> This link navigates to a project website that provides a codebase that can help optimize a phase-only hologram representing multiplanar three-dimensional scenes."},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing-to-odak","title":"Contributing to Odak","text":"<p>Odak is in constant development.  We shape Odak according to the most current needs in our scientific research. We welcome both users and developers in the open-source community as long as they have good intentions (e.g., scientific research). For the most recent description of Odak, please consult our description. If you are planning to use Odak for industrial purposes, please reach out to Kaan Ak\u015fit. All of the Odak contributors are listed in our <code>THANKS.txt</code> and added to <code>CITATION.cff</code> regardless of how much they contribute to the project. Their names are also included in our Digital Object Identifier (DOI) page.</p>"},{"location":"contributing/#contributing-process","title":"Contributing process","text":"<p>Contributions to Odak can come in different forms. It can either be code or documentation related contributions. Historically, Odak has evolved through scientific collaboration, in which authors of Odak identified a collaborative project with a new potential contributor. You can always reach out to Kaan Ak\u015fit to query your idea for potential collaborations in the future.  Another potential place to identify likely means to improve odak is to address outstanding issues of Odak.</p>"},{"location":"contributing/#code","title":"Code","text":"<p>Odak's <code>odak</code> directory contains the source code.  To add to it, please make sure that you can install and test Odak on your local computer. The installation documentation contains routines for installation and testing, please follow that page carefully.</p> <p>We typically work with <code>pull requests</code>.  If you want to add new code to Odak, please do not hesitate to fork Odak's git repository and have your modifications on your fork at first. Once you test the modified version, please do not hesitate to initiate a pull request. We will revise your code, and if found suitable, it will be merged to the master branch. Remember to follow <code>numpy</code> convention while adding documentation to your newly added functions to Odak. Another thing to mention is regarding to the code quality and standard. Although it hasn't been strictly followed since the start of Odak, note that Odak follows code conventions of <code>flake8</code>, which can be installed using:</p> <pre><code>pip3 install flake8\n</code></pre> <p>You can always check for code standard violations in Odak by running these two commands:</p> <pre><code>flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n</code></pre> <p>There are tools that can automatically fix code in terms of following standards. One primary tool that we are aware of is <code>autopep8</code>, which can be installed using:</p> <pre><code>pip3 install autopep8\n</code></pre> <p>Please once you are ready to have a pull request, make sure to add a unit test for your additions in <code>test</code> folder, and make sure to test all unit tests by running <code>pytest</code>. If your system do not have <code>pytest</code> installed, it can be installed using:</p> <pre><code>pip3 install pytest\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>Under Odak's source's root directory, you will find a folder named <code>docs</code>. This directory contains all the necessary information to generate the pages in this documentation. If you are interested in improving the documentation of Odak, this directory is the place where you will be adding things.</p> <p>Odak's documentation is built using <code>mkdocs</code>. At this point, I assume that you have successfully installed Odak on your system. If you haven't yet, please follow installation documentation. To be able to run documentation locally, make sure to have the correct dependencies installed properly:</p> <pre><code>pip3 install plyfile\npip3 install Pillow\npip3 install tqdm\npip3 install mkdocs-material\npip3 install mkdocstrings\n</code></pre> <p>Once you have dependencies appropriately installed, navigate to the source directory of Odak in your hard drive and run a test server:</p> <pre><code>cd odak\nmkdocs serve\n</code></pre> <p>If all goes well, you should see a bunch of lines on your terminal, and the final lines should look similar to these:</p> <pre><code>INFO     -  Documentation built in 4.45 seconds\nINFO     -  [22:15:22] Serving on http://127.0.0.1:8000/odak/\nINFO     -  [22:15:23] Browser connected: http://127.0.0.1:8000/odak/\n</code></pre> <p>At this point, you can start your favourite browser and navigate to <code>http://127.0.0.1:8000/odak</code> to view documentation locally. This local viewing is essential as it can help you view your changes locally on the spot before actually committing. One last thing to mention here is the fact that Odak's <code>docs</code> folder's structure is self-explanatory. It follows <code>markdown</code> rules, and <code>mkdocsstrings</code> style is <code>numpy</code>.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>We use odak with Linux operating systems.  Therefore, we don't know if it can work with Windows or Mac operating systems. Odak can be installed in multiple ways.  However, our recommended method for installing Odak is using pip distribution system.  We update Odak within pip with each new version. Thus, the most straightforward way to install Odak is to use the below command in a Linux shell:</p> <p></p><pre><code>pip3 install odak\n</code></pre> Note that Odak is in constant development.  One may want to install the latest and greatest odak in the source repository for their reasons. In this case, our recommended method is to rely on pip for installing Odak from the source using:<p></p> <pre><code>pip3 install git+https://github.com/kaanaksit/odak\n</code></pre> <p>One can also install Odak without pip by first getting a local copy and installing using Python.  Such an installation can be conducted using:</p> <pre><code>git clone git@github.com:kaanaksit/odak.git\ncd odak\npip3 install -r requirements.txt\npip3 install -e .\n</code></pre>"},{"location":"installation/#uninstalling-the-development-version","title":"Uninstalling the Development version","text":"<p>If you have to remove the development version of <code>odak</code>, you can first try:</p> <pre><code>pip3 uninstall odak\nsudo pip3 uninstall odak\n</code></pre> <p>And if for some reason, you are still able to import <code>odak</code> after that, check <code>easy-install.pth</code> file which is typically found <code>~/.local/lib/pythonX/site-packages</code>, where <code>~</code> refers to your home directory and <code>X</code> refers to your Python version. In that file, if you see odak's directory listed, delete it. This will help you remove development version of <code>odak</code>.</p>"},{"location":"installation/#notes-before-running","title":"Notes before running","text":"<p>Some notes should be highlighted to users, and these include:</p> <ul> <li>Odak installs <code>PyTorch</code> that only uses <code>CPU</code>.  To properly install <code>PyTorch</code> with GPU support, please consult PyTorch website.</li> </ul>"},{"location":"installation/#testing-an-installation","title":"Testing an installation","text":"<p>After installing Odak, one can test if Odak has been appropriately installed with its dependencies by running the unit tests. To be able to run unit tests, make sure to have <code>pytest</code> installed:</p> <pre><code>pip3 install -U pytest\n</code></pre> <p>Once <code>pytest</code> is installed, unit tests can be run by calling:</p> <p></p><pre><code>cd odak\npytest\n</code></pre> The tests should return no error. However, if an error is encountered, please start a new issue to help us be aware of the issue.<p></p>"},{"location":"lensless/","title":"Getting Started","text":""},{"location":"lensless/#lensless-imaging","title":"Lensless Imaging","text":"<p>Odak contains essential ingredients for research and development targeting Lensless Imaging.</p>"},{"location":"machine_learning/","title":"Introduction","text":""},{"location":"machine_learning/#machine-learning","title":"Machine learning","text":"<p>Odak provides a set of function that implements classical methods in machine learning. Note that these functions are typically basing on <code>Numpy</code>. Thus, they do not take advantage from automatic differentiation found in <code>Torch</code>. The soul reason why these functions exists is because they stand as an example for impelementing basic methods in machine learning.</p>"},{"location":"perception/","title":"Getting Started","text":""},{"location":"perception/#visual-perception","title":"Visual perception","text":"<p>The <code>perception</code> module of <code>odak</code> focuses on visual perception, and in particular on gaze-contingent perceptual loss functions. </p>"},{"location":"perception/#metamers","title":"Metamers","text":"<p>It contains an implementation of a metameric loss function. When used in optimisation tasks, this loss function enforces the optimised image to be a ventral metamer to the ground truth image.</p> <p>This loss function is based on previous work on fast metamer generation. It uses the same statistical model and many of the same acceleration techniques (e.g. MIP map sampling) to enable the metameric loss to run efficiently.</p>"},{"location":"perception/#engineering-notes","title":"Engineering notes","text":"Note Description <code>Using metameric loss in Odak</code> This engineering note will give you an idea about how to use the metameric perceptual loss in Odak."},{"location":"raytracing/","title":"Introduction","text":""},{"location":"raytracing/#raytracing","title":"Raytracing","text":"<p>Odak provides a set of function that implements methods used in raytracing. The ones implemented in <code>Numpy</code>, such as <code>odak.raytracing</code>, are not differentiable. However, the ones impelemented in <code>Torch</code>, such as <code>odak.learn.raytracing</code>, are differentiable.</p>"},{"location":"toolkit/","title":"Introduction","text":""},{"location":"toolkit/#general-toolkit","title":"General toolkit.","text":"<p>Odak provides a set of functions that can be used for general purpose work, such as saving an image file or loading a three-dimensional point cloud of an object. These functions are helpful for general use and provide consistency across routine works in loading and saving routines. When working with odak, we strongly suggest sticking to the general toolkit to provide a coherent solution to your task.</p>"},{"location":"toolkit/#engineering-notes","title":"Engineering notes","text":"Note Description <code>Working with images</code> This engineering note will give you an idea about how read and write images using odak. <code>Working with dictionaries</code> This engineering note will give you an idea about how read and write dictionaries using odak."},{"location":"course/","title":"Prerequisites and general information","text":"Narrate section"},{"location":"course/#prerequisites-and-general-information","title":"Prerequisites and general information","text":"<p>You have reached the website for the Computational Light Course.</p> <p>This page is the starting point for the Computational Light course. Readers can follow the course material found on these pages to learn more about the field of Computational Light. I encourage readers to carefully read this page to decide if they want to continue with the course.</p>"},{"location":"course/#brief-course-description","title":"Brief course description","text":"<p> Informative</p> <p>Computational Light is a term that brings the concepts in computational methods with the characteristics of light. So more specifically, wherever we can program the qualities of light, such as intensity or direction, this will bring us into the topics of Computational Light. Some well-known subfields of Computational Light are Computer Graphics, Computational Displays, Computational Photography, Computational Imaging and Sensing, Computational Optics and Fabrication, Optical Communication, and All-optical Machine Learning.</p> <p>   Future is yet to be decided. Will you help me build it? A rendering from Telelife vision paper <sup>1</sup>.</p> <p>Computational Light Course bridges the gap between Computer Science and physics. In other words, Computational Light Course offers students a gateway to get familiar with various aspects of the physics of light, the human visual system, computational methods in designing light-based devices, and applications of light. Precisely, students will familiarize themselves with designing and implementing graphics, display, sensor, and camera systems using state-of-the-art deep learning and optimization methods. A deep understanding of these topics can help students become experts in the computational design of new graphics, displays, sensors, and camera systems.</p>"},{"location":"course/#what-is-computational-light","title":"What is Computational Light?","text":"<p> Informative</p> <p>Computational light is a term that brings the concepts in computational methods with the characteristics of light. In other words, wherever we can program the qualities of light, this will get us into the topics of computational light. Programming light may sound unconventional, but I invite you to consider how we program current computers. These conventional computers interpret voltage levels in an electric signal as ones and zeros. Color, \\(\\lambda\\), propagation direction, \\(\\vec{r}\\), amplitude, \\(A\\), phase, \\(\\phi\\), polarization, diffraction, and interference are all qualities that could help us program light to achieve tasks for specific applications. For now, many of these suggested programable qualities of light may be unknown to you. We will cover all these qualities in this course, so please patient in terms of growing your understanding on the topic.</p>"},{"location":"course/#applications-of-computational-light","title":"Applications of Computational Light","text":"<p> Informative \u00b7  Media</p> <p>There are enormous amounts of applications of light. Let us glance at some of the important ones to get a sense of possibilities for people studying the topics of computational light. For each topic highlighted below, please click on the box to discover more about that specific subfield of computational light.</p> Computer Graphics <p>Computer Graphics deals with generating synthetic images using computers and simulations of light. Common examples of Computer Graphics are the video games we all play and are familiar with. In today's world, you can often find Computer Graphics as a tool to simulate and synthesize scenes for developing a trending topic, artificial intelligence.</p> <ul> <li>Noticeable profiles. Like in any field, there are noticeable people in this field that you may want to observe their profiles to get a sense of who they are, what they achieve, or what they built for the development of modern Computer Graphics. Here are some people I would encourage you to explore their websites: Peter Shirley, Henry Fuchs, Turner Whitted and Morgan Mcguire.</li> <li>Successful products. Here are a few examples of successful outcomes from the field of Computer Graphics: Roblox, NVIDIA's DLSS, Apple's Metal, OpenGL, Vulkan and Stable Diffusion.</li> <li>Did you know? The lecturer of the Computational Light Course, Kaan Ak\u015fit, is actively researching topics of Computer Graphics (e.g., Beyond blur: Real-time ventral metamers for foveated rendering<sup>2</sup>).</li> <li>Want to learn more? Although we will cover a great deal of Computer Graphics in this course, you may want to dig deeper with a dedicated course, which you can follow online: </li> </ul> Computational Displays <p>Computational Displays topic deals with inventing next-generation display technology for the future of human-computer interaction. Common examples of emerging Computational Displays are near-eye displays such as Virtual Reality headsets and Augmented Reality Glasses. Today, we all use displays as a core component for any visual task, such as working, entertainment, education, and many more.</p> <ul> <li>Noticeable profiles. Like in any field, there are noticeable people in this field that you may want to observe their profiles to get a sense of who they are, what they achieve, or what they built for the development of Computational Displays. Here are some examples of such people; I would encourage you to explore their websites: Rafa\u0142 Mantiuk, and Andrew Maimone.</li> <li>Successful products. Here are a few examples of successful outcomes from the field of Computational Displays: Xreal Augmented Reality glasses, Meta Quest Virtual Reality headsets and Asus Gaming Monitors.</li> <li>Did you know? The lecturer of the Computational Light Course, Kaan Ak\u015fit, is actively researching topics of Computational Displays (e.g., Near-Eye Varifocal Augmented Reality Display using See-Through Screens <sup>3</sup>). Kaan has made noticeable contributions to three-dimensional displays, virtual reality headsets, and augmented reality glasses.</li> <li>Want to learn more? Although we will cover a great deal of Computational Displays in this course, you may want to dig deeper with a dedicated course, which you can follow online <sup>4</sup>:  </li> </ul> Computational Photography <p>Computational Photography topic deals with digital image capture based on optical hardware such as cameras. Common examples of emerging Computational Photography are smartphone applications such as shooting in the dark or capturing selfies. Today, we all use products of Computational Photography to capture glimpses from our daily lives and store them as memories.</p> <ul> <li>Noticeable profiles. Like in any field, there are noticeable people in this field that you may want to observe their profiles to get a sense of who they are, what they achieve, or what they built for the development of Computational Displays. Here are some examples of such people; I would encourage you to explore their websites: Diego Gutierrez and Jinwei Gu.</li> <li>Successful products. Here are a few examples of successful outcomes from the field of Computational Displays: Google's Night Sight and Samsung Camera modes.</li> <li>Want to learn more? Although we will cover relevant information for Computational Photography in this course, you may want to dig deeper with a dedicated course, which you can follow online:  </li> </ul> Computational Imaging and Sensing <p>Computational Imaging and Sensing topic deal with imaging and sensing certain scene qualities. Common examples of Computational Imaging and Sensing can be found in the two other domains of Computational Light: Computational Astronomy and Computational Microscopy. Today, medical diagnoses of biological samples in hospitals or imaging stars and beyond or sensing vital signals are all products of Computational Imaging and Sensing.</p> <ul> <li>Noticeable profiles. Like in any field, there are noticeable people in this field that you may want to observe their profiles to get a sense of who they are, what they achieve, or what they built for the development of Computational Imaging and Sensing. Here are some examples of such people; I would encourage you to explore their websites: Laura Waller and Nick Antipa.</li> <li>Successful products. Here are a few examples of successful outcomes from the field of Computational Imaging and Sensing: Zeiss Microscopes and Heart rate sensors on Apple's Smartwatch.</li> <li>Did you know? The lecturer of the Computational Light Course, Kaan Ak\u015fit, is actively researching topics of Computational Imaging and Displays (e.g., Unrolled Primal-Dual Networks for Lensless Cameras <sup>5</sup>).</li> <li>Want to learn more? Although we will cover a great deal of Computational Imaging and Sensing in this course, you may want to dig deeper with a dedicated course, which you can follow online:  </li> </ul> Computational Optics and Fabrication <p>The Computational Optics and Fabrication topic deals with designing and fabricating optical components such as lenses, mirrors, diffraction gratings, holographic optical elements, and metasurfaces.  There is a little bit of Computational Optics and Fabrication in every sector of Computational Light, especially when there is a need for custom optical design.</p> <ul> <li>Noticeable profiles. Like in any field, there are noticeable people in this field that you may want to observe their profiles to get a sense of who they are, what they achieve, or what they built for the development of Computational Optics and Fabrication. Here are some examples of such people; I would encourage you to explore their websites: Jannick Rolland and Mark Pauly.</li> <li>Did you know? The lecturer of the Computational Light Course, Kaan Ak\u015fit, is actively researching topics of Computational Optics and Fabrication (e.g., Manufacturing application-driven foveated near-eye displays <sup>6</sup>).</li> <li>Want to learn more? Although we will cover a great deal of Computational Imaging and Sensing in this course, you may want to dig deeper with a dedicated course, which you can follow online:  </li> </ul> Optical Communication <p>Optical Communication deals with using light as a medium for telecommunication applications. Common examples of Optical Communication are the fiber cables and satellites equipped with optical links in space running our Internet. In today's world, Optical Communication runs our entire modern life by making the Internet a reality.</p> <ul> <li>Noticeable profiles. Like in any field, there are noticeable people in this field that you may want to observe their profiles to get a sense of who they are, what they achieve, or what they built for the development of modern Optical Communication. Here are some people I would encourage you to explore their websites: Harald Haas and Anna Maria Vegni.</li> <li>Did you know? The lecturer of the Computational Light Course, Kaan Ak\u015fit, was researching topics of Optical Communication (e.g., From sound to sight: Using audio processing to enable visible light communication <sup>7</sup>).</li> <li>Want to learn more? Although we will cover relevant information for Optical Communication in this course, you may want to dig deeper and could start with this online video: </li> </ul> All-optical Machine Learning <p>All-optical Machine Learning deals with building neural networks and computers running solely based on light. As this is an emerging field, there are yet to be products in this field that we use in our daily lives. But this also means there are opportunities for newcomers and investors in this space.</p> <ul> <li>Noticeable profiles. Like in any field, there are noticeable people in this field that you may want to observe their profiles to get a sense of who they are, what they achieve, or what they built for the development of All-optical Machine Learning. Here are some people I would encourage you to explore their websites: Aydogan Ozcan.</li> <li>Want to learn more? Although we will cover a great deal of All-optical Machine Learning in this course, you may want to dig deeper with a dedicated course, which you can follow online: </li> </ul> Lab work: What are the other fields and interesting profiles out there? <p>Please explore other relevant fields to Computational Light, and explore interesting profiles out there. Please make a list of relevant fields and interesting profiles and report your top three.</p> <p>Indeed, there are more topics related to computational light than the ones highlighted here. If you are up to a challenge for the next phase of your life, you could help the field identify new opportunities with light-based sciences. In addition, there are indeed more topics, more noticeable profiles, successful product examples, and dedicated courses that focus on every one of these topics. Examples are not limited to the ones that I have provided above. Your favorite search engine is your friend to find out more in this case.</p> Lab work: Where do we find good resources? <p>Please explore software projects on GitHub and papers on Google Scholar to find out about works that are relevant to the theme of Computational Light. Please make a list of these projects and report the top three projects that you feel most exciting and interesting.</p>"},{"location":"course/#prerequisites","title":"Prerequisites","text":"<p>These are the prerequisites of Computational Light course:</p> <ul> <li>Background knowledge. First and foremost being fluent in programming with Python programming language and a graduate-level understanding of <code>Linear Algebra,</code> and  <code>Deep Learning</code> are highly required.</li> <li>Skills and abilities. Throughout the entire course, three libraries will be used, and these libraries include <code>odak</code>, <code>numpy</code>, and <code>torch</code>. Familiarity with these libraries is a big plus.</li> <li>Required Resources. Readers need a computer with decent computational resources (e.g., GPU) when working on the provided materials, laboratory work, and projects.  In case you do not have the right resources, consider using Google's Colab service as it is free to students. Note that at each section of the course, you will be provided with relevant reading materials on the spot.</li> <li>Expectations. Readers also need sustainable motivation to learn new things related to the topics of <code>Computational Light,</code> and willing to advance the field by developing, innovating and researching.  In other terms, you are someone motivated to create a positive impact in the society with light related innovations.  You can also be someone eager to understand and learn physics behind light and how you can simulate light related phenomena.</li> </ul>"},{"location":"course/#questions-and-answers","title":"Questions and Answers","text":"<p>Here are some questions and answers related to the course that readers may ask:</p> What is the overarching rationale for the module? <p>Historically, physics and electronics departments in various universities study and teach the physics of light. This way, new light-based devices and equipment have been invented, such as displays, cameras, and fiber networks, in the past, and these devices continuously serve our societies. However, emerging topics from mathematics and computer science departments, such as deep learning and advanced optimization methods, unlocked new capabilities for existing light-based devices and started to play a crucial role in designing the next generation of these devices. The Computational Light Course aims to bridge this gap between Computer Science and physics by providing a fundamental understanding of light and computational methods that helps to explore new possibilities with light.</p> Who is the target audience of Computational Light course? <p>The Computational Light course is designed for individuals willing to learn how to develop and invent light-based practical systems for next-generation human-computer interfaces. This course targets a graduate-level audience in Computer Science, Physics and Electrical and Electronics Engineering departments.  However, you do not have to be strictly from one of the highlighted targeted audiences. Simply put, if you think you can learn and are eager to learn, no one will stop you.</p> How can I learn Python programming, linear Algebra and machine learning? <p>There isn't a point in providing references on how to learn <code>Python programming,</code> <code>Linear Algebra,</code> and <code>Deep Learning</code> as there is a vast amount of resources online or in your previous university courses. Your favorite search engine is your friend in this case.</p> How do I install Python, numpy and torch? <p>The installation guide for python, numpy and torch is also available on their websites.</p> How do I install odak? <p>Odak's installation page and README provide the most up-to-date information on installing odak.   But in a nutshell, all you need is to use the following command in a terminal <code>pip3 install odak</code> for the latest version, or if you want to install the latest code from the source, use <code>pip3 install git+https://github.com/kaanaksit/odak</code>.</p> Which Python environment and operating system should I use? <p>I use the Python distribution shipped with a traditional Linux distribution (e.g., Ubuntu). Again, there isn't no one correct answer here for everyone. You can use any operating system (e.g., Windows, Mac) and Python distribution (e.g., conda).</p> Which text editor should I use for programming? <p>I use vim as my text editor. However, I understand that <code>vim</code> could be challenging to adopt, especially as a newcomer. The pattern I observe among collaborators and students is that they use Microsoft's Visual Studio, a competent text editor with artificial intelligence support through subscription and works across various operating systems. I encourage you to make your choice depending on how comfortable you are with sharing your data with companies. Please also remember that I am only making suggestions here. If another text editor works better for you, please use that one (e.g., nano, Sublime Text, Atom, Notepad++, Jupyter Notebooks).</p> Which terminal program to use? <p>You are highly encouraged to use the terminal that you feel most comfortable with. This terminal could be the default terminal in your operating system. I use terminator as it enables my workflow with incredible features and is open source.</p> What is the method of delivery? <p>The proposed course, Computational Light Course, comprises multiple elements in delivery. We list these elements as the followings:</p> <ul> <li>Prerequisites and general information. Students will be provided with a written description of requirements related to the course as in this document.</li> <li>Lectures. The students will attend two hours of classes each week, which will be in-person, virtual, or hybrid, depending on the circumstances (e.g., global pandemic, strikes).</li> <li>Supplementary Lectures. Beyond weekly classes, students will be encouraged to follow several other sources through online video recordings.</li> <li>Background review. Students often need a clear development guideline or a stable production pipeline. Thus, in every class and project, a phase of try-and-error causes the student to lose interest in the topic, and often students need help to pass the stage of getting ready for the course and finding the right recipe to complete their work. Thus, we formulate a special session to review the course's basics and requirements. This way, we hope to overcome the challenges related to the \"warming up\" stage of the class.</li> <li>Lecture content. We will provide the students with a lecture book composed of chapters. These chapters will be discussed at each weekly lecture. The book chapters will be distributed online using Moodle (requires UCL access), and a free copy of this book will also be reachable without requiring UCL access.</li> <li>Laboratory work. Students will be provided with questions about their weekly class topics. These questions will require them to code for a specific task. After each class, students will have an hour-long laboratory session to address these questions by coding. The teaching assistants of the lecture will support each laboratory session.</li> <li>Supporting tools. We continuously develop new tools for the emerging fields of Computational Light. Our development tools will be used in the delivery. These tools are publicly available in Odak, our research toolkit with Mozilla Public License 2.0. Students will get a chance to use these tools in their laboratory works and projects. In the meantime, they will also get the opportunity to contribute to the next versions of the tool.</li> <li>Project Assignments. Students will be evaluated on their projects. The lecturer will suggest projects related to the topics of Computational Light. However, the students will also be highly encouraged to propose projects for successfully finishing their course. These projects are expected to address a research question related to the topic discussed. Thus, there are multiple components of a project. These are implementation in coding, manuscript in a modern paper format, a website to promote the work to wider audiences, and presentation of the work to other students and the lecturer.</li> <li>Office hours. There will be office hours for students willing to talk to the course lecturer, Kaan Ak\u015fit, in a one-on-one setting. Each week, the lecturer will schedule two hours for such cases.</li> </ul> What is the aim of this course? <p>Computational Light Course aims to train individuals that could potentially help invent and develop the next generation of light-based devices, systems and software. To achieve this goal, Computational Light Course, will aim:</p> <ul> <li>To educate students on physics of light, human visual system and computational methods relevant to physics of light based on optimizations and machine learning techniques,</li> <li>To enable students the right set of practical skills in coding and design for the next generation of light-based systems,</li> <li>And to increase literacy on light-based technologies among students and professionals.</li> </ul> What are the intended learning outcomes of this course? <p>Students who have completed Computational Light Course successfully will have literacy and practical skills on the following items:</p> <ul> <li>Physics of Light and applications of Computational Light,</li> <li>Fundamental knowledge of managing a software project (e.g., version and authoring tools, unit tests, coding style, and grammar),</li> <li>Fundamental knowledge of optimization methods and state-of-the-art libraries aiming at relevant topics,</li> <li>Fundamental knowledge of visual perception and the human visual system,</li> <li>Simulating light as geometric rays, continous waves, and quantum level,</li> <li>Simulating imaging and displays systems, including Computer-Generated Holography,</li> <li>Designing and optimizing imaging and display systems,</li> <li>Designing and optimizing all-optical machine learning systems.</li> </ul> <p>Note that the above list is always subject to change in order or topic as society's needs move in various directions.</p> How to cite this course? <p>For citing using latex's bibtex bibliography system: </p><pre><code>@book{aksit2024computationallight,\n  title = {Computational Light},\n  author = {Ak{\\c{s}}it, Kaan and Kam, Henry},\n  booktitle = {Computational Light Course Notes},\n  year = {2024}\n}\n</code></pre> For plain text citation: <code>Kaan Ak\u015fit and Henry Kam, \"Computational Light Course\", 2024.</code> <p></p>"},{"location":"course/#team","title":"Team","text":"<p>Kaan Ak\u015fit</p> <p>Instructor</p> <p> E-mail </p> <p>Henry Kam</p> <p>Contributor</p> <p> E-mail </p> <p>Contact Us</p> <p>The preferred way of communication is through the discussions section of odak. Please only reach us through email if the thing you want to achieve, establish, or ask is not possible through the suggested route.</p>"},{"location":"course/#outreach","title":"Outreach","text":"<p>We host a Slack group with more than 300 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link. Readers can get in-touch with the wider community using this public group.</p>"},{"location":"course/#acknowledgements","title":"Acknowledgements","text":"<p>Acknowledgements</p> <p>We thank our readers. We also thank Yicheng Zhan for his feedback.</p> <p>Interested in supporting?</p> <p>Enjoyed our course material and want us to do better in the future? Please consider supporting us monetarily, citing our work in your next scientific work, or leaving us a star for odak.</p> <ol> <li> <p>Jason Orlosky, Misha Sra, Kenan Bekta\u015f, Huaishu Peng, Jeeeun Kim, Nataliya Kos\u2019 myna, Tobias H\u00f6llerer, Anthony Steed, Kiyoshi Kiyokawa, and Kaan Ak\u015fit. Telelife: the future of remote living. Frontiers in Virtual Reality, 2:763340, 2021.\u00a0\u21a9</p> </li> <li> <p>David R Walton, Rafael Kuffner Dos Anjos, Sebastian Friston, David Swapp, Kaan Ak\u015fit, Anthony Steed, and Tobias Ritschel. Beyond blur: real-time ventral metamers for foveated rendering. ACM Transactions on Graphics, 40(4):1\u201314, 2021.\u00a0\u21a9</p> </li> <li> <p>Kaan Ak\u015fit, Ward Lopes, Jonghyun Kim, Peter Shirley, and David Luebke. Near-eye varifocal augmented reality display using see-through screens. ACM Transactions on Graphics (TOG), 36(6):1\u201313, 2017.\u00a0\u21a9</p> </li> <li> <p>Koray Kavakli, David Robert Walton, Nick Antipa, Rafa\u0142 Mantiuk, Douglas Lanman, and Kaan Ak\u015fit. Optimizing vision and visuals: lectures on cameras, displays and perception. In ACM SIGGRAPH 2022 Courses, pages 1\u201366. 2022.\u00a0\u21a9</p> </li> <li> <p>Oliver Kingshott, Nick Antipa, Emrah Bostan, and Kaan Ak\u015fit. Unrolled primal-dual networks for lensless cameras. Optics Express, 30(26):46324\u201346335, 2022.\u00a0\u21a9</p> </li> <li> <p>Kaan Ak\u015fit, Praneeth Chakravarthula, Kishore Rathinavel, Youngmo Jeong, Rachel Albert, Henry Fuchs, and David Luebke. Manufacturing application-driven foveated near-eye displays. IEEE transactions on visualization and computer graphics, 25(5):1928\u20131939, 2019.\u00a0\u21a9</p> </li> <li> <p>Stefan Schmid, Daniel Schwyn, Kaan Ak\u015fit, Giorgio Corbellini, Thomas R Gross, and Stefan Mangold. From sound to sight: using audio processing to enable visible light communication. In 2014 IEEE Globecom Workshops (GC Wkshps), 518\u2013523. IEEE, 2014.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/computational_displays/","title":"Computational Displays","text":"Is there a good resource for classifying existing Augmented Reality glasses? <p>Using your favorite search engine, investigate if there is a reliable up-to-date table that helps comparing existing Augmented Reality glasses in terms of functionality and technical capabilities (e.g., field-of-View, resolution, focus cues).</p>"},{"location":"course/computational_imaging/","title":"Computational Imaging and Sensing","text":""},{"location":"course/computational_light/","title":"Light, Computation, and Computational Light","text":"Narrate section"},{"location":"course/computational_light/#light-computation-and-computational-light","title":"Light, Computation, and Computational Light","text":"<p>We can establish an understanding of the term <code>Computational Light</code> as we explore the term <code>light</code> and its relation to <code>computation.</code></p>"},{"location":"course/computational_light/#what-is-light","title":"What is light?","text":"<p> Informative</p> <p>Light surrounds us; we see the light and swim in the sea of light. It is indeed a daily matter that we interact by looking out of our window to see what is outside, turning on the lights of a room, looking at our displays, taking pictures of our loved ones, walking in a night lit by moonlight, or downloading media from the internet. Light is an eye-catching festival, reflecting, diffracting, interfering, and refracting. Is light a wave, a ray, or a quantum-related phenomenon? Is light heavy, or weightless? Is light the fastest thing in this universe? Which way does the light go? In a more general sense, how can we use light to trigger more innovations, positively impact our lives, and unlock the mysteries of life? We all experience light, but we must dig deep to describe it clearly.</p> <p>In this introduction, my first intention here is to establish some basic scientific knowledge about light, which will help us understand why it is essential for the future of technology, especially computing. Note that we will cover more details of light as we make progress through different chapters of this course. But let's get this starting with the following statement. Light is electromagnetic radiation, often described as a bundle of photons, a term first coined by Gilbert Lewis in 1926.</p> Where can I learn more about electric and magnetic fields? <p>Beware that the topic of electric and magnetic fields deserves a stand-alone course and has many details to explore. As an undergraduate student, back in the day, I learned about electric and magnetic fields by following a dedicated class and reading this book: <code>Cheng, David Keun. \"Fundamentals of engineering electromagnetics.\" (1993).</code> <sup>1</sup></p> What is a photon? <p>Let me adjust this question a bit: <code>What model is good for describing a photon?</code> There is literature describing a photon as a single particle, and works show photons as a pack of particles.  Suppose you want a more profound understanding than stating that it is a particle. In that case, you may want to dive deep into existing models in relation to the relativity concept: <code>Roychoudhuri, C., Kracklauer, A. F., &amp; Creath, K. (Eds.). (2017). The nature of light: What is a photon?. CRC Press.</code> <sup>2</sup></p> Where can I learn more about history of research on light? <p>There is a website showing noticeable people researching on light since ancient times and their contributions to the research on light. To reach out to this website to get a crash course, click here.</p> <p>Let me highlight that for anything to be electromagnetic, it must have electric and magnetic fields. Let us start with this simple drawing to explain the characteristics of this electromagnetic radiation, light. Note that this figure depicts a photon at the origin of XYZ axes. But bear in mind that a photon's shape, weight, and characteristics are yet to be fully discovered and remain an open research question. Beware that the figure depicts a photon as a sphere to provide ease of understanding. It does not mean that photons are spheres.</p> <p> </p> A sketch showing XYZ axes and a photon depicted as a sphere. <p>Let us imagine that our photon is traveling in the direction of the Z axes (notice \\(\\vec{r}\\), the direction vector). Let us also imagine that this photon has an electric field, \\(\\vec{E}(r,t)\\) oscillating along the Y axes. Typically this electric field is a sinusoidal oscillation following the equation, </p> \\[ \\vec{E}(r,t) = A cos(wt), \\] <p>where \\(A\\) is the amplitude of light, \\(t\\) is the time, \\(\\vec{r}\\) is the propagation direction, \\(w\\) is equal to \\(2\\pi f\\) and \\(f\\) represents the frequency of light.</p> <p> </p> A sketch highligting electric and magnetic fields of light. <p>A period of this sinusoidal oscillation, \\(\\vec{E}(r, t)\\), describes wavelength of light, \\(\\lambda\\). In the most simple terms, \\(\\lambda\\) is also known as the color of light. As light is electromagnetic, there is one more component than \\(\\vec{E}(r,t)\\) describing light. The next component is the magnetic field, \\(\\vec{B}(r, t)\\). The magnetic field of light, \\(\\vec{B}(r, t)\\), is always perpendicular to the electric field of light, \\(\\vec{E}(r, t)\\) (90 degrees along XY plane). Since only one \\(\\lambda\\) is involved in our example, we call our light monochromatic. This light would have been polychromatic if many other \\(\\lambda\\)s were superimposed to create \\(\\vec{E}(r, t)\\). In other words, monochromatic light is a single-color light, whereas polychromatic light contains many colors. The concept of color originated from how we sense various \\(\\lambda\\)s in nature.</p> <p> </p> A sketch showing electromagnetic spectrum with waves labelled in terms of their frequencies and temperatures. <p>But are all electromagnetic waves with various \\(\\lambda\\)s considered as light? The short answer is that we can not call all the electromagnetic radiation light. When we refer to light, we mainly talk about visible light, \\(\\lambda\\)s that our eyes could sense. These \\(\\lambda\\)s defining visible light fall into a tiny portion of the electromagnetic spectrum shown in the above sketch. Mainly, visible light falls into the spectrum covering wavelengths between 380 nm and 750 nm. The tails of visible light in the electromagnetic spectrum, such as near-infrared or ultraviolet regions, could also be referred to as light in some cases (e.g., for camera designers). In this course, although we will talk about visible light, we will also discuss the applications of these regions. </p> <p> </p> A sketch showing (left) electric and magnetic fields of light, and (right) polarization state of light. <p>Let us revisit our photon and its electromagnetic field one more time. As depicted in the above figure, the electric field, \\(\\vec{E}(r, t)\\), oscillates along only one axis: the Y axes. The direction of oscillation in \\(\\vec{E}(r, t)\\) is known as polarization of light. In the above example, the polarization of light is linear. In other words, the light is linearly polarized in the vertical axis. Note that when people talk about polarization of light, they always refer to the oscillation direction of the electric field, \\(\\vec{E}(r, t)\\). But are there any other polarization states of light? The light could be polarized in different directions along the X-axis, which would make the light polarized linearly in the horizontal axis, as depicted in the figure below on the left-hand side. If the light has a tilted electric field, \\(\\vec{E}(r, t)\\), with components both in the X and Y axes, light could still be linearly polarized but with some angle. However, if these two components have delays, \\(\\phi\\), in between in terms of oscillation, say one component is  \\(\\vec{E_x}(r, t) = A_x cos(wt)\\) and the other component is \\(\\vec{E_y}(r, t) = A_y cos(wt + \\phi)\\), light could have a circular polarization if \\(A_x = A_y\\). But the light will be elliptically polarized if there is a delay, \\(\\phi\\), and \\(A_x \\neq A_y\\). Although we do not discuss this here in detail, note that the delay of \\(\\phi\\) will help steer the light's direction in the Computer-Generated Holography chapter.</p> <p> </p> A sketch showing (left) various components of polarization, and (right) a right-handed circular polarization as a sample case. <p>There are means to filter light with a specific polarization as well. Here, we provide a conceptual example.  The below sketch depicts a polarization filter like a grid of lines letting the output light oscillate only in a specific direction.</p> <p> </p> A sketch showing a conceptual example of linear polarization filters. <p>We should also highlight that light could bounce off surfaces by reflecting or diffusing. If the material is proper (e.g., dielectric mirror), the light will perfectly reflect as depicted in the sketch below on the left-hand side. The light will perfectly diffuse at every angle if the material is proper (e.g., Lambertian diffuser), as depicted in the sketch below on the right-hand side. Though we will discuss these features of light in the Geometric Light chapter in detail, we should also highlight that light could refract through various mediums or diffract through a tiny hole or around a corner.</p> <p> </p> A sketch showing (left) light's reflection off a dielectric mirror (right) light's diffusion off a Lambertian's surface. <p>Existing knowledge on our understanding of our universe also states that light is the fastest thing in the universe, and no other material, thing or being could exceed lightspeed (\\(c = 299,792,458\\) metres per second).</p> \\[ c = \\lambda n f, \\] <p>where \\(n\\) represents refractive index of a medium that light travels.</p> Where can I find more basic information about optics and light? <p>As a graduate student, back in the day, I learned the basics of optics by reading this book without following any course: <code>Hecht, E. (2012). Optics. Pearson Education India.</code> <sup>3</sup></p> <p>We have identified a bunch of different qualities of light so far. Let us summarize what we have identified in a nutshell.</p> <ul> <li>Light is electromagnetic radiation.</li> <li>Light has electric, \\(\\vec{E}(r,t) = A cos(wt)\\), and magnetic fields, \\(\\vec{B}(r,t)\\), that are always perpendicular to each other.</li> <li>Light has color, also known as wavelength, \\(\\lambda\\).</li> <li>When we say light, we typically refer to the color we can see, visible light (390 - 750 nm).</li> <li>The oscillation axis of light's electric field is light's polarization.</li> <li>Light could have various brightness levels, the so-called amplitude of light, \\(A\\).</li> <li>Light's polarization could be at various states with different \\(A\\)s and \\(\\phi\\)s.</li> <li>Light could interfere by accumulating delays, \\(\\phi\\), and this could help change the direction of light.</li> <li>Light could reflect off the surfaces.</li> <li>Light could refract as it changes the medium.</li> <li>Light could diffract around the corners.</li> <li>Light is the fastest thing in our universe.</li> </ul> <p>Remember that the description of light provided in this chapter is simplistic, missing many important details. The reason is to provide an entry and a crash course at first glance is obvious. We will deep dive into focused topics in the following chapters. This way, you will be ready with a conceptual understanding of light.</p> Lab work: Are there any other light-related phenomena? <p>Please find more light-related phenomena not discussed in this chapter using your favorite search engine. Report back your findings.</p> Did you know? <p>Did you know there is an international light day every 16th of May recognized by the United Nations Educational, Scientific and Cultural Organization (UNESCO)?  For more details, click here</p> <p>Reminder</p> <p>We host a Slack group with more than 300 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link. Readers can get in-touch with the wider community using this public group.</p> <ol> <li> <p>David Keun Cheng and others. Fundamentals of engineering electromagnetics. Addison-Wesley Reading, MA, 1993.\u00a0\u21a9</p> </li> <li> <p>Chandra Roychoudhuri, Al F Kracklauer, and Kathy Creath. The nature of light: What is a photon? CRC Press, 2017.\u00a0\u21a9</p> </li> <li> <p>Eugene Hecht. Optics. Pearson Education India, 2012.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/computer_generated_holography/","title":"Computer-Generated Holography","text":"Narrate section"},{"location":"course/computer_generated_holography/#computer-generated-holography","title":"Computer-Generated Holography","text":"<p>In this section, we introduce Computer-Generated Holography (CGH) <sup>1</sup> as another emerging method to simulate light. CGH offers an upgraded but more computationally expensive way to simulating light concerning the raytracing method described in the previous section. This section dives deep into CGH and will explain how CGH differs from raytracing as we go.</p>"},{"location":"course/computer_generated_holography/#what-is-holography","title":"What is holography?","text":"<p> Informative </p> <p>Holography is a method in Optical sciences to represent light distribution using amplitude and phase of light. In much simpler terms, holography describes light distribution emitted from an object, scene, or illumination source over a surface by treating the light as a wave. The primary difference of holography concerning raytracing is that it accounts not only amplitude or intensity of light but also the phase of light. Unlike classical raytracing, holography also includes diffraction and interference phenomena. In raytracing, the smallest building block that defines light is a ray, whereas, in holography, the building block is a light distribution over surfaces. In other terms, while raytracing traces rays, holography deals with surface-to-surface light transfer.</p> Did you know this source? <p>There is an active repository on GitHub, where latest CGH papers relevant to display technologies are listed. Visit GitHub:bchao1/awesome-holography for more.</p>"},{"location":"course/computer_generated_holography/#what-is-a-hologram","title":"What is a hologram?","text":"<p> Informative </p> <p>Hologram is either a surface or a volume that modifies the light distribution of incoming light in terms of phase and amplitude. Diffraction gratings, Holographic Optical Elements, or Metasurfaces are good examples of holograms. Within this section, we also use the term hologram as a means to describe a lightfield or a slice of a lightfield.</p>"},{"location":"course/computer_generated_holography/#what-is-computer-generated-holography","title":"What is Computer-Generated Holography?","text":"<p> Informative </p> <p>It is the computerized version (discrete sampling) of holography. In other terms, whenever you can program the phase or amplitude of light, this will get us to Computer-Generated Holography.</p> Where can I find an extensive summary on CGH? <p>You may be wondering about the greater physical details of CGH. In this case, we suggest our readers watch the video below. Please watch this video for an extensive summary on CGH <sup>8</sup>. </p> <p></p>"},{"location":"course/computer_generated_holography/#defining-a-slice-of-a-lightfield","title":"Defining a slice of a lightfield","text":"<p> Informative \u00b7  Practical</p> <p>CGH deals with generating optical fields that capture light from various scenes. CGH often describes these optical fields (a.k.a. lightfields, holograms) as planes. So in CGH, light travels from plane to plane, as depicted below. Roughly, CGH deals with plane to plane interaction of light, whereas raytracing is a ray or beam oriented description of light.</p> <p> </p> A rendering showing how a slice (a.k.a. lightfield, optical field, hologram) propagates from one plane to another plane. <p>In other words, in CGH, you define everything as a \"lightfield,\" including light sources, materials, and objects. Thus, we must first determine how to describe the mentioned lightfield in a computer. So that we can run CGH simulations effectively.</p> <p>A lightfield is a planar slice in the context of CGH, as depicted in the above figure. This planar field is a pixelated 2D surface (could be represented as a matrix). The pixels in this 2D slice hold values for the amplitude of light, \\(A\\), and the phase of the light, \\(\\phi\\) at each pixel. Whereas in classical raytracing, a ray only holds the amplitude or intensity of light. With a caveat, though, raytracing could also be made to care about the phase of light.  Still, it will then arrive with all the complications of raytracing, like sampling enough rays or describing scenes accurately.</p> <p>Each pixel in this planar lightfield slice encapsulates the \\(A\\) and \\(\\phi\\) as \\(A cos(wt + \\phi)\\). If you recall our description of light, we explain that light is an electromagnetic phenomenon. Here, we model the oscillating electric field of light with \\(A cos(wt + \\phi)\\) shown in our previous light description. Note that if we stick to \\(A cos(wt + \\phi)\\), each time two fields intersect, we have to deal with trigonometric conversion complexities like sampled in this example:</p> \\[ A_0 cos(wt + \\phi_0) + A_1 cos(wt + \\phi_1), \\] <p>Where the indices zero and one indicate the first and second fields, and we have to identify the right trigonometric conversion to deal with this sum.</p> <p>Instead of complicated trigonometric conversions, what people do in CGH is to rely on complex numbers as a proxy to these trigonometric conversions. In its proxy form, a pixel value in a field is converted into \\(A e^{-j \\phi}\\), where \\(j\\) represents a complex number (\\(\\sqrt{-1}\\)). Thus, with this new proxy representation, the same intersection problem we dealt with using sophisticated trigonometry before could be turned into something as simple as \\(A_0 A_1 e^{-j(\\phi_0 +\\phi_1)}\\).</p> <p>In the above summation of two fields, the resulting field follows an exact sum of the two collided fields. On the other hand, in raytracing, often, when a ray intersects with another ray, it will be left unchanged and continue its path. However, in the case of lightfields, they form a new field. This feature is called interference of light, which is not introduced in raytracing, and often raytracing omits this feature. As you can tell from also the summation, two fields could enhance the resulting field (constructive interference) by converging to a brighter intensity, or these two fields could cancel out each other (destructive interference) and lead to the absence of light --total darkness--.</p> <p>There are various examples of interference in nature. For example, the blue color of a butterfly wing results from interference, as biology typically does not produce blue-colored pigments in nature. More examples of light interference from daily lives are provided in the figure below.</p> <p> </p> Two photographs showin some examples of light interference: (left) thin oil film creates rainbow interference patterns (CC BY-SA 2.5 by Wikipedia user John) and a soup bubble interference with light and creates vivid reflections (CC BY-SA 3.0 by Wikipedia user Brocken Inaglory). <p>We have established an easy way to describe a field with a proxy complex number form. This way, we avoided complicated trigonometric conversions.  Let us look into how we use that in an actual simulation. Firstly, we can define two separate matrices to represent a field using real numbers:</p> <pre><code>import torch\n\namplitude = torch.tensor(100, 100, dtype = torch.float64)\nphase = torch.tensor(100, 100, dtype = torch.float64)\n</code></pre> <p>In this above example, we define two matrices with <code>100 x 100</code> dimensions. Each matrix holds floating point numbers, and they are real numbers. To convert the amplitude and phase into a field, we must define the field as suggested in our previous description. Instead of going through the same mathematical process for every piece of our future codes, we can rely on a utility function in odak to create fields consistently and coherently across all our future developments. The utility function we will review is <code>odak.learn.wave.generate_complex_field()</code>:</p> <p>Here, we provide visual results from this piece of code as below:</p> <code>odak.learn.wave.generate_complex_field</code> <p>Definition to generate a complex field with a given amplitude and phase.</p> <p>Parameters:</p> <ul> <li> <code>amplitude</code>           \u2013            <pre><code>            Amplitude of the field.\n            The expected size is [m x n] or [1 x m x n].\n</code></pre> </li> <li> <code>phase</code>           \u2013            <pre><code>            Phase of the field.\n            The expected size is [m x n] or [1 x m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field</code> (              <code>ndarray</code> )          \u2013            <p>Complex field. Depending on the input, the expected size is [m x n] or [1 x m x n].</p> </li> </ul> Source code in <code>odak/learn/wave/util.py</code> <pre><code>def generate_complex_field(amplitude, phase):\n    \"\"\"\n    Definition to generate a complex field with a given amplitude and phase.\n\n    Parameters\n    ----------\n    amplitude         : torch.tensor\n                        Amplitude of the field.\n                        The expected size is [m x n] or [1 x m x n].\n    phase             : torch.tensor\n                        Phase of the field.\n                        The expected size is [m x n] or [1 x m x n].\n\n    Returns\n    -------\n    field             : ndarray\n                        Complex field.\n                        Depending on the input, the expected size is [m x n] or [1 x m x n].\n    \"\"\"\n    field = amplitude * torch.cos(phase) + 1j * amplitude * torch.sin(phase)\n    return field\n</code></pre> <p>Let us use this utility function to expand our previous code snippet and show how we can generate a complex field using that:</p> <pre><code>import torch\nimport odak # (1)\n\namplitude = torch.tensor(100, 100, dtype = torch.float64)\nphase = torch.tensor(100, 100, dtype = torch.float64)\nfield = odak.learn.wave.generate_complex_field(amplitude, phase) # (2)\n</code></pre> <ol> <li>Adding <code>odak</code> to our imports.</li> <li>Generating a field using <code>odak.learn.wave.generate_complex_field</code>.</li> </ol>"},{"location":"course/computer_generated_holography/#propagating-a-field-in-free-space","title":"Propagating a field in free space","text":"<p> Informative \u00b7  Practical</p> <p>The next question we have to ask is related to the field we generated in our previous example. In raytracing, we propagate rays in space, whereas in CGH, we propagate a field described over a surface onto another target surface. So we need a transfer function that projects our field on another target surface. That is the point where free space beam propagation comes into play. As the name implies, free space beam propagation deals with propagating light in free space from one surface to another. This entire process of propagation is also referred to as light transport in the domains of Computer Graphics. In the rest of this section, we will explore means to simulate beam propagation on a computer.</p> A good news for Matlab fans! <p>We will indeed use <code>odak</code> to explore beam propagation. However, there is also a book in the literature, <code>[Numerical simulation of optical wave propagation: With examples in MATLAB by Jason D. Schmidt](https://www.spiedigitallibrary.org/ebooks/PM/Numerical-Simulation-of-Optical-Wave-Propagation-with-Examples-in-MATLAB/eISBN-9780819483270/10.1117/3.866274?SSO=1)</code><sup>9</sup>, that provides a crash course on beam propagation using MATLAB.</p> <p>As we revisit the field we generated in the previous subsection, we remember that our field is a pixelated 2D surface. Each pixel in our fields, either a hologram or image plane, typically has a small size of a few micrometers (e.g., \\(8 \\mu m\\)). How light travels from each one of these pixels on one surface to pixels on another is conceptually depicted as a figure at the beginning of this section (green wolf image with two planes). We will name that figure's first plane on the left as the hologram plane and the second as the image plane. In a nutshell, the contribution of a pixel on a hologram plane could be calculated by drawing rays to every pixel on the image plane. We draw rays from a point to a plane because in wave theory --what CGH follows--, light can diffract (a small aperture creating spherical waves as Huygens suggested). Each ray will have a certain distance, thus causing various delays in phase \\(\\phi\\). As long as the distance between planes is large enough, each ray will maintain an electric field that is in the same direction as the others (same polarization), thus able to interfere with other rays emerging from other pixels in a hologram plane. This simplified description oversimplifies solving the Maxwell equations in electromagnetics.</p> <p>A simplified result of solving Maxwell's equation is commonly described using Rayleigh-Sommerfeld diffraction integrals.  For more on Rayleigh-Sommerfeld, consult <code>Heurtley, J. C. (1973). Scalar Rayleigh\u2013Sommerfeld and Kirchhoff diffraction integrals: a comparison of exact evaluations for axial points. JOSA, 63(8), 1003-1008.</code> <sup>2</sup>. The first solution of the Rayleigh-Sommerfeld integral, also known as the Huygens-Fresnel principle, is expressed as follows:</p> \\[ u(x,y)=\\frac{1}{j\\lambda} \\int\\!\\!\\!\\!\\int u_0(x,y)\\frac{e^{jkr}}{r}cos(\\theta)dxdy, \\] <p>where the field at a target image plane, \\(u(x,y)\\), is calculated by integrating over every point of the hologram's area, \\(u_0(x,y)\\). Note that, for the above equation, \\(r\\) represents the optical path between a selected point over a hologram and a selected point in the image plane, theta represents the angle between these two points, k represents the wavenumber (\\(\\frac{2\\pi}{\\lambda}\\)) and \\(\\lambda\\) represents the wavelength of light. In this described light transport model, optical fields, \\(u_0(x,y)\\) and \\(u(x,y)\\), are represented with a complex value,</p> \\[ u_0(x,y)=A(x,y)e^{j\\phi(x,y)}, \\] <p>where \\(A\\) represents the spatial distribution of amplitude and \\(\\phi\\) represents the spatial distribution of phase across a hologram plane. The described holographic light transport model is often simplified into a single convolution with a fixed spatially invariant complex kernel, \\(h(x,y)\\) <sup>3</sup>.</p> \\[ u(x,y)=u_0(x,y) * h(x,y) =\\mathcal{F}^{-1}(\\mathcal{F}(u_0(x,y)) \\mathcal{F}(h(x,y))). \\] <p>There are multiple variants of this simplified approach:</p> <ul> <li><code>Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673.</code> <sup>10</sup>,</li> <li><code>Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics letters 45.6 (2020): 1543-1546.</code> <sup>11</sup>,</li> <li><code>Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419.</code> <sup>12</sup>.</li> </ul> <p>In many cases, people choose to use the most common form of \\(h(x, y)\\) described as</p> \\[ h(x,y)=\\frac{e^{jkz}}{j\\lambda z} e^{\\frac{jk}{2z} (x^2+y^2)}, \\] <p>where z represents the distance between a hologram plane and a target image plane. Before, we introduce you how to use existing beam propagation in our library, let us dive deep in compiling a beam propagation code following the Rayleigh-Sommerfeld integral, also known as the Huygens-Fresnel principle. In the rest of this script, I will walk you through the below code:</p> <code>test_diffraction_integral.py</code> <pre><code>import sys\nimport odak # (1)\nimport torch\nfrom tqdm import tqdm\n\n\ndef main(): # (2)\n    length = [7e-6, 7e-6] # (3)\n    for fresnel_id, fresnel_number in enumerate(range(99)): # (4)\n        fresnel_number += 1\n        propagate(\n                  fresnel_number = fresnel_number,\n                  length = [length[0] + 1. / fresnel_number * 8e-6, length[1] + 1. / fresnel_number * 8e-6]\n                 )\n\n\ndef propagate(\n              wavelength = 532e-9, # (6)\n              pixel_pitch = 3.74e-6, # (7)\n              length = [15e-6, 15e-6],\n              image_samples = [2, 2], # Replace it with 1000 by 1000 (8)\n              aperture_samples = [2, 2], # Replace it with 1000 by 1000 (9)\n              device = torch.device('cpu'),\n              output_directory = 'test_output', \n              fresnel_number = 4,\n              save_flag = False\n             ): # (5)\n    distance = pixel_pitch ** 2 / wavelength / fresnel_number\n    distance = torch.as_tensor(distance, device = device)\n    k = odak.learn.wave.wavenumber(wavelength)\n    x = torch.linspace(- length[0] / 2, length[0] / 2, image_samples[0], device = device)\n    y = torch.linspace(- length[1] / 2, length[1] / 2, image_samples[1], device = device)\n    X, Y = torch.meshgrid(x, y, indexing = 'ij') # (10)\n    wxs = torch.linspace(- pixel_pitch / 2., pixel_pitch / 2., aperture_samples[0], device = device)\n    wys = torch.linspace(- pixel_pitch / 2., pixel_pitch / 2., aperture_samples[1], device = device) # (11)\n    h  = torch.zeros(image_samples[0], image_samples[1], dtype = torch.complex64, device = device)\n    for wx in tqdm(wxs):\n        for wy in wys:\n            h += huygens_fresnel_principle(wx, wy, X, Y, distance, k, wavelength) # (12)\n    h = h * pixel_pitch ** 2 / aperture_samples[0] / aperture_samples[1] # (13) \n\n    if save_flag:\n        save_results(h, output_directory, fresnel_number, length, pixel_pitch, distance, image_samples, device) # (14)\n    return True\n\n\ndef huygens_fresnel_principle(x, y, X, Y, z, k, wavelength): # (12)\n    r = torch.sqrt((X - x) ** 2 + (Y - y) ** 2 + z ** 2)\n    h = torch.exp(1j * k * r) * z / r ** 2 * (1. / (2 * odak.pi * r) + 1. / (1j * wavelength))\n    return h\n\n\ndef save_results(h, output_directory, fresnel_number, length, pixel_pitch, distance, image_samples, device):\n    from matplotlib import pyplot as plt\n    odak.tools.check_directory(output_directory)\n    output_intensity = odak.learn.wave.calculate_amplitude(h) ** 2\n    odak.learn.tools.save_image(\n                                '{}/diffraction_output_intensity_fresnel_number_{:02d}.png'.format(output_directory, int(fresnel_number)),\n                                output_intensity,\n                                cmin = 0.,\n                                cmax = output_intensity.max()\n                               )\n    cross_section_1d = output_intensity[output_intensity.shape[0] // 2]\n    lengths = torch.linspace(- length[0] * 10 ** 6 / 2., length[0] * 10 ** 6 / 2., image_samples[0], device = device)\n    plt.figure()\n    plt.plot(lengths.detach().cpu().numpy(), cross_section_1d.detach().cpu().numpy())\n    plt.xlabel('length (um)')\n    plt.figtext(\n                0.14,\n                0.9, \n                r'Fresnel Number: {:02d}, Pixel pitch: {:.2f} um, Distance: {:.2f} um'.format(fresnel_number, pixel_pitch * 10 ** 6, distance * 10 ** 6),\n                fontsize = 11\n               )\n    plt.savefig('{}/diffraction_1d_output_intensity_fresnel_number_{:02d}.png'.format(output_directory, int(fresnel_number)))\n    plt.cla()\n    plt.clf()\n    plt.close()\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n</code></pre> <ol> <li>Importing relevant libraries</li> <li>This is our main routine.</li> <li>Length of the final image plane along X and Y axes.</li> <li>Fresnel number is an arbitrary number that helps to get a sense if the optical configuration could be considered as a Fresnel (near field) or Fraunhofer regions.</li> <li>Propagating light with the given configuration.</li> <li>Wavelength of light.</li> <li>Square aperture length of a single pixel in the simulation. This is where light diffracts from.</li> <li>Number of pixels in the image plane along X and Y axes.</li> <li>Number of point light sources used to represent a single pixel's square aperture.</li> <li>Sample point locations along X and Y axes at the image plane.</li> <li>Sample point locations along X and Y axes at the aperture plane.</li> <li>For each, virtual point light source defined inside the aperture, we simulate the light as if divergind point light source.</li> <li>Normalize with the number of samples (trapezoid integration).</li> <li>Rest of this code is for logistics for saving images.</li> </ol> <p>We start the implementation by importing necessary libraries such as <code>odak</code> or <code>torch</code>. The first function, <code>def main</code>, sets the length of our image plane, where we will observe the diffraction pattern. As we set the size of our image plane, we also set a arbitrary number called <code>Fresnel Number</code>,</p> \\[ n_F = \\frac{w^2}{\\lambda z}, \\] <p>where \\(z\\) is the propagation distance, \\(w\\) is the side length of an aperture diffracting light like a pixel's square aperture -- this is often the pixel pitch -- and \\(\\lambda\\) is the wavelength of the light. This number helps us to get an idea if the set optical configuration falls under a certain regime like <code>Fresnel</code> or <code>Fraunhofer</code>. Fresnel number also provides a practical ease related to comparing solutions. Regardless of the optical configuration, a result with a specific Fresnel number will follow a similar pattern with different optical configuration. Thus, providing a way to verify your solutions. In the next step, we call the light propagation function, <code>def propagate</code>. In the beginning of this function, we set the optical configuration. For instance, we set <code>pixel_pitch</code>, this is the side length of a square aperture that the light will diffract from. Inside the <code>def propagate</code> function, we reset the distance such that it follows the input <code>Fresnel Number</code> and <code>wavelength</code>. We define the locations of the samples across X and Y axes that will represent points to calculate on the image plane, <code>x</code> and <code>y</code>. Than, we define the locations of the samples across X and Y axes that will represent the point light source locations inside the aperture, <code>wxs</code> and <code>wys</code>, which in this case a square aperture that represents a single pixel and its sidelength is provided by <code>pixel_pitch</code>. The nested for loop goes over the <code>wxs</code> and <code>wys</code>. Each time, we choose a point from the aperture, we propagate a spherical wave from that point using <code>def huygens_fresnel_principle</code>. Note that we accumulate the effect of each spherical wave on a variable called <code>h</code>.  This is diffraction pattern in complex form from our square aperture, and we also normalize it using <code>pixel_pitch</code> and <code>aperture_samples</code>. Here, we provide visual results from this piece of code as below:</p> <p> </p> Saved 1D intensities on image plane representing diffraction patterns for various Fresnel numbers. These patterns are generated by using \"test/test_diffraction_integral.py\". <p> </p> Saved 2D intensities on image plane representing diffraction patterns for various Fresnel numbers. These patterns are generated by using \"test/test_diffraction_integral.py\". <p>Note that beam propagation can also be learned for physical setups to avoid imperfections in a setup and to improve the image quality at an image plane:</p> <ul> <li><code>Peng, Yifan, et al. \"Neural holography with camera-in-the-loop training.\" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-14.</code> <sup>13</sup>,</li> <li><code>Chakravarthula, Praneeth, et al. \"Learned hardware-in-the-loop phase retrieval for holographic near-eye displays.\" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-18.</code> <sup>14</sup>,</li> <li><code>Kavakl\u0131, Koray, Hakan Urey, and Kaan Ak\u015fit. \"Learned holographic light transport.\" Applied Optics (2021).</code> <sup>15</sup>.</li> </ul> <p>The above descriptions establish a mathematical understanding of beam propagation. Let us examine the implementation of a beam propagation method called <code>Bandlimited Angular Spectrum</code> by reviewing these two utility functions from <code>odak</code>:</p> <code>odak.learn.wave.get_band_limited_angular_spectrum_kernel</code> <code>odak.learn.wave.band_limited_angular_spectrum</code> <code>odak.learn.wave.propagate_beam</code> <code>odak.learn.wave.wavenumber</code> <p>Helper function for odak.learn.wave.band_limited_angular_spectrum.</p> <p>Parameters:</p> <ul> <li> <code>nu</code>           \u2013            <pre><code>             Resolution at X axis in pixels.\n</code></pre> </li> <li> <code>nv</code>           \u2013            <pre><code>             Resolution at Y axis in pixels.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>             Pixel pitch in meters.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>             Wavelength in meters.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>             Distance in meters.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>             Device, for more see torch.device().\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>H</code> (              <code>complex64</code> )          \u2013            <p>Complex kernel in Fourier domain.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def get_band_limited_angular_spectrum_kernel(\n                                             nu,\n                                             nv,\n                                             dx = 8e-6,\n                                             wavelength = 515e-9,\n                                             distance = 0.,\n                                             device = torch.device('cpu')\n                                            ):\n    \"\"\"\n    Helper function for odak.learn.wave.band_limited_angular_spectrum.\n\n    Parameters\n    ----------\n    nu                 : int\n                         Resolution at X axis in pixels.\n    nv                 : int\n                         Resolution at Y axis in pixels.\n    dx                 : float\n                         Pixel pitch in meters.\n    wavelength         : float\n                         Wavelength in meters.\n    distance           : float\n                         Distance in meters.\n    device             : torch.device\n                         Device, for more see torch.device().\n\n\n    Returns\n    -------\n    H                  : torch.complex64\n                         Complex kernel in Fourier domain.\n    \"\"\"\n    x = dx * float(nu)\n    y = dx * float(nv)\n    fx = torch.linspace(\n                        -1 / (2 * dx) + 0.5 / (2 * x),\n                         1 / (2 * dx) - 0.5 / (2 * x),\n                         nu,\n                         dtype = torch.float32,\n                         device = device\n                        )\n    fy = torch.linspace(\n                        -1 / (2 * dx) + 0.5 / (2 * y),\n                        1 / (2 * dx) - 0.5 / (2 * y),\n                        nv,\n                        dtype = torch.float32,\n                        device = device\n                       )\n    FY, FX = torch.meshgrid(fx, fy, indexing='ij')\n    HH_exp = 2 * torch.pi * torch.sqrt(1 / wavelength ** 2 - (FX ** 2 + FY ** 2))\n    distance = torch.tensor([distance], device = device)\n    H_exp = torch.mul(HH_exp, distance)\n    fx_max = 1 / torch.sqrt((2 * distance * (1 / x))**2 + 1) / wavelength\n    fy_max = 1 / torch.sqrt((2 * distance * (1 / y))**2 + 1) / wavelength\n    H_filter = ((torch.abs(FX) &lt; fx_max) &amp; (torch.abs(FY) &lt; fy_max)).clone().detach()\n    H = generate_complex_field(H_filter, H_exp)\n    return H\n</code></pre> <p>A definition to calculate bandlimited angular spectrum based beam propagation. For more  <code>Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673</code>.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           A complex field.\n           The expected size is [m x n].\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>zero_padding</code>           \u2013            <pre><code>           Zero pad in Fourier domain.\n</code></pre> </li> <li> <code>aperture</code>           \u2013            <pre><code>           Fourier domain aperture (e.g., pinhole in a typical holographic display).\n           The default is one, but an aperture could be as large as input field [m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field [m x n].</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def band_limited_angular_spectrum(\n                                  field,\n                                  k,\n                                  distance,\n                                  dx,\n                                  wavelength,\n                                  zero_padding = False,\n                                  aperture = 1.\n                                 ):\n    \"\"\"\n    A definition to calculate bandlimited angular spectrum based beam propagation. For more \n    `Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673`.\n\n    Parameters\n    ----------\n    field            : torch.complex\n                       A complex field.\n                       The expected size is [m x n].\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    zero_padding     : bool\n                       Zero pad in Fourier domain.\n    aperture         : torch.tensor\n                       Fourier domain aperture (e.g., pinhole in a typical holographic display).\n                       The default is one, but an aperture could be as large as input field [m x n].\n\n\n    Returns\n    -------\n    result           : torch.complex\n                       Final complex field [m x n].\n    \"\"\"\n    H = get_propagation_kernel(\n                               nu = field.shape[-2], \n                               nv = field.shape[-1], \n                               dx = dx, \n                               wavelength = wavelength, \n                               distance = distance, \n                               propagation_type = 'Bandlimited Angular Spectrum',\n                               device = field.device\n                              )\n    result = custom(field, H, zero_padding = zero_padding, aperture = aperture)\n    return result\n</code></pre> <p>Definitions for various beam propagation methods mostly in accordence with \"Computational Fourier Optics\" by David Vuelz.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field [m x n].\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>propagation_type</code>               (<code>str</code>, default:                   <code>'Bandlimited Angular Spectrum'</code> )           \u2013            <pre><code>           Type of the propagation.\n           The options are Impulse Response Fresnel, Transfer Function Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, Fraunhofer.\n</code></pre> </li> <li> <code>kernel</code>           \u2013            <pre><code>           Custom complex kernel.\n</code></pre> </li> <li> <code>zero_padding</code>           \u2013            <pre><code>           Zero padding the input field if the first item in the list set True.\n           Zero padding in the Fourier domain if the second item in the list set to True.\n           Cropping the result with half resolution if the third item in the list is set to true.\n           Note that in Fraunhofer propagation, setting the second item True or False will have no effect.\n</code></pre> </li> <li> <code>aperture</code>           \u2013            <pre><code>           Aperture at Fourier domain default:[2m x 2n], otherwise depends on `zero_padding`.\n           If provided as a floating point 1, there will be no aperture in Fourier domain.\n</code></pre> </li> <li> <code>scale</code>           \u2013            <pre><code>           Resolution factor to scale generated kernel.\n</code></pre> </li> <li> <code>samples</code>           \u2013            <pre><code>           When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for a hologram pixel and second two is for an image plane pixel.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field [m x n].</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def propagate_beam(\n                   field,\n                   k,\n                   distance,\n                   dx,\n                   wavelength,\n                   propagation_type='Bandlimited Angular Spectrum',\n                   kernel = None,\n                   zero_padding = [True, False, True],\n                   aperture = 1.,\n                   scale = 1,\n                   samples = [20, 20, 5, 5]\n                  ):\n    \"\"\"\n    Definitions for various beam propagation methods mostly in accordence with \"Computational Fourier Optics\" by David Vuelz.\n\n    Parameters\n    ----------\n    field            : torch.complex\n                       Complex field [m x n].\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    propagation_type : str\n                       Type of the propagation.\n                       The options are Impulse Response Fresnel, Transfer Function Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, Fraunhofer.\n    kernel           : torch.complex\n                       Custom complex kernel.\n    zero_padding     : list\n                       Zero padding the input field if the first item in the list set True.\n                       Zero padding in the Fourier domain if the second item in the list set to True.\n                       Cropping the result with half resolution if the third item in the list is set to true.\n                       Note that in Fraunhofer propagation, setting the second item True or False will have no effect.\n    aperture         : torch.tensor\n                       Aperture at Fourier domain default:[2m x 2n], otherwise depends on `zero_padding`.\n                       If provided as a floating point 1, there will be no aperture in Fourier domain.\n    scale            : int\n                       Resolution factor to scale generated kernel.\n    samples          : list\n                       When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for a hologram pixel and second two is for an image plane pixel.\n\n    Returns\n    -------\n    result           : torch.complex\n                       Final complex field [m x n].\n    \"\"\"\n    if zero_padding[0]:\n        field = zero_pad(field)\n    if propagation_type == 'Angular Spectrum':\n        result = angular_spectrum(\n                                  field = field,\n                                  k = k,\n                                  distance = distance,\n                                  dx = dx,\n                                  wavelength = wavelength,\n                                  zero_padding = zero_padding[1],\n                                  aperture = aperture\n                                 )\n    elif propagation_type == 'Bandlimited Angular Spectrum':\n        result = band_limited_angular_spectrum(\n                                               field = field,\n                                               k = k,\n                                               distance = distance,\n                                               dx = dx,\n                                               wavelength = wavelength,\n                                               zero_padding = zero_padding[1],\n                                               aperture = aperture\n                                              )\n    elif propagation_type == 'Impulse Response Fresnel':\n        result = impulse_response_fresnel(\n                                          field = field,\n                                          k = k,\n                                          distance = distance,\n                                          dx = dx,\n                                          wavelength = wavelength,\n                                          zero_padding = zero_padding[1],\n                                          aperture = aperture,\n                                          scale = scale,\n                                          samples = samples\n                                         )\n    elif propagation_type == 'Seperable Impulse Response Fresnel':\n        result = seperable_impulse_response_fresnel(\n                                                    field = field,\n                                                    k = k,\n                                                    distance = distance,\n                                                    dx = dx,\n                                                    wavelength = wavelength,\n                                                    zero_padding = zero_padding[1],\n                                                    aperture = aperture,\n                                                    scale = scale,\n                                                    samples = samples\n                                                   )\n    elif propagation_type == 'Transfer Function Fresnel':\n        result = transfer_function_fresnel(\n                                           field = field,\n                                           k = k,\n                                           distance = distance,\n                                           dx = dx,\n                                           wavelength = wavelength,\n                                           zero_padding = zero_padding[1],\n                                           aperture = aperture\n                                          )\n    elif propagation_type == 'custom':\n        result = custom(\n                        field = field,\n                        kernel = kernel,\n                        zero_padding = zero_padding[1],\n                        aperture = aperture\n                       )\n    elif propagation_type == 'Fraunhofer':\n        result = fraunhofer(\n                            field = field,\n                            k = k,\n                            distance = distance,\n                            dx = dx,\n                            wavelength = wavelength\n                           )\n    elif propagation_type == 'Incoherent Angular Spectrum':\n        result = incoherent_angular_spectrum(\n                                             field = field,\n                                             k = k,\n                                             distance = distance,\n                                             dx = dx,\n                                             wavelength = wavelength,\n                                             zero_padding = zero_padding[1],\n                                             aperture = aperture\n                                            )\n    else:\n        logging.warning('Propagation type not recognized')\n        assert True == False\n    if zero_padding[2]:\n        result = crop_center(result)\n    return result\n</code></pre> <p>Definition for calculating the wavenumber of a plane wave.</p> <p>Parameters:</p> <ul> <li> <code>wavelength</code>           \u2013            <pre><code>       Wavelength of a wave in mm.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>k</code> (              <code>float</code> )          \u2013            <p>Wave number for a given wavelength.</p> </li> </ul> Source code in <code>odak/learn/wave/util.py</code> <pre><code>def wavenumber(wavelength):\n    \"\"\"\n    Definition for calculating the wavenumber of a plane wave.\n\n    Parameters\n    ----------\n    wavelength   : float\n                   Wavelength of a wave in mm.\n\n    Returns\n    -------\n    k            : float\n                   Wave number for a given wavelength.\n    \"\"\"\n    k = 2 * torch.pi / wavelength\n    return k\n</code></pre> <p>Let us see how we can use the given beam propagation function with an example:</p> <code>test_learn_wave_propagate_beam.py</code> <pre><code>import sys\nimport os\nimport odak\nimport numpy as np\nimport torch\n\n\ndef test(output_directory = 'test_output'):\n    odak.tools.check_directory(output_directory)\n    wavelength = 532e-9 # (1)\n    pixel_pitch = 8e-6 # (2)\n    distance = 0.5e-2 # (3)\n    propagation_types = ['Angular Spectrum', 'Bandlimited Angular Spectrum', 'Transfer Function Fresnel'] # (4)\n    k = odak.learn.wave.wavenumber(wavelength) # (5)\n\n\n    amplitude = torch.zeros(500, 500)\n    amplitude[200:300, 200:300 ] = 1. # (5)\n    phase = torch.randn_like(amplitude) * 2 * odak.pi # (6)\n    hologram = odak.learn.wave.generate_complex_field(amplitude, phase) # (7)\n\n    for propagation_type in propagation_types:\n        image_plane = odak.learn.wave.propagate_beam(\n                                                     hologram,\n                                                     k,\n                                                     distance,\n                                                     pixel_pitch,\n                                                     wavelength,\n                                                     propagation_type,\n                                                     zero_padding = [True, False, True] # (8)\n                                                    ) # (9)\n\n        image_intensity = odak.learn.wave.calculate_amplitude(image_plane) ** 2 # (10)\n        hologram_intensity = amplitude ** 2\n\n        odak.learn.tools.save_image(\n                                    '{}/image_intensity_{}.png'.format(output_directory, propagation_type.replace(' ', '_')), \n                                    image_intensity, \n                                    cmin = 0., \n                                    cmax = image_intensity.max()\n                                ) # (11)\n        odak.learn.tools.save_image(\n                                    '{}/hologram_intensity_{}.png'.format(output_directory, propagation_type.replace(' ', '_')), \n                                    hologram_intensity, \n                                    cmin = 0., \n                                    cmax = 1.\n                                ) # (12)\n    assert True == True\n\n\nif __name__ == '__main__':\n    sys.exit(test()) \n</code></pre> <ol> <li>Setting the wavelength of light in meters. We use 532 nm (green light) in this example.</li> <li>Setting the physical size of a single pixel in our simulation. We use \\(6 \\mu m\\) pixel size (width and height are both \\(6 \\mu m\\).) </li> <li>Setting the distance between two planes, hologram and image plane. We set it as half a centimeterhere.</li> <li>We set the propagation type to <code>Bandlimited Angular Spectrum</code>.</li> <li>Here, we calculate a value named wavenumber, which we introduced while we were talking about the beam propagation functions.</li> <li>Here, we assume that there is a rectangular light at the center of our hologram.</li> <li>Here, we generate the field by combining amplitude and phase. </li> <li>Here, we zeropad and crop our field before and after the beam propagation to make sure that there is no aliasing in our results (see Nyquist criterion).</li> <li>We propagate the beam using the values and field provided.</li> <li>We calculate the final intensity on our image plane. Remember that human eyes can see intensity but not amplitude or phase of light. Intensity of light is a square of its amplitude.</li> <li>We save image plane intensity to an image file.</li> <li>For comparison, we also save the hologram intensity to an image file so that we can observe how our light transformed from one plane to another.</li> </ol> <p>Let us also take a look at the saved images as a result of the above sample code:</p> <p> </p> Saved intensities before (left_ and after (right) beam propagation (hologram and image plane intensities). This result is generated using \"test/test_learn_beam_propagation.py\". Challenge: Light transport on Arbitrary Surfaces <p>We know that we can propagate a hologram to any image plane at any distance. This propagation is a plane-to-plane interaction. However, there may be cases where a simulation involves finding light distribution over an arbitrary surface. Conventionally, this could be achieved by propagating the hologram to multiple different planes and picking the results from each plane on the surface of that arbitrary surface. We challenge our readers to code the mentioned baseline (multiple planes for arbitrary surfaces) and ask them to develop a beam propagation that is less computationally expensive and works for arbitrary surfaces (e.g., tilted planes or arbitrary shapes). This development could either rely on classical approaches or involve learning-based methods. The resultant method could be part of <code>odak.learn.wave</code> submodule as a new class <code>odak.learn.wave.propagate_arbitrary</code>. In addition, a unit test <code>test/test_learn_propagate_arbitrary.py</code> has to adopt this new class. To add these to <code>odak,</code> you can rely on the <code>pull request</code> feature on GitHub. You can also create a new <code>engineering note</code> for arbitrary surfaces in <code>docs/notes/beam_propagation_arbitrary_surfaces.md</code>.</p>"},{"location":"course/computer_generated_holography/#optimizing-holograms","title":"Optimizing holograms","text":"<p> Informative \u00b7  Practical</p> <p>In the previous subsection, we propagate an input field (a.k.a. lightfield, hologram) to another plane called the image plane. We can store any scene or object as a field on such planes. Thus, we have learned that we can have a plane (hologram) to capture or display a slice of a lightfield for any given scene or object. After all this introduction, it is also safe to say, regardless of hardware, holograms are the natural way to represent three-dimensional scenes, objects, and data!</p> <p>Holograms come in many forms. We can broadly classify holograms as analog and digital. Analog holograms are physically tailored structures. They are typically a result of manufacturing engineered surfaces (micron or nanoscale structures). Some examples of analog holograms include diffractive optical elements <sup>4</sup>, holographic optical elements <sup>5</sup>, and metasurfaces <sup>6</sup>. Here, we show an example of an analog hologram that gives us a slice of a lightfield, and we can observe the scene this way from various perspectives:</p> <p> </p> A video showing analog hologram example from Zebra Imaging -- ZScape. <p>Digital holograms are the ones that are dynamic and generated using programmable versions of analog holograms. Typically, the tiniest fraction of digital holograms is a pixel that either manipulates the phase or amplitude of light. In our laboratory, we build holographic displays <sup>7</sup>, a programmable device to display holograms. The components used in such a display are illustrated in the following rendering and contain a Spatial Light Modulator (SLM) that could display programmable holograms. Note that the SLM in this specific hardware can only manipulate phase of an incoming light.</p> <p> </p> A rendering showing a standard holographic display hardware. <p>We can display holograms that generate images to fill a three-dimensional volume using the above hardware. We know that they are three-dimensional from the fact that we can focus on different parts of the images by changing the focus of our camera (closely observing the camera's location in the above figure). Let us look into a sample result to see what these three-dimensional images look like as we focus on different scene parts.</p> <p> </p> A series of photographs at various focuses capturing images from our computer-generated holograms. <p>Let us look into how we can optimize a hologram for our holographic display by visiting the below example:</p> <code>test_learn_wave_stochastic_gradient_descent.py</code> <pre><code>import sys\nimport odak\nimport torch\n\n\ndef test(output_directory = 'test_output'):\n    odak.tools.check_directory(output_directory)\n    device = torch.device('cpu') # (1)\n    target = odak.learn.tools.load_image('./test/data/usaf1951.png', normalizeby = 255., torch_style = True)[1] # (4)\n    hologram, reconstruction = odak.learn.wave.stochastic_gradient_descent(\n                                                                           target,\n                                                                           wavelength = 532e-9,\n                                                                           distance = 20e-2,\n                                                                           pixel_pitch = 8e-6,\n                                                                           propagation_type = 'Bandlimited Angular Spectrum',\n                                                                           n_iteration = 50,\n                                                                           learning_rate = 0.1\n                                                                          ) # (2)\n    odak.learn.tools.save_image(\n                                '{}/phase.png'.format(output_directory), \n                                odak.learn.wave.calculate_phase(hologram) % (2 * odak.pi), \n                                cmin = 0., \n                                cmax = 2 * odak.pi\n                               ) # (3)\n    odak.learn.tools.save_image('{}/sgd_target.png'.format(output_directory), target, cmin = 0., cmax = 1.)\n    odak.learn.tools.save_image(\n                                '{}/sgd_reconstruction.png'.format(output_directory), \n                                odak.learn.wave.calculate_amplitude(reconstruction) ** 2, \n                                cmin = 0., \n                                cmax = 1.\n                               )\n    assert True == True\n\n\nif __name__ == '__main__':\n    sys.exit(test())\n</code></pre> <ol> <li>Replace <code>cpu</code> with <code>cuda</code> if you have a NVIDIA GPU with enough memory or AMD GPU with enough memory and ROCm support.</li> <li>We will provide the details of this optimization function in the next part.</li> <li>Saving the phase-only hologram. Note that a phase-only hologram is between zero and two pi.</li> <li>Loading an image from a file with 1920 by 1080 resolution and using green channel.</li> </ol> <p>The above sample optimization script uses a function called <code>odak.learn.wave.stochastic_gradient_descent</code>. This function sits at the center of this optimization, and we have to understand what it entails by closely observing its inputs, outputs, and source code. Let us review the function.</p> <code>odak.learn.wave.stochastic_gradient_descent</code> <p>Definition to generate phase and reconstruction from target image via stochastic gradient descent.</p> <p>Parameters:</p> <ul> <li> <code>target</code>           \u2013            <pre><code>                    Target field amplitude [m x n].\n                    Keep the target values between zero and one.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>                    Set if the converted array requires gradient.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>                    Hologram plane distance wrt SLM plane.\n</code></pre> </li> <li> <code>pixel_pitch</code>           \u2013            <pre><code>                    SLM pixel pitch in meters.\n</code></pre> </li> <li> <code>propagation_type</code>           \u2013            <pre><code>                    Type of the propagation (see odak.learn.wave.propagate_beam()).\n</code></pre> </li> <li> <code>n_iteration</code>           \u2013            <pre><code>                    Number of iteration.\n</code></pre> </li> <li> <code>loss_function</code>           \u2013            <pre><code>                    If none it is set to be l2 loss.\n</code></pre> </li> <li> <code>learning_rate</code>           \u2013            <pre><code>                    Learning rate.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>hologram</code> (              <code>Tensor</code> )          \u2013            <p>Phase only hologram as torch array</p> </li> <li> <code>reconstruction_intensity</code> (              <code>Tensor</code> )          \u2013            <p>Reconstruction as torch array</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def stochastic_gradient_descent(\n                                target,\n                                wavelength,\n                                distance,\n                                pixel_pitch,\n                                propagation_type = 'Bandlimited Angular Spectrum',\n                                n_iteration = 100,\n                                loss_function = None,\n                                learning_rate = 0.1\n                               ):\n    \"\"\"\n    Definition to generate phase and reconstruction from target image via stochastic gradient descent.\n\n    Parameters\n    ----------\n    target                    : torch.Tensor\n                                Target field amplitude [m x n].\n                                Keep the target values between zero and one.\n    wavelength                : double\n                                Set if the converted array requires gradient.\n    distance                  : double\n                                Hologram plane distance wrt SLM plane.\n    pixel_pitch               : float\n                                SLM pixel pitch in meters.\n    propagation_type          : str\n                                Type of the propagation (see odak.learn.wave.propagate_beam()).\n    n_iteration:              : int\n                                Number of iteration.\n    loss_function:            : function\n                                If none it is set to be l2 loss.\n    learning_rate             : float\n                                Learning rate.\n\n    Returns\n    -------\n    hologram                  : torch.Tensor\n                                Phase only hologram as torch array\n\n    reconstruction_intensity  : torch.Tensor\n                                Reconstruction as torch array\n\n    \"\"\"\n    phase = torch.randn_like(target, requires_grad = True)\n    k = wavenumber(wavelength)\n    optimizer = torch.optim.Adam([phase], lr = learning_rate)\n    if type(loss_function) == type(None):\n        loss_function = torch.nn.MSELoss()\n    t = tqdm(range(n_iteration), leave = False, dynamic_ncols = True)\n    for i in t:\n        optimizer.zero_grad()\n        hologram = generate_complex_field(1., phase)\n        reconstruction = propagate_beam(\n                                        hologram, \n                                        k, \n                                        distance, \n                                        pixel_pitch, \n                                        wavelength, \n                                        propagation_type, \n                                        zero_padding = [True, False, True]\n                                       )\n        reconstruction_intensity = calculate_amplitude(reconstruction) ** 2\n        loss = loss_function(reconstruction_intensity, target)\n        description = \"Loss:{:.4f}\".format(loss.item())\n        loss.backward(retain_graph = True)\n        optimizer.step()\n        t.set_description(description)\n    logging.warning(description)\n    torch.no_grad()\n    hologram = generate_complex_field(1., phase)\n    reconstruction = propagate_beam(\n                                    hologram, \n                                    k, \n                                    distance, \n                                    pixel_pitch, \n                                    wavelength, \n                                    propagation_type, \n                                    zero_padding = [True, False, True]\n                                   )\n    return hologram, reconstruction\n</code></pre> <p>Let us also examine the optimized hologram and the image that the hologram reconstructed at the image plane.</p> <p> </p> Optimized phase-only hologram. Generated using \"test/test_learn_wave_stochastic_gradient_descent.py\". <p> </p> Optimized phase-only hologram reconstructed at the image plane, generated using \"test/test_learn_wave_stochastic_gradient_descent.py\". Challenge: Non-iterative Learned Hologram Calculation <p>We provided an overview of optimizing holograms using iterative methods. Iterative methods are computationally expensive and unsuitable for real-time hologram generation. We challenge our readers to derive a learned hologram generation method for multiplane images (not single-plane like in our example). This development could either rely on classical convolutional neural networks or blend with physical priors explained in this section. The resultant method could be part of <code>odak.learn.wave</code> submodule as a new class <code>odak.learn.wave.learned_hologram</code>. In addition, a unit test <code>test/test_learn_hologram.py</code> has to adopt this new class. To add these to <code>odak,</code> you can rely on the <code>pull request</code> feature on GitHub. You can also create a new <code>engineering note</code> for arbitrary surfaces in <code>docs/notes/learned_hologram_generation.md</code>.</p>"},{"location":"course/computer_generated_holography/#simulating-a-standard-holographic-display","title":"Simulating a standard holographic display","text":"<p> Informative \u00b7  Practical</p> <p>We optimized holograms for a holographic display in the previous section. However, the beam propagation distance we used in our optimization example was large. If we were to run the same optimization for a shorter propagation distance, say not cms but mms, we would not get a decent solution. Because in an actual holographic display, there is an aperture that helps to filter out some of the light. The previous section contained an optical layout rendering of a holographic display, where this aperture is also depicted. As depicted in the rendering located in the previous section, this aperture is located between a two lens system, which is also known as 4F imaging system.</p> Did you know? <p>4F imaging system can take a Fourier transform of an input field by using physics but not computers. For more details, please review these course notes from MIT.</p> <p>Let us review the class dedicated to accurately simulating a holographic display and its functions:</p> <code>odak.learn.wave.propagator.reconstruct</code> <code>odak.learn.wave.propgator.__call__</code> <p>Internal function to reconstruct a given hologram.</p> <p>Parameters:</p> <ul> <li> <code>hologram_phases</code>           \u2013            <pre><code>                     Hologram phases [ch x m x n].\n</code></pre> </li> <li> <code>amplitude</code>           \u2013            <pre><code>                     Amplitude profiles for each color primary [ch x m x n]\n</code></pre> </li> <li> <code>no_grad</code>           \u2013            <pre><code>                     If set True, uses torch.no_grad in reconstruction.\n</code></pre> </li> <li> <code>get_complex</code>           \u2013            <pre><code>                     If set True, reconstructor returns the complex field but not the intensities.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>reconstructions</code> (              <code>tensor</code> )          \u2013            <p>Reconstructed frames.</p> </li> </ul> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def reconstruct(self, hologram_phases, amplitude = None, no_grad = True, get_complex = False):\n    \"\"\"\n    Internal function to reconstruct a given hologram.\n\n\n    Parameters\n    ----------\n    hologram_phases            : torch.tensor\n                                 Hologram phases [ch x m x n].\n    amplitude                  : torch.tensor\n                                 Amplitude profiles for each color primary [ch x m x n]\n    no_grad                    : bool\n                                 If set True, uses torch.no_grad in reconstruction.\n    get_complex                : bool\n                                 If set True, reconstructor returns the complex field but not the intensities.\n\n    Returns\n    -------\n    reconstructions            : torch.tensor\n                                 Reconstructed frames.\n    \"\"\"\n    if no_grad:\n        torch.no_grad()\n    if len(hologram_phases.shape) &gt; 3:\n        hologram_phases = hologram_phases.squeeze(0)\n    if get_complex == True:\n        reconstruction_type = torch.complex64\n    else:\n        reconstruction_type = torch.float32\n    if hologram_phases.shape[0] != self.number_of_frames:\n        logging.warning('Provided hologram frame count is {} but the configured number of frames is {}.'.format(hologram_phases.shape[0], self.number_of_frames))\n    reconstructions = torch.zeros(\n                                  self.number_of_frames,\n                                  self.number_of_depth_layers,\n                                  self.number_of_channels,\n                                  self.resolution[0] * self.resolution_factor,\n                                  self.resolution[1] * self.resolution_factor,\n                                  dtype = reconstruction_type,\n                                  device = self.device\n                                 )\n    if isinstance(amplitude, type(None)):\n        amplitude = torch.zeros(\n                                self.number_of_channels,\n                                self.resolution[0] * self.resolution_factor,\n                                self.resolution[1] * self.resolution_factor,\n                                device = self.device\n                               )\n        amplitude[:, ::self.resolution_factor, ::self.resolution_factor] = 1.\n    if self.resolution_factor != 1:\n        hologram_phases_scaled = torch.zeros_like(amplitude)\n        hologram_phases_scaled[\n                               :,\n                               ::self.resolution_factor,\n                               ::self.resolution_factor\n                              ] = hologram_phases\n    else:\n        hologram_phases_scaled = hologram_phases\n    for frame_id in range(self.number_of_frames):\n        for depth_id in range(self.number_of_depth_layers):\n            for channel_id in range(self.number_of_channels):\n                laser_power = self.get_laser_powers()[frame_id][channel_id]\n                phase = hologram_phases_scaled[frame_id]\n                hologram = generate_complex_field(\n                                                  laser_power * amplitude[channel_id],\n                                                  phase * self.phase_scale[channel_id]\n                                                 )\n                reconstruction_field = self.__call__(hologram, channel_id, depth_id)\n                if get_complex == True:\n                    result = reconstruction_field\n                else:\n                    result = calculate_amplitude(reconstruction_field) ** 2\n\n                if no_grad: \n                    result = result.detach().clone()\n\n                reconstructions[\n                                frame_id,\n                                depth_id,\n                                channel_id\n                               ] = result\n\n    return reconstructions\n</code></pre> <p>Function that represents the forward model in hologram optimization.</p> <p>Parameters:</p> <ul> <li> <code>input_field</code>           \u2013            <pre><code>              Input complex input field.\n</code></pre> </li> <li> <code>channel_id</code>           \u2013            <pre><code>              Identifying the color primary to be used.\n</code></pre> </li> <li> <code>depth_id</code>           \u2013            <pre><code>              Identifying the depth layer to be used.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output_field</code> (              <code>tensor</code> )          \u2013            <p>Propagated output complex field.</p> </li> </ul> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def __call__(self, input_field, channel_id, depth_id):\n    \"\"\"\n    Function that represents the forward model in hologram optimization.\n\n    Parameters\n    ----------\n    input_field         : torch.tensor\n                          Input complex input field.\n    channel_id          : int\n                          Identifying the color primary to be used.\n    depth_id            : int\n                          Identifying the depth layer to be used.\n\n    Returns\n    -------\n    output_field        : torch.tensor\n                          Propagated output complex field.\n    \"\"\"\n    distance = self.distances[depth_id]\n    if not self.generated_kernels[depth_id, channel_id]:\n        if self.propagator_type == 'forward':\n            H = get_propagation_kernel(\n                                       nu = self.resolution[0] * 2,\n                                       nv = self.resolution[1] * 2,\n                                       dx = self.pixel_pitch,\n                                       wavelength = self.wavelengths[channel_id],\n                                       distance = distance,\n                                       device = self.device,\n                                       propagation_type = self.propagation_type,\n                                       samples = self.aperture_samples,\n                                       scale = self.resolution_factor\n                                      )\n        elif self.propagator_type == 'back and forth':\n            H_forward = get_propagation_kernel(\n                                               nu = self.resolution[0] * 2,\n                                               nv = self.resolution[1] * 2,\n                                               dx = self.pixel_pitch,\n                                               wavelength = self.wavelengths[channel_id],\n                                               distance = self.zero_mode_distance,\n                                               device = self.device,\n                                               propagation_type = self.propagation_type,\n                                               samples = self.aperture_samples,\n                                               scale = self.resolution_factor\n                                              )\n            distance_back = -(self.zero_mode_distance + self.image_location_offset - distance)\n            H_back = get_propagation_kernel(\n                                            nu = self.resolution[0] * 2,\n                                            nv = self.resolution[1] * 2,\n                                            dx = self.pixel_pitch,\n                                            wavelength = self.wavelengths[channel_id],\n                                            distance = distance_back,\n                                            device = self.device,\n                                            propagation_type = self.propagation_type,\n                                            samples = self.aperture_samples,\n                                            scale = self.resolution_factor\n                                           )\n            H = H_forward * H_back\n        self.kernels[depth_id, channel_id] = H\n        self.generated_kernels[depth_id, channel_id] = True\n    else:\n        H = self.kernels[depth_id, channel_id].detach().clone()\n    field_scale = input_field\n    field_scale_padded = zero_pad(field_scale)\n    output_field_padded = custom(field_scale_padded, H, aperture = self.aperture)\n    output_field = crop_center(output_field_padded)\n    return output_field\n</code></pre> <p>This sample unit test provides an example use case of the holographic display class.</p> <code>test_learn_wave_holographic_display.py</code> <pre><code>\n</code></pre> <p>Let us also examine how the reconstructed images look like at the image plane.</p> <p> </p> Reconstructed phase-only hologram at two image plane, generated using \"test/test_learn_wave_holographic_display.py\". <p>You may also be curious about how these holograms would look like in an actual holographic display, here we provide a sample gallery filled with photographs captured from our holographic display:</p> <p> </p> Photographs of holograms captured using the holographic display in Computational Light Laboratory"},{"location":"course/computer_generated_holography/#conclusion","title":"Conclusion","text":"<p> Informative</p> <p>Holography offers new frontiers as an emerging method in simulating light for various applications, including displays and cameras. We provide a basic introduction to Computer-Generated Holography and a simple understanding of holographic methods. A motivated reader could scale up from this knowledge to advance concepts in displays, cameras, visual perception, optical computing, and many other light-based applications.</p> <p>Reminder</p> <p>We host a Slack group with more than 300 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link. Readers can get in-touch with the wider community using this public group.</p> <ol> <li> <p>Max Born and Emil Wolf. Principles of optics: electromagnetic theory of propagation, interference and diffraction of light. Elsevier, 2013.\u00a0\u21a9</p> </li> <li> <p>John C Heurtley. Scalar rayleigh\u2013sommerfeld and kirchhoff diffraction integrals: a comparison of exact evaluations for axial points. JOSA, 63(8):1003\u20131008, 1973.\u00a0\u21a9</p> </li> <li> <p>Maciej Sypek. Light propagation in the fresnel region. new numerical approach. Optics communications, 116(1-3):43\u201348, 1995.\u00a0\u21a9</p> </li> <li> <p>Gary J Swanson. Binary optics technology: the theory and design of multi-level diffractive optical elements. Technical Report, MASSACHUSETTS INST OF TECH LEXINGTON LINCOLN LAB, 1989.\u00a0\u21a9</p> </li> <li> <p>Herwig Kogelnik. Coupled wave theory for thick hologram gratings. Bell System Technical Journal, 48(9):2909\u20132947, 1969.\u00a0\u21a9</p> </li> <li> <p>Lingling Huang, Shuang Zhang, and Thomas Zentgraf. Metasurface holography: from fundamentals to applications. Nanophotonics, 7(6):1169\u20131190, 2018.\u00a0\u21a9</p> </li> <li> <p>Koray Kavakl\u0131, Yuta Itoh, Hakan Urey, and Kaan Ak\u015fit. Realistic defocus blur for multiplane computer-generated holography. In 2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR), 418\u2013426. IEEE, 2023.\u00a0\u21a9</p> </li> <li> <p>Koray Kavakli, David Robert Walton, Nick Antipa, Rafa\u0142 Mantiuk, Douglas Lanman, and Kaan Ak\u015fit. Optimizing vision and visuals: lectures on cameras, displays and perception. In ACM SIGGRAPH 2022 Courses, pages 1\u201366. 2022.\u00a0\u21a9</p> </li> <li> <p>Jason D Schmidt. Numerical simulation of optical wave propagation with examples in matlab. (No Title), 2010.\u00a0\u21a9</p> </li> <li> <p>Kyoji Matsushima and Tomoyoshi Shimobaba. Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields. Optics express, 17(22):19662\u201319673, 2009.\u00a0\u21a9</p> </li> <li> <p>Wenhui Zhang, Hao Zhang, and Guofan Jin. Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range. Optics letters, 45(6):1543\u20131546, 2020.\u00a0\u21a9</p> </li> <li> <p>Wenhui Zhang, Hao Zhang, and Guofan Jin. Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product. Optics Letters, 45(16):4416\u20134419, 2020.\u00a0\u21a9</p> </li> <li> <p>Yifan Peng, Suyeon Choi, Nitish Padmanaban, and Gordon Wetzstein. Neural holography with camera-in-the-loop training. ACM Transactions on Graphics (TOG), 39(6):1\u201314, 2020.\u00a0\u21a9</p> </li> <li> <p>Praneeth Chakravarthula, Ethan Tseng, Tarun Srivastava, Henry Fuchs, and Felix Heide. Learned hardware-in-the-loop phase retrieval for holographic near-eye displays. ACM Transactions on Graphics (TOG), 39(6):1\u201318, 2020.\u00a0\u21a9</p> </li> <li> <p>Koray Kavakl\u0131, Hakan Urey, and Kaan Ak\u015fit. Learned holographic light transport. Applied Optics, 61(5):B50\u2013B55, 2022.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/fundamentals/","title":"Fundamentals in optimizing and learning light","text":"Narrate section"},{"location":"course/fundamentals/#fundamentals-and-standards","title":"Fundamentals and Standards","text":"<p>This chapter will reveal some important basic information you will use in the rest of this course. In addition, we will also introduce you to a structure where we establish some standards to decrease the chances of producing buggy or incompatible codes.</p>"},{"location":"course/fundamentals/#required-production-environment","title":"Required Production Environment","text":"<p> Informative \u00b7  Practical</p> <p>We have provided some information in prerequisites. This information includes programming language requirements, required libraries, text editors, build environments, and operating system requirements. For installing our library, odak, we strongly advise using the version in the source repository. You can install odak from the source repository using your favorite terminal and operating system:</p> <pre><code>pip3 install git+https://github.com/kaanaksit/odak\n</code></pre> <p>Note that your production environment meaning your computer and required software for this course is important. To avoid wasting time in the next chapters and get the most from this lecture, please ensure that you have dedicated enough time to set everything up as it should.</p>"},{"location":"course/fundamentals/#production-standards","title":"Production Standards","text":"<p> Informative</p> <p>In this course, you will be asked to code and implement simulations related to the physics of light. Your work, meaning your production, should strictly follow certain habits to help build better tools and developments.</p>"},{"location":"course/fundamentals/#subversion-and-revision-control","title":"Subversion and Revision Control","text":"<p> Informative \u00b7  Practical</p> <p>As you develop your code for your future homework and projects, you will discover that many things could go wrong. For example, the hard drive that contains the only copy of your code could be damaged, or your most trusted friend (so-called) can claim that she compiled most of the work, and gets her credit for it, although that is not the case. These are just a few potential cases that may happen to you. On the other hand, in business life, poor code control can cause companies to lose money by releasing incorrect codes or researchers to lose their reputations as their work is challenging to replicate. How do you claim in that case that you did your part? What is the proper method to avoid losing data, time, effort, and motivation? In short, what is the way to stay out of trouble?</p> <p>This is where the subversion, authoring, and revision control systems come into play, especially, for the example cases discussed in the previous paragraph. In today's world, Git is a widespread version control system adopted by major websites such as GitHub or Gitlab. We will not dive deep into how to use Git and all its features, but I will try to highlight parts that are essential for your workflow. I encourage you to use Git for creating a repository for every one of your tasks in the future. You can either keep this repository in your local and constantly back up somewhere else (suggested to people knowing what they are doing) or use these online services such as GitHub or Gitlab. I also encourage you to use the online services if you are a beginner.</p> <p>For each operating system, installing Git has its processes, but for an Ubuntu operating system, it is as easy as typing the following commands in your terminal:</p> <pre><code>sudo apt install git\n</code></pre> <p>Let us imagine that you want to start a repository on GitHub. Make sure to create a private repository, and please only go public with any repository once you feel it is at a state where it can be shared with others. Once you have created your repository on GitHub, you can clone the repository using the following command in a terminal:</p> <pre><code>git clone REPLACEWITHLOCATIONOFREPO\n</code></pre> <p>You can find out about the repository's location by visiting the repository's website that you have created. The location is typically revealed by clicking the code button, as depicted in the below screenshot.</p> <p> </p> A screenshot showing how you can acquire the link for cloning a repository from GitHub. <p>For example, in the above case, the command should be updated with the following:</p> <pre><code>git clone https://github.com/kaanaksit/odak.git\n</code></pre> <p>If you want to share your private repository with someone you can go into the settings of your repository in its webpage and navigate to the collaborators section. This way, you can assign roles to your collaborators that best suit your scenario.</p> <p>Secure your account</p> <p>If you are using GitHub for your development, I highly encourage you to consider using two-factor authentication.</p>"},{"location":"course/fundamentals/#git-basics","title":"Git Basics","text":"<p> Informative \u00b7  Practical</p> <p>If you want to add new files to your subversion control system, use the following in a terminal:</p> <pre><code>git add YOURFILE.jpeg\n</code></pre> <p>You may want to track the status of the files (whether they are added, deleted, etc.) </p><pre><code>git status\n</code></pre> And later, you can update the online copy (remote server or source) using the following:<p></p> <pre><code>git commit -am \"Explain what you add in a short comment.\"\ngit push\n</code></pre> <p>In some cases, you may want to include large binary files in your project, such as a paper, video, or any other media you want to archive within your project repository. For those cases, using just <code>git</code> may not be the best opinion, as Git works on creating a history of files and how they are changed at each commit, this history will likely be too bulky and oversized. Thus, cloning a repository could be slow when large binary files and Git come together. Assuming you are on an Ubuntu operating system, you can install the Large File Support (LFS) for Git by typing these commands in your terminal:</p> <pre><code>sudo apt install git-lfs\n</code></pre> <p>Once you have the LFS installed in your operating system, you can then go into your repository and enable LFS:</p> <pre><code>cd YOURREPOSITORY\ngit lfs install\n</code></pre> <p>Now is the time to let your LFS track specific files to avoid overcrowding your Git history. For example, you can track the <code>*.pdf</code> extension, meaning all the PDF files in your repository by typing the following command in your terminal:</p> <pre><code>git lfs track *.pdf\n</code></pre> <p>Finally, ensure the tracking information and LFS are copied to your remote/source repository.  You can do that using the following commands in your terminal:</p> <pre><code>git add .gitattributes\ngit commit -am \"Enabling large file support.\"\ngit push\n</code></pre> <p>When projects expand in size, it's quite feasible for hundreds of individuals to collaborate within the same repository.  This is particularly prevalent in sizable software development initiatives or open-source projects with a substantial contributor base.  The branching system is frequently employed in these circumstances. </p> <p>Consider you are in a software development team and you want to introduce new features or changes to a project without affecting the main or \"master\" branch.  You need to firstly create a new branch by using the following command which creates a new branch named BRANCHNAME but does not switch to it.  This new branch has the same contents as the current branch (a copy of the current branch). </p> <pre><code>git branch BRANCHNAME\n</code></pre> <p>Then you can switch to the new brach by using the command:</p> <pre><code>git checkout BRANCHNAME\n</code></pre> <p>Or use this command to create and switch to a new branch immediately</p> <pre><code>git checkout -b BRANCHNAME\n</code></pre> <p>After editing the new branch, you may want to update the changes to the master or main branch.  This command merges the branch named BRANCHNAME into the current branch.  You must resolve any conflicts to complete the merge.</p> <pre><code>git merge BRANCHNAME\n</code></pre> <p>We recommend an interactive, visual method for learning Git commands and branching online: learngitbranching. More information can be found in the offical Git documentation: Git docs.</p>"},{"location":"course/fundamentals/#coding-standards","title":"Coding Standards","text":"<p> Informative \u00b7  Practical</p> <p>I encourage our readers to follow the methods of coding highlighted here. Following the methods that I am going to explain is not only crucial for developing replicable projects, but it is also vital for allowing other people to read your code with the least amount of hassle.</p> Where do I find out more about Python coding standards? <p>Python Enhancement Proposals documentation provides a great deal of information on modern ways to code in Python.</p>"},{"location":"course/fundamentals/#avoid-using-long-lines","title":"Avoid using long lines.","text":"<p>Please avoid having too many characters in one line. Let us start with a bad example:</p> <pre><code>def light_transport(wavelength, distances, resolution, propagation_type, polarization, input_field, output_field, angles):\n      pass\n      return results\n</code></pre> <p>As you can observe, the above function requires multiple inputs to be provided. Try making the inputs more readable by breaking lines and in some cases, you can also provide the requested type for an input and a default value to guide your users:</p> <pre><code>def light_transport(\n                    wavelength,\n                    distances,\n                    resolution,\n                    propagation_type : str, \n                    polarization = 'vertical',\n                    input_field = torch.rand(1, 1, 100, 100),\n                    output_field = torch.zeros(1, 1, 100, 100),\n                    angles= [0., 0., 0.]\n                   ):\n    pass\n    return results\n</code></pre>"},{"location":"course/fundamentals/#leave-spaces-between-commands-variables-and-functions","title":"Leave spaces between commands, variables, and functions","text":"<p>Please avoid writing code like a train of characters. Here is a terrible coding example:</p> <pre><code>def addition(x,y,z):\n    result=2*y+z+x**2*3\n    return result\n</code></pre> <p>Please leave spaces after each comma, <code>,</code>, and mathematical operation. So now, we can correct the above example as in below:</p> <pre><code>def addition(x, y, z):\n    result = 2 * y + z + x ** 2 * 3\n    return result\n</code></pre> <p>Please also leave two lines of space between the two functions. Here is a bad example again:</p> <pre><code>def add(x, y):\n    return x + y\ndef multiply(x, y):\n    return x * y\n</code></pre> <p>Instead, it should be:</p> <pre><code>def add(x, y):\n    return x + y\n\n\ndef multiply(x, y):\n    return x * y\n</code></pre>"},{"location":"course/fundamentals/#add-documentation","title":"Add documentation","text":"<p>For your code, please make sure to add the necessary documentation. Here is a good example of doing that:</p> <pre><code>def add(x, y):\n    \"\"\"\n    A function to add two values together.\n\n    Parameters\n    ==========\n    x         : float\n                First input value.\n    y         : float\n                Second input value.\n\n    Returns\n    =======\n    result    : float\n                Result of the addition.\n    \"\"\"\n    result = x + y\n    return result\n</code></pre>"},{"location":"course/fundamentals/#use-a-code-style-checker-and-validator","title":"Use a code-style checker and validator","text":"<p>There are also code-style checkers and code validators that you can adapt to your workflows when coding. One of these code-style checkers and validators I use in my projects is <code>pyflakes.</code> On an Ubuntu operating system, you can install <code>pyflakes</code> easily by typing these commands into your terminal:</p> <pre><code>sudo apt install python3-pyflakes\n</code></pre> <p>It could tell you about missing imports or undefined or unused variables. You can use it on any Python script very easily:</p> <pre><code>pyflakes3 sample.py\n</code></pre> <p>In addition, I use <code>flake8</code> and <code>autopep8</code> for standard code violations. To learn more about these, please read the code section of the contribution guide.</p>"},{"location":"course/fundamentals/#naming-variables","title":"Naming variables","text":"<p>When naming variables use lower case letters and make sure that the variables are named in an explanatory manner. Please also always use underscore as a replacement of space. For example if you are going to create a variable for storing reconstructed image at some image plane, you can name that variable as <code>reconstructions_image_planes</code>. </p>"},{"location":"course/fundamentals/#use-fewer-imports","title":"Use fewer imports","text":"<p>When it comes to importing libraries in your code, please make sure to use a minimal amount of libraries. Using a few libraries can help you keep your code robust and working over newer generations of libraries. Please stick to the libraries suggested in this course when coding for this course. If you need access to some other library, please do let us know!</p>"},{"location":"course/fundamentals/#fixing-bugs","title":"Fixing bugs","text":"<p>Often, you can encounter bugs in your code. To fix your code in such cases, I would like you to consider using a method called <code>Rubber duck debugging</code> or <code>Rubber ducking.</code> The basic idea is to be able to express your code to a third person or yourself line by line. Explaining line by line could help you see what is wrong with your code. I am sure there are many recipes for solving bugs in codes. I tried introducing you to one that works for me.</p>"},{"location":"course/fundamentals/#have-a-requirementstxt","title":"Have a <code>requirements.txt</code>","text":"<p>Please also make sure to have a <code>requirements.txt</code> in the root directory of your repository. For example, in this course your <code>requirements.txt</code> would look like this:</p> <pre><code>odak&gt;=0.2.4\ntorch \n</code></pre> <p>This way, a future user of your code could install the required libraries by following a simple command in a terminal:</p> <pre><code>pip3 install -m requirements.txt \n</code></pre>"},{"location":"course/fundamentals/#always-use-the-same-function-for-saving-and-loading","title":"Always use the same function for saving and loading","text":"<p>Most issues in every software project come from repetition. Imagine if you want to save and load images inside a code after some processing. If you rely on manually coding a save and load routine in every corner of the same code, it is likely that when you change one of these saving or loading routines, you must modify the others. In other words, do not rediscover what you have already known.  Instead, turn it into a Lego brick you can use whenever needed. For saving and loading images, please rely on functions in odak to avoid any issues. For example, if I want to load a sample image called <code>letter.jpeg</code>, I can rely on this example:</p> <pre><code>import odak\nimage = odak.learn.tools.load_image(\n                                    'letter.jpeg',\n                                    torch_style = True, # (1)\n                                    normalizeby = 255. # (2)\n                                   )\n</code></pre> <ol> <li>If you set this flag to True, the image will be loaded     as [ch x m x n], where ch represents the number of color channels (e.g., typically three).     In case of False, it will be loaded as [m x n x ch].</li> <li>If you provide a floating number here, the image to be loaded will be divived with that number.     For example, if you have a 8-bit image (0-255) and if you provide <code>normalizeby = 2.0</code>, the maximum     value that you can expect is 255 / 2. = 127.5.</li> </ol> <p>Odak also provides a standard method for saving your torch tensors as image files:</p> <pre><code>odak.learn.tools.save_image(\n                            'copy.png',\n                            image,\n                            cmin = 0., # (1)\n                            cmax = 1., # (2)\n                            color_depth = 8 # (3)\n                           )\n</code></pre> <ol> <li>Minimum expected value for torch tensor <code>image</code>.</li> <li>Maximum expected value for torch tensor <code>image</code>.</li> <li>Pixel depth of the image to be saved, default is 8-bit.</li> </ol> <p>You may want to try the same code with different settings in some code development. In those cases, I create a separate <code>settings</code> folder in the root directory of my projects and add <code>JSON</code> files that I can load for testing different cases. To explain the case better, let us assume we will change the number of light sources in some simulations. Let's first assume that we create a settings file as <code>settings/experiment_000.txt</code> in the root directory and fill it with the following content:</p> <pre><code>{\n  \"light source\" : {\n                    \"count\" : 5,\n                    \"type\"  : \"LED\"\n                   }\n}\n</code></pre> <p>In the rest of my code, I can read, modify and save JSON files using odak functions:</p> <pre><code>import odak\nsettings = odak.tools.load_dictionary('./settings/experiment_000.txt')\nsettings['light source']['count'] = 10\nodak.tools.save_dictionary(settings, './settings/experiment_000.txt')\n</code></pre> <p>This way, you do not have to memorize the variables you used for every experiment you conducted with the same piece of code. You can have a dedicated settings file for each experiment.</p>"},{"location":"course/fundamentals/#create-unit-tests","title":"Create unit tests","text":"<p>Suppose your project is a library containing multiple valuable functions for developing other projects.  In that case, I encourage you to create unit tests for your library so that whenever you update it, you can see if your updates break anything in that library. For this purpose, consider creating a <code>test</code> directory in the root folder of your repository. In that directory, you can create separate Python scripts for testing out various functions of your library. Say there is a function called <code>add</code> in your project <code>MOSTAWESOMECODEEVER,</code> so your test script <code>test/test_add.py</code> should look like this:</p> <pre><code>import MOSTAWESOMECODEEVER\n\ndef test():\n    ground_truth = 3 + 5\n    result = MOSTAWESOMECODEEVER.add(3, 5)\n    if ground_trurth == result:\n        assert True == True\n    assert False == True\n\n\nif __name__ == '__main__':\n    sys.exit(test())\n</code></pre> <p>You may accumulate various unit tests in your <code>test</code> directory. To test them all before pushing them to your repository, you can rely on <code>pytest.</code> You can install <code>pytest</code> using the following command in your terminal:</p> <pre><code>pip3 install pytest\n</code></pre> <p>Once installed, you can navigate to your repository's root directory and call <code>pytest</code> to test things out:</p> <pre><code>cd MOSTAWESOMECODEEVER\npytest\n</code></pre> <p>If anything is wrong with your unit tests, which validate your functions, <code>pytest</code> will provide a detailed explanation.Suppose your project is a library containing multiple valuable functions for developing other projects.  In that case, I encourage you to create unit tests for your library so that whenever you update it, you can see if your updates break anything in that library. For this purpose, consider creating a <code>test</code> directory in the root folder of your repository. In that directory, you can create separate Python scripts for testing out various functions of your library. Say there is a function called <code>add</code> in your project <code>MOSTAWESOMECODEEVER,</code> so your test script <code>test/test_add.py</code> should look like this:</p> <pre><code>import MOSTAWESOMECODEEVER\n\ndef test():\n    ground_truth = 3 + 5\n    result = MOSTAWESOMECODEEVER.add(3, 5)\n    if ground_trurth == result:\n        assert True == True\n    assert False == True\n\n\nif __name__ == '__main__':\n    sys.exit(test())\n</code></pre> <p>You may accumulate various unit tests in your <code>test</code> directory. To test them all before pushing them to your repository, you can rely on <code>pytest.</code> You can install <code>pytest</code> using the following command in your terminal:</p> <pre><code>pip3 install pytest\n</code></pre> <p>Once installed, you can navigate to your repository's root directory and call <code>pytest</code> to test things out:</p> <pre><code>cd MOSTAWESOMECODEEVER\npytest\n</code></pre> <p>If anything is wrong with your unit tests, which validate your functions, <code>pytest</code> will provide a detailed explanation.</p>"},{"location":"course/fundamentals/#set-a-licence","title":"Set a licence","text":"<p>If you want to distribute your code online, consider adding a license to avoid having difficulties related to sharing with others. In other words, you can add <code>LICENSE.txt</code> in the root directory of your repository. To determine which license works best for you, consider visiting this guideline. When choosing a license for your project, consider tinkering about whether you agree people are building a product out of your work or derivate, etc.</p> Lab work: Prepare a project repository <p>Please prepare a sample repository on GitHub using the information provided in the above sections.  Here are some sample files that may inspire you and help you structure your project in good order:</p> <code>main.py</code> <code>LICENSE.txt</code> <code>requirements.txt</code> <code>THANKS.txt</code> <code>CODE_OF_CONDUCT.md</code> <pre><code>import odak\nimport torch\nimport sys\n\n\ndef main():\n    print('your codebase')\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n</code></pre> LICENSE.txt<pre><code>Mozilla Public License Version 2.0\n==================================\n\n1. Definitions\n--------------\n\n1.1. \"Contributor\"\n    means each individual or legal entity that creates, contributes to\n    the creation of, or owns Covered Software.\n\n1.2. \"Contributor Version\"\n    means the combination of the Contributions of others (if any) used\n    by a Contributor and that particular Contributor's Contribution.\n\n1.3. \"Contribution\"\n    means Covered Software of a particular Contributor.\n\n1.4. \"Covered Software\"\n    means Source Code Form to which the initial Contributor has attached\n    the notice in Exhibit A, the Executable Form of such Source Code\n    Form, and Modifications of such Source Code Form, in each case\n    including portions thereof.\n\n1.5. \"Incompatible With Secondary Licenses\"\n    means\n\n    (a) that the initial Contributor has attached the notice described\n        in Exhibit B to the Covered Software; or\n\n    (b) that the Covered Software was made available under the terms of\n        version 1.1 or earlier of the License, but not also under the\n        terms of a Secondary License.\n\n1.6. \"Executable Form\"\n    means any form of the work other than Source Code Form.\n\n1.7. \"Larger Work\"\n    means a work that combines Covered Software with other material, in \n    a separate file or files, that is not Covered Software.\n\n1.8. \"License\"\n    means this document.\n\n1.9. \"Licensable\"\n    means having the right to grant, to the maximum extent possible,\n    whether at the time of the initial grant or subsequently, any and\n    all of the rights conveyed by this License.\n\n1.10. \"Modifications\"\n    means any of the following:\n\n    (a) any file in Source Code Form that results from an addition to,\n        deletion from, or modification of the contents of Covered\n        Software; or\n\n    (b) any new file in Source Code Form that contains any Covered\n        Software.\n\n1.11. \"Patent Claims\" of a Contributor\n    means any patent claim(s), including without limitation, method,\n    process, and apparatus claims, in any patent Licensable by such\n    Contributor that would be infringed, but for the grant of the\n    License, by the making, using, selling, offering for sale, having\n    made, import, or transfer of either its Contributions or its\n    Contributor Version.\n\n1.12. \"Secondary License\"\n    means either the GNU General Public License, Version 2.0, the GNU\n    Lesser General Public License, Version 2.1, the GNU Affero General\n    Public License, Version 3.0, or any later versions of those\n    licenses.\n\n1.13. \"Source Code Form\"\n    means the form of the work preferred for making modifications.\n\n1.14. \"You\" (or \"Your\")\n    means an individual or a legal entity exercising rights under this\n    License. For legal entities, \"You\" includes any entity that\n    controls, is controlled by, or is under common control with You. For\n    purposes of this definition, \"control\" means (a) the power, direct\n    or indirect, to cause the direction or management of such entity,\n    whether by contract or otherwise, or (b) ownership of more than\n    fifty percent (50%) of the outstanding shares or beneficial\n    ownership of such entity.\n\n2. License Grants and Conditions\n--------------------------------\n\n2.1. Grants\n\nEach Contributor hereby grants You a world-wide, royalty-free,\nnon-exclusive license:\n\n(a) under intellectual property rights (other than patent or trademark)\n    Licensable by such Contributor to use, reproduce, make available,\n    modify, display, perform, distribute, and otherwise exploit its\n    Contributions, either on an unmodified basis, with Modifications, or\n    as part of a Larger Work; and\n\n(b) under Patent Claims of such Contributor to make, use, sell, offer\n    for sale, have made, import, and otherwise transfer either its\n    Contributions or its Contributor Version.\n\n2.2. Effective Date\n\nThe licenses granted in Section 2.1 with respect to any Contribution\nbecome effective for each Contribution on the date the Contributor first\ndistributes such Contribution.\n\n2.3. Limitations on Grant Scope\n\nThe licenses granted in this Section 2 are the only rights granted under\nthis License. No additional rights or licenses will be implied from the\ndistribution or licensing of Covered Software under this License.\nNotwithstanding Section 2.1(b) above, no patent license is granted by a\nContributor:\n\n(a) for any code that a Contributor has removed from Covered Software;\n    or\n\n(b) for infringements caused by: (i) Your and any other third party's\n    modifications of Covered Software, or (ii) the combination of its\n    Contributions with other software (except as part of its Contributor\n    Version); or\n\n(c) under Patent Claims infringed by Covered Software in the absence of\n    its Contributions.\n\nThis License does not grant any rights in the trademarks, service marks,\nor logos of any Contributor (except as may be necessary to comply with\nthe notice requirements in Section 3.4).\n\n2.4. Subsequent Licenses\n\nNo Contributor makes additional grants as a result of Your choice to\ndistribute the Covered Software under a subsequent version of this\nLicense (see Section 10.2) or under the terms of a Secondary License (if\npermitted under the terms of Section 3.3).\n\n2.5. Representation\n\nEach Contributor represents that the Contributor believes its\nContributions are its original creation(s) or it has sufficient rights\nto grant the rights to its Contributions conveyed by this License.\n\n2.6. Fair Use\n\nThis License is not intended to limit any rights You have under\napplicable copyright doctrines of fair use, fair dealing, or other\nequivalents.\n\n2.7. Conditions\n\nSections 3.1, 3.2, 3.3, and 3.4 are conditions of the licenses granted\nin Section 2.1.\n\n3. Responsibilities\n-------------------\n\n3.1. Distribution of Source Form\n\nAll distribution of Covered Software in Source Code Form, including any\nModifications that You create or to which You contribute, must be under\nthe terms of this License. You must inform recipients that the Source\nCode Form of the Covered Software is governed by the terms of this\nLicense, and how they can obtain a copy of this License. You may not\nattempt to alter or restrict the recipients' rights in the Source Code\nForm.\n\n3.2. Distribution of Executable Form\n\nIf You distribute Covered Software in Executable Form then:\n\n(a) such Covered Software must also be made available in Source Code\n    Form, as described in Section 3.1, and You must inform recipients of\n    the Executable Form how they can obtain a copy of such Source Code\n    Form by reasonable means in a timely manner, at a charge no more\n    than the cost of distribution to the recipient; and\n\n(b) You may distribute such Executable Form under the terms of this\n    License, or sublicense it under different terms, provided that the\n    license for the Executable Form does not attempt to limit or alter\n    the recipients' rights in the Source Code Form under this License.\n\n3.3. Distribution of a Larger Work\n\nYou may create and distribute a Larger Work under terms of Your choice,\nprovided that You also comply with the requirements of this License for\nthe Covered Software. If the Larger Work is a combination of Covered\nSoftware with a work governed by one or more Secondary Licenses, and the\nCovered Software is not Incompatible With Secondary Licenses, this\nLicense permits You to additionally distribute such Covered Software\nunder the terms of such Secondary License(s), so that the recipient of\nthe Larger Work may, at their option, further distribute the Covered\nSoftware under the terms of either this License or such Secondary\nLicense(s).\n\n3.4. Notices\n\nYou may not remove or alter the substance of any license notices\n(including copyright notices, patent notices, disclaimers of warranty,\nor limitations of liability) contained within the Source Code Form of\nthe Covered Software, except that You may alter any license notices to\nthe extent required to remedy known factual inaccuracies.\n\n3.5. Application of Additional Terms\n\nYou may choose to offer, and to charge a fee for, warranty, support,\nindemnity or liability obligations to one or more recipients of Covered\nSoftware. However, You may do so only on Your own behalf, and not on\nbehalf of any Contributor. You must make it absolutely clear that any\nsuch warranty, support, indemnity, or liability obligation is offered by\nYou alone, and You hereby agree to indemnify every Contributor for any\nliability incurred by such Contributor as a result of warranty, support,\nindemnity or liability terms You offer. You may include additional\ndisclaimers of warranty and limitations of liability specific to any\njurisdiction.\n\n4. Inability to Comply Due to Statute or Regulation\n---------------------------------------------------\n\nIf it is impossible for You to comply with any of the terms of this\nLicense with respect to some or all of the Covered Software due to\nstatute, judicial order, or regulation then You must: (a) comply with\nthe terms of this License to the maximum extent possible; and (b)\ndescribe the limitations and the code they affect. Such description must\nbe placed in a text file included with all distributions of the Covered\nSoftware under this License. Except to the extent prohibited by statute\nor regulation, such description must be sufficiently detailed for a\nrecipient of ordinary skill to be able to understand it.\n\n5. Termination\n--------------\n\n5.1. The rights granted under this License will terminate automatically\nif You fail to comply with any of its terms. However, if You become\ncompliant, then the rights granted under this License from a particular\nContributor are reinstated (a) provisionally, unless and until such\nContributor explicitly and finally terminates Your grants, and (b) on an\nongoing basis, if such Contributor fails to notify You of the\nnon-compliance by some reasonable means prior to 60 days after You have\ncome back into compliance. Moreover, Your grants from a particular\nContributor are reinstated on an ongoing basis if such Contributor\nnotifies You of the non-compliance by some reasonable means, this is the\nfirst time You have received notice of non-compliance with this License\nfrom such Contributor, and You become compliant prior to 30 days after\nYour receipt of the notice.\n\n5.2. If You initiate litigation against any entity by asserting a patent\ninfringement claim (excluding declaratory judgment actions,\ncounter-claims, and cross-claims) alleging that a Contributor Version\ndirectly or indirectly infringes any patent, then the rights granted to\nYou by any and all Contributors for the Covered Software under Section\n2.1 of this License shall terminate.\n\n5.3. In the event of termination under Sections 5.1 or 5.2 above, all\nend user license agreements (excluding distributors and resellers) which\nhave been validly granted by You or Your distributors under this License\nprior to termination shall survive termination.\n\n************************************************************************\n*                                                                      *\n*  6. Disclaimer of Warranty                                           *\n*  -------------------------                                           *\n*                                                                      *\n*  Covered Software is provided under this License on an \"as is\"       *\n*  basis, without warranty of any kind, either expressed, implied, or  *\n*  statutory, including, without limitation, warranties that the       *\n*  Covered Software is free of defects, merchantable, fit for a        *\n*  particular purpose or non-infringing. The entire risk as to the     *\n*  quality and performance of the Covered Software is with You.        *\n*  Should any Covered Software prove defective in any respect, You     *\n*  (not any Contributor) assume the cost of any necessary servicing,   *\n*  repair, or correction. This disclaimer of warranty constitutes an   *\n*  essential part of this License. No use of any Covered Software is   *\n*  authorized under this License except under this disclaimer.         *\n*                                                                      *\n************************************************************************\n\n************************************************************************\n*                                                                      *\n*  7. Limitation of Liability                                          *\n*  --------------------------                                          *\n*                                                                      *\n*  Under no circumstances and under no legal theory, whether tort      *\n*  (including negligence), contract, or otherwise, shall any           *\n*  Contributor, or anyone who distributes Covered Software as          *\n*  permitted above, be liable to You for any direct, indirect,         *\n*  special, incidental, or consequential damages of any character      *\n*  including, without limitation, damages for lost profits, loss of    *\n*  goodwill, work stoppage, computer failure or malfunction, or any    *\n*  and all other commercial damages or losses, even if such party      *\n*  shall have been informed of the possibility of such damages. This   *\n*  limitation of liability shall not apply to liability for death or   *\n*  personal injury resulting from such party's negligence to the       *\n*  extent applicable law prohibits such limitation. Some               *\n*  jurisdictions do not allow the exclusion or limitation of           *\n*  incidental or consequential damages, so this exclusion and          *\n*  limitation may not apply to You.                                    *\n*                                                                      *\n************************************************************************\n\n8. Litigation\n-------------\n\nAny litigation relating to this License may be brought only in the\ncourts of a jurisdiction where the defendant maintains its principal\nplace of business and such litigation shall be governed by laws of that\njurisdiction, without reference to its conflict-of-law provisions.\nNothing in this Section shall prevent a party's ability to bring\ncross-claims or counter-claims.\n\n9. Miscellaneous\n----------------\n\nThis License represents the complete agreement concerning the subject\nmatter hereof. If any provision of this License is held to be\nunenforceable, such provision shall be reformed only to the extent\nnecessary to make it enforceable. Any law or regulation which provides\nthat the language of a contract shall be construed against the drafter\nshall not be used to construe this License against a Contributor.\n\n10. Versions of the License\n---------------------------\n\n10.1. New Versions\n\nMozilla Foundation is the license steward. Except as provided in Section\n10.3, no one other than the license steward has the right to modify or\npublish new versions of this License. Each version will be given a\ndistinguishing version number.\n\n10.2. Effect of New Versions\n\nYou may distribute the Covered Software under the terms of the version\nof the License under which You originally received the Covered Software,\nor under the terms of any subsequent version published by the license\nsteward.\n\n10.3. Modified Versions\n\nIf you create software not governed by this License, and you want to\ncreate a new license for such software, you may create and use a\nmodified version of this License if you rename the license and remove\nany references to the name of the license steward (except to note that\nsuch modified license differs from this License).\n\n10.4. Distributing Source Code Form that is Incompatible With Secondary\nLicenses\n\nIf You choose to distribute Source Code Form that is Incompatible With\nSecondary Licenses under the terms of this version of the License, the\nnotice described in Exhibit B of this License must be attached.\n\nExhibit A - Source Code Form License Notice\n-------------------------------------------\n\n  This Source Code Form is subject to the terms of the Mozilla Public\n  License, v. 2.0. If a copy of the MPL was not distributed with this\n  file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nIf it is not possible or desirable to put the notice in a particular\nfile, then You may include the notice in a location (such as a LICENSE\nfile in a relevant directory) where a recipient would be likely to look\nfor such a notice.\n\nYou may add additional accurate notices of copyright ownership.\n\nExhibit B - \"Incompatible With Secondary Licenses\" Notice\n---------------------------------------------------------\n\n  This Source Code Form is \"Incompatible With Secondary Licenses\", as\n  defined by the Mozilla Public License, v. 2.0.\n</code></pre> requirements.txt<pre><code>opencv-python&gt;=4.10.0.84\nnumpy&gt;=1.26.4\ntorch&gt;=2.3.0\nplyfile&gt;=1.0.3\ntqdm&gt;=4.66.4\n</code></pre> THANKS.txt<pre><code>Ahmet Hamdi G\u00fczel\nAhmet Serdar Karadeniz\nDavid Robert Walton\nDavid Santiago Morales Norato\nHenry Kam\nDo\u011fa Y\u0131lmaz\nJeanne Beyazian\nJialun Wu\nJosef Spjut\nKoray Kavakl\u0131\nLiang Shi\nMustafa Do\u011fa Do\u011fan\nPraneeth Chakravarthula\nRunze Zhu\nWeijie Xie\nYujie Wang\nYuta Itoh\nZiyang Chen\nYicheng Zhan\nYiyang Wu\nXinge Yang\n</code></pre> CODE_OF_CONDUCT.md<pre><code># Contributor Covenant Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socio-economic status,\nnationality, personal appearance, race, religion, or sexual identity\nand orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the\n  overall community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or\n  advances of any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email\n  address, without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official e-mail address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement at\n.\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series\nof actions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or\npermanent ban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior,  harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within\nthe community.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant][homepage],\nversion 2.0, available at\nhttps://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n\nCommunity Impact Guidelines were inspired by [Mozilla's code of conduct\nenforcement ladder](https://github.com/mozilla/diversity).\n\n[homepage]: https://www.contributor-covenant.org\n\nFor answers to common questions about this code of conduct, see the FAQ at\nhttps://www.contributor-covenant.org/faq. Translations are available at\nhttps://www.contributor-covenant.org/translations.\n</code></pre>"},{"location":"course/fundamentals/#background-review","title":"Background Review","text":"<p> Informative \u00b7  Media</p> <p>Here, I will review some basic mathematical concepts using equations, images, or codes. Please note that you must understand these concepts to avoid difficulty following this course.</p>"},{"location":"course/fundamentals/#convolution-operation","title":"Convolution Operation","text":"<p>Convolution is a mathematical operation used as a building block for describing systems. It has proven to be highly effective in machine learning and deep learning. Convolution operation often denoted with a <code>*</code> symbol. Assume there is a matrix, A, which we want to convolve with some other matrix, also known as the kernel, K.</p> <p> </p> A sketch showing a matrix and a kernel to be convolved. <p>One can define such a matrix and a kernel using Torch in Python:</p> <pre><code>a = torch.tensor(\n                 [\n                  [1, 5, 9, 2, 3],\n                  [4, 8, 2, 3, 6],\n                  [7, 2, 0, 1, 3],\n                  [9, 6, 4, 2, 5],\n                  [2, 3, 5, 7, 4]\n                 ]\n                )\nk = torch.tensor(\n                 [\n                  [-1, 2, -3], \n                  [ 3, 5,  7], \n                  [-4, 9, -2]\n                 ]\n                )\n</code></pre> <p>To convolve these two matrices without losing information, we first have to go through a mathematical operation called zero padding.</p> <p> </p> A sketch showing zeropadding operating on a matrix. <p>To zeropad the matrix A, you can rely on Odak:</p> <pre><code>import odak\n\na_zeropad = odak.learn.tools.zero_pad(a, size = [7, 7])\n</code></pre> <p>Note that we pass here size as <code>[7, 7]</code>, the logic of this is very simple. Our original matrix was five by five if you add a zero along two axis, you get seven by seven as the new requested size. Also note that our kernel is three by three. There could be cases where there is a larger kernel size. In those cases, you want to zeropad half the size of kernel (e.g., original size plus half the kernel size, <code>a.shape[0] + k.shape[0] // 2</code>). Now we choose the first element in the original matrix A, multiply it with the kernel, and add it to a matrix R. But note that we add the results of our summation by centring it with the original location of the first element.</p> <p> </p> A sketch showing the first step of a convolution operation. <p>We have to repeat this operation for each element in our original matrix and accummulate a result.</p> <p> </p> A sketch showing the second step of a convolution operation. <p>Note that there are other ways to describe and implement the convolution operation. Thus far, this definition formulates a simplistic description for convolution. </p> Lab work: Implement convolution operation using Numpy <p>There are three possible ways to implement convolution operation on a computer. The first one involves loops visiting each point in a given data. The second involves formulating a convolution operation as matrix multiplication, and the final one involves implementing convolution as a multiplication operation in the Fourier domain. Implement all these three methods using Jupyter Notebooks and visually prove that they are all functioning correctly with various kernels (e.g., convolving image with a kernel).  Listed source files below may inspire your implementation in various means.  Note that the below code is based on Torch but not Numpy.</p> <code>odak.learn.tools.convolve2d</code> <code>odak.learn.tools.generate_2d_gaussian</code> <code>animation_convolution.py</code> <p>Definition to convolve a field with a kernel by multiplying in frequency space.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field with MxN shape.\n</code></pre> </li> <li> <code>kernel</code>           \u2013            <pre><code>      Input kernel with MxN shape.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>tensor</code> )          \u2013            <p>Convolved field.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def convolve2d(field, kernel):\n    \"\"\"\n    Definition to convolve a field with a kernel by multiplying in frequency space.\n\n    Parameters\n    ----------\n    field       : torch.tensor\n                  Input field with MxN shape.\n    kernel      : torch.tensor\n                  Input kernel with MxN shape.\n\n    Returns\n    ----------\n    new_field   : torch.tensor\n                  Convolved field.\n    \"\"\"\n    fr = torch.fft.fft2(field)\n    fr2 = torch.fft.fft2(torch.flip(torch.flip(kernel, [1, 0]), [0, 1]))\n    m, n = fr.shape\n    new_field = torch.real(torch.fft.ifft2(fr*fr2))\n    new_field = torch.roll(new_field, shifts=(int(n/2+1), 0), dims=(1, 0))\n    new_field = torch.roll(new_field, shifts=(int(m/2+1), 0), dims=(0, 1))\n    return new_field\n</code></pre> <p>Generate 2D Gaussian kernel. Inspired from https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy</p> <p>Parameters:</p> <ul> <li> <code>kernel_length</code>               (<code>list</code>, default:                   <code>[21, 21]</code> )           \u2013            <pre><code>        Length of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>nsigma</code>           \u2013            <pre><code>        Sigma of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>mu</code>           \u2013            <pre><code>        Mu of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>normalize</code>           \u2013            <pre><code>        If set True, normalize the output.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>kernel_2d</code> (              <code>tensor</code> )          \u2013            <p>Generated Gaussian kernel.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def generate_2d_gaussian(kernel_length = [21, 21], nsigma = [3, 3], mu = [0, 0], normalize = False):\n    \"\"\"\n    Generate 2D Gaussian kernel. Inspired from https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy\n\n    Parameters\n    ----------\n    kernel_length : list\n                    Length of the Gaussian kernel along X and Y axes.\n    nsigma        : list\n                    Sigma of the Gaussian kernel along X and Y axes.\n    mu            : list\n                    Mu of the Gaussian kernel along X and Y axes.\n    normalize     : bool\n                    If set True, normalize the output.\n\n    Returns\n    ----------\n    kernel_2d     : torch.tensor\n                    Generated Gaussian kernel.\n    \"\"\"\n    x = torch.linspace(-kernel_length[0]/2., kernel_length[0]/2., kernel_length[0])\n    y = torch.linspace(-kernel_length[1]/2., kernel_length[1]/2., kernel_length[1])\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    if nsigma[0] == 0:\n        nsigma[0] = 1e-5\n    if nsigma[1] == 0:\n        nsigma[1] = 1e-5\n    kernel_2d = 1. / (2. * torch.pi * nsigma[0] * nsigma[1]) * torch.exp(-((X - mu[0])**2. / (2. * nsigma[0]**2.) + (Y - mu[1])**2. / (2. * nsigma[1]**2.)))\n    if normalize:\n        kernel_2d = kernel_2d / kernel_2d.max()\n    return kernel_2d\n</code></pre> animation_convolution.py<pre><code>import odak\nimport torch\nimport sys\n\n\ndef main():\n    filename_image = '../media/10591010993_80c7cb37a6_c.jpg'\n    image = odak.learn.tools.load_image(filename_image, normalizeby = 255., torch_style = True)[0:3].unsqueeze(0)\n    kernel = odak.learn.tools.generate_2d_gaussian(kernel_length = [12, 12], nsigma = [21, 21])\n    kernel = kernel / kernel.max()\n    result = torch.zeros_like(image)\n    result = odak.learn.tools.zero_pad(result, size = [image.shape[-2] + kernel.shape[0], image.shape[-1] + kernel.shape[1]])\n    step = 0\n    for i in range(image.shape[-2]):\n        for j in range(image.shape[-1]):\n            for ch in range(image.shape[-3]):\n                element = image[:, ch, i, j]\n                add = kernel * element\n                result[:, ch, i : i + kernel.shape[0], j : j + kernel.shape[1]] += add\n            if (i * image.shape[-1] + j) % 1e4 == 0:\n                filename = 'step_{:04d}.png'.format(step)\n                odak.learn.tools.save_image( filename, result, cmin = 0., cmax = 100.)\n                step += 1\n    cmd = ['convert', '-delay', '1', '-loop', '0', '*.png', '../media/convolution_animation.gif']\n    odak.tools.shell_command(cmd)\n    cmd = ['rm', '*.png']\n    odak.tools.shell_command(cmd)\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n</code></pre> <p>In summary, the convolution operation is heavily used in describing optical systems, computer vision-related algorithms, and state-of-the-art machine learning techniques. Thus, understanding this mathematical operation is extremely important not only for this course but also for undergraduate and graduate-level courses. As an example, let's see step by step how a sample image provided below is convolved:</p> <p> </p> An animation showing the steps of convolution operation. <p>and the original image is as below:</p> <p> </p> Original image before the convolution operation (Generated by Stable Diffusion). <p>Note that the source image shown above is generated with a generative model. As a side note, I strongly suggest you to have familiarity with several models for generating test images, audio or any other type of media. This way, you can remove your dependency to others in various means.</p> Lab work: Convolve an image with a Gaussian kernel <p>Using Odak and Torch, blur an image using a Gaussian kernel. Also try compiling an animation like the one shown above using Matplotlib. Use the below solution as a last resort, try compiling your code. The code below is tested under Ubuntu operating system.</p> <code>animation_convolution.py</code> animation_convolution.py<pre><code>import odak\nimport torch\nimport sys\n\n\ndef main():\n    filename_image = '../media/10591010993_80c7cb37a6_c.jpg'\n    image = odak.learn.tools.load_image(filename_image, normalizeby = 255., torch_style = True)[0:3].unsqueeze(0)\n    kernel = odak.learn.tools.generate_2d_gaussian(kernel_length = [12, 12], nsigma = [21, 21])\n    kernel = kernel / kernel.max()\n    result = torch.zeros_like(image)\n    result = odak.learn.tools.zero_pad(result, size = [image.shape[-2] + kernel.shape[0], image.shape[-1] + kernel.shape[1]])\n    step = 0\n    for i in range(image.shape[-2]):\n        for j in range(image.shape[-1]):\n            for ch in range(image.shape[-3]):\n                element = image[:, ch, i, j]\n                add = kernel * element\n                result[:, ch, i : i + kernel.shape[0], j : j + kernel.shape[1]] += add\n            if (i * image.shape[-1] + j) % 1e4 == 0:\n                filename = 'step_{:04d}.png'.format(step)\n                odak.learn.tools.save_image( filename, result, cmin = 0., cmax = 100.)\n                step += 1\n    cmd = ['convert', '-delay', '1', '-loop', '0', '*.png', '../media/convolution_animation.gif']\n    odak.tools.shell_command(cmd)\n    cmd = ['rm', '*.png']\n    odak.tools.shell_command(cmd)\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n</code></pre>"},{"location":"course/fundamentals/#gradient-descent-optimizers","title":"Gradient Descent Optimizers","text":"<p>Throughout this course, we will have to optimize variables to generate a solution for our problems. Thus, we need a scalable method to optimize various variables in future problems and tasks. We will not review optimizers in this section but provide a working solution. You can learn more about optimizers through other courses offered within our curriculum or through suggested readings.  State-of-the-art Gradient Descent (GD) optimizers could play a key role here.  Significantly, Stochastic Gradient Descent (SGD) optimizers can help resolve our problems in the future with a reasonable memory footprint. This is because GD updates its weights by visiting every sample in a dataset, whereas SGD can update using only randomly chosen data from that dataset. Thus, SGD requires less memory for each update.</p> Where can I read more about the state-of-the-art Stochastic Gradient Descent optimizer? <p>To learn more, please read <code>Paszke, Adam, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. \"Automatic differentiation in pytorch.\" (2017).</code> <sup>1</sup></p> Would you like to code your Gradient Descent based optimizer ground up? <p>In case you are interested in coding your Gradient Descent-based optimizer from the ground up, consider watching this tutorial online where I code the optimizer using only <code>Numpy</code>: </p>   If you want to learn more about odak's built-in functions on the matter, visit the below unit test script:<p></p> <code>test_fit_gradient_descent_1d.py</code> test_fit_gradient_descent_1d.py<pre><code>import numpy as np\nimport sys\nimport odak\n\n\ndef gradient_function(x, y, function, parameters):\n    solution = function(x, parameters)\n    gradient = np.array([\n                         -2 * x**2 * (y - solution),\n                         -2 * x * (y- solution),\n                         -2 * (y - solution)\n                        ])\n    return gradient\n\n\ndef function(x, parameters):\n    y = parameters[0] * x**2 + parameters[1] * x + parameters[2]\n    return y\n\n\ndef l2_loss(a, b):\n    loss = np.sum((a - b)**2)\n    return loss\n\n\ndef test():\n    x = np.linspace(0, 1., 20) \n    y = function(x, parameters=[2., 1., 10.])\n\n    learning_rate = 5e-1\n    iteration_number = 2000\n    initial_parameters = np.array([10., 10., 0.])\n    estimated_parameters = odak.fit.gradient_descent_1d(\n                                                        input_data=x,\n                                                        ground_truth_data=y,\n                                                        function=function,\n                                                        loss_function=l2_loss,\n                                                        gradient_function=gradient_function,\n                                                        parameters=initial_parameters,\n                                                        learning_rate=learning_rate,\n                                                        iteration_number=iteration_number\n                                                       )\n    assert True == True\n\n\nif __name__ == '__main__':\n   sys.exit(test())\n</code></pre> <p>Torch is a blessing for people that optimizes or trains with their algorithm. Torch also comes with a set of state-of-the-art optimizers. One of these optimizers is called the ADAM optimizer, <code>torch.optim.Adam</code>. Let's observe the below example to make sense of how this optimizer can help us to optimize various variables.</p> <pre><code>import torch\nimport odak  \nimport sys # (1)\n\n\ndef forward(x, m, n): # (2)\n    y = m * x + n\n    return y\n\n\ndef main():\n    m = torch.tensor([100.], requires_grad = True)\n    n = torch.tensor([0.], requires_grad = True) # (3)\n    x_vals = torch.tensor([1., 2., 3., 100.])\n    y_vals = torch.tensor([5., 6., 7., 101.]) # (4)\n    optimizer = torch.optim.Adam([m, n], lr = 5e1) # (5)\n    loss_function = torch.nn.MSELoss() # (6)\n    for step in range(1000):\n        optimizer.zero_grad() # (7)\n        y_estimate = forward(x_vals, m, n) # (8)\n        loss = loss_function(y_estimate, y_vals) # (9)\n        loss.backward(retain_graph = True)\n        optimizer.step() # (10)\n        print('Step: {}, Loss: {}'.format(step, loss.item()))\n    print(m, n)\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n</code></pre> <ol> <li>Required libraries are imported.</li> <li>Let's assume that we are aiming to fit a line to some data (y = mx + n).</li> <li>As we are aiming to fit a line, we have to find a proper m and n for our line (y = mx + n).     Pay attention to the fact that we have to make these variables differentiable by setting <code>requires_grad = True</code>.</li> <li>Here is a sample dataset of X and Y values.</li> <li>We define an Adam optimizer and ask our optimizer to optimize m and n.</li> <li>We need some metric to identify if we are optimizer is optimizing correctly.     Here, we choose a L2 norm (least mean square) as our metric.</li> <li>We clear graph before each iteration.</li> <li>We make our estimation for Y values using the most current m and n values suggested by the optimizer.</li> <li>We compare our estimation with original Y values to help our optimizer update m and n values.</li> <li>Loss and optimizer help us move in the right direction for updating m and n values.</li> </ol>"},{"location":"course/fundamentals/#conclusion","title":"Conclusion","text":"<p>We covered a lot of grounds in terms of coding standards, how to organize a project repository, and how basic things work in odak and Torch. Please ensure you understand the essential information in this section. Please note that we will use this information in this course's following sections and stages.</p> Consider revisiting this chapter <p>Remember that you can always revisit this chapter as you progress with the course and as you need it. This chapter is vital for establishing a means to complete your assignments and could help formulate a suitable base to collaborate and work with my research group in the future or other experts in the field.</p> Did you know that Computer Science misses basic tool education? <p>The classes that Computer Science programs offer around the globe are commonly missing basic tool education. Students often spend a large amount of time to learn tools while they are also learning an advanced topic. This section of our course gave you a quick overview. But you may want to go beyond and learn more about many more basic aspects of Computer Science such as using shell tools, editors, metaprogramming or security. The missing semester of your CS education offers an online resource for you to follow up and learn more. The content of the mentioned course is mostly developed by instructors from Massachusetts Institute of Technology.</p> <p>Reminder</p> <p>We host a Slack group with more than 300 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link. Readers can get in-touch with the wider community using this public group.</p> <ol> <li> <p>Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. NIPS 2017 Workshop Autodiff, 2017.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/geometric_optics/","title":"Modeling light with rays","text":"Narrate section"},{"location":"course/geometric_optics/#modeling-light-with-rays","title":"Modeling light with rays","text":"<p>Modeling light plays a crucial role in describing events based on light and helps designing mechanisms based on light (e.g., Realistic graphics in a video game, display or camera). This chapter introduces the most basic description of light using geometric rays, also known as raytracing. Raytracing has a long history, from ancient times to current Computer Graphics. Here, we will not cover the history of raytracing. Instead, we will focus on how we implement simulations to build \"things\" with raytracing in the future. As we provide algorithmic examples to support our descriptions, readers should be able to simulate light on their computers using the provided descriptions.</p> Are there other good resources on modeling light with rays? <p>When I first started coding Odak, the first paper I read was on raytracing.  Thus, I recommend that paper for any starter:</p> <ul> <li>Spencer, G. H., and M. V. R. K. Murty. \"General ray-tracing procedure.\" JOSA 52, no. 6 (1962): 672-678. <sup>1</sup></li> </ul> <p>Beyond this paper, there are several resources that I can recommend for curious readers:</p> <ul> <li>Shirley, Peter. \"Ray tracing in one weekend.\" Amazon Digital Services LLC 1 (2018): 4. <sup>7</sup></li> <li>Morgan McGuire (2021). The Graphics Codex. Casual Effects. <sup>8</sup></li> </ul>"},{"location":"course/geometric_optics/#ray-description","title":"Ray description","text":"<p> Informative \u00b7  Practical</p> <p>We have to define what \"a ray\" is. A ray has a starting point in Euclidean space (\\(x_0, y_0, z_0 \\in \\mathbb{R}\\)). We also have to define direction cosines to provide the directions for rays. Direction cosines are three angles of a ray between the XYZ axis and that ray (\\(\\theta_x, \\theta_y, \\theta_z \\in \\mathbb{R}\\)). To calculate direction cosines, we must choose a point on that ray as \\(x_1, y_1,\\) and \\(z_1\\) and we calculate its distance to the starting point of \\(x_0, y_0\\) and \\(z_0\\):</p> \\[ x_{distance} = x_1 - x_0, \\\\ y_{distance} = y_1 - y_0, \\\\ z_{distance} = z_1 - z_0. \\] <p>Then, we can also calculate the Euclidian distance between starting point and the point chosen:</p> \\[ s = \\sqrt{x_{distance}^2 + y_{distance}^2 + z_{distance}^2}. \\] <p>Thus, we describe each direction cosines as:</p> \\[ cos(\\theta_x) = \\frac{x_{distance}}{s}, \\\\ cos(\\theta_y) = \\frac{y_{distance}}{s}, \\\\ cos(\\theta_z) = \\frac{z_{distance}}{s}. \\] <p>Now that we know how to define a ray with a starting point, \\(x_0, y_0, z_0\\) and a direction cosine, \\(cos(\\theta_x), cos(\\theta_y), cos(\\theta_z)\\), let us carefully analyze the parameters, returns, and source code of the provided two following functions in <code>odak</code> dedicated to creating a ray or multiple rays.</p> <code>odak.learn.raytracing.create_ray</code> <code>odak.learn.raytracing.create_ray_from_two_points</code> <p>Definition to create a ray.</p> <p>Parameters:</p> <ul> <li> <code>xyz</code>           \u2013            <pre><code>       List that contains X,Y and Z start locations of a ray.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> <li> <code>abg</code>           \u2013            <pre><code>       List that contains angles in degrees with respect to the X,Y and Z axes.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> <li> <code>direction</code>           \u2013            <pre><code>       If set to True, cosines of `abg` is not calculated.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray. Size will be either [1 x 3] or [m x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray(xyz, abg, direction = False):\n    \"\"\"\n    Definition to create a ray.\n\n    Parameters\n    ----------\n    xyz          : torch.tensor\n                   List that contains X,Y and Z start locations of a ray.\n                   Size could be [1 x 3], [3], [m x 3].\n    abg          : torch.tensor\n                   List that contains angles in degrees with respect to the X,Y and Z axes.\n                   Size could be [1 x 3], [3], [m x 3].\n    direction    : bool\n                   If set to True, cosines of `abg` is not calculated.\n\n    Returns\n    ----------\n    ray          : torch.tensor\n                   Array that contains starting points and cosines of a created ray.\n                   Size will be either [1 x 3] or [m x 3].\n    \"\"\"\n    points = xyz\n    angles = abg\n    if len(xyz) == 1:\n        points = xyz.unsqueeze(0)\n    if len(abg) == 1:\n        angles = abg.unsqueeze(0)\n    ray = torch.zeros(points.shape[0], 2, 3, device = points.device)\n    ray[:, 0] = points\n    if direction:\n        ray[:, 1] = abg\n    else:\n        ray[:, 1] = torch.cos(torch.deg2rad(abg))\n    return ray\n</code></pre> <p>Definition to create a ray from two given points. Note that both inputs must match in shape.</p> <p>Parameters:</p> <ul> <li> <code>x0y0z0</code>           \u2013            <pre><code>       List that contains X,Y and Z start locations of a ray.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> <li> <code>x1y1z1</code>           \u2013            <pre><code>       List that contains X,Y and Z ending locations of a ray or batch of rays.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray(s).</p> </li> </ul> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray_from_two_points(x0y0z0, x1y1z1):\n    \"\"\"\n    Definition to create a ray from two given points. Note that both inputs must match in shape.\n\n    Parameters\n    ----------\n    x0y0z0       : torch.tensor\n                   List that contains X,Y and Z start locations of a ray.\n                   Size could be [1 x 3], [3], [m x 3].\n    x1y1z1       : torch.tensor\n                   List that contains X,Y and Z ending locations of a ray or batch of rays.\n                   Size could be [1 x 3], [3], [m x 3].\n\n    Returns\n    ----------\n    ray          : torch.tensor\n                   Array that contains starting points and cosines of a created ray(s).\n    \"\"\"\n    if len(x0y0z0.shape) == 1:\n        x0y0z0 = x0y0z0.unsqueeze(0)\n    if len(x1y1z1.shape) == 1:\n        x1y1z1 = x1y1z1.unsqueeze(0)\n    xdiff = x1y1z1[:, 0] - x0y0z0[:, 0]\n    ydiff = x1y1z1[:, 1] - x0y0z0[:, 1]\n    zdiff = x1y1z1[:, 2] - x0y0z0[:, 2]\n    s = (xdiff ** 2 + ydiff ** 2 + zdiff ** 2) ** 0.5\n    s[s == 0] = float('nan')\n    cosines = torch.zeros_like(x0y0z0 * x1y1z1)\n    cosines[:, 0] = xdiff / s\n    cosines[:, 1] = ydiff / s\n    cosines[:, 2] = zdiff / s\n    ray = torch.zeros(xdiff.shape[0], 2, 3, device = x0y0z0.device)\n    ray[:, 0] = x0y0z0\n    ray[:, 1] = cosines\n    return ray\n</code></pre> <p>In the future, we must find out where a ray lands after a certain amount of propagation distance for various purposes, which we will describe in this chapter. For that purpose, let us also create a utility function that propagates a  ray to some distance, \\(d\\), using \\(x_0, y_0, z_0\\) and \\(cos(\\theta_x), cos(\\theta_y), cos(\\theta_z)\\):</p> \\[ x_{new} = x_0 + cos(\\theta_x) d,\\\\ y_{new} = y_0 + cos(\\theta_y) d,\\\\ z_{new} = z_0 + cos(\\theta_z) d. \\] <p>Let us also check the function provided below to understand its source code, parameters, and returns. This function will serve as a utility function to propagate a ray or a batch of rays in our future simulations.</p> <code>odak.learn.raytracing.propagate_ray</code> <p>Definition to propagate a ray at a certain given distance.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>     A ray with a size of [2 x 3], [1 x 2 x 3] or a batch of rays with [m x 2 x 3].\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>     Distance with a size of [1], [1, m] or distances with a size of [m], [1, m].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_ray</code> (              <code>tensor</code> )          \u2013            <p>Propagated ray with a size of [1 x 2 x 3] or batch of rays with [m x 2 x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def propagate_ray(ray, distance):\n    \"\"\"\n    Definition to propagate a ray at a certain given distance.\n\n    Parameters\n    ----------\n    ray        : torch.tensor\n                 A ray with a size of [2 x 3], [1 x 2 x 3] or a batch of rays with [m x 2 x 3].\n    distance   : torch.tensor\n                 Distance with a size of [1], [1, m] or distances with a size of [m], [1, m].\n\n    Returns\n    ----------\n    new_ray    : torch.tensor\n                 Propagated ray with a size of [1 x 2 x 3] or batch of rays with [m x 2 x 3].\n    \"\"\"\n    if len(ray.shape) == 2:\n        ray = ray.unsqueeze(0)\n    if len(distance.shape) == 2:\n        distance = distance.squeeze(-1)\n    new_ray = torch.zeros_like(ray)\n    new_ray[:, 0, 0] = distance * ray[:, 1, 0] + ray[:, 0, 0]\n    new_ray[:, 0, 1] = distance * ray[:, 1, 1] + ray[:, 0, 1]\n    new_ray[:, 0, 2] = distance * ray[:, 1, 2] + ray[:, 0, 2]\n    return new_ray\n</code></pre> <p>It is now time for us to put what we have learned so far into an actual code. We can create many rays using the two functions, <code>odak.learn.raytracing.create_ray_from_two_points</code> and <code>odak.learn.raytracing.create_ray</code>. However, to do so, we need to have many points in both cases. For that purpose, let's carefully review this utility function provided below. This utility function can generate grid samples from a plane with some tilt, and we can also define the center of our samples to position points anywhere in Euclidean space.</p> <code>odak.learn.tools.grid_sample</code> <p>Definition to generate samples over a surface.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Physical size of the surface.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>tensor</code> )          \u2013            <p>Samples generated.</p> </li> <li> <code>rotx</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at X axis.</p> </li> <li> <code>roty</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at Y axis.</p> </li> <li> <code>rotz</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at Z axis.</p> </li> </ul> Source code in <code>odak/learn/tools/sample.py</code> <pre><code>def grid_sample(\n                no = [10, 10],\n                size = [100., 100.], \n                center = [0., 0., 0.], \n                angles = [0., 0., 0.]):\n    \"\"\"\n    Definition to generate samples over a surface.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    size        : list\n                  Physical size of the surface.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    -------\n    samples     : torch.tensor\n                  Samples generated.\n    rotx        : torch.tensor\n                  Rotation matrix at X axis.\n    roty        : torch.tensor\n                  Rotation matrix at Y axis.\n    rotz        : torch.tensor\n                  Rotation matrix at Z axis.\n    \"\"\"\n    center = torch.tensor(center)\n    angles = torch.tensor(angles)\n    size = torch.tensor(size)\n    samples = torch.zeros((no[0], no[1], 3))\n    x = torch.linspace(-size[0] / 2., size[0] / 2., no[0])\n    y = torch.linspace(-size[1] / 2., size[1] / 2., no[1])\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    samples[:, :, 0] = X.detach().clone()\n    samples[:, :, 1] = Y.detach().clone()\n    samples = samples.reshape((samples.shape[0] * samples.shape[1], samples.shape[2]))\n    samples, rotx, roty, rotz = rotate_points(samples, angles = angles, offset = center)\n    return samples, rotx, roty, rotz\n</code></pre> <p>The below script provides a sample use case for the functions provided above. I also leave comments near some lines explaining the code in steps.</p> <code>test_learn_ray_create_ray_from_two_points.py</code> <pre><code>import sys\nimport odak\nimport torch # (1)\n\n\ndef test(directory = 'test_output'):\n    odak.tools.check_directory(directory)\n    starting_point = torch.tensor([[5., 5., 0.]]) # (2)\n    end_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                       no = [2, 2], \n                                                       size = [20., 20.], \n                                                       center = [0., 0., 10.]\n                                                      ) # (3)\n    rays_from_points = odak.learn.raytracing.create_ray_from_two_points(\n                                                                        starting_point,\n                                                                        end_points\n                                                                       ) # (4)\n\n\n    starting_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                            no = [3, 3], \n                                                            size = [100., 100.], \n                                                            center = [0., 0., 10.],\n                                                           )\n    angles = torch.randn_like(starting_points) * 180. # (5)\n    rays_from_angles = odak.learn.raytracing.create_ray(\n                                                        starting_points,\n                                                        angles\n                                                       ) # (6)\n\n\n    distances = torch.ones(rays_from_points.shape[0]) * 12.5\n    propagated_rays = odak.learn.raytracing.propagate_ray(\n                                                          rays_from_points,\n                                                          distances\n                                                         ) # (7)\n\n\n\n\n    visualize = False # (8)\n    if visualize:\n        ray_diagram = odak.visualize.plotly.rayshow(line_width = 3., marker_size = 3.)\n        ray_diagram.add_point(starting_point, color = 'red')\n        ray_diagram.add_point(end_points[0], color = 'blue')\n        ray_diagram.add_line(starting_point, end_points[0], color = 'green')\n        x_axis = starting_point.clone()\n        x_axis[0, 0] = end_points[0, 0]\n        ray_diagram.add_point(x_axis, color = 'black')\n        ray_diagram.add_line(starting_point, x_axis, color = 'black', dash = 'dash')\n        y_axis = starting_point.clone()\n        y_axis[0, 1] = end_points[0, 1]\n        ray_diagram.add_point(y_axis, color = 'black')\n        ray_diagram.add_line(starting_point, y_axis, color = 'black', dash = 'dash')\n        z_axis = starting_point.clone()\n        z_axis[0, 2] = end_points[0, 2]\n        ray_diagram.add_point(z_axis, color = 'black')\n        ray_diagram.add_line(starting_point, z_axis, color = 'black', dash = 'dash')\n        html = ray_diagram.save_offline()\n        markdown_file = open('{}/ray.txt'.format(directory), 'w')\n        markdown_file.write(html)\n        markdown_file.close()\n    assert True == True\n\n\nif __name__ == '__main__':\n    sys.exit(test())\n</code></pre> <ol> <li>Required libraries are imported.</li> <li>Defining a starting point, in order X, Y and Z locations.    Size of starting point could be s1] or [1, 1].</li> <li>Defining some end points on a plane in grid fashion.</li> <li><code>odak.learn.raytracing.create_ray_from_two_points</code> is verified with an example! Let's move on to <code>odak.learn.raytracing.create_ray</code>.</li> <li>Creating starting points with <code>odak.learn.tools.grid_sample</code> and defining some angles as the direction using <code>torch.randn</code>.    Note that the angles are in degrees.</li> <li><code>odak.learn.raytracing.create_ray</code> is verified with an example!</li> <li><code>odak.learn.raytracing.propagate_a_ray</code> is verified with an example!</li> <li>Set it to <code>True</code> to enable visualization.</li> </ol> <p>The above code also has parts that are disabled (see <code>visualize</code> variable). We disabled these lines intentionally to avoid running it at every run. Let me talk about these disabled functions as well. Odak offers a tidy approach to simple visualizations through packages called Plotly and <code>kaleido</code>. To make these lines work by setting <code>visualize = True</code>, you must first install <code>plotly</code> in your work environment. This installation is as simple as <code>pip3 install plotly kaleido</code> in a Linux system. As you install these packages and enable these lines, the code will produce a visualization similar to the one below. Note that this is an interactive visualization where you can interact with your mouse clicks to rotate, shift, and zoom. In this visualization, we visualize a single ray (green line) starting from our defined starting point (red dot) and ending at one of the <code>end_points</code> (blue dot). We also highlight three axes with black lines to provide a reference frame. Although <code>odak.visualize.plotly</code> offers us methods to visualize rays quickly for debugging, it is highly suggested to stick to a low number of lines when using it (e.g., say not exceeding 100 rays in total). The proper way to draw many rays lies in modern path-tracing renderers such as Blender.</p> How can I learn more about more sophisticated renderers like Blender? <p>Blender is a widely used open-source renderer that comes with sophisticated features. It is user interface could be challenging for newcomers. A blog post published by SIGGRAPH Research Career Development Committee offers a neat entry-level post titled <code>Rendering a paper figure with Blender</code> written by Silvia Sell\u00e1n.</p> <p>In addition to Blender, there are various renderers you may be happy to know about if you are curious about Computer Graphics. Mitsuba 3 is another sophisticated rendering system based on a SIGGRAPH paper titled <code>Dr.Jit: A Just-In-Time Compiler for Differentiable Rendering</code> <sup>4</sup> from Wenzel Jakob.</p> <p>If you know any other, please share it with the class so that they also learn more about other renderers.</p> Challenge: Blender meets Odak <p>In light of the given information, we challenge readers to create a new submodule for Odak. Note that Odak has <code>odak.visualize.blender</code> submodule. However, at the time of this writing, this submodule works as a server that sends commands to a program that has to be manually triggered inside Blender. Odak seeks an upgrade to this submodule, where users can draw rays, meshes, or parametric surfaces easily in Blender with commands from Odak. This newly upgraded submodule should require no manual processes. To add these to <code>odak,</code> you can rely on the <code>pull request</code> feature on GitHub. You can also create a new <code>engineering note</code> for your new submodule in <code>docs/notes/odak_meets_blender.md</code>.</p>"},{"location":"course/geometric_optics/#intersecting-rays-with-surfaces","title":"Intersecting rays with surfaces","text":"<p> Informative \u00b7  Practical</p> <p>Rays we have described so far help us explore light and matter interactions. Often in simulations, these rays interact with surfaces. In a simulation environment for optical design,  equations often describe surfaces continuously. These surface equations typically contain a number of parameters for defining surfaces. For example, let us consider a sphere, which follows a standard equation as follows,</p> \\[ r^2 = (x - x_0)^2 + (y - y_0)^2 + (z - z_0)^2, \\] <p>Where \\(r\\) represents the diameter of that sphere, \\(x_0, y_0, z_0\\) defines the center location of that sphere, and \\(x, y, z\\) are points on the surface of a sphere. When testing if a point is on a sphere, we use the above equation by inserting the point to be tested as \\(x, y, z\\) into that equation. In other words, to find a ray and sphere intersection, we must identify a distance that propagates our rays a certain amount and lends on a point on that sphere, and we can use the above sphere equation for identifying the intersection point of that rays. As long the surface equation is well degined, the same strategy can be used for any surfaces. In addition, if needed for future purposes (e.g., reflecting or refracting light off the surface of that sphere), we can also calculate the surface normal of that sphere by drawing a line by defining a ray starting from the center of that sphere and propagating towards the intersection point. Let us examine, how we can identify intersection points for a set of given rays and a sphere by examining the below function.</p> <code>odak.learn.raytracing.intersect_w_sphere</code> <p>Definition to find the intersection between ray(s) and sphere(s).</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>              Input ray(s).\n              Expected size is [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> <li> <code>sphere</code>           \u2013            <pre><code>              Input sphere.\n              Expected size is [1 x 4].\n</code></pre> </li> <li> <code>learning_rate</code>           \u2013            <pre><code>              Learning rate used in the optimizer for finding the propagation distances of the rays.\n</code></pre> </li> <li> <code>number_of_steps</code>           \u2013            <pre><code>              Number of steps used in the optimizer.\n</code></pre> </li> <li> <code>error_threshold</code>           \u2013            <pre><code>              The error threshold that will help deciding intersection or no intersection.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>intersecting_ray</code> (              <code>tensor</code> )          \u2013            <p>Ray(s) that intersecting with the given sphere. Expected size is [n x 2 x 3], where n could be any real number.</p> </li> <li> <code>intersecting_normal</code> (              <code>tensor</code> )          \u2013            <p>Normal(s) for the ray(s) intersecting with the given sphere Expected size is [n x 2 x 3], where n could be any real number.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_sphere(ray, sphere, learning_rate = 2e-1, number_of_steps = 5000, error_threshold = 1e-2):\n    \"\"\"\n    Definition to find the intersection between ray(s) and sphere(s).\n\n    Parameters\n    ----------\n    ray                 : torch.tensor\n                          Input ray(s).\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3].\n    sphere              : torch.tensor\n                          Input sphere.\n                          Expected size is [1 x 4].\n    learning_rate       : float\n                          Learning rate used in the optimizer for finding the propagation distances of the rays.\n    number_of_steps     : int\n                          Number of steps used in the optimizer.\n    error_threshold     : float\n                          The error threshold that will help deciding intersection or no intersection.\n\n    Returns\n    -------\n    intersecting_ray    : torch.tensor\n                          Ray(s) that intersecting with the given sphere.\n                          Expected size is [n x 2 x 3], where n could be any real number.\n    intersecting_normal : torch.tensor\n                          Normal(s) for the ray(s) intersecting with the given sphere\n                          Expected size is [n x 2 x 3], where n could be any real number.\n\n    \"\"\"\n    if len(ray.shape) == 2:\n        ray = ray.unsqueeze(0)\n    if len(sphere.shape) == 1:\n        sphere = sphere.unsqueeze(0)\n    distance = torch.zeros(ray.shape[0], device = ray.device, requires_grad = True)\n    loss_l2 = torch.nn.MSELoss(reduction = 'sum')\n    optimizer = torch.optim.AdamW([distance], lr = learning_rate)    \n    t = tqdm(range(number_of_steps), leave = False, dynamic_ncols = True)\n    for step in t:\n        optimizer.zero_grad()\n        propagated_ray = propagate_ray(ray, distance)\n        test = torch.abs((propagated_ray[:, 0, 0] - sphere[:, 0]) ** 2 + (propagated_ray[:, 0, 1] - sphere[:, 1]) ** 2 + (propagated_ray[:, 0, 2] - sphere[:, 2]) ** 2 - sphere[:, 3] ** 2)\n        loss = loss_l2(\n                       test,\n                       torch.zeros_like(test)\n                      )\n        loss.backward(retain_graph = True)\n        optimizer.step()\n        t.set_description('Sphere intersection loss: {}'.format(loss.item()))\n    check = test &lt; error_threshold\n    intersecting_ray = propagate_ray(ray[check == True], distance[check == True])\n    intersecting_normal = create_ray_from_two_points(\n                                                     sphere[:, 0:3],\n                                                     intersecting_ray[:, 0]\n                                                    )\n    return intersecting_ray, intersecting_normal, distance, check\n</code></pre> <p>The <code>odak.learn.raytracing.intersect_w_sphere</code> function uses an optimizer to identify intersection points for each ray. Instead, a function could have accomplished the task with a closed-form solution without iterating over the intersection test, which could have been much faster than the current function. If you are curious about how to fix the highlighted issue, you may want to see the challenge provided below.</p> <p>Let us examine how we can use the provided sphere intersection function with an example provided at the end of this subsection.</p> <code>test_learn_ray_intersect_w_a_sphere.py</code> <pre><code>import sys\nimport odak\nimport torch\n\ndef test(output_directory = 'test_output'):\n    odak.tools.check_directory(output_directory)\n    starting_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                            no = [5, 5],\n                                                            size = [3., 3.],\n                                                            center = [0., 0., 0.]\n                                                           )\n    end_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                       no = [5, 5],\n                                                       size = [0.1, 0.1],\n                                                       center = [0., 0., 5.]\n                                                      )\n    rays = odak.learn.raytracing.create_ray_from_two_points(\n                                                            starting_points,\n                                                            end_points\n                                                           )\n    center = torch.tensor([[0., 0., 5.]])\n    radius = torch.tensor([[3.]])\n    sphere = odak.learn.raytracing.define_sphere(\n                                                 center = center,\n                                                 radius = radius\n                                                ) # (1)\n    intersecting_rays, intersecting_normals, _, check = odak.learn.raytracing.intersect_w_sphere(rays, sphere)\n\n\n    visualize = False # (2)\n    if visualize:\n        ray_diagram = odak.visualize.plotly.rayshow(line_width = 3., marker_size = 3.)\n        ray_diagram.add_point(rays[:, 0], color = 'blue')\n        ray_diagram.add_line(rays[:, 0][check == True], intersecting_rays[:, 0], color = 'blue')\n        ray_diagram.add_sphere(sphere, color = 'orange')\n        ray_diagram.add_point(intersecting_normals[:, 0], color = 'green')\n        html = ray_diagram.save_offline()\n        markdown_file = open('{}/ray.txt'.format(output_directory), 'w')\n        markdown_file.write(html)\n        markdown_file.close()\n    assert True == True\n\n\nif __name__ == '__main__':\n    sys.exit(test())\n</code></pre> <ol> <li>Here we provide an example use case for <code>odak.learn.raytracing.intersect_w_sphere</code> by providing a sphere and a batch of sample rays.</li> <li>Uncomment for running visualization.</li> </ol> <p> </p> Screenshow showing a sphere and ray intersections generated by \"test_learn_ray_intersect_w_a_sphere.py\" script. <p>This section shows us how to operate with known geometric shapes, precisely spheres. However, not every shape could be defined using parametric modeling (e.g., nonlinearities such as discontinuities on a surface). We will look into another method in the next section, an approach used by folks working in Computer Graphics.</p> Challenge: Raytracing arbitrary surfaces <p>In light of the given information, we challenge readers to create a new function inside <code>odak.learn.raytracing</code> submodule that replaces the current <code>intersect_w_sphere</code> function. In addition, the current unit test <code>test/test_learn_ray_intersect_w_a_sphere.py</code> has to adopt this new function. <code>odak.learn.raytracing</code> submodule also needs new functions for supporting arbitrary surfaces (parametric). New unit tests are needed to improve the submodule accordingly. To add these to <code>odak,</code> you can rely on the <code>pull request</code> feature on GitHub. You can also create a new <code>engineering note</code> for arbitrary surfaces in <code>docs/notes/raytracing_arbitrary_surfaces.md</code>.</p>"},{"location":"course/geometric_optics/#intersecting-rays-with-meshes","title":"Intersecting rays with meshes","text":"<p> Informative \u00b7  Practical</p> <p>Parametric surfaces provide ease in defining shapes and geometries in various fields, including Optics and Computer Graphics. However, not every object in a given scene could easily be described using parametric surfaces. In many cases, including modern Computer Graphics, triangles formulate the smallest particle of an object or a shape. These triangles altogether form meshes that define objects and shapes. For this purpose, we will review source codes, parameters, and returns of three utility functions here. We will first review <code>odak.learn.raytracing.intersect_w_surface</code> to understand how one can calculate the intersection of a ray with a given plane. Later, we review <code>odak.learn.raytracing.is_it_on_triangle</code> function, which checks if an intersection point on a given surface is inside a triangle on that surface. Finally, we will review <code>odak.learn.raytracing.intersect_w_triangle</code> function. This last function combines both reviewed functions into a single function to identify the intersection between rays and a triangle.</p> <code>odak.learn.raytracing.intersect_w_surface</code> <code>odak.learn.raytracing.is_it_on_triangle</code> <code>odak.learn.raytracing.intersect_w_triangle</code> <p>Definition to find intersection point inbetween a surface and a ray. For more see: http://geomalgorithms.com/a06-_intersect-2.html</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>points</code>           \u2013            <pre><code>       Set of points in X,Y and Z to define a planar surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between starting point of a ray with it's intersection with a planar surface.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_surface(ray, points):\n    \"\"\"\n    Definition to find intersection point inbetween a surface and a ray. For more see: http://geomalgorithms.com/a06-_intersect-2.html\n\n    Parameters\n    ----------\n    ray          : torch.tensor\n                   A vector/ray.\n    points       : torch.tensor\n                   Set of points in X,Y and Z to define a planar surface.\n\n    Returns\n    ----------\n    normal       : torch.tensor\n                   Surface normal at the point of intersection.\n    distance     : float\n                   Distance in between starting point of a ray with it's intersection with a planar surface.\n    \"\"\"\n    normal = get_triangle_normal(points)\n    if len(ray.shape) == 2:\n        ray = ray.unsqueeze(0)\n    if len(points.shape) == 2:\n        points = points.unsqueeze(0)\n    if len(normal.shape) == 2:\n        normal = normal.unsqueeze(0)\n    f = normal[:, 0] - ray[:, 0]\n    distance = (torch.mm(normal[:, 1], f.T) / torch.mm(normal[:, 1], ray[:, 1].T)).T\n    new_normal = torch.zeros_like(ray)\n    new_normal[:, 0] = ray[:, 0] + distance * ray[:, 1]\n    new_normal[:, 1] = normal[:, 1]\n    new_normal = torch.nan_to_num(\n                                  new_normal,\n                                  nan = float('nan'),\n                                  posinf = float('nan'),\n                                  neginf = float('nan')\n                                 )\n    distance = torch.nan_to_num(\n                                distance,\n                                nan = float('nan'),\n                                posinf = float('nan'),\n                                neginf = float('nan')\n                               )\n    return new_normal, distance\n</code></pre> <p>Definition to check if a given point is inside a triangle.  If the given point is inside a defined triangle, this definition returns True. For more details, visit: https://blackpawn.com/texts/pointinpoly/.</p> <p>Parameters:</p> <ul> <li> <code>point_to_check</code>           \u2013            <pre><code>          Point(s) to check.\n          Expected size is [3], [1 x 3] or [m x 3].\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>          Triangle described with three points.\n          Expected size is [3 x 3], [1 x 3 x 3] or [m x 3 x3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Is it on a triangle? Returns NaN if condition not satisfied. Expected size is [1] or [m] depending on the input.</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def is_it_on_triangle(point_to_check, triangle):\n    \"\"\"\n    Definition to check if a given point is inside a triangle. \n    If the given point is inside a defined triangle, this definition returns True.\n    For more details, visit: [https://blackpawn.com/texts/pointinpoly/](https://blackpawn.com/texts/pointinpoly/).\n\n    Parameters\n    ----------\n    point_to_check  : torch.tensor\n                      Point(s) to check.\n                      Expected size is [3], [1 x 3] or [m x 3].\n    triangle        : torch.tensor\n                      Triangle described with three points.\n                      Expected size is [3 x 3], [1 x 3 x 3] or [m x 3 x3].\n\n    Returns\n    -------\n    result          : torch.tensor\n                      Is it on a triangle? Returns NaN if condition not satisfied.\n                      Expected size is [1] or [m] depending on the input.\n    \"\"\"\n    if len(point_to_check.shape) == 1:\n        point_to_check = point_to_check.unsqueeze(0)\n    if len(triangle.shape) == 2:\n        triangle = triangle.unsqueeze(0)\n    v0 = triangle[:, 2] - triangle[:, 0]\n    v1 = triangle[:, 1] - triangle[:, 0]\n    v2 = point_to_check - triangle[:, 0]\n    if len(v0.shape) == 1:\n        v0 = v0.unsqueeze(0)\n    if len(v1.shape) == 1:\n        v1 = v1.unsqueeze(0)\n    if len(v2.shape) == 1:\n        v2 = v2.unsqueeze(0)\n    dot00 = torch.mm(v0, v0.T)\n    dot01 = torch.mm(v0, v1.T)\n    dot02 = torch.mm(v0, v2.T) \n    dot11 = torch.mm(v1, v1.T)\n    dot12 = torch.mm(v1, v2.T)\n    invDenom = 1. / (dot00 * dot11 - dot01 * dot01)\n    u = (dot11 * dot02 - dot01 * dot12) * invDenom\n    v = (dot00 * dot12 - dot01 * dot02) * invDenom\n    result = (u &gt;= 0.) &amp; (v &gt;= 0.) &amp; ((u + v) &lt; 1)\n    return result\n</code></pre> <p>Definition to find intersection point of a ray with a triangle. </p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>              A ray [1 x 2 x 3] or a batch of ray [m x 2 x 3].\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>              Set of points in X,Y and Z to define a single triangle [1 x 3 x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection with the surface of triangle. This could also involve surface normals that are not on the triangle. Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between a starting point of a ray and the intersection point with a given triangle. Expected size is [1 x 1] or [m x 1] depending on the input.</p> </li> <li> <code>intersecting_ray</code> (              <code>tensor</code> )          \u2013            <p>Rays that intersect with the triangle plane and on the triangle. Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.</p> </li> <li> <code>intersecting_normal</code> (              <code>tensor</code> )          \u2013            <p>Normals that intersect with the triangle plane and on the triangle. Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.</p> </li> <li> <code>check</code> (              <code>tensor</code> )          \u2013            <p>A list that provides a bool as True or False for each ray used as input. A test to see is a ray could be on the given triangle. Expected size is [1] or [m].</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_triangle(ray, triangle):\n    \"\"\"\n    Definition to find intersection point of a ray with a triangle. \n\n    Parameters\n    ----------\n    ray                 : torch.tensor\n                          A ray [1 x 2 x 3] or a batch of ray [m x 2 x 3].\n    triangle            : torch.tensor\n                          Set of points in X,Y and Z to define a single triangle [1 x 3 x 3].\n\n    Returns\n    ----------\n    normal              : torch.tensor\n                          Surface normal at the point of intersection with the surface of triangle.\n                          This could also involve surface normals that are not on the triangle.\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.\n    distance            : float\n                          Distance in between a starting point of a ray and the intersection point with a given triangle.\n                          Expected size is [1 x 1] or [m x 1] depending on the input.\n    intersecting_ray    : torch.tensor\n                          Rays that intersect with the triangle plane and on the triangle.\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.\n    intersecting_normal : torch.tensor\n                          Normals that intersect with the triangle plane and on the triangle.\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.\n    check               : torch.tensor\n                          A list that provides a bool as True or False for each ray used as input.\n                          A test to see is a ray could be on the given triangle.\n                          Expected size is [1] or [m].\n    \"\"\"\n    if len(triangle.shape) == 2:\n       triangle = triangle.unsqueeze(0)\n    if len(ray.shape) == 2:\n       ray = ray.unsqueeze(0)\n    normal, distance = intersect_w_surface(ray, triangle)\n    check = is_it_on_triangle(normal[:, 0], triangle)\n    intersecting_ray = ray.unsqueeze(0)\n    intersecting_ray = intersecting_ray.repeat(triangle.shape[0], 1, 1, 1)\n    intersecting_ray = intersecting_ray[check == True]\n    intersecting_normal = normal.unsqueeze(0)\n    intersecting_normal = intersecting_normal.repeat(triangle.shape[0], 1, 1, 1)\n    intersecting_normal = intersecting_normal[check ==  True]\n    return normal, distance, intersecting_ray, intersecting_normal, check\n</code></pre> <p>Using the provided utility functions above, let us build an example below that helps us find intersections between a triangle and a batch of rays.</p> <code>test_learn_ray_intersect_w_a_triangle.py</code> <pre><code>import sys\nimport odak\nimport torch\n\n\ndef test(output_directory = 'test_output'):\n    odak.tools.check_directory(output_directory)\n    starting_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                            no = [5, 5],\n                                                            size = [10., 10.],\n                                                            center = [0., 0., 0.]\n                                                           )\n    end_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                       no = [5, 5],\n                                                       size = [6., 6.],\n                                                       center = [0., 0., 10.]\n                                                      )\n    rays = odak.learn.raytracing.create_ray_from_two_points(\n                                                            starting_points,\n                                                            end_points\n                                                           )\n    triangle = torch.tensor([[\n                              [-5., -5., 10.],\n                              [ 5., -5., 10.],\n                              [ 0.,  5., 10.]\n                            ]])\n    normals, distance, _, _, check = odak.learn.raytracing.intersect_w_triangle(\n                                                                                rays,\n                                                                                triangle\n                                                                               ) # (2)\n\n\n\n    visualize = False # (1)\n    if visualize:\n        ray_diagram = odak.visualize.plotly.rayshow(line_width = 3., marker_size = 3.) # (1)\n        ray_diagram.add_triangle(triangle, color = 'orange')\n        ray_diagram.add_point(rays[:, 0], color = 'blue')\n        ray_diagram.add_line(rays[:, 0], normals[:, 0], color = 'blue')\n        colors = []\n        for color_id in range(check.shape[1]):\n            if check[0, color_id] == True:\n                colors.append('green')\n            elif check[0, color_id] == False:\n                colors.append('red')\n        ray_diagram.add_point(normals[:, 0], color = colors)\n        html = ray_diagram.save_offline()\n        markdown_file = open('{}/ray.txt'.format(output_directory), 'w')\n        markdown_file.write(html)\n        markdown_file.close()\n    assert True == True\n\n\nif __name__ == '__main__':\n    sys.exit(test())\n</code></pre> <ol> <li>Uncomment for running visualization.</li> <li>Returning intersection normals as new rays, distances from starting point of input rays and a check which returns True if intersection points are inside the triangle.</li> </ol> Why should we be interested in ray and triangle intersections? <p>Modern Computer Graphics uses various representations for defining three-dimensional objects and scenes. These representations include:</p> <ul> <li>Point Clouds: a series of XYZ coordinates from the surface of a three-dimensional object,</li> <li>Meshes: a soup of triangles that represents a surface of a three-dimensional object,</li> <li>Signed Distance Functions: a function informing about the distance between an XYZ point and a surface of a three-dimensional object,</li> <li>Neural Radiance Fields: A machine learning approach to learning ray patterns from various perspectives.</li> </ul> <p>Historically, meshes have been mainly used to represent three-dimensional objects. Thus, intersecting rays and triangles are important for most Computer Graphics.</p> Challenge: Many triangles! <p>The example provided above deals with a ray and a batch of rays. However, objects represented with triangles are typically described with many triangles but not one. Note that <code>odak.learn.raytracing.intersect_w_triangle</code> deal with each triangle one by one, and may lead to slow execution times as the function has to visit each triangle one by one. Given the information, we challenge readers to create a new function inside <code>odak.learn.raytracing</code> submodule named <code>intersect_w_mesh</code>. This new function has to be able to work with multiple triangles (meshes) and has to be aware of \"occlusions\" (e.g., a triangle blocking another triangle). In addition, a new unit test, <code>test/test_learn_ray_intersect_w_mesh.py</code>, has to adopt this new function. To add these to <code>odak,</code> you can rely on the <code>pull request</code> feature on GitHub. You can also create a new <code>engineering note</code> for arbitrary surfaces in <code>docs/notes/raytracing_meshes.md</code>.</p>"},{"location":"course/geometric_optics/#refracting-and-reflecting-rays","title":"Refracting and reflecting rays","text":"<p> Informative \u00b7  Practical </p> <p>In the previous subsections, we reviewed ray intersection with various surface representations, including parametric (e.g., spheres) and non-parametric (e.g., meshes). Please remember that raytracing is the most simplistic modeling of light. Thus, often raytracing does not account for any wave or quantum-related nature of light. To our knowledge, light refracts, reflects, or diffracts when light interfaces with a surface or, in other words, a changing medium (e.g., light traveling from air to glass). In that case, our next step should be identifying a methodology to help us model these events using rays. We compiled two utility functions that could help us to model a refraction or a reflection. These functions are named <code>odak.learn.raytracing.refract</code> <sup>1</sup> and <code>odak.learn.raytracing.reflect</code> <sup>1</sup>. This first one, <code>odak.learn.raytracing.refract</code> follows Snell's law of refraction, while <code>odak.learn.raytracing.reflect</code> follows a perfect reflection case. We will not go into details of this theory as its simplest form in the way we discuss it could now be considered common knowledge. However, for curious readers, the work by Bell et al. <sup>2</sup> provides a generalized solution for the laws of refraction and reflection. Let us carefully examine these two utility functions to understand their internal workings.</p> <code>odak.learn.raytracing.refract</code> <code>odak.learn.raytracing.reflect</code> <p>Definition to refract an incoming ray. Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.</p> <p>Parameters:</p> <ul> <li> <code>vector</code>           \u2013            <pre><code>         Incoming ray.\n         Expected size is [2, 3], [1, 2, 3] or [m, 2, 3].\n</code></pre> </li> <li> <code>normvector</code>           \u2013            <pre><code>         Normal vector.\n         Expected size is [2, 3], [1, 2, 3] or [m, 2, 3]].\n</code></pre> </li> <li> <code>n1</code>           \u2013            <pre><code>         Refractive index of the incoming medium.\n</code></pre> </li> <li> <code>n2</code>           \u2013            <pre><code>         Refractive index of the outgoing medium.\n</code></pre> </li> <li> <code>error</code>           \u2013            <pre><code>         Desired error.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Refracted ray. Expected size is [1, 2, 3]</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def refract(vector, normvector, n1, n2, error = 0.01):\n    \"\"\"\n    Definition to refract an incoming ray.\n    Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.\n\n\n    Parameters\n    ----------\n    vector         : torch.tensor\n                     Incoming ray.\n                     Expected size is [2, 3], [1, 2, 3] or [m, 2, 3].\n    normvector     : torch.tensor\n                     Normal vector.\n                     Expected size is [2, 3], [1, 2, 3] or [m, 2, 3]].\n    n1             : float\n                     Refractive index of the incoming medium.\n    n2             : float\n                     Refractive index of the outgoing medium.\n    error          : float \n                     Desired error.\n\n    Returns\n    -------\n    output         : torch.tensor\n                     Refracted ray.\n                     Expected size is [1, 2, 3]\n    \"\"\"\n    if len(vector.shape) == 2:\n        vector = vector.unsqueeze(0)\n    if len(normvector.shape) == 2:\n        normvector = normvector.unsqueeze(0)\n    mu    = n1 / n2\n    div   = normvector[:, 1, 0] ** 2  + normvector[:, 1, 1] ** 2 + normvector[:, 1, 2] ** 2\n    a     = mu * (vector[:, 1, 0] * normvector[:, 1, 0] + vector[:, 1, 1] * normvector[:, 1, 1] + vector[:, 1, 2] * normvector[:, 1, 2]) / div\n    b     = (mu ** 2 - 1) / div\n    to    = - b * 0.5 / a\n    num   = 0\n    eps   = torch.ones(vector.shape[0], device = vector.device) * error * 2\n    while len(eps[eps &gt; error]) &gt; 0:\n       num   += 1\n       oldto  = to\n       v      = to ** 2 + 2 * a * to + b\n       deltav = 2 * (to + a)\n       to     = to - v / deltav\n       eps    = abs(oldto - to)\n    output = torch.zeros_like(vector)\n    output[:, 0, 0] = normvector[:, 0, 0]\n    output[:, 0, 1] = normvector[:, 0, 1]\n    output[:, 0, 2] = normvector[:, 0, 2]\n    output[:, 1, 0] = mu * vector[:, 1, 0] + to * normvector[:, 1, 0]\n    output[:, 1, 1] = mu * vector[:, 1, 1] + to * normvector[:, 1, 1]\n    output[:, 1, 2] = mu * vector[:, 1, 2] + to * normvector[:, 1, 2]\n    return output\n</code></pre> <p>Definition to reflect an incoming ray from a surface defined by a surface normal.  Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.</p> <p>Parameters:</p> <ul> <li> <code>input_ray</code>           \u2013            <pre><code>       A ray or rays.\n       Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> <li> <code>normal</code>           \u2013            <pre><code>       A surface normal(s).\n       Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output_ray</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a reflected ray. Expected size is [1 x 2 x 3] or [m x 2 x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def reflect(input_ray, normal):\n    \"\"\" \n    Definition to reflect an incoming ray from a surface defined by a surface normal. \n    Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.\n\n\n    Parameters\n    ----------\n    input_ray    : torch.tensor\n                   A ray or rays.\n                   Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n    normal       : torch.tensor\n                   A surface normal(s).\n                   Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n\n    Returns\n    ----------\n    output_ray   : torch.tensor\n                   Array that contains starting points and cosines of a reflected ray.\n                   Expected size is [1 x 2 x 3] or [m x 2 x 3].\n    \"\"\"\n    if len(input_ray.shape) == 2:\n        input_ray = input_ray.unsqueeze(0)\n    if len(normal.shape) == 2:\n        normal = normal.unsqueeze(0)\n    mu = 1\n    div = normal[:, 1, 0]**2 + normal[:, 1, 1]**2 + normal[:, 1, 2]**2 + 1e-8\n    a = mu * (input_ray[:, 1, 0] * normal[:, 1, 0] + input_ray[:, 1, 1] * normal[:, 1, 1] + input_ray[:, 1, 2] * normal[:, 1, 2]) / div\n    a = a.unsqueeze(1)\n    n = int(torch.amax(torch.tensor([normal.shape[0], input_ray.shape[0]])))\n    output_ray = torch.zeros((n, 2, 3)).to(input_ray.device)\n    output_ray[:, 0] = normal[:, 0]\n    output_ray[:, 1] = input_ray[:, 1] - 2 * a * normal[:, 1]\n    return output_ray\n</code></pre> <p>Please note that we provide two refractive indices as inputs in <code>odak.learn.raytracing.refract</code>. These inputs represent the refractive indices of two mediums (e.g., air and glass). However, the refractive index of a medium is dependent on light's wavelength (color). In the following example, where we showcase a sample use case of these utility functions, we will assume that light has a single wavelength. But bear in mind that when you need to ray trace with lots of wavelengths (multi-color RGB or hyperspectral), one must ray trace for each wavelength (color). Thus, the computational complexity of the raytracing increases dramatically as we aim growing realism in the simulations (e.g., describe scenes per color, raytracing for each color). Let's dive deep into how we use these functions in an actual example by observing the example below.</p> <code>test_learn_ray_refract_reflect.py</code> <pre><code>import sys\nimport odak\nimport torch\n\ndef test(output_directory = 'test_output'):\n    odak.tools.check_directory(output_directory)\n    starting_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                            no = [5, 5],\n                                                            size = [15., 15.],\n                                                            center = [0., 0., 0.]\n                                                           )\n    end_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                       no = [5, 5],\n                                                       size = [6., 6.],\n                                                       center = [0., 0., 10.]\n                                                      )\n    rays = odak.learn.raytracing.create_ray_from_two_points(\n                                                            starting_points,\n                                                            end_points\n                                                           )\n    triangle = torch.tensor([[\n                              [-5., -5., 10.],\n                              [ 5., -5., 10.],\n                              [ 0.,  5., 10.]\n                            ]])\n    normals, distance, intersecting_rays, intersecting_normals, check = odak.learn.raytracing.intersect_w_triangle(\n                                                                                    rays,\n                                                                                    triangle\n                                                                                   ) \n    n_air = 1.0 # (1)\n    n_glass = 1.51 # (2)\n    refracted_rays = odak.learn.raytracing.refract(intersecting_rays, intersecting_normals, n_air, n_glass) # (3)\n    reflected_rays = odak.learn.raytracing.reflect(intersecting_rays, intersecting_normals) # (4)\n    refract_distance = 11.\n    reflect_distance = 7.2\n    propagated_refracted_rays = odak.learn.raytracing.propagate_ray(\n                                                                    refracted_rays, \n                                                                    torch.ones(refracted_rays.shape[0]) * refract_distance\n                                                                   )\n    propagated_reflected_rays = odak.learn.raytracing.propagate_ray(\n                                                                    reflected_rays,\n                                                                    torch.ones(reflected_rays.shape[0]) * reflect_distance\n                                                                   )\n\n\n\n    visualize = False\n    if visualize:\n        ray_diagram = odak.visualize.plotly.rayshow(\n                                                    columns = 2,\n                                                    line_width = 3.,\n                                                    marker_size = 3.,\n                                                    subplot_titles = ['Refraction example', 'Reflection example']\n                                                   ) # (1)\n        ray_diagram.add_triangle(triangle, column = 1, color = 'orange')\n        ray_diagram.add_triangle(triangle, column = 2, color = 'orange')\n        ray_diagram.add_point(rays[:, 0], column = 1, color = 'blue')\n        ray_diagram.add_point(rays[:, 0], column = 2, color = 'blue')\n        ray_diagram.add_line(rays[:, 0], normals[:, 0], column = 1, color = 'blue')\n        ray_diagram.add_line(rays[:, 0], normals[:, 0], column = 2, color = 'blue')\n        ray_diagram.add_line(refracted_rays[:, 0], propagated_refracted_rays[:, 0], column = 1, color = 'blue')\n        ray_diagram.add_line(reflected_rays[:, 0], propagated_reflected_rays[:, 0], column = 2, color = 'blue')\n        colors = []\n        for color_id in range(check.shape[1]):\n            if check[0, color_id] == True:\n                colors.append('green')\n            elif check[0, color_id] == False:\n                colors.append('red')\n        ray_diagram.add_point(normals[:, 0], column = 1, color = colors)\n        ray_diagram.add_point(normals[:, 0], column = 2, color = colors)\n        html = ray_diagram.save_offline()\n        markdown_file = open('{}/ray.txt'.format(output_directory), 'w')\n        markdown_file.write(html)\n        markdown_file.close()\n    assert True == True\n\n\nif __name__ == '__main__':\n    sys.exit(test())\n</code></pre> <ol> <li>Refractive index of air (arbitrary and regardless of wavelength), the medium before the ray and triangle intersection. </li> <li>Refractive index of glass (arbitrary and regardless of wavelength), the medium after the ray and triangle intersection.</li> <li>Refraction process.</li> <li>Reflection process.</li> </ol> Challenge: Diffracted intefering rays <p>This subsection covered simulating refraction and reflection events. However, diffraction or interference <sup>5</sup> is not introduced in this raytracing model. This is because diffraction and interference would require another layer of complication. In other words, rays have to have an extra dimension beyond their starting points and direction cosines, and they also have to have the quality named phase of light. This fact makes a typical ray have dimensions of [1 x 3 x 3] instead of [1 x 2 x 3], where only direction cosines and starting points are defined. Given the information, we challenge readers to create a new submodule, <code>odak.learn.raytracing.diffraction</code>, extending rays to diffraction and interference. In addition, a new set of unit tests should be derived to adopt this new function submodule. To add these to <code>odak,</code> you can rely on the <code>pull request</code> feature on GitHub. You can also create a new <code>engineering note</code> for arbitrary surfaces in <code>docs/notes/raytracing_diffraction_interference.md</code>.</p>"},{"location":"course/geometric_optics/#optimization-with-rays","title":"Optimization with rays","text":"<p> Informative \u00b7  Practical </p> <p>We learned about refraction, reflection, rays, and surface intersections in the previous subsection. We didn't mention it then, but these functions are differentiable <sup>3</sup>. In other words, a modern machine learning library can keep a graph of a variable passing through each one of these functions (see chain rule). This differentiability feature is vital because differentiability makes our simulations for light with raytracing based on these functions compatible with modern machine learning frameworks such as Torch. In this subsection, we will use an off-the-shelf optimizer from Torch to optimize variables in our ray tracing simulations. In the first example, we will see that the optimizer helps us define the proper tilt angles for a triangle-shaped mirror and redirect light from a point light source towards a given target. Our first example resembles a straightforward case for optimization by containing only a batch of rays and a single triangle. The problem highlighted in the first example has a closed-form solution, and using an optimizer is obviously overkill. We want our readers to understand that the first example is a warm-up scenario where our readers understand how to interact with race and triangles in the context of an optimization problem. In our following second example, we will deal with a more sophisticated case where a batch of rays arriving from a point light source bounces off a surface with multiple triangles in parentheses mesh and comes at some point in our final target plane. This time we will ask our optimizer to optimize the shape of our triangles so that most of the light bouncing off there's optimized surface ends up at a location close to a target we define in our simulation. This way, we show our readers that a more sophisticated shape could be optimized using our framework, Odak. In real life, the second example could be a lens or mirror shape to be optimized.  More specifically, as an application example, it could be a mirror or a lens that focuses light from the Sun onto a solar cell to increase the efficiency of a solar power system, or it could have been a lens helping you to focus on a specific depth given your eye prescription. Let us start from our first example and examine how we can tilt the surfaces using an optimizer, and in this second example, let us see how an optimizer helps us define and optimize shape for a given mesh.</p> <code>test_learn_ray_optimization.py</code> <pre><code>import sys\nimport odak\nimport torch\nfrom tqdm import tqdm\n\n\ndef test(output_directory = 'test_output'):\n    odak.tools.check_directory(output_directory)\n    final_surface = torch.tensor([[\n                                   [-5., -5., 0.],\n                                   [ 5., -5., 0.],\n                                   [ 0.,  5., 0.]\n                                 ]])\n    final_target = torch.tensor([[3., 3., 0.]])\n    triangle = torch.tensor([\n                             [-5., -5., 10.],\n                             [ 5., -5., 10.],\n                             [ 0.,  5., 10.]\n                            ])\n    starting_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                            no = [5, 5],\n                                                            size = [1., 1.],\n                                                            center = [0., 0., 0.]\n                                                           )\n    end_point = odak.learn.raytracing.center_of_triangle(triangle)\n    rays = odak.learn.raytracing.create_ray_from_two_points(\n                                                            starting_points,\n                                                            end_point\n                                                           )\n    angles = torch.zeros(1, 3, requires_grad = True)\n    learning_rate = 2e-1\n    optimizer = torch.optim.Adam([angles], lr = learning_rate)\n    loss_function = torch.nn.MSELoss()\n    number_of_steps = 100\n    t = tqdm(range(number_of_steps), leave = False, dynamic_ncols = True)\n    for step in t:\n        optimizer.zero_grad()\n        rotated_triangle, _, _, _ = odak.learn.tools.rotate_points(\n                                                                   triangle, \n                                                                   angles = angles, \n                                                                   origin = end_point\n                                                                  )\n        _, _, intersecting_rays, intersecting_normals, check = odak.learn.raytracing.intersect_w_triangle(\n                                                                                                          rays,\n                                                                                                          rotated_triangle\n                                                                                                         )\n        reflected_rays = odak.learn.raytracing.reflect(intersecting_rays, intersecting_normals)\n        final_normals, _ = odak.learn.raytracing.intersect_w_surface(reflected_rays, final_surface)\n        if step == 0:\n            start_rays = rays.detach().clone()\n            start_rotated_triangle = rotated_triangle.detach().clone()\n            start_intersecting_rays = intersecting_rays.detach().clone()\n            start_intersecting_normals = intersecting_normals.detach().clone()\n            start_final_normals = final_normals.detach().clone()\n        final_points = final_normals[:, 0]\n        target = final_target.repeat(final_points.shape[0], 1)\n        loss = loss_function(final_points, target)\n        loss.backward(retain_graph = True)\n        optimizer.step()\n        t.set_description('Loss: {}'.format(loss.item()))\n    print('Loss: {}, angles: {}'.format(loss.item(), angles))\n\n\n    visualize = False\n    if visualize:\n        ray_diagram = odak.visualize.plotly.rayshow(\n                                                    columns = 2,\n                                                    line_width = 3.,\n                                                    marker_size = 3.,\n                                                    subplot_titles = [\n                                                                       'Surace before optimization', \n                                                                       'Surface after optimization',\n                                                                       'Hits at the target plane before optimization',\n                                                                       'Hits at the target plane after optimization',\n                                                                     ]\n                                                   ) \n        ray_diagram.add_triangle(start_rotated_triangle, column = 1, color = 'orange')\n        ray_diagram.add_triangle(rotated_triangle, column = 2, color = 'orange')\n        ray_diagram.add_point(start_rays[:, 0], column = 1, color = 'blue')\n        ray_diagram.add_point(rays[:, 0], column = 2, color = 'blue')\n        ray_diagram.add_line(start_intersecting_rays[:, 0], start_intersecting_normals[:, 0], column = 1, color = 'blue')\n        ray_diagram.add_line(intersecting_rays[:, 0], intersecting_normals[:, 0], column = 2, color = 'blue')\n        ray_diagram.add_line(start_intersecting_normals[:, 0], start_final_normals[:, 0], column = 1, color = 'blue')\n        ray_diagram.add_line(start_intersecting_normals[:, 0], final_normals[:, 0], column = 2, color = 'blue')\n        ray_diagram.add_point(final_target, column = 1, color = 'red')\n        ray_diagram.add_point(final_target, column = 2, color = 'green')\n        html = ray_diagram.save_offline()\n        markdown_file = open('{}/ray.txt'.format(output_directory), 'w')\n        markdown_file.write(html)\n        markdown_file.close()\n    assert True == True\n\n\nif __name__ == '__main__':\n    sys.exit(test())\n</code></pre> <p>Let us also look into the more sophisticated second example, where a triangular mesh is optimized to meet a specific demand, redirecting rays to a particular target.</p> <code>test_learn_ray_mesh.py</code> <pre><code>import sys\nimport odak\nimport torch\nfrom tqdm import tqdm\n\n\ndef test(output_directory = 'test_output'):\n    odak.tools.check_directory(output_directory)\n    device = torch.device('cpu')\n    final_target = torch.tensor([-2., -2., 10.], device = device)\n    final_surface = odak.learn.raytracing.define_plane(point = final_target)\n    mesh = odak.learn.raytracing.planar_mesh(\n                                             size = torch.tensor([1.1, 1.1]), \n                                             number_of_meshes = torch.tensor([9, 9]), \n                                             device = device\n                                            )\n    start_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                         no = [11, 11],\n                                                         size = [1., 1.],\n                                                         center = [2., 2., 10.]\n                                                        )\n    end_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                       no = [11, 11],\n                                                       size = [1., 1.],\n                                                       center = [0., 0., 0.]\n                                                      )\n    start_points = start_points.to(device)\n    end_points = end_points.to(device)\n    loss_function = torch.nn.MSELoss(reduction = 'sum')\n    learning_rate = 2e-3\n    optimizer = torch.optim.AdamW([mesh.heights], lr = learning_rate)\n    rays = odak.learn.raytracing.create_ray_from_two_points(start_points, end_points)\n    number_of_steps = 100\n    t = tqdm(range(number_of_steps), leave = False, dynamic_ncols = True)\n    for step in t:\n        optimizer.zero_grad()\n        triangles = mesh.get_triangles()\n        reflected_rays, reflected_normals = mesh.mirror(rays)\n        final_normals, _ = odak.learn.raytracing.intersect_w_surface(reflected_rays, final_surface)\n        final_points = final_normals[:, 0]\n        target = final_target.repeat(final_points.shape[0], 1)\n        if step == 0:\n            start_triangles = triangles.detach().clone()\n            start_reflected_rays = reflected_rays.detach().clone()\n            start_final_normals = final_normals.detach().clone()\n        loss = loss_function(final_points, target)\n        loss.backward(retain_graph = True)\n        optimizer.step() \n        description = 'Loss: {}'.format(loss.item())\n        t.set_description(description)\n    print(description)\n\n\n    visualize = False\n    if visualize:\n        ray_diagram = odak.visualize.plotly.rayshow(\n                                                    rows = 1,\n                                                    columns = 2,\n                                                    line_width = 3.,\n                                                    marker_size = 1.,\n                                                    subplot_titles = ['Before optimization', 'After optimization']\n                                                   ) \n        for triangle_id in range(triangles.shape[0]):\n            ray_diagram.add_triangle(\n                                     start_triangles[triangle_id], \n                                     row = 1, \n                                     column = 1, \n                                     color = 'orange'\n                                    )\n            ray_diagram.add_triangle(triangles[triangle_id], row = 1, column = 2, color = 'orange')\n        html = ray_diagram.save_offline()\n        markdown_file = open('{}/ray.txt'.format(output_directory), 'w')\n        markdown_file.write(html)\n        markdown_file.close()\n    assert True == True\n\n\nif __name__ == '__main__':\n    sys.exit(test())\n</code></pre> Challenge: Differentiable detector <p>In our examples, where we try bouncing light towards a fixed target, our target is defined as a single point along XYZ axes. However, in many cases in Optics and Computer Graphics, we may want to design surfaces to resemble a specific distribution of intensities over a plane (e.g., a detector or a camera sensor). For example, the work by Schwartzburg et al. <sup>6</sup> designs optical surfaces such that when light refracts, the distribution of these intensities forms an image at some target plane. To be able to replicate such works with Odak, odak needs a detector that is differentiable. This detector could be added as a class in the <code>odak.learn.raytracing</code> submodule, and a new unit test could be added as <code>test/test_learn_detector.py</code>. To add these to <code>odak,</code> you can rely on the <code>pull request</code> feature on GitHub.</p>"},{"location":"course/geometric_optics/#rendering-scenes","title":"Rendering scenes","text":"<p> Informative \u00b7  Practical </p> <p>This section shows how one can use raytracing for rendering purposes in Computer Graphics. Note that the provided example is simple, aiming to introduce a newcomer to how raytracing could be used for rendering purposes. The example uses a single perspective camera and relies on a concept called splatting, where rays originate from a camera towards a scene. The scene is composed of randomly colored triangles, and each time a ray hits a random colored triangle, our perspective camera's corresponding pixel is painted with the color of that triangle. Let us review our simple example by reading the code and observing its outcome.</p> <code>test_learn_ray_render.py</code> <pre><code>import sys\nimport odak\nimport torch\nfrom tqdm import tqdm\n\ndef test(output_directory = 'test_output'):\n    odak.tools.check_directory(output_directory)\n    final_surface_point = torch.tensor([0., 0., 10.])\n    final_surface = odak.learn.raytracing.define_plane(point = final_surface_point)\n    no = [500, 500]\n    start_points, _, _, _ = odak.learn.tools.grid_sample(\n                                                         no = no,\n                                                         size = [10., 10.],\n                                                         center = [0., 0., -10.]\n                                                        )\n    end_point = torch.tensor([0., 0., 0.])\n    rays = odak.learn.raytracing.create_ray_from_two_points(start_points, end_point)\n    mesh = odak.learn.raytracing.planar_mesh(\n                                             size = torch.tensor([10., 10.]),\n                                             number_of_meshes = torch.tensor([40, 40]),\n                                             angles = torch.tensor([  0., -70., 0.]),\n                                             offset = torch.tensor([ -2.,   0., 5.]),\n                                            )\n    triangles = mesh.get_triangles()\n    play_button = torch.tensor([[\n                                 [  1.,  0.5, 3.],\n                                 [  0.,  0.5, 3.],\n                                 [ 0.5, -0.5, 3.],\n                                ]])\n    triangles = torch.cat((play_button, triangles), dim = 0)\n    background_color = torch.rand(3)\n    triangles_color = torch.rand(triangles.shape[0], 3)\n    image = torch.zeros(rays.shape[0], 3) \n    for triangle_id, triangle in enumerate(triangles):\n        _, _, _, _, check = odak.learn.raytracing.intersect_w_triangle(rays, triangle)\n        check = check.squeeze(0).unsqueeze(-1).repeat(1, 3)\n        color = triangles_color[triangle_id].unsqueeze(0).repeat(check.shape[0], 1)\n        image[check == True] = color[check == True] * check[check == True]\n    image[image == [0., 0., 0]] = background_color\n    image = image.view(no[0], no[1], 3)\n    odak.learn.tools.save_image('{}/image.png'.format(output_directory), image, cmin = 0., cmax = 1.)\n    assert True == True\n\n\nif __name__ == '__main__':\n    sys.exit(test())\n</code></pre> <p> </p> Rendered result for the renderer script of \"/test/test_learn_ray_render.py\". <p>A modern raytracer used in gaming is far more sophisticated than the example we provide here. There aspects such as material properties or tracing the ray from its source to a camera or allowing rays to interface with multiple materials. Covering these aspects in a crash course like the one we provide here will take much work. Instead, we suggest our readers follow the resources provided in other classes, references provided at the end, or any other online available materials.</p>"},{"location":"course/geometric_optics/#conclusion","title":"Conclusion","text":"<p> Informative</p> <p>We can simulate light on a computer using various methods. We explain \"raytracing\" as one of these methods. Often, raytracing deals with light intensities, omitting many other aspects of light, like the phase or polarization of light. In addition, sending the right amount of rays from a light source into a scene in raytracing is always a struggle as an outstanding sampling problem. Raytracing creates many success stories in gaming (e.g., NVIDIA RTX or AMD Radeon Rays) and optical component design (e.g., Zemax or Ansys Speos).</p> <p>Overall, we cover a basic introduction to how to model light as rays and how to use rays to optimize against a given target. Note that our examples resemble simple cases. This section aims to provide the readers with a suitable basis to get started with the raytracing of light in simulations. A dedicated and motivated reader could scale up from this knowledge to advance concepts in displays, cameras, visual perception, optical computing, and many other light-based applications.</p> <p>Reminder</p> <p>We host a Slack group with more than 300 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link. Readers can get in-touch with the wider community using this public group.</p> <ol> <li> <p>GH Spencer and MVRK Murty. General ray-tracing procedure. JOSA, 52(6):672\u2013678, 1962.\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Robert J Bell, Kendall R Armstrong, C Stephen Nichols, and Roger W Bradley. Generalized laws of refraction and reflection. JOSA, 59(2):187\u2013189, 1969.\u00a0\u21a9</p> </li> <li> <p>Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. NIPS 2017 Workshop Autodiff, 2017.\u00a0\u21a9</p> </li> <li> <p>Wenzel Jakob, S\u00e9bastien Speierer, Nicolas Roussel, and Delio Vicini. Dr. jit: a just-in-time compiler for differentiable rendering. ACM Transactions on Graphics (TOG), 41(4):1\u201319, 2022.\u00a0\u21a9</p> </li> <li> <p>Max Born and Emil Wolf. Principles of optics: electromagnetic theory of propagation, interference and diffraction of light. Elsevier, 2013.\u00a0\u21a9</p> </li> <li> <p>Yuliy Schwartzburg, Romain Testuz, Andrea Tagliasacchi, and Mark Pauly. High-contrast computational caustic design. ACM Transactions on Graphics (TOG), 33(4):1\u201311, 2014.\u00a0\u21a9</p> </li> <li> <p>Peter Shirley. Ray tracing in one weekend. Amazon Digital Services LLC, 1:4, 2018.\u00a0\u21a9</p> </li> <li> <p>Morgan McGuire. The graphics codex. 2018.\u00a0\u21a9</p> </li> </ol>"},{"location":"course/photonic_computers/","title":"All-optical Machine Learning","text":""},{"location":"course/visual_perception/","title":"Visual Perception","text":""},{"location":"course/visual_perception/#color-perception","title":"Color Perception","text":"<p> Informative \u00b7  Practical</p> <p>We can establish an understanding on color perception through studying its physical and perceptual meaning. This way, we can gather more information on its relation to technologies and devices including displays, cameras, sensors, communication devices, computers and computer graphics.</p> <p>Color, a perceptual phenomenon, can be explained in a physical and visual perception capacity. In the physical sense, color is a quantity representing the response to wavelength of light. The human visual system can perceive colors within a certain range of the electromagnetic spectrum, from around 400 nanometers to 700 nanometers. For greater details on the electromagnetic spectrum and concept of wavelength, we recommend revisiting Light, Computation, and Computational Light section of our course. For the human visual system, color is a perceptual phenomenon created by our brain when specific wavelengths of light are emitted, reflected, or transmitted by objects. The perception of color originates from the absorption of light by photoreceptors in the eye. These photoreceptor cells convert the light into electrical signals to be interpreted by the brain<sup>1</sup>. Here, you can see a close-up photograph of these photoreceptor cells found in the eye.</p> <p> </p> Micrograph of retinal photoreceptor cells, with rods and cones highlighted in green (top row). Image courtesy of NIH, licensed under CC PDM 1.0. View source. <p>The photoreceptors, where color perception originates, are called rods and cones<sup>2</sup>.  Here, we provide a sketch showing where these rods and cones are located inside the eye. By closely observing this sketch, you can also understand the basic average geometry of a human eye and its parts helping to redirect light from an actual scene towards retinal cells.</p> <p> </p> Anatomy of an Eye (Designed with BioRender.com). <p>Rods, which are relatively more common in the periphery, help people see in low-light (scotopic) conditions. The current understanding is that the rods can only interpret in a greyscale manner. Cones, which are more dense in the fovea, are pivotal in color perception in brighter (photopic) environments.  We highlight the distribution of these photoreceptor cells, rods and cones with changing eccentricities in the eye. Here, the word <code>eccentricities</code> refer to angles with respect to our gaze direction. For instance, if a person is not directly gazing at a location or an object in a given scene, that location or the object would have some angle to the gaze of that person. Thus, there would be at some angles, some eccentricity between the gaze of that person and that location or object in that scene.</p> <p> </p> Retinal Photoreceptor Distribution, adapted from the work by Goldstein et al [3]. <p>In the above sketch, we introduced various parts on the retina, including fovea, parafovea, perifovea and peripheral vision. Note that these regions are defined by the angles, in other words eccentricities. Please also note that there is a region on our retina where there are no rods and cones are available. This region could be found in every human eye and known as the blind spot on the retina. Visual acuity and contrast sensitivity decreases progressively across these identified regions, with the most detail in the fovea, diminishing toward the periphery.</p> <p> </p> Spectral Sensitivities of LMS cones <p>The cones are categorized into three types based on their sensitivity to specific wavelengths of light, corresponding to long (L), medium (M), and short (S) wavelength cones. These three types of cones<sup>3</sup> allow us to better understand the trichromatic theory<sup>4</sup>, suggesting that human color perception stems from combining stimulations of the LMS cones. Scientists have tried to graphically represent how sensitive each type of cone is to different wavelengths of light, which is known as the spectral sensitivity function<sup>5</sup>. In practical applications such as display technologies and computational imaging, the LMS cone response can be replicated with the following formula:</p> \\[ LMS = \\sum_{i=1}^{3} \\text{RGB}_i \\cdot \\text{Spectrum}_i \\cdot \\text{Sensitivity}_i  \\] <p>Where:</p> <ul> <li>\\(RGB_i\\): The i-th color channel (Red, Green, or Blue) of the image.  </li> <li>\\(Spectrum_i\\): The spectral distribution of the corresponding primary </li> <li>\\(Sensitivity_i\\): The sensitivity of the L, M, and S cones for each wavelength.</li> </ul> <p>This formula gives us more insight on how we percieve colors from different digital and physical inputs.</p> Looking for more reading to expand your understanding on human visual system? <p>We recommend these papers, which we find it insightful:    -  B. P. Schmidt, M. Neitz, and J. Neitz, \"Neurobiological hypothesis of color appearance and hue perception,\" J. Opt. Soc. Am. A 31(4), A195\u2013207 (2014) - Biomimetic Eye Modeling &amp; Deep Neuromuscular Oculomotor Control</p> <p>The story of color perception only deepens with the concept of color opponency<sup>6</sup>. This theory reveals that our perception of color is not just a matter of additive combinations of primary colors but also involves a dynamic interplay of opposing colors: red versus green, blue versus yellow.  This phenomenon is rooted in the neural pathways of the eye and brain, where certain cells are excited or inhibited by specific wavelengths, enhancing our ability to distinguish between subtle shades and contrasts. Below is a mathematical formulation for the color opponency model proposed by Schmidt et al.<sup>3</sup></p> \\[\\begin{bmatrix} I_{(M+S)-L} \\\\ I_{(L+S)-M} \\\\ I_{(L+M+S)} \\end{bmatrix} = \\begin{bmatrix} (I_M + I_S) - I_L \\\\ (I_L + I_S) - I_M \\\\ (I_L, I_M, I_S) \\end{bmatrix}\\] <p>In this equation, \\(I_L\\), \\(I_M\\), and \\(I_S\\) represent the intensities received by the long, medium, and short cone cells, respectively. Opponent signals are represented by the differences between combinations of cone responses.</p> <p>We could exercise on our understanding of trichromat sensation with LMS cones and the concept of color opponency by vising the functions available in our toolkit, <code>odak</code>. The utility function we will review is <code>odak.learn.perception.display_color_hvs.primarier_to_lms()</code> from <code>odak.learn.perception</code>. Let us use this test to demonstrate how we can obtain LMS sensation from the color primaries of an image.</p> <code>test_learn_perception_display_color_hvs.py</code> <pre><code>import odak # (1)\nimport torch\nimport sys\nfrom odak.learn.perception.color_conversion import display_color_hvs\n\n\ndef test(\n         device = torch.device('cpu'),\n         output_directory = 'test_output'\n        ):\n    odak.tools.check_directory(output_directory)\n    torch.manual_seed(0)\n\n    image_rgb = odak.learn.tools.load_image(\n                                            'test/data/fruit_lady.png',\n                                            normalizeby = 255.,\n                                            torch_style = True\n                                           ).unsqueeze(0).to(device) # (2)\n\n    the_number_of_primaries = 3\n    multi_spectrum = torch.zeros(\n                                 the_number_of_primaries,\n                                 301\n                                ) # (3)\n    multi_spectrum[0, 200:250] = 1.\n    multi_spectrum[1, 130:145] = 1.\n    multi_spectrum[2, 0:50] = 1.\n\n    display_color = display_color_hvs(\n                                      read_spectrum ='tensor',\n                                      primaries_spectrum=multi_spectrum,\n                                      device = device\n                                     ) # (4)\n\n    image_lms_second_stage = display_color.primaries_to_lms(image_rgb) # (5)\n    image_lms_third_stage = display_color.second_to_third_stage(image_lms_second_stage) # (6)\n\n\n    odak.learn.tools.save_image(\n                                '{}/image_rgb.png'.format(output_directory),\n                                image_rgb,\n                                cmin = 0.,\n                                cmax = image_rgb.max()\n                               )\n\n\n    odak.learn.tools.save_image(\n                                '{}/image_lms_second_stage.png'.format(output_directory),\n                                image_lms_second_stage,\n                                cmin = 0.,\n                                cmax = image_lms_second_stage.max()\n                               )\n\n    odak.learn.tools.save_image(\n                                '{}/image_lms_third_stage.png'.format(output_directory),\n                                image_lms_third_stage,\n                                cmin = 0.,\n                                cmax = image_lms_third_stage.max()\n                               )\n\n\n    image_rgb_noisy = image_rgb * 0.6 + torch.rand_like(image_rgb) * 0.4 # (7)\n    loss_lms = display_color(image_rgb, image_rgb_noisy) # (8)\n    print('The third stage LMS sensation difference between two input images is {:.10f}.'.format(loss_lms))\n    assert True == True\n\nif __name__ == \"__main__\":\n    sys.exit(test())\n</code></pre> <ol> <li>Adding <code>odak</code> to our imports.</li> <li>Loading an existing RGB image.</li> <li>Defining the spectrum of our primaries of our imaginary display. These values are defined for each primary from 400 nm to 701 nm (301 elements).</li> <li>Obtain LMS cone sensations for our primaries of our imaginary display.</li> <li>Calculating the LMS sensation of our input RGB image at the second stage of color perception using our imaginary display.</li> <li>Calculating the LMS sensation of our input RGB image at the third stage of color perception using our imaginary display.</li> <li>We are intentionally adding some noise to the input RGB image here.</li> <li>We calculate the perceptual loss/difference between the two input image (original RGB vs noisy RGB).      This a visualization of a randomly generated image and its' LMS cone sensation.</li> </ol> <p>Our code above saves three different images. The very first saved image is the ground truth RGB image as depicted below.</p> <p> </p> Original ground truth image. <p>We process this ground truth image by accounting human visual system's cones and display backlight spectrum. This way, we can calculate how our ground truth image is sensed by LMS cones. The LMS sensation, in other words, ground truth image in LMS color space is provided below. Note that each color here represent a different cone, for instance, green color channel of below image represents medium cone and blue channel represents short cones. Keep in mind that LMS sensation is also known as trichromat sensation in the literature.</p> <p> </p> Image in LMS cones trichromat space. <p>Earlier, we discussed about the color oppenency theory. We follow this theory, and with our code, we utilize trichromat values to derive an image representation below.</p> <p> </p> Image representation of color opponency. Lab work: Observing the effect of display spectrum <p>We introduce our unit test, <code>test_learn_perception_display_color_hvs.py</code>, to provide an example on how to convert an RGB image to trichromat values as sensed by the retinal cone cells. Note that during this exercise, we define a variable named <code>multi_spectrum</code> to represent the wavelengths of our each color primary. These wavelength values are stored in a vector for each primary and provided the intensity of a corresponding wavelength from 400 nm to 701 nm. The trichromat values that we have derived from our original ground truth RGB image is highly correlated with these spectrum values. To observe this correlation, we encourage you to find spectrums of actual display types (e.g., OLEDs, LEDs, LCDs) and map the <code>multi_spectrum</code> to their spectrum to observe the difference in color perception in various display technologies. In addition, we believe that this will also give you a practical sandbox to examine the correlation between wavelengths and trichromat values.</p>"},{"location":"course/visual_perception/#closing-remarks","title":"Closing remarks","text":"<p>As we dive deeper into light and color perception, it becomes evident that the task of replicating the natural spectrum of colors in technology is still an evolving journey. This exploration into the nature of color sets the stage for a deeper examination of how our biological systems perceive color and how technology strives to emulate that perception.</p> Consider revisiting this chapter <p>Remember that you can always revisit this chapter as you progress with the course and as you need it. This chapter is vital for establishing a means to complete your assignments and could help formulate a suitable base to collaborate and work with my research group in the future or other experts in the field.</p> <p>Reminder</p> <p>We host a Slack group with more than 300 members. This Slack group focuses on the topics of rendering, perception, displays and cameras. The group is open to public and you can become a member by following this link. Readers can get in-touch with the wider community using this public group.</p>"},{"location":"course/visual_perception/#future-improvements","title":"Future improvements","text":"<ul> <li>Figure 1 of this paper provides sensitivity curves of various animals.</li> </ul> <ol> <li> <p>Jeremy Freeman and Eero P Simoncelli. Metamers of the ventral stream. Nature Neuroscience, 14:1195\u20131201, 2011. doi:10.1038/nn.2889.\u00a0\u21a9</p> </li> <li> <p>Trevor D Lamb. Why rods and cones? Eye, 30:179\u2013185, 2015. doi:10.1038/eye.2015.236.\u00a0\u21a9</p> </li> <li> <p>Brian P Schmidt, Maureen Neitz, and Jay Neitz. Neurobiological hypothesis of color appearance and hue perception. Journal of the Optical Society of America A, 31(4):A195\u2013A207, 2014. doi:10.1364/JOSAA.31.00A195.\u00a0\u21a9\u21a9</p> </li> <li> <p>H. V. Walters. Some experiments on the trichromatic theory of vision. Proceedings of the Royal Society of London. Series B - Biological Sciences, 131:27\u201350, 1942. doi:10.1098/rspb.1942.0016.\u00a0\u21a9</p> </li> <li> <p>Andrew Stockman and Lindsay T Sharpe. The spectral sensitivities of the middle- and long-wavelength-sensitive cones derived from measurements in observers of known genotype. Vision Research, 40:1711\u20131737, 2000. doi:10.1016/S0042-6989(00)00021-3.\u00a0\u21a9</p> </li> <li> <p>Steven K Shevell and Paul R Martin. Color opponency: tutorial. Journal of the Optical Society of America A, 34(8):1099\u20131110, 2017. doi:10.1364/JOSAA.34.001099.\u00a0\u21a9</p> </li> </ol>"},{"location":"notes/holographic_light_transport/","title":"Holographic Light Transport","text":""},{"location":"notes/holographic_light_transport/#holographic-light-transport","title":"Holographic light transport","text":"<p>Odak contains essential ingredients for research and development targeting Computer-Generated Holography. We consult the beginners in this matter to <code>Goodman's Introduction to Fourier Optics</code> book (ISBN-13:  978-0974707723) and <code>Principles of optics: electromagnetic theory of propagation, interference and diffraction of light</code> from Max Born and Emil Wolf (ISBN 0-08-26482-4). This engineering note will provide a crash course on how light travels from a phase-only hologram to an image plane.</p> Holographic image reconstruction. A collimated beam with a homogenous amplitude distribution (A=1) illuminates a phase-only hologram \\(u_0(x,y)\\). Light from this hologram diffracts and arrive at an image plane \\(u(x,y)\\) at a distance of z. Diffracted beams from each hologram pixel interfere at the image plane and, finally, reconstruct a target image. <p>As depicted in above figure, when such holograms are illuminated with a collimated coherent light (e.g. laser), these holograms can reconstruct an intended optical field at target depth levels. How light travels from a hologram to a parallel image plane is commonly described using Rayleigh-Sommerfeld diffraction integrals (For more, consult <code>Heurtley, J. C. (1973). Scalar Rayleigh\u2013Sommerfeld and Kirchhoff diffraction integrals: a comparison of exact evaluations for axial points. JOSA, 63(8), 1003-1008.</code>). The first solution of the Rayleigh-Sommerfeld integral, also known as the Huygens-Fresnel principle, is expressed as follows:</p> <p>\\(u(x,y)=\\frac{1}{j\\lambda} \\int\\!\\!\\!\\!\\int u_0(x,y)\\frac{e^{jkr}}{r}cos(\\theta)dxdy,\\)</p> <p>where field at a target image plane, \\(u(x,y)\\), is calculated by integrating over every point of hologram's field, \\(u_0(x,y)\\). Note that, for the above equation, \\(r\\) represents the optical path between a selected point over a hologram and a selected point in the image plane, theta represents the angle between these two points, k represents the wavenumber (\\(\\frac{2\\pi}{\\lambda}\\)) and \\(\\lambda\\) represents the wavelength of light. In this described light transport model, optical fields, \\(u_0(x,y)\\) and \\(u(x,y)\\), are represented with a complex value,</p> <p>\\(u_0(x,y)=A(x,y)e^{j\\phi(x,y)},\\)</p> <p>where A represents the spatial distribution of amplitude and \\(\\phi\\) represents the spatial distribution of phase across a hologram plane. The described holographic light transport model is often simplified into a single convolution with a fixed spatially invariant complex kernel, \\(h(x,y)\\) (<code>Sypek, Maciej. \"Light propagation in the Fresnel region. New numerical approach.\" Optics communications 116.1-3 (1995): 43-48.</code>).</p> <p>\\(u(x,y)=u_0(x,y) * h(x,y) =\\mathcal{F}^{-1}(\\mathcal{F}(u_0(x,y)) \\mathcal{F}(h(x,y)))\\)</p> <p>There are multiple variants of this simplified approach:</p> <ul> <li><code>Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673.</code>,</li> <li><code>Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics letters 45.6 (2020): 1543-1546.</code>,</li> <li><code>Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419.</code></li> </ul> <p>In many cases, people choose to use the most common form of h described as</p> <p>\\(h(x,y)=\\frac{e^{jkz}}{j\\lambda z} e^{\\frac{jk}{2z} (x^2+y^2)},\\)</p> <p>where z represents the distance between a hologram plane and a target image plane. Note that beam propagation can also be learned for physical setups to avoid imperfections in a setup and to improve the image quality at an image plane:</p> <ul> <li><code>Peng, Yifan, et al. \"Neural holography with camera-in-the-loop training.\" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-14.</code>,</li> <li><code>Chakravarthula, Praneeth, et al. \"Learned hardware-in-the-loop phase retrieval for holographic near-eye displays.\" ACM Transactions on Graphics (TOG) 39.6 (2020): 1-18.</code>,</li> <li><code>Kavakl\u0131, Koray, Hakan Urey, and Kaan Ak\u015fit. \"Learned holographic light transport.\" Applied Optics (2021).</code>.</li> </ul>"},{"location":"notes/holographic_light_transport/#see-also","title":"See also","text":"<p>For more engineering notes, follow:</p> <ul> <li><code>Computer Generated-Holography</code></li> </ul>"},{"location":"notes/optimizing_holograms_using_odak/","title":"Hologram Optimization","text":""},{"location":"notes/optimizing_holograms_using_odak/#optimizing-holograms-using-odak","title":"Optimizing holograms using Odak","text":"<p>This engineering note will give you an idea about how to optimize phase-only holograms using Odak. We consult the beginners in this matter to <code>Goodman's Introduction to Fourier Optics</code> (ISBN-13:  978-0974707723) and <code>Principles of optics: electromagnetic theory of propagation, interference and diffraction of light</code> from Max Born and Emil Wolf (ISBN 0-08-26482-4). Note that the creators of this documentation are from the <code>Computational Displays</code> domain. However, the provided submodules can potentially aid other lines of research as well, such as <code>Computational Imaging</code> or <code>Computational Microscopy</code>.</p> <p>The optimization that is referred to in this document is the one that generates a phase-only hologram that can reconstruct a target image. There are multiple ways in the literature to optimize a phase-only hologram for a single plane, and these include:</p> <p>Gerchberg-Saxton and Yang-Yu algorithms: - Yang, G. Z., Dong, B. Z., Gu, B. Y., Zhuang, J. Y., &amp; Ersoy, O. K. (1994). Gerchberg\u2013Saxton and Yang\u2013Gu algorithms for phase retrieval in a nonunitary transform system: a comparison. Applied optics, 33(2), 209-218.</p> <p>Stochastic Gradient Descent based optimization: - Chen, Y., Chi, Y., Fan, J., &amp; Ma, C. (2019). Gradient descent with random initialization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming, 176(1), 5-37.</p> <p>Odak provides functions to optimize phase-only holograms using <code>Gerchberg-Saxton</code> algorithm or the <code>Stochastic Gradient Descent</code> based approach. The relevant functions here are <code>odak.learn.wave.stochastic_gradient_descent</code> and <code>odak.learn.wave.gerchberg_saxton</code>. We will review both of these definitions in this document. But first, let's get prepared.</p>"},{"location":"notes/optimizing_holograms_using_odak/#preparation","title":"Preparation","text":"<p>We first start with imports, here is all you need:</p> <pre><code>from odak.learn.wave import stochastic_gradient_descent, calculate_amplitude, calculate_phase\nimport torch\n</code></pre> <p>We will also be needing some variables that defines the wavelength of light that we work with:</p> <pre><code>wavelength = 0.000000532\n</code></pre> <p>Pixel pitch and resolution of the phase-only hologram or a phase-only spatial light modulator that we are simulating:</p> <pre><code>dx = 0.0000064\nresolution = [1080, 1920]\n</code></pre> <p>Define the distance that the light will travel from optimized hologram.</p> <pre><code>distance = 0.15\n</code></pre> <p>We have to set a target image.  You can either load a sample image here or paint a white rectangle on a white background like in this example.</p> <pre><code>target = torch.zeros(resolution[0],resolution[1])\ntarget[500:600,400:450] = 1.\n</code></pre> <p>Surely, we also have to set the number of iterations and learning rate for our optimizations. If you want the GPU support, you also have to set the <code>cuda</code> as <code>True</code>. Propagation type has to be defined as well. In this example, we will use transfer function Fresnel approach. For more on propagation types, curious readers can consult  <code>Computational Fourier</code> Optics <code>David Vuelz</code> (ISBN13:9780819482044).</p> <pre><code>iteration_number = 100\nlearning_rate = 0.1\ncuda = True\npropagation_type = 'TR Fresnel'\n</code></pre> <p>This step concludes our preparations. Let's dive into optimizing our phase-only holograms. Depending on your choice, you can either optimize using <code>Gerchberg-Saxton</code> approach or the <code>Stochastic Gradient Descent</code> approach. This document will only show you <code>Stochastic Gradient Descent</code> approach as it is the state of art. However, optimizing a phase-only hologram is as importing:</p> <pre><code>from odak.learn.wave import gerchberg_saxton\n</code></pre> <p>and almost as easy as replacing <code>stochastic_gradient_descent</code> with <code>gerchberg_saxton</code> in the upcoming described hologram routine. For greater details, consult to documentation of odak.learn.wave.</p>"},{"location":"notes/optimizing_holograms_using_odak/#stochastic-gradient-descent-approach","title":"Stochastic Gradient Descent approach","text":"<p>We have prepared a function for you to avoid compiling a differentiable hologram optimizer from scratch.</p> <pre><code>hologram, reconstructed = stochastic_gradient_descent(\n        target,\n        wavelength,\n        distance,\n        dx,\n        resolution,\n        'TR Fresnel',\n        iteration_number,\n        learning_rate=learning_rate,\n        cuda=cuda\n    )\n</code></pre> <pre><code>Iteration: 99 loss:0.0003\n</code></pre> <p>Congratulations! You have just optimized a phase-only hologram that reconstruct your target image at the target depth.</p> <p>Surely, you want to see what kind of image is being reconstructed with this newly optimized hologram. You can save the outcome to an image file easily. Odak provides tools to save and load images. First, you have to import:</p> <pre><code>from odak.learn.tools import save_image,load_image\n</code></pre> <p>As you can recall, we have created a target image earlier that is normalized between zero and one.  The same is true for our result, <code>reconstructed</code>. Therefore, we have to save it correctly by taking that into account. Note that <code>reconstructed</code> is the complex field generated by our optimized <code>hologram</code> variable. So, we need to save the <code>reconstructed</code> intensity as humans and cameras capture intensity but not a complex field with phase and amplitude.</p> <pre><code>reconstructed_intensity = calculate_amplitude(reconstructed)**2\nsave_image('reconstructed_image.png',reconstructed_intensity,cmin=0.,cmax=1.)\n</code></pre> <pre><code>True\n</code></pre> <p>To save our hologram as an image so that we can load it to a spatial light modulator, we have to normalize it between zero and 255 (dynamic range of a typical image on a computer).</p> <p>P.S. Depending on your SLM's calibration and dynamic range things may vary.</p> <pre><code>slm_range = 2*3.14\ndynamic_range = 255\nphase_hologram = calculate_phase(hologram)\nphase_only_hologram = (phase_hologram%slm_range)/(slm_range)*dynamic_range\n</code></pre> <p>It is now time for saving our hologram:</p> <pre><code>save_image('phase_only_hologram.png',phase_only_hologram)\n</code></pre> <pre><code>True\n</code></pre> <p>In some cases, you may want to add a grating term to your hologram as you will display it on a spatial light modulator. There are various reasons for that, but the most obvious is getting rid of zeroth-order reflections that are not modulated by your hologram. In case you need it is as simple as below:</p> <pre><code>from odak.learn.wave import linear_grating\ngrating = linear_grating(resolution[0],resolution[1],axis='y').to(phase_hologram.device)\nphase_only_hologram_w_grating = phase_hologram+calculate_phase(grating)\n</code></pre> <p>And let's save what we got from this step:</p> <pre><code>phase_only_hologram_w_grating = (phase_only_hologram_w_grating%slm_range)/(slm_range)*dynamic_range\nsave_image('phase_only_hologram_w_grating.png',phase_only_hologram_w_grating)\n</code></pre> <pre><code>True\n</code></pre>"},{"location":"notes/optimizing_holograms_using_odak/#optimizing-holograms-with-only-numpy","title":"Optimizing holograms with only NumPy","text":"<p>In some cases, you may want to implement hologram optimization without relying on deep learning frameworks such as PyTorch. Here we show how to optimize a phase-only hologram using only NumPy, following a differentiable angular spectrum method (ASM).</p> <p>We first define the propagation model. This is a band-limited angular spectrum method (band-ASM), which accounts for the finite sampling aperture:</p> <pre><code>import numpy as np \nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\ndef bandASM(field, wavelength, pixel_pitch, z):\n    nv, nu = field.shape\n\n    k = 2 * np.pi / wavelength\n    fx = np.linspace(-1 / 2. / pixel_pitch, 1 / 2. / pixel_pitch, nu)\n    fy = np.linspace(-1 / 2. / pixel_pitch, 1 / 2. / pixel_pitch, nv)\n\n    FX, FY = np.meshgrid(fx, fy)\n    H_exp = np.exp(1j * k * z * (1 - (FX * wavelength)**2 - (FY * wavelength)**2) ** 0.5)\n    # Dispersion relation: k_z\u00b2 + k_x\u00b2 + k_y\u00b2 = k\u00b2\n    # k_x = 2\u03c0\u00b7fx, k_y = 2\u03c0\u00b7fy, k = 2\u03c0/\u03bb\n    # k_z = k\u00b7\u221a(1 - (\u03bb\u00b7fx)\u00b2 - (\u03bb\u00b7fy)\u00b2)\n\n    x = pixel_pitch * float(nu)\n    y = pixel_pitch * float(nv)\n    fx_max = 1 / np.sqrt((2 * z * (1/x))**2 + 1) / wavelength\n    fy_max = 1 / np.sqrt((2 * z * (1/y))**2 + 1) / wavelength\n    H_filter = ((np.abs(FX) &lt; fx_max) &amp; (np.abs(FY) &lt; fy_max))\n\n    H = H_filter * H_exp\n    field_fft = np.fft.fftshift(np.fft.fft2(field))\n    prop_fft = H * field_fft\n    return np.fft.ifft2(np.fft.ifftshift(prop_fft))\n</code></pre> <p>Next, we define a differentiable loss function with respect to the phase. The gradient is computed using Wirtinger calculus, back-propagating the error through the propagation operator:</p> <pre><code>def compute_loss(phase, target, wavelength, pixel_pitch, z):\n    P = np.exp(1j*phase)\n    U = bandASM(P, wavelength, pixel_pitch, z)\n    raw_intensity = np.abs(U) ** 2 \n\n    # Normalize intensity to [0,1]\n    intensity = (raw_intensity - np.min(raw_intensity)) / (np.max(raw_intensity) - np.min(raw_intensity))\n\n    # Mean squared error\n    loss = np.mean((intensity - target)**2)\n    N = intensity.size\n    dL_dI = 2 * (intensity - target) / N\n\n    # Gradient wrt U\n    I_range = np.max(raw_intensity) - np.min(raw_intensity)\n    dI_draw = 1.0 / I_range\n    dL_draw = dL_dI * dI_draw\n    dL_dU = dL_draw * np.conj(U)\n\n    # Back-propagate through ASM\n    dL_dP = bandASM(dL_dU, wavelength, pixel_pitch, -z)\n\n    # Gradient wrt real phase\n    dL_dphase = -np.imag(dL_dP * np.conj(P))\n    return loss, dL_dphase, intensity\n</code></pre> <p>For visualization, we provide a plotting helper to compare target, reconstruction, and phase:</p> <pre><code>def plot_results(target, intensity, phase, i):\n    fig, axes = plt.subplots(1, 3, figsize=(7,3))\n    axes[0].imshow(target, cmap='gray', vmin=0, vmax=1)\n    axes[1].imshow(intensity, cmap='gray', vmin=0, vmax=1)\n    axes[2].imshow(phase, cmap='gray')\n    plt.tight_layout()\n    plt.savefig(f'recon_{i}.png')\n    plt.close()\n</code></pre> <p>Finally, we bring everything together into an optimization loop. Here we use simple gradient descent on the phase values:</p> <pre><code>def optimize_hologram():\n    image_path = \"./input.png\"\n    wavelength = 515e-9\n    pixel_pitch = 6e-6\n    z = 1e-3\n\n    # Load grayscale target and normalize\n    img = Image.open(image_path).convert('L').resize((128, 128))\n    target = np.array(img) / 255.\n\n    # Initialize phase with small random values\n    phase = np.random.uniform(-0.1, 0.1, target.shape)\n\n    learning_rate = 0.5\n    num_iter = 7000\n\n    for i in range(num_iter):\n        loss, grad, intensity = compute_loss(phase, target, wavelength, pixel_pitch, z)\n        phase -= learning_rate * grad\n\n        if i % 1000 == 0:\n            print(f\"iter {i} Loss {loss}\")\n            plot_results(target, intensity, phase, i)\n\n\nif __name__ == \"__main__\":\n    optimize_hologram()\n</code></pre>"},{"location":"notes/optimizing_holograms_using_odak/#see-also","title":"See also","text":"<p>For more engineering notes, follow:</p> <ul> <li><code>Computer Generated-Holography</code></li> </ul>"},{"location":"notes/using_metameric_loss/","title":"Using metameric loss","text":"<p>This engineering note will give you an idea about using the metameric perceptual loss in <code>odak</code>.  This note is compiled by <code>David Walton</code>.  If you have further questions regarding this note, please email <code>David</code> at <code>david.walton.13@ucl.ac.uk</code>.</p> <p>Our metameric loss function works in a very similar way to built in loss functions in <code>pytorch</code>, such as <code>torch.nn.MSELoss()</code>.  However, it has a number of parameters which can be adjusted on creation (see the documentation).  Additionally, when calculating the loss a gaze location must be specified. For example:</p> <pre><code>loss_func = odak.learn.perception.MetamericLoss()\nloss = loss_func(my_image, gt_image, gaze=[0.7, 0.3])\n</code></pre> <p>The loss function caches some information, and performs most efficiently when repeatedly calculating losses for the same image size, with the same gaze location and foveation settings.</p> <p>We recommend adjusting the parameters of the loss function to match your application.  Most importantly, please set the <code>real_image_width</code> and <code>real_viewing_distance</code> parameters to correspond to how your image will be displayed to the user.  The <code>alpha</code> parameter controls the intensity of the foveation effect.  You should only need to set <code>alpha</code> once - you can then adjust the width and viewing distance to achieve the same apparent foveation effect on a range of displays &amp; viewing conditions. Note that we assume the pixels in the displayed image are square, and derive the height from the image dimensions.</p> <p>We also provide two baseline loss functions <code>BlurLoss</code> and <code>MetamerMSELoss</code> which function in much the same way.</p> <p>At the present time the loss functions are implemented only for images displayed to a user on a flat 2D display (e.g. an LCD computer monitor).  Support for equirectangular 3D images is planned for the future.</p>"},{"location":"notes/using_metameric_loss/#see-also","title":"See also","text":"<p><code>Visual perception</code></p>"},{"location":"odak/fit/","title":"odak.fit","text":"<p><code>odak.fit</code></p> <p>Provides functions to fit models to a provided data. These functions could be best described as a catalog of machine learning models.</p>"},{"location":"odak/fit/#odak.fit.gradient_descent_1d","title":"<code>gradient_descent_1d(input_data, ground_truth_data, parameters, function, gradient_function, loss_function, learning_rate=0.1, iteration_number=10)</code>","text":"<p>Vanilla Gradient Descent algorithm for 1D data.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>           \u2013            <pre><code>            One-dimensional input data.\n</code></pre> </li> <li> <code>ground_truth_data</code>               (<code>array</code>)           \u2013            <pre><code>            One-dimensional ground truth data.\n</code></pre> </li> <li> <code>parameters</code>           \u2013            <pre><code>            Parameters to be optimized.\n</code></pre> </li> <li> <code>function</code>           \u2013            <pre><code>            Function to estimate an output using the parameters.\n</code></pre> </li> <li> <code>gradient_function</code>               (<code>function</code>)           \u2013            <pre><code>            Function used in estimating gradient to update parameters at each iteration.\n</code></pre> </li> <li> <code>learning_rate</code>           \u2013            <pre><code>            Learning rate.\n</code></pre> </li> <li> <code>iteration_number</code>           \u2013            <pre><code>            Iteration number.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>parameters</code> (              <code>array</code> )          \u2013            <p>Optimized parameters.</p> </li> </ul> Source code in <code>odak/fit/__init__.py</code> <pre><code>def gradient_descent_1d(\n                        input_data,\n                        ground_truth_data,\n                        parameters,\n                        function,\n                        gradient_function,\n                        loss_function,\n                        learning_rate = 1e-1,\n                        iteration_number = 10\n                       ):\n    \"\"\"\n    Vanilla Gradient Descent algorithm for 1D data.\n\n    Parameters\n    ----------\n    input_data        : numpy.array\n                        One-dimensional input data.\n    ground_truth_data : numpy.array\n                        One-dimensional ground truth data.\n    parameters        : numpy.array\n                        Parameters to be optimized.\n    function          : function\n                        Function to estimate an output using the parameters.\n    gradient_function : function\n                        Function used in estimating gradient to update parameters at each iteration.\n    learning_rate     : float\n                        Learning rate.\n    iteration_number  : int\n                        Iteration number.\n\n\n    Returns\n    -------\n    parameters        : numpy.array\n                        Optimized parameters.\n    \"\"\"\n    t = tqdm(range(iteration_number))\n    for i in t:\n        gradient = np.zeros(parameters.shape[0])\n        for j in range(input_data.shape[0]):\n            x = input_data[j]\n            y = ground_truth_data[j]\n            gradient = gradient + gradient_function(x, y, function, parameters)\n        parameters = parameters - learning_rate * gradient / input_data.shape[0]\n        loss = loss_function(ground_truth_data, function(input_data, parameters))\n        description = 'Iteration number:{}, loss:{:0.4f}, parameters:{}'.format(i, loss, np.round(parameters, 2))\n        t.set_description(description)\n    return parameters\n</code></pre>"},{"location":"odak/fit/#odak.fit.least_square_1d","title":"<code>least_square_1d(x, y)</code>","text":"<p>A function to fit a line to given x and y data (y=mx+n). Inspired from: https://mmas.github.io/least-squares-fitting-numpy-scipy</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>     1D input data.\n</code></pre> </li> <li> <code>y</code>           \u2013            <pre><code>     1D output data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>parameters</code> (              <code>array</code> )          \u2013            <p>Parameters of m and n in a line (y=mx+n).</p> </li> </ul> Source code in <code>odak/fit/__init__.py</code> <pre><code>def least_square_1d(x, y):\n    \"\"\"\n    A function to fit a line to given x and y data (y=mx+n). Inspired from: https://mmas.github.io/least-squares-fitting-numpy-scipy\n\n    Parameters\n    ----------\n    x          : numpy.array\n                 1D input data.\n    y          : numpy.array\n                 1D output data.\n\n    Returns\n    -------\n    parameters : numpy.array\n                 Parameters of m and n in a line (y=mx+n).\n    \"\"\"\n    w = np.vstack([x, np.ones(x.shape[0])]).T\n    parameters = np.dot(np.linalg.inv(np.dot(w.T, w)), np.dot(w.T, y))\n    return parameters\n</code></pre>"},{"location":"odak/fit/#odak.fit.perceptron","title":"<code>perceptron(x, y, learning_rate=0.1, iteration_number=100)</code>","text":"<p>A function to train a perceptron model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>           Input X-Y pairs [m x 2].\n</code></pre> </li> <li> <code>y</code>           \u2013            <pre><code>           Labels for the input data [m x 1]\n</code></pre> </li> <li> <code>learning_rate</code>           \u2013            <pre><code>           Learning rate.\n</code></pre> </li> <li> <code>iteration_number</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <pre><code>           Iteration number.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>weights</code> (              <code>array</code> )          \u2013            <p>Trained weights of our model [3 x 1].</p> </li> </ul> Source code in <code>odak/fit/__init__.py</code> <pre><code>def perceptron(x, y, learning_rate = 0.1, iteration_number = 100):\n    \"\"\"\n    A function to train a perceptron model.\n\n    Parameters\n    ----------\n    x                : numpy.array\n                       Input X-Y pairs [m x 2].\n    y                : numpy.array\n                       Labels for the input data [m x 1]\n    learning_rate    : float\n                       Learning rate.\n    iteration_number : int\n                       Iteration number.\n\n    Returns\n    -------\n    weights          : numpy.array\n                       Trained weights of our model [3 x 1].\n    \"\"\"\n    weights = np.zeros((x.shape[1] + 1, 1))\n    t = tqdm(range(iteration_number))\n    for step in t:\n        unsuccessful = 0\n        for data_id in range(x.shape[0]):\n            x_i = np.insert(x[data_id], 0, 1).reshape(-1, 1)\n            y_i = y[data_id]\n            y_hat = threshold_linear_model(x_i, weights)\n            if y_hat - y_i != 0:\n                unsuccessful += 1\n                weights = weights + learning_rate * (y_i - y_hat) * x_i \n            description = 'Unsuccessful count: {}/{}'.format(unsuccessful, x.shape[0])\n    return weights\n</code></pre>"},{"location":"odak/fit/#odak.fit.threshold_linear_model","title":"<code>threshold_linear_model(x, w, threshold=0)</code>","text":"<p>A function for thresholding a linear model described with a dot product.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>           Input data [3 x 1].\n</code></pre> </li> <li> <code>w</code>           \u2013            <pre><code>           Weights [3 x 1].\n</code></pre> </li> <li> <code>threshold</code>           \u2013            <pre><code>           Value for thresholding.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>int</code> )          \u2013            <p>Estimated class of the input data. It could either be one or zero.</p> </li> </ul> Source code in <code>odak/fit/__init__.py</code> <pre><code>def threshold_linear_model(x, w, threshold = 0):\n    \"\"\"\n    A function for thresholding a linear model described with a dot product.\n\n    Parameters\n    ----------\n    x                : numpy.array\n                       Input data [3 x 1].\n    w                : numpy.array\n                       Weights [3 x 1].\n    threshold        : float\n                       Value for thresholding.\n\n    Returns\n    -------\n    result           : int\n                       Estimated class of the input data. It could either be one or zero.\n    \"\"\"\n    value = np.dot(x.T, w)\n    result = 0\n    if value &gt;= threshold:\n       result = 1\n    return result\n</code></pre>"},{"location":"odak/learn_lensless/","title":"odak.learn.lensless","text":""},{"location":"odak/learn_lensless/#odak.learn.lensless.models.spec_track","title":"<code>spec_track</code>","text":"<p>               Bases: <code>Module</code></p> <p>The learned holography model used in the paper, Ziyang Chen and Mustafa Dogan and Josef Spjut and Kaan Ak\u015fit. \"SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging.\" In SIGGRAPH Asia 2024 Posters (SA Posters '24).</p> <p>Parameters:</p> <ul> <li> <code>reduction</code>               (<code>str</code>, default:                   <code>'sum'</code> )           \u2013            <pre><code>    Reduction used for torch.nn.MSELoss and torch.nn.L1Loss. The default is 'sum'.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>    Device to run the model on. Default is CPU.\n</code></pre> </li> </ul> Source code in <code>odak/learn/lensless/models.py</code> <pre><code>class spec_track(nn.Module):\n    \"\"\"\n    The learned holography model used in the paper, Ziyang Chen and Mustafa Dogan and Josef Spjut and Kaan Ak\u015fit. \"SpecTrack: Learned Multi-Rotation Tracking via Speckle Imaging.\" In SIGGRAPH Asia 2024 Posters (SA Posters '24).\n\n    Parameters\n    ----------\n    reduction : str\n                Reduction used for torch.nn.MSELoss and torch.nn.L1Loss. The default is 'sum'.\n    device    : torch.device\n                Device to run the model on. Default is CPU.\n    \"\"\"\n    def __init__(\n                 self,\n                 reduction = 'sum',\n                 device = torch.device('cpu')\n                ):\n        super(spec_track, self).__init__()\n        self.device = device\n        self.init_layers()\n        self.reduction = reduction\n        self.l2 = torch.nn.MSELoss(reduction = self.reduction)\n        self.l1 = torch.nn.L1Loss(reduction = self.reduction)\n        self.train_history = []\n        self.validation_history = []\n\n\n    def init_layers(self):\n        \"\"\"\n        Initialize the layers of the network.\n        \"\"\"\n        # Convolutional layers with batch normalization and pooling\n        self.network = nn.Sequential(OrderedDict([\n            ('conv1', nn.Conv2d(5, 32, kernel_size=3, padding=1)),\n            ('bn1', nn.BatchNorm2d(32)),\n            ('relu1', nn.ReLU()),\n            ('pool1', nn.MaxPool2d(kernel_size=3)),\n\n            ('conv2', nn.Conv2d(32, 64, kernel_size=5, padding=1)),\n            ('bn2', nn.BatchNorm2d(64)),\n            ('relu2', nn.ReLU()),\n            ('pool2', nn.MaxPool2d(kernel_size=3)),\n\n            ('conv3', nn.Conv2d(64, 128, kernel_size=7, padding=1)),\n            ('bn3', nn.BatchNorm2d(128)),\n            ('relu3', nn.ReLU()),\n            ('pool3', nn.MaxPool2d(kernel_size=3)),\n\n            ('flatten', nn.Flatten()),\n\n            ('fc1', nn.Linear(6400, 2048)),\n            ('fc_bn1', nn.BatchNorm1d(2048)),\n            ('relu_fc1', nn.ReLU()),\n\n            ('fc2', nn.Linear(2048, 1024)),\n            ('fc_bn2', nn.BatchNorm1d(1024)),\n            ('relu_fc2', nn.ReLU()),\n\n            ('fc3', nn.Linear(1024, 512)),\n            ('fc_bn3', nn.BatchNorm1d(512)),\n            ('relu_fc3', nn.ReLU()),\n\n            ('fc4', nn.Linear(512, 128)),\n            ('fc_bn4', nn.BatchNorm1d(128)),\n            ('relu_fc4', nn.ReLU()),\n\n            ('fc5', nn.Linear(128, 3))\n        ])).to(self.device)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the network.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor.\n        \"\"\"\n        return self.network(x)\n\n\n    def evaluate(self, input_data, ground_truth, weights = [100., 1.]):\n        \"\"\"\n        Evaluate the model's performance.\n\n        Parameters\n        ----------\n        input_data    : torch.Tensor\n                        Predicted data from the model.\n        ground_truth  : torch.Tensor\n                        Ground truth data.\n        weights       : list\n                        Weights for L2 and L1 losses. Default is [100., 1.].\n\n        Returns\n        -------\n        torch.Tensor\n            Combined weighted loss.\n        \"\"\"\n        loss = weights[0] * self.l2(input_data, ground_truth) + weights[1] * self.l1(input_data, ground_truth)\n        return loss\n\n\n    def fit(self, trainloader, testloader, number_of_epochs=100, learning_rate=1e-5, weight_decay=1e-5, directory='./output'):\n        \"\"\"\n        Train the model.\n\n        Parameters\n        ----------\n        trainloader      : torch.utils.data.DataLoader\n                           Training data loader.\n        testloader       : torch.utils.data.DataLoader\n                           Testing data loader.\n        number_of_epochs : int\n                           Number of epochs to train for. Default is 100.\n        learning_rate    : float\n                           Learning rate for the optimizer. Default is 1e-5.\n        weight_decay     : float\n                           Weight decay for the optimizer. Default is 1e-5.\n        directory        : str\n                           Directory to save the model weights. Default is './output'.\n        \"\"\"\n        makedirs(directory, exist_ok=True)\n        makedirs(join(directory, \"log\"), exist_ok=True)\n\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n        best_val_loss = float('inf')\n\n        for epoch in range(number_of_epochs):\n            # Training phase\n            self.train()\n            train_loss = 0.0\n            train_batches = 0\n            train_pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{number_of_epochs} [Train]\", leave=False, dynamic_ncols=True)\n\n            for batch, labels in train_pbar:\n                self.optimizer.zero_grad()\n                batch, labels = batch.to(self.device), labels.to(self.device)\n                predicts = torch.squeeze(self.forward(batch))\n                loss = self.evaluate(predicts, labels)\n                loss.backward()\n                self.optimizer.step()\n\n                train_loss += loss.item()\n                train_batches += 1\n                train_pbar.set_postfix({'Loss': f\"{loss.item():.4f}\"})\n\n            avg_train_loss = train_loss / train_batches\n            self.train_history.append(avg_train_loss)\n\n            # Validation phase\n            self.eval()\n            val_loss = 0.0\n            val_batches = 0\n            val_pbar = tqdm(testloader, desc=f\"Epoch {epoch+1}/{number_of_epochs} [Val]\", leave=False, dynamic_ncols=True)\n\n            with torch.no_grad():\n                for batch, labels in val_pbar:\n                    batch, labels = batch.to(self.device), labels.to(self.device)\n                    predicts = torch.squeeze(self.forward(batch), dim=1)\n                    loss = self.evaluate(predicts, labels)\n\n                    val_loss += loss.item()\n                    val_batches += 1\n                    val_pbar.set_postfix({'Loss': f\"{loss.item():.4f}\"})\n\n            avg_val_loss = val_loss / val_batches\n            self.validation_history.append(avg_val_loss)\n\n            # Print epoch summary\n            print(f\"Epoch {epoch+1}/{number_of_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n\n            # Save best model\n            if avg_val_loss &lt; best_val_loss:\n                best_val_loss = avg_val_loss\n                self.save_weights(join(directory, f\"best_model_epoch_{epoch+1}.pt\"))\n                print(f\"Best model saved at epoch {epoch+1}\")\n\n        # Save training history\n        torch.save(self.train_history, join(directory, \"log\", \"train_log.pt\"))\n        torch.save(self.validation_history, join(directory, \"log\", \"validation_log.pt\"))\n        print(\"Training completed. History saved.\")\n\n\n    def save_weights(self, filename = './weights.pt'):\n        \"\"\"\n        Save the current weights of the network to a file.\n\n        Parameters\n        ----------\n        filename : str\n                   Path to save the weights. Default is './weights.pt'.\n        \"\"\"\n        torch.save(self.network.state_dict(), os.path.expanduser(filename))\n\n\n    def load_weights(self, filename = './weights.pt'):\n        \"\"\"\n        Load weights for the network from a file.\n\n        Parameters\n        ----------\n        filename : str\n                   Path to load the weights from. Default is './weights.pt'.\n        \"\"\"\n        self.network.load_state_dict(torch.load(os.path.expanduser(filename), weights_only = True))\n        self.network.eval()\n</code></pre>"},{"location":"odak/learn_lensless/#odak.learn.lensless.models.spec_track.evaluate","title":"<code>evaluate(input_data, ground_truth, weights=[100.0, 1.0])</code>","text":"<p>Evaluate the model's performance.</p> <p>Parameters:</p> <ul> <li> <code>input_data</code>           \u2013            <pre><code>        Predicted data from the model.\n</code></pre> </li> <li> <code>ground_truth</code>           \u2013            <pre><code>        Ground truth data.\n</code></pre> </li> <li> <code>weights</code>           \u2013            <pre><code>        Weights for L2 and L1 losses. Default is [100., 1.].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Combined weighted loss.</p> </li> </ul> Source code in <code>odak/learn/lensless/models.py</code> <pre><code>def evaluate(self, input_data, ground_truth, weights = [100., 1.]):\n    \"\"\"\n    Evaluate the model's performance.\n\n    Parameters\n    ----------\n    input_data    : torch.Tensor\n                    Predicted data from the model.\n    ground_truth  : torch.Tensor\n                    Ground truth data.\n    weights       : list\n                    Weights for L2 and L1 losses. Default is [100., 1.].\n\n    Returns\n    -------\n    torch.Tensor\n        Combined weighted loss.\n    \"\"\"\n    loss = weights[0] * self.l2(input_data, ground_truth) + weights[1] * self.l1(input_data, ground_truth)\n    return loss\n</code></pre>"},{"location":"odak/learn_lensless/#odak.learn.lensless.models.spec_track.fit","title":"<code>fit(trainloader, testloader, number_of_epochs=100, learning_rate=1e-05, weight_decay=1e-05, directory='./output')</code>","text":"<p>Train the model.</p> <p>Parameters:</p> <ul> <li> <code>trainloader</code>           \u2013            <pre><code>           Training data loader.\n</code></pre> </li> <li> <code>testloader</code>           \u2013            <pre><code>           Testing data loader.\n</code></pre> </li> <li> <code>number_of_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <pre><code>           Number of epochs to train for. Default is 100.\n</code></pre> </li> <li> <code>learning_rate</code>           \u2013            <pre><code>           Learning rate for the optimizer. Default is 1e-5.\n</code></pre> </li> <li> <code>weight_decay</code>           \u2013            <pre><code>           Weight decay for the optimizer. Default is 1e-5.\n</code></pre> </li> <li> <code>directory</code>           \u2013            <pre><code>           Directory to save the model weights. Default is './output'.\n</code></pre> </li> </ul> Source code in <code>odak/learn/lensless/models.py</code> <pre><code>def fit(self, trainloader, testloader, number_of_epochs=100, learning_rate=1e-5, weight_decay=1e-5, directory='./output'):\n    \"\"\"\n    Train the model.\n\n    Parameters\n    ----------\n    trainloader      : torch.utils.data.DataLoader\n                       Training data loader.\n    testloader       : torch.utils.data.DataLoader\n                       Testing data loader.\n    number_of_epochs : int\n                       Number of epochs to train for. Default is 100.\n    learning_rate    : float\n                       Learning rate for the optimizer. Default is 1e-5.\n    weight_decay     : float\n                       Weight decay for the optimizer. Default is 1e-5.\n    directory        : str\n                       Directory to save the model weights. Default is './output'.\n    \"\"\"\n    makedirs(directory, exist_ok=True)\n    makedirs(join(directory, \"log\"), exist_ok=True)\n\n    self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    best_val_loss = float('inf')\n\n    for epoch in range(number_of_epochs):\n        # Training phase\n        self.train()\n        train_loss = 0.0\n        train_batches = 0\n        train_pbar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{number_of_epochs} [Train]\", leave=False, dynamic_ncols=True)\n\n        for batch, labels in train_pbar:\n            self.optimizer.zero_grad()\n            batch, labels = batch.to(self.device), labels.to(self.device)\n            predicts = torch.squeeze(self.forward(batch))\n            loss = self.evaluate(predicts, labels)\n            loss.backward()\n            self.optimizer.step()\n\n            train_loss += loss.item()\n            train_batches += 1\n            train_pbar.set_postfix({'Loss': f\"{loss.item():.4f}\"})\n\n        avg_train_loss = train_loss / train_batches\n        self.train_history.append(avg_train_loss)\n\n        # Validation phase\n        self.eval()\n        val_loss = 0.0\n        val_batches = 0\n        val_pbar = tqdm(testloader, desc=f\"Epoch {epoch+1}/{number_of_epochs} [Val]\", leave=False, dynamic_ncols=True)\n\n        with torch.no_grad():\n            for batch, labels in val_pbar:\n                batch, labels = batch.to(self.device), labels.to(self.device)\n                predicts = torch.squeeze(self.forward(batch), dim=1)\n                loss = self.evaluate(predicts, labels)\n\n                val_loss += loss.item()\n                val_batches += 1\n                val_pbar.set_postfix({'Loss': f\"{loss.item():.4f}\"})\n\n        avg_val_loss = val_loss / val_batches\n        self.validation_history.append(avg_val_loss)\n\n        # Print epoch summary\n        print(f\"Epoch {epoch+1}/{number_of_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n\n        # Save best model\n        if avg_val_loss &lt; best_val_loss:\n            best_val_loss = avg_val_loss\n            self.save_weights(join(directory, f\"best_model_epoch_{epoch+1}.pt\"))\n            print(f\"Best model saved at epoch {epoch+1}\")\n\n    # Save training history\n    torch.save(self.train_history, join(directory, \"log\", \"train_log.pt\"))\n    torch.save(self.validation_history, join(directory, \"log\", \"validation_log.pt\"))\n    print(\"Training completed. History saved.\")\n</code></pre>"},{"location":"odak/learn_lensless/#odak.learn.lensless.models.spec_track.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the network.</p> <p>Parameters:</p> <ul> <li> <code>x</code>               (<code>Tensor</code>)           \u2013            <p>Input tensor.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tensor</code>           \u2013            <p>Output tensor.</p> </li> </ul> Source code in <code>odak/learn/lensless/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the network.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor.\n    \"\"\"\n    return self.network(x)\n</code></pre>"},{"location":"odak/learn_lensless/#odak.learn.lensless.models.spec_track.init_layers","title":"<code>init_layers()</code>","text":"<p>Initialize the layers of the network.</p> Source code in <code>odak/learn/lensless/models.py</code> <pre><code>def init_layers(self):\n    \"\"\"\n    Initialize the layers of the network.\n    \"\"\"\n    # Convolutional layers with batch normalization and pooling\n    self.network = nn.Sequential(OrderedDict([\n        ('conv1', nn.Conv2d(5, 32, kernel_size=3, padding=1)),\n        ('bn1', nn.BatchNorm2d(32)),\n        ('relu1', nn.ReLU()),\n        ('pool1', nn.MaxPool2d(kernel_size=3)),\n\n        ('conv2', nn.Conv2d(32, 64, kernel_size=5, padding=1)),\n        ('bn2', nn.BatchNorm2d(64)),\n        ('relu2', nn.ReLU()),\n        ('pool2', nn.MaxPool2d(kernel_size=3)),\n\n        ('conv3', nn.Conv2d(64, 128, kernel_size=7, padding=1)),\n        ('bn3', nn.BatchNorm2d(128)),\n        ('relu3', nn.ReLU()),\n        ('pool3', nn.MaxPool2d(kernel_size=3)),\n\n        ('flatten', nn.Flatten()),\n\n        ('fc1', nn.Linear(6400, 2048)),\n        ('fc_bn1', nn.BatchNorm1d(2048)),\n        ('relu_fc1', nn.ReLU()),\n\n        ('fc2', nn.Linear(2048, 1024)),\n        ('fc_bn2', nn.BatchNorm1d(1024)),\n        ('relu_fc2', nn.ReLU()),\n\n        ('fc3', nn.Linear(1024, 512)),\n        ('fc_bn3', nn.BatchNorm1d(512)),\n        ('relu_fc3', nn.ReLU()),\n\n        ('fc4', nn.Linear(512, 128)),\n        ('fc_bn4', nn.BatchNorm1d(128)),\n        ('relu_fc4', nn.ReLU()),\n\n        ('fc5', nn.Linear(128, 3))\n    ])).to(self.device)\n</code></pre>"},{"location":"odak/learn_lensless/#odak.learn.lensless.models.spec_track.load_weights","title":"<code>load_weights(filename='./weights.pt')</code>","text":"<p>Load weights for the network from a file.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>               (<code>str</code>, default:                   <code>'./weights.pt'</code> )           \u2013            <pre><code>   Path to load the weights from. Default is './weights.pt'.\n</code></pre> </li> </ul> Source code in <code>odak/learn/lensless/models.py</code> <pre><code>def load_weights(self, filename = './weights.pt'):\n    \"\"\"\n    Load weights for the network from a file.\n\n    Parameters\n    ----------\n    filename : str\n               Path to load the weights from. Default is './weights.pt'.\n    \"\"\"\n    self.network.load_state_dict(torch.load(os.path.expanduser(filename), weights_only = True))\n    self.network.eval()\n</code></pre>"},{"location":"odak/learn_lensless/#odak.learn.lensless.models.spec_track.save_weights","title":"<code>save_weights(filename='./weights.pt')</code>","text":"<p>Save the current weights of the network to a file.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>               (<code>str</code>, default:                   <code>'./weights.pt'</code> )           \u2013            <pre><code>   Path to save the weights. Default is './weights.pt'.\n</code></pre> </li> </ul> Source code in <code>odak/learn/lensless/models.py</code> <pre><code>def save_weights(self, filename = './weights.pt'):\n    \"\"\"\n    Save the current weights of the network to a file.\n\n    Parameters\n    ----------\n    filename : str\n               Path to save the weights. Default is './weights.pt'.\n    \"\"\"\n    torch.save(self.network.state_dict(), os.path.expanduser(filename))\n</code></pre>"},{"location":"odak/learn_models/","title":"odak.learn.models","text":"<p><code>odak.learn.models</code></p> <p>Provides necessary definitions for components used in machine learning and deep learning.</p>"},{"location":"odak/learn_models/#odak.learn.models.channel_gate","title":"<code>channel_gate</code>","text":"<p>               Bases: <code>Module</code></p> <p>Channel attention module with various pooling strategies. This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class channel_gate(torch.nn.Module):\n    \"\"\"\n    Channel attention module with various pooling strategies.\n    This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).\n    \"\"\"\n    def __init__(\n                 self, \n                 gate_channels, \n                 reduction_ratio = 16, \n                 pool_types = ['avg', 'max']\n                ):\n        \"\"\"\n        Initializes the channel gate module.\n\n        Parameters\n        ----------\n        gate_channels   : int\n                          Number of channels of the input feature map.\n        reduction_ratio : int\n                          Reduction ratio for the intermediate layer.\n        pool_types      : list\n                          List of pooling operations to apply.\n        \"\"\"\n        super().__init__()\n        self.gate_channels = gate_channels\n        hidden_channels = gate_channels // reduction_ratio\n        if hidden_channels == 0:\n            hidden_channels = 1\n        self.mlp = torch.nn.Sequential(\n                                       convolutional_block_attention.Flatten(),\n                                       torch.nn.Linear(gate_channels, hidden_channels),\n                                       torch.nn.ReLU(),\n                                       torch.nn.Linear(hidden_channels, gate_channels)\n                                      )\n        self.pool_types = pool_types\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the ChannelGate module.\n\n        Applies channel-wise attention to the input tensor.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the ChannelGate module.\n\n        Returns\n        -------\n        output       : torch.tensor\n                       Output tensor after applying channel attention.\n        \"\"\"\n        channel_att_sum = None\n        for pool_type in self.pool_types:\n            if pool_type == 'avg':\n                pool = torch.nn.functional.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n            elif pool_type == 'max':\n                pool = torch.nn.functional.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n            channel_att_raw = self.mlp(pool)\n            channel_att_sum = channel_att_raw if channel_att_sum is None else channel_att_sum + channel_att_raw\n        scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n        output = x * scale\n        return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.channel_gate.__init__","title":"<code>__init__(gate_channels, reduction_ratio=16, pool_types=['avg', 'max'])</code>","text":"<p>Initializes the channel gate module.</p> <p>Parameters:</p> <ul> <li> <code>gate_channels</code>           \u2013            <pre><code>          Number of channels of the input feature map.\n</code></pre> </li> <li> <code>reduction_ratio</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <pre><code>          Reduction ratio for the intermediate layer.\n</code></pre> </li> <li> <code>pool_types</code>           \u2013            <pre><code>          List of pooling operations to apply.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self, \n             gate_channels, \n             reduction_ratio = 16, \n             pool_types = ['avg', 'max']\n            ):\n    \"\"\"\n    Initializes the channel gate module.\n\n    Parameters\n    ----------\n    gate_channels   : int\n                      Number of channels of the input feature map.\n    reduction_ratio : int\n                      Reduction ratio for the intermediate layer.\n    pool_types      : list\n                      List of pooling operations to apply.\n    \"\"\"\n    super().__init__()\n    self.gate_channels = gate_channels\n    hidden_channels = gate_channels // reduction_ratio\n    if hidden_channels == 0:\n        hidden_channels = 1\n    self.mlp = torch.nn.Sequential(\n                                   convolutional_block_attention.Flatten(),\n                                   torch.nn.Linear(gate_channels, hidden_channels),\n                                   torch.nn.ReLU(),\n                                   torch.nn.Linear(hidden_channels, gate_channels)\n                                  )\n    self.pool_types = pool_types\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.channel_gate.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the ChannelGate module.</p> <p>Applies channel-wise attention to the input tensor.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the ChannelGate module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying channel attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the ChannelGate module.\n\n    Applies channel-wise attention to the input tensor.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the ChannelGate module.\n\n    Returns\n    -------\n    output       : torch.tensor\n                   Output tensor after applying channel attention.\n    \"\"\"\n    channel_att_sum = None\n    for pool_type in self.pool_types:\n        if pool_type == 'avg':\n            pool = torch.nn.functional.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n        elif pool_type == 'max':\n            pool = torch.nn.functional.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n        channel_att_raw = self.mlp(pool)\n        channel_att_sum = channel_att_raw if channel_att_sum is None else channel_att_sum + channel_att_raw\n    scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n    output = x * scale\n    return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.convolution_layer","title":"<code>convolution_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A convolution layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class convolution_layer(torch.nn.Module):\n    \"\"\"\n    A convolution layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 bias = False,\n                 stride = 1,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A convolutional layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        layers = [\n            torch.nn.Conv2d(\n                            input_channels,\n                            output_channels,\n                            kernel_size = kernel_size,\n                            stride = stride,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           )\n        ]\n        if normalization:\n            layers.append(torch.nn.BatchNorm2d(output_channels))\n        if activation:\n            layers.append(activation)\n        self.model = torch.nn.Sequential(*layers)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        result = self.model(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.convolution_layer.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, bias=False, stride=1, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A convolutional layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             bias = False,\n             stride = 1,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A convolutional layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    layers = [\n        torch.nn.Conv2d(\n                        input_channels,\n                        output_channels,\n                        kernel_size = kernel_size,\n                        stride = stride,\n                        padding = kernel_size // 2,\n                        bias = bias\n                       )\n    ]\n    if normalization:\n        layers.append(torch.nn.BatchNorm2d(output_channels))\n    if activation:\n        layers.append(activation)\n    self.model = torch.nn.Sequential(*layers)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.convolution_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    result = self.model(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.convolutional_block_attention","title":"<code>convolutional_block_attention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convolutional Block Attention Module (CBAM) class.  This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class convolutional_block_attention(torch.nn.Module):\n    \"\"\"\n    Convolutional Block Attention Module (CBAM) class. \n    This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).\n    \"\"\"\n    def __init__(\n                 self, \n                 gate_channels, \n                 reduction_ratio = 16, \n                 pool_types = ['avg', 'max'], \n                 no_spatial = False\n                ):\n        \"\"\"\n        Initializes the convolutional block attention module.\n\n        Parameters\n        ----------\n        gate_channels   : int\n                          Number of channels of the input feature map.\n        reduction_ratio : int\n                          Reduction ratio for the channel attention.\n        pool_types      : list\n                          List of pooling operations to apply for channel attention.\n        no_spatial      : bool\n                          If True, spatial attention is not applied.\n        \"\"\"\n        super(convolutional_block_attention, self).__init__()\n        self.channel_gate = channel_gate(gate_channels, reduction_ratio, pool_types)\n        self.no_spatial = no_spatial\n        if not no_spatial:\n            self.spatial_gate = spatial_gate()\n\n\n    class Flatten(torch.nn.Module):\n        \"\"\"\n        Flattens the input tensor to a 2D matrix.\n        \"\"\"\n        def forward(self, x):\n            return x.view(x.size(0), -1)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the convolutional block attention module.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the CBAM module.\n\n        Returns\n        -------\n        x_out        : torch.tensor\n                       Output tensor after applying channel and spatial attention.\n        \"\"\"\n        x_out = self.channel_gate(x)\n        if not self.no_spatial:\n            x_out = self.spatial_gate(x_out)\n        return x_out\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.convolutional_block_attention.Flatten","title":"<code>Flatten</code>","text":"<p>               Bases: <code>Module</code></p> <p>Flattens the input tensor to a 2D matrix.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class Flatten(torch.nn.Module):\n    \"\"\"\n    Flattens the input tensor to a 2D matrix.\n    \"\"\"\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.convolutional_block_attention.__init__","title":"<code>__init__(gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False)</code>","text":"<p>Initializes the convolutional block attention module.</p> <p>Parameters:</p> <ul> <li> <code>gate_channels</code>           \u2013            <pre><code>          Number of channels of the input feature map.\n</code></pre> </li> <li> <code>reduction_ratio</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <pre><code>          Reduction ratio for the channel attention.\n</code></pre> </li> <li> <code>pool_types</code>           \u2013            <pre><code>          List of pooling operations to apply for channel attention.\n</code></pre> </li> <li> <code>no_spatial</code>           \u2013            <pre><code>          If True, spatial attention is not applied.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self, \n             gate_channels, \n             reduction_ratio = 16, \n             pool_types = ['avg', 'max'], \n             no_spatial = False\n            ):\n    \"\"\"\n    Initializes the convolutional block attention module.\n\n    Parameters\n    ----------\n    gate_channels   : int\n                      Number of channels of the input feature map.\n    reduction_ratio : int\n                      Reduction ratio for the channel attention.\n    pool_types      : list\n                      List of pooling operations to apply for channel attention.\n    no_spatial      : bool\n                      If True, spatial attention is not applied.\n    \"\"\"\n    super(convolutional_block_attention, self).__init__()\n    self.channel_gate = channel_gate(gate_channels, reduction_ratio, pool_types)\n    self.no_spatial = no_spatial\n    if not no_spatial:\n        self.spatial_gate = spatial_gate()\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.convolutional_block_attention.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the convolutional block attention module.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the CBAM module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_out</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying channel and spatial attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the convolutional block attention module.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the CBAM module.\n\n    Returns\n    -------\n    x_out        : torch.tensor\n                   Output tensor after applying channel and spatial attention.\n    \"\"\"\n    x_out = self.channel_gate(x)\n    if not self.no_spatial:\n        x_out = self.spatial_gate(x_out)\n    return x_out\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.double_convolution","title":"<code>double_convolution</code>","text":"<p>               Bases: <code>Module</code></p> <p>A double convolution layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class double_convolution(torch.nn.Module):\n    \"\"\"\n    A double convolution layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 mid_channels = None,\n                 output_channels = 2,\n                 kernel_size = 3, \n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        Double convolution model.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels    : int\n                          Number of channels in the hidden layer between two convolutions.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        if isinstance(mid_channels, type(None)):\n            mid_channels = output_channels\n        self.activation = activation\n        self.model = torch.nn.Sequential(\n                                         convolution_layer(\n                                                           input_channels = input_channels,\n                                                           output_channels = mid_channels,\n                                                           kernel_size = kernel_size,\n                                                           bias = bias,\n                                                           normalization = normalization,\n                                                           activation = self.activation\n                                                          ),\n                                         convolution_layer(\n                                                           input_channels = mid_channels,\n                                                           output_channels = output_channels,\n                                                           kernel_size = kernel_size,\n                                                           bias = bias,\n                                                           normalization = normalization,\n                                                           activation = self.activation\n                                                          )\n                                        )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        result = self.model(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.double_convolution.__init__","title":"<code>__init__(input_channels=2, mid_channels=None, output_channels=2, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>Double convolution model.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of channels in the hidden layer between two convolutions.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             mid_channels = None,\n             output_channels = 2,\n             kernel_size = 3, \n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    Double convolution model.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels    : int\n                      Number of channels in the hidden layer between two convolutions.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    if isinstance(mid_channels, type(None)):\n        mid_channels = output_channels\n    self.activation = activation\n    self.model = torch.nn.Sequential(\n                                     convolution_layer(\n                                                       input_channels = input_channels,\n                                                       output_channels = mid_channels,\n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       normalization = normalization,\n                                                       activation = self.activation\n                                                      ),\n                                     convolution_layer(\n                                                       input_channels = mid_channels,\n                                                       output_channels = output_channels,\n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       normalization = normalization,\n                                                       activation = self.activation\n                                                      )\n                                    )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.double_convolution.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    result = self.model(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.downsample_layer","title":"<code>downsample_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A downscaling component followed by a double convolution.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class downsample_layer(torch.nn.Module):\n    \"\"\"\n    A downscaling component followed by a double convolution.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.maxpool_conv = torch.nn.Sequential(\n                                                torch.nn.MaxPool2d(2),\n                                                double_convolution(\n                                                                   input_channels = input_channels,\n                                                                   mid_channels = output_channels,\n                                                                   output_channels = output_channels,\n                                                                   kernel_size = kernel_size,\n                                                                   bias = bias,\n                                                                   normalization = normalization,\n                                                                   activation = activation\n                                                                  )\n                                               )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x              : torch.tensor\n                         First input data.\n\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        result = self.maxpool_conv(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.downsample_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 3,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.maxpool_conv = torch.nn.Sequential(\n                                            torch.nn.MaxPool2d(2),\n                                            double_convolution(\n                                                               input_channels = input_channels,\n                                                               mid_channels = output_channels,\n                                                               output_channels = output_channels,\n                                                               kernel_size = kernel_size,\n                                                               bias = bias,\n                                                               normalization = normalization,\n                                                               activation = activation\n                                                              )\n                                           )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.downsample_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x              : torch.tensor\n                     First input data.\n\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    result = self.maxpool_conv(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.global_feature_module","title":"<code>global_feature_module</code>","text":"<p>               Bases: <code>Module</code></p> <p>A global feature layer that processes global features from input channels and applies them to another input tensor via learned transformations.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class global_feature_module(torch.nn.Module):\n    \"\"\"\n    A global feature layer that processes global features from input channels and\n    applies them to another input tensor via learned transformations.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 mid_channels,\n                 output_channels,\n                 kernel_size,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A global feature layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels  : int\n                          Number of mid channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.transformations_1 = global_transformations(input_channels, output_channels)\n        self.global_features_1 = double_convolution(\n                                                    input_channels = input_channels,\n                                                    mid_channels = mid_channels,\n                                                    output_channels = output_channels,\n                                                    kernel_size = kernel_size,\n                                                    bias = bias,\n                                                    normalization = normalization,\n                                                    activation = activation\n                                                   )\n        self.global_features_2 = double_convolution(\n                                                    input_channels = input_channels,\n                                                    mid_channels = mid_channels,\n                                                    output_channels = output_channels,\n                                                    kernel_size = kernel_size,\n                                                    bias = bias,\n                                                    normalization = normalization,\n                                                    activation = activation\n                                                   )\n        self.transformations_2 = global_transformations(input_channels, output_channels)\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        global_tensor_1 = self.transformations_1(x1, x2)\n        y1 = self.global_features_1(global_tensor_1)\n        y2 = self.global_features_2(y1)\n        global_tensor_2 = self.transformations_2(y1, y2)\n        return global_tensor_2\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.global_feature_module.__init__","title":"<code>__init__(input_channels, mid_channels, output_channels, kernel_size, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A global feature layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of mid channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             mid_channels,\n             output_channels,\n             kernel_size,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A global feature layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels  : int\n                      Number of mid channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.transformations_1 = global_transformations(input_channels, output_channels)\n    self.global_features_1 = double_convolution(\n                                                input_channels = input_channels,\n                                                mid_channels = mid_channels,\n                                                output_channels = output_channels,\n                                                kernel_size = kernel_size,\n                                                bias = bias,\n                                                normalization = normalization,\n                                                activation = activation\n                                               )\n    self.global_features_2 = double_convolution(\n                                                input_channels = input_channels,\n                                                mid_channels = mid_channels,\n                                                output_channels = output_channels,\n                                                kernel_size = kernel_size,\n                                                bias = bias,\n                                                normalization = normalization,\n                                                activation = activation\n                                               )\n    self.transformations_2 = global_transformations(input_channels, output_channels)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.global_feature_module.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    global_tensor_1 = self.transformations_1(x1, x2)\n    y1 = self.global_features_1(global_tensor_1)\n    y2 = self.global_features_2(y1)\n    global_tensor_2 = self.transformations_2(y1, y2)\n    return global_tensor_2\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.global_transformations","title":"<code>global_transformations</code>","text":"<p>               Bases: <code>Module</code></p> <p>A global feature layer that processes global features from input channels and applies learned transformations to another input tensor.</p> <p>This implementation is adapted from RSGUnet: https://github.com/MTLab/rsgunet_image_enhance.</p> <p>Reference: J. Huang, P. Zhu, M. Geng et al. \"Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.\"</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class global_transformations(torch.nn.Module):\n    \"\"\"\n    A global feature layer that processes global features from input channels and\n    applies learned transformations to another input tensor.\n\n    This implementation is adapted from RSGUnet:\n    https://github.com/MTLab/rsgunet_image_enhance.\n\n    Reference:\n    J. Huang, P. Zhu, M. Geng et al. \"Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.\"\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels\n                ):\n        \"\"\"\n        A global feature layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        \"\"\"\n        super().__init__()\n        self.global_feature_1 = torch.nn.Sequential(\n            torch.nn.Linear(input_channels, output_channels),\n            torch.nn.LeakyReLU(0.2, inplace = True),\n        )\n        self.global_feature_2 = torch.nn.Sequential(\n            torch.nn.Linear(output_channels, output_channels),\n            torch.nn.LeakyReLU(0.2, inplace = True)\n        )\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        y = torch.mean(x2, dim = (2, 3))\n        y1 = self.global_feature_1(y)\n        y2 = self.global_feature_2(y1)\n        y1 = y1.unsqueeze(2).unsqueeze(3)\n        y2 = y2.unsqueeze(2).unsqueeze(3)\n        result = x1 * y1 + y2\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.global_transformations.__init__","title":"<code>__init__(input_channels, output_channels)</code>","text":"<p>A global feature layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels\n            ):\n    \"\"\"\n    A global feature layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    \"\"\"\n    super().__init__()\n    self.global_feature_1 = torch.nn.Sequential(\n        torch.nn.Linear(input_channels, output_channels),\n        torch.nn.LeakyReLU(0.2, inplace = True),\n    )\n    self.global_feature_2 = torch.nn.Sequential(\n        torch.nn.Linear(output_channels, output_channels),\n        torch.nn.LeakyReLU(0.2, inplace = True)\n    )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.global_transformations.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    y = torch.mean(x2, dim = (2, 3))\n    y1 = self.global_feature_1(y)\n    y2 = self.global_feature_2(y1)\n    y1 = y1.unsqueeze(2).unsqueeze(3)\n    y2 = y2.unsqueeze(2).unsqueeze(3)\n    result = x1 * y1 + y2\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.multi_layer_perceptron","title":"<code>multi_layer_perceptron</code>","text":"<p>               Bases: <code>Module</code></p> <p>A multi-layer perceptron model.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class multi_layer_perceptron(torch.nn.Module):\n    \"\"\"\n    A multi-layer perceptron model.\n    \"\"\"\n\n    def __init__(self,\n                 dimensions,\n                 activation = torch.nn.ReLU(),\n                 bias = False,\n                 model_type = 'conventional',\n                 siren_multiplier = 1.,\n                 input_multiplier = None\n                ):\n        \"\"\"\n        Parameters\n        ----------\n        dimensions        : list\n                            List of integers representing the dimensions of each layer (e.g., [2, 10, 1], where the first layer has two channels and last one has one channel.).\n        activation        : torch.nn\n                            Nonlinear activation function.\n                            Default is `torch.nn.ReLU()`.\n        bias              : bool\n                            If set to True, linear layers will include biases.\n        siren_multiplier  : float\n                            When using `SIREN` model type, this parameter functions as a hyperparameter.\n                            The original SIREN work uses 30.\n                            You can bypass this parameter by providing input that are not normalized and larger then one.\n        input_multiplier  : float\n                            Initial value of the input multiplier before the very first layer.\n        model_type        : str\n                            Model type: `conventional`, `swish`, `SIREN`, `FILM SIREN`, `Gaussian`.\n                            `conventional` refers to a standard multi layer perceptron.\n                            For `SIREN,` see: Sitzmann, Vincent, et al. \"Implicit neural representations with periodic activation functions.\" Advances in neural information processing systems 33 (2020): 7462-7473.\n                            For `Swish,` see: Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for activation functions.\" arXiv preprint arXiv:1710.05941 (2017). \n                            For `FILM SIREN,` see: Chan, Eric R., et al. \"pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n                            For `Gaussian,` see: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n        \"\"\"\n        super(multi_layer_perceptron, self).__init__()\n        self.activation = activation\n        self.bias = bias\n        self.model_type = model_type\n        self.layers = torch.nn.ModuleList()\n        self.siren_multiplier = siren_multiplier\n        self.dimensions = dimensions\n        for i in range(len(self.dimensions) - 1):\n            self.layers.append(torch.nn.Linear(self.dimensions[i], self.dimensions[i + 1], bias = self.bias))\n        if not isinstance(input_multiplier, type(None)):\n            self.input_multiplier = torch.nn.ParameterList()\n            self.input_multiplier.append(torch.nn.Parameter(torch.ones(1, self.dimensions[0]) * input_multiplier))\n        if self.model_type == 'FILM SIREN':\n            self.alpha = torch.nn.ParameterList()\n            for j in self.dimensions[1::]:\n                self.alpha.append(torch.nn.Parameter(torch.randn(2, 1, j)))\n        if self.model_type == 'Gaussian':\n            self.alpha = torch.nn.ParameterList()\n            for j in self.dimensions[1::]:\n                self.alpha.append(torch.nn.Parameter(torch.randn(1, 1, j)))\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        if hasattr(self, 'input_multiplier'):\n            result = x * self.input_multiplier[0]\n        else:\n            result = x\n        for layer_id, layer in enumerate(self.layers):\n            result = layer(result)\n            if self.model_type == 'conventional' and layer_id != len(self.layers) -1:\n                result = self.activation(result)\n            elif self.model_type == 'swish' and layer_id != len(self.layers) - 1:\n                result = swish(result)\n            elif self.model_type == 'SIREN' and layer_id != len(self.layers) - 1:\n                result = torch.sin(result * self.siren_multiplier)\n            elif self.model_type == 'FILM SIREN' and layer_id != len(self.layers) - 1:\n                result = torch.sin(self.alpha[layer_id][0] * result + self.alpha[layer_id][1])\n            elif self.model_type == 'Gaussian' and layer_id != len(self.layers) - 1: \n                result = gaussian(result, self.alpha[layer_id][0])\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.multi_layer_perceptron.__init__","title":"<code>__init__(dimensions, activation=torch.nn.ReLU(), bias=False, model_type='conventional', siren_multiplier=1.0, input_multiplier=None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>dimensions</code>           \u2013            <pre><code>            List of integers representing the dimensions of each layer (e.g., [2, 10, 1], where the first layer has two channels and last one has one channel.).\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>            Nonlinear activation function.\n            Default is `torch.nn.ReLU()`.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>            If set to True, linear layers will include biases.\n</code></pre> </li> <li> <code>siren_multiplier</code>           \u2013            <pre><code>            When using `SIREN` model type, this parameter functions as a hyperparameter.\n            The original SIREN work uses 30.\n            You can bypass this parameter by providing input that are not normalized and larger then one.\n</code></pre> </li> <li> <code>input_multiplier</code>           \u2013            <pre><code>            Initial value of the input multiplier before the very first layer.\n</code></pre> </li> <li> <code>model_type</code>           \u2013            <pre><code>            Model type: `conventional`, `swish`, `SIREN`, `FILM SIREN`, `Gaussian`.\n            `conventional` refers to a standard multi layer perceptron.\n            For `SIREN,` see: Sitzmann, Vincent, et al. \"Implicit neural representations with periodic activation functions.\" Advances in neural information processing systems 33 (2020): 7462-7473.\n            For `Swish,` see: Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for activation functions.\" arXiv preprint arXiv:1710.05941 (2017). \n            For `FILM SIREN,` see: Chan, Eric R., et al. \"pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n            For `Gaussian,` see: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(self,\n             dimensions,\n             activation = torch.nn.ReLU(),\n             bias = False,\n             model_type = 'conventional',\n             siren_multiplier = 1.,\n             input_multiplier = None\n            ):\n    \"\"\"\n    Parameters\n    ----------\n    dimensions        : list\n                        List of integers representing the dimensions of each layer (e.g., [2, 10, 1], where the first layer has two channels and last one has one channel.).\n    activation        : torch.nn\n                        Nonlinear activation function.\n                        Default is `torch.nn.ReLU()`.\n    bias              : bool\n                        If set to True, linear layers will include biases.\n    siren_multiplier  : float\n                        When using `SIREN` model type, this parameter functions as a hyperparameter.\n                        The original SIREN work uses 30.\n                        You can bypass this parameter by providing input that are not normalized and larger then one.\n    input_multiplier  : float\n                        Initial value of the input multiplier before the very first layer.\n    model_type        : str\n                        Model type: `conventional`, `swish`, `SIREN`, `FILM SIREN`, `Gaussian`.\n                        `conventional` refers to a standard multi layer perceptron.\n                        For `SIREN,` see: Sitzmann, Vincent, et al. \"Implicit neural representations with periodic activation functions.\" Advances in neural information processing systems 33 (2020): 7462-7473.\n                        For `Swish,` see: Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for activation functions.\" arXiv preprint arXiv:1710.05941 (2017). \n                        For `FILM SIREN,` see: Chan, Eric R., et al. \"pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n                        For `Gaussian,` see: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n    \"\"\"\n    super(multi_layer_perceptron, self).__init__()\n    self.activation = activation\n    self.bias = bias\n    self.model_type = model_type\n    self.layers = torch.nn.ModuleList()\n    self.siren_multiplier = siren_multiplier\n    self.dimensions = dimensions\n    for i in range(len(self.dimensions) - 1):\n        self.layers.append(torch.nn.Linear(self.dimensions[i], self.dimensions[i + 1], bias = self.bias))\n    if not isinstance(input_multiplier, type(None)):\n        self.input_multiplier = torch.nn.ParameterList()\n        self.input_multiplier.append(torch.nn.Parameter(torch.ones(1, self.dimensions[0]) * input_multiplier))\n    if self.model_type == 'FILM SIREN':\n        self.alpha = torch.nn.ParameterList()\n        for j in self.dimensions[1::]:\n            self.alpha.append(torch.nn.Parameter(torch.randn(2, 1, j)))\n    if self.model_type == 'Gaussian':\n        self.alpha = torch.nn.ParameterList()\n        for j in self.dimensions[1::]:\n            self.alpha.append(torch.nn.Parameter(torch.randn(1, 1, j)))\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.multi_layer_perceptron.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    if hasattr(self, 'input_multiplier'):\n        result = x * self.input_multiplier[0]\n    else:\n        result = x\n    for layer_id, layer in enumerate(self.layers):\n        result = layer(result)\n        if self.model_type == 'conventional' and layer_id != len(self.layers) -1:\n            result = self.activation(result)\n        elif self.model_type == 'swish' and layer_id != len(self.layers) - 1:\n            result = swish(result)\n        elif self.model_type == 'SIREN' and layer_id != len(self.layers) - 1:\n            result = torch.sin(result * self.siren_multiplier)\n        elif self.model_type == 'FILM SIREN' and layer_id != len(self.layers) - 1:\n            result = torch.sin(self.alpha[layer_id][0] * result + self.alpha[layer_id][1])\n        elif self.model_type == 'Gaussian' and layer_id != len(self.layers) - 1: \n            result = gaussian(result, self.alpha[layer_id][0])\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.non_local_layer","title":"<code>non_local_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Self-Attention Layer [zi = Wzyi + xi] (non-local block : ref https://arxiv.org/abs/1711.07971)</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class non_local_layer(torch.nn.Module):\n    \"\"\"\n    Self-Attention Layer [zi = Wzyi + xi] (non-local block : ref https://arxiv.org/abs/1711.07971)\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 1024,\n                 bottleneck_channels = 512,\n                 kernel_size = 1,\n                 bias = False,\n                ):\n        \"\"\"\n\n        Parameters\n        ----------\n        input_channels      : int\n                              Number of input channels.\n        bottleneck_channels : int\n                              Number of middle channels.\n        kernel_size         : int\n                              Kernel size.\n        bias                : bool \n                              Set to True to let convolutional layers have bias term.\n        \"\"\"\n        super(non_local_layer, self).__init__()\n        self.input_channels = input_channels\n        self.bottleneck_channels = bottleneck_channels\n        self.g = torch.nn.Conv2d(\n                                 self.input_channels, \n                                 self.bottleneck_channels,\n                                 kernel_size = kernel_size,\n                                 padding = kernel_size // 2,\n                                 bias = bias\n                                )\n        self.W_z = torch.nn.Sequential(\n                                       torch.nn.Conv2d(\n                                                       self.bottleneck_channels,\n                                                       self.input_channels, \n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       padding = kernel_size // 2\n                                                      ),\n                                       torch.nn.BatchNorm2d(self.input_channels)\n                                      )\n        torch.nn.init.constant_(self.W_z[1].weight, 0)   \n        torch.nn.init.constant_(self.W_z[1].bias, 0)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model [zi = Wzyi + xi]\n\n        Parameters\n        ----------\n        x               : torch.tensor\n                          First input data.                       \n\n\n        Returns\n        ----------\n        z               : torch.tensor\n                          Estimated output.\n        \"\"\"\n        batch_size, channels, height, width = x.size()\n        theta = x.view(batch_size, channels, -1).permute(0, 2, 1)\n        phi = x.view(batch_size, channels, -1).permute(0, 2, 1)\n        g = self.g(x).view(batch_size, self.bottleneck_channels, -1).permute(0, 2, 1)\n        attn = torch.bmm(theta, phi.transpose(1, 2)) / (height * width)\n        attn = torch.nn.functional.softmax(attn, dim=-1)\n        y = torch.bmm(attn, g).permute(0, 2, 1).contiguous().view(batch_size, self.bottleneck_channels, height, width)\n        W_y = self.W_z(y)\n        z = W_y + x\n        return z\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.non_local_layer.__init__","title":"<code>__init__(input_channels=1024, bottleneck_channels=512, kernel_size=1, bias=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>              Number of input channels.\n</code></pre> </li> <li> <code>bottleneck_channels</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <pre><code>              Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>              Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>              Set to True to let convolutional layers have bias term.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 1024,\n             bottleneck_channels = 512,\n             kernel_size = 1,\n             bias = False,\n            ):\n    \"\"\"\n\n    Parameters\n    ----------\n    input_channels      : int\n                          Number of input channels.\n    bottleneck_channels : int\n                          Number of middle channels.\n    kernel_size         : int\n                          Kernel size.\n    bias                : bool \n                          Set to True to let convolutional layers have bias term.\n    \"\"\"\n    super(non_local_layer, self).__init__()\n    self.input_channels = input_channels\n    self.bottleneck_channels = bottleneck_channels\n    self.g = torch.nn.Conv2d(\n                             self.input_channels, \n                             self.bottleneck_channels,\n                             kernel_size = kernel_size,\n                             padding = kernel_size // 2,\n                             bias = bias\n                            )\n    self.W_z = torch.nn.Sequential(\n                                   torch.nn.Conv2d(\n                                                   self.bottleneck_channels,\n                                                   self.input_channels, \n                                                   kernel_size = kernel_size,\n                                                   bias = bias,\n                                                   padding = kernel_size // 2\n                                                  ),\n                                   torch.nn.BatchNorm2d(self.input_channels)\n                                  )\n    torch.nn.init.constant_(self.W_z[1].weight, 0)   \n    torch.nn.init.constant_(self.W_z[1].bias, 0)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.non_local_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model [zi = Wzyi + xi]</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>          First input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model [zi = Wzyi + xi]\n\n    Parameters\n    ----------\n    x               : torch.tensor\n                      First input data.                       \n\n\n    Returns\n    ----------\n    z               : torch.tensor\n                      Estimated output.\n    \"\"\"\n    batch_size, channels, height, width = x.size()\n    theta = x.view(batch_size, channels, -1).permute(0, 2, 1)\n    phi = x.view(batch_size, channels, -1).permute(0, 2, 1)\n    g = self.g(x).view(batch_size, self.bottleneck_channels, -1).permute(0, 2, 1)\n    attn = torch.bmm(theta, phi.transpose(1, 2)) / (height * width)\n    attn = torch.nn.functional.softmax(attn, dim=-1)\n    y = torch.bmm(attn, g).permute(0, 2, 1).contiguous().view(batch_size, self.bottleneck_channels, height, width)\n    W_y = self.W_z(y)\n    z = W_y + x\n    return z\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.normalization","title":"<code>normalization</code>","text":"<p>               Bases: <code>Module</code></p> <p>A normalization layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class normalization(torch.nn.Module):\n    \"\"\"\n    A normalization layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 dim = 1,\n                ):\n        \"\"\"\n        Normalization layer.\n\n\n        Parameters\n        ----------\n        dim             : int\n                          Dimension (axis) to normalize.\n        \"\"\"\n        super().__init__()\n        self.k = torch.nn.Parameter(torch.ones(1, dim, 1, 1))\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        result =  (x - mean) * (var + eps).rsqrt() * self.k\n        return result \n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.normalization.__init__","title":"<code>__init__(dim=1)</code>","text":"<p>Normalization layer.</p> <p>Parameters:</p> <ul> <li> <code>dim</code>           \u2013            <pre><code>          Dimension (axis) to normalize.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             dim = 1,\n            ):\n    \"\"\"\n    Normalization layer.\n\n\n    Parameters\n    ----------\n    dim             : int\n                      Dimension (axis) to normalize.\n    \"\"\"\n    super().__init__()\n    self.k = torch.nn.Parameter(torch.ones(1, dim, 1, 1))\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.normalization.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n    var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n    mean = torch.mean(x, dim = 1, keepdim = True)\n    result =  (x - mean) * (var + eps).rsqrt() * self.k\n    return result \n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.positional_encoder","title":"<code>positional_encoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>A positional encoder module. This implementation follows this specific work: <code>Martin-Brualla, Ricardo, Noha Radwan, Mehdi SM Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. \"Nerf in the wild: Neural radiance fields for unconstrained photo collections.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7210-7219. 2021.</code>.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class positional_encoder(torch.nn.Module):\n    \"\"\"\n    A positional encoder module.\n    This implementation follows this specific work: `Martin-Brualla, Ricardo, Noha Radwan, Mehdi SM Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. \"Nerf in the wild: Neural radiance fields for unconstrained photo collections.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7210-7219. 2021.`.\n    \"\"\"\n\n    def __init__(self, L):\n        \"\"\"\n        A positional encoder module.\n\n        Parameters\n        ----------\n        L                   : int\n                              Positional encoding level.\n        \"\"\"\n        super(positional_encoder, self).__init__()\n        self.L = L\n\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x               : torch.tensor\n                          Input data [b x n], where `b` is batch size, `n` is the feature size.\n\n        Returns\n        ----------\n        result          : torch.tensor\n                          Result of the forward operation.\n        \"\"\"\n        freqs = 2 ** torch.arange(self.L, device = x.device)\n        freqs = freqs.view(1, 1, -1)\n        results_cos = torch.cos(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n        results_sin = torch.sin(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n        results = torch.cat((x, results_cos, results_sin), dim = 1)\n        return results\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.positional_encoder.__init__","title":"<code>__init__(L)</code>","text":"<p>A positional encoder module.</p> <p>Parameters:</p> <ul> <li> <code>L</code>           \u2013            <pre><code>              Positional encoding level.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(self, L):\n    \"\"\"\n    A positional encoder module.\n\n    Parameters\n    ----------\n    L                   : int\n                          Positional encoding level.\n    \"\"\"\n    super(positional_encoder, self).__init__()\n    self.L = L\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.positional_encoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>          Input data [b x n], where `b` is batch size, `n` is the feature size.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x               : torch.tensor\n                      Input data [b x n], where `b` is batch size, `n` is the feature size.\n\n    Returns\n    ----------\n    result          : torch.tensor\n                      Result of the forward operation.\n    \"\"\"\n    freqs = 2 ** torch.arange(self.L, device = x.device)\n    freqs = freqs.view(1, 1, -1)\n    results_cos = torch.cos(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n    results_sin = torch.sin(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n    results = torch.cat((x, results_cos, results_sin), dim = 1)\n    return results\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.residual_attention_layer","title":"<code>residual_attention_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A residual block with an attention layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class residual_attention_layer(torch.nn.Module):\n    \"\"\"\n    A residual block with an attention layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 1,\n                 bias = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        An attention layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int or optioal\n                          Number of input channels.\n        output_channels : int or optional\n                          Number of middle channels.\n        kernel_size     : int or optional\n                          Kernel size.\n        bias            : bool or optional\n                          Set to True to let convolutional layers have bias term.\n        activation      : torch.nn or optional\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.activation = activation\n        self.convolution0 = torch.nn.Sequential(\n                                                torch.nn.Conv2d(\n                                                                input_channels,\n                                                                output_channels,\n                                                                kernel_size = kernel_size,\n                                                                padding = kernel_size // 2,\n                                                                bias = bias\n                                                               ),\n                                                torch.nn.BatchNorm2d(output_channels)\n                                               )\n        self.convolution1 = torch.nn.Sequential(\n                                                torch.nn.Conv2d(\n                                                                input_channels,\n                                                                output_channels,\n                                                                kernel_size = kernel_size,\n                                                                padding = kernel_size // 2,\n                                                                bias = bias\n                                                               ),\n                                                torch.nn.BatchNorm2d(output_channels)\n                                               )\n        self.final_layer = torch.nn.Sequential(\n                                               self.activation,\n                                               torch.nn.Conv2d(\n                                                               output_channels,\n                                                               output_channels,\n                                                               kernel_size = kernel_size,\n                                                               padding = kernel_size // 2,\n                                                               bias = bias\n                                                              )\n                                              )\n\n\n    def forward(self, x0, x1):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x0             : torch.tensor\n                         First input data.\n\n        x1             : torch.tensor\n                         Seconnd input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        y0 = self.convolution0(x0)\n        y1 = self.convolution1(x1)\n        y2 = torch.add(y0, y1)\n        result = self.final_layer(y2) * x0\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.residual_attention_layer.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=1, bias=False, activation=torch.nn.ReLU())</code>","text":"<p>An attention layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int or optional</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 1,\n             bias = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    An attention layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int or optioal\n                      Number of input channels.\n    output_channels : int or optional\n                      Number of middle channels.\n    kernel_size     : int or optional\n                      Kernel size.\n    bias            : bool or optional\n                      Set to True to let convolutional layers have bias term.\n    activation      : torch.nn or optional\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.activation = activation\n    self.convolution0 = torch.nn.Sequential(\n                                            torch.nn.Conv2d(\n                                                            input_channels,\n                                                            output_channels,\n                                                            kernel_size = kernel_size,\n                                                            padding = kernel_size // 2,\n                                                            bias = bias\n                                                           ),\n                                            torch.nn.BatchNorm2d(output_channels)\n                                           )\n    self.convolution1 = torch.nn.Sequential(\n                                            torch.nn.Conv2d(\n                                                            input_channels,\n                                                            output_channels,\n                                                            kernel_size = kernel_size,\n                                                            padding = kernel_size // 2,\n                                                            bias = bias\n                                                           ),\n                                            torch.nn.BatchNorm2d(output_channels)\n                                           )\n    self.final_layer = torch.nn.Sequential(\n                                           self.activation,\n                                           torch.nn.Conv2d(\n                                                           output_channels,\n                                                           output_channels,\n                                                           kernel_size = kernel_size,\n                                                           padding = kernel_size // 2,\n                                                           bias = bias\n                                                          )\n                                          )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.residual_attention_layer.forward","title":"<code>forward(x0, x1)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x0</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x1</code>           \u2013            <pre><code>         Seconnd input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x0, x1):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x0             : torch.tensor\n                     First input data.\n\n    x1             : torch.tensor\n                     Seconnd input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    y0 = self.convolution0(x0)\n    y1 = self.convolution1(x1)\n    y2 = torch.add(y0, y1)\n    result = self.final_layer(y2) * x0\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.residual_layer","title":"<code>residual_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A residual layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class residual_layer(torch.nn.Module):\n    \"\"\"\n    A residual layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 mid_channels = 16,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = True,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A convolutional layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels    : int\n                          Number of middle channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.activation = activation\n        self.convolution = double_convolution(\n                                              input_channels,\n                                              mid_channels = mid_channels,\n                                              output_channels = input_channels,\n                                              kernel_size = kernel_size,\n                                              normalization = normalization,\n                                              bias = bias,\n                                              activation = activation\n                                             )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        x0 = self.convolution(x)\n        return x + x0\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.residual_layer.__init__","title":"<code>__init__(input_channels=2, mid_channels=16, kernel_size=3, bias=False, normalization=True, activation=torch.nn.ReLU())</code>","text":"<p>A convolutional layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             mid_channels = 16,\n             kernel_size = 3,\n             bias = False,\n             normalization = True,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A convolutional layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels    : int\n                      Number of middle channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.activation = activation\n    self.convolution = double_convolution(\n                                          input_channels,\n                                          mid_channels = mid_channels,\n                                          output_channels = input_channels,\n                                          kernel_size = kernel_size,\n                                          normalization = normalization,\n                                          bias = bias,\n                                          activation = activation\n                                         )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.residual_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    x0 = self.convolution(x)\n    return x + x0\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatial_gate","title":"<code>spatial_gate</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatial attention module that applies a convolution layer after channel pooling. This class is heavily inspired by https://github.com/Jongchan/attention-module/blob/master/MODELS/cbam.py.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatial_gate(torch.nn.Module):\n    \"\"\"\n    Spatial attention module that applies a convolution layer after channel pooling.\n    This class is heavily inspired by https://github.com/Jongchan/attention-module/blob/master/MODELS/cbam.py.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the spatial gate module.\n        \"\"\"\n        super().__init__()\n        kernel_size = 7\n        self.spatial = convolution_layer(2, 1, kernel_size, bias = False, activation = torch.nn.Identity())\n\n\n    def channel_pool(self, x):\n        \"\"\"\n        Applies max and average pooling on the channels.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input tensor.\n\n        Returns\n        -------\n        output        : torch.tensor\n                        Output tensor.\n        \"\"\"\n        max_pool = torch.max(x, 1)[0].unsqueeze(1)\n        avg_pool = torch.mean(x, 1).unsqueeze(1)\n        output = torch.cat((max_pool, avg_pool), dim=1)\n        return output\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the SpatialGate module.\n\n        Applies spatial attention to the input tensor.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the SpatialGate module.\n\n        Returns\n        -------\n        scaled_x     : torch.tensor\n                       Output tensor after applying spatial attention.\n        \"\"\"\n        x_compress = self.channel_pool(x)\n        x_out = self.spatial(x_compress)\n        scale = torch.sigmoid(x_out)\n        scaled_x = x * scale\n        return scaled_x\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatial_gate.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the spatial gate module.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the spatial gate module.\n    \"\"\"\n    super().__init__()\n    kernel_size = 7\n    self.spatial = convolution_layer(2, 1, kernel_size, bias = False, activation = torch.nn.Identity())\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatial_gate.channel_pool","title":"<code>channel_pool(x)</code>","text":"<p>Applies max and average pooling on the channels.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input tensor.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Output tensor.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def channel_pool(self, x):\n    \"\"\"\n    Applies max and average pooling on the channels.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input tensor.\n\n    Returns\n    -------\n    output        : torch.tensor\n                    Output tensor.\n    \"\"\"\n    max_pool = torch.max(x, 1)[0].unsqueeze(1)\n    avg_pool = torch.mean(x, 1).unsqueeze(1)\n    output = torch.cat((max_pool, avg_pool), dim=1)\n    return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatial_gate.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the SpatialGate module.</p> <p>Applies spatial attention to the input tensor.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the SpatialGate module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>scaled_x</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying spatial attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the SpatialGate module.\n\n    Applies spatial attention to the input tensor.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the SpatialGate module.\n\n    Returns\n    -------\n    scaled_x     : torch.tensor\n                   Output tensor after applying spatial attention.\n    \"\"\"\n    x_compress = self.channel_pool(x)\n    x_out = self.spatial(x_compress)\n    scale = torch.sigmoid(x_out)\n    scaled_x = x * scale\n    return scaled_x\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_adaptive_convolution","title":"<code>spatially_adaptive_convolution</code>","text":"<p>               Bases: <code>Module</code></p> <p>A spatially adaptive convolution layer.</p> References <p>C. Zheng et al. \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions.\" C. Xu et al. \"Squeezesegv3: Spatially-adaptive Convolution for Efficient Point-Cloud Segmentation.\" C. Zheng et al. \"Windowing Decomposition Convolutional Neural Network for Image Enhancement.\"</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatially_adaptive_convolution(torch.nn.Module):\n    \"\"\"\n    A spatially adaptive convolution layer.\n\n    References\n    ----------\n\n    C. Zheng et al. \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions.\"\n    C. Xu et al. \"Squeezesegv3: Spatially-adaptive Convolution for Efficient Point-Cloud Segmentation.\"\n    C. Zheng et al. \"Windowing Decomposition Convolutional Neural Network for Image Enhancement.\"\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 stride = 1,\n                 padding = 1,\n                 bias = False,\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        Initializes a spatially adaptive convolution layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Size of the convolution kernel.\n        stride          : int\n                          Stride of the convolution.\n        padding         : int\n                          Padding added to both sides of the input.\n        bias            : bool\n                          If True, includes a bias term in the convolution.\n        activation      : torch.nn.Module\n                          Activation function to apply. If None, no activation is applied.\n        \"\"\"\n        super(spatially_adaptive_convolution, self).__init__()\n        self.kernel_size = kernel_size\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.stride = stride\n        self.padding = padding\n        self.standard_convolution = torch.nn.Conv2d(\n                                                    in_channels = input_channels,\n                                                    out_channels = self.output_channels,\n                                                    kernel_size = kernel_size,\n                                                    stride = stride,\n                                                    padding = padding,\n                                                    bias = bias\n                                                   )\n        self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n        self.activation = activation\n\n\n    def forward(self, x, sv_kernel_feature):\n        \"\"\"\n        Forward pass for the spatially adaptive convolution layer.\n\n        Parameters\n        ----------\n        x                  : torch.tensor\n                            Input data tensor.\n                            Dimension: (1, C, H, W)\n        sv_kernel_feature   : torch.tensor\n                            Spatially varying kernel features.\n                            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n        Returns\n        -------\n        sa_output          : torch.tensor\n                            Estimated output tensor.\n                            Dimension: (1, output_channels, H_out, W_out)\n        \"\"\"\n        # Pad input and sv_kernel_feature if necessary\n        if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n                -2) * self.stride != x.size(-2):\n            diffY = sv_kernel_feature.size(-2) % self.stride\n            diffX = sv_kernel_feature.size(-1) % self.stride\n            sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                            diffY // 2, diffY - diffY // 2))\n            diffY = x.size(-2) % self.stride\n            diffX = x.size(-1) % self.stride\n            x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                            diffY // 2, diffY - diffY // 2))\n\n        # Unfold the input tensor for matrix multiplication\n        input_feature = torch.nn.functional.unfold(\n                                                   x,\n                                                   kernel_size = (self.kernel_size, self.kernel_size),\n                                                   stride = self.stride,\n                                                   padding = self.padding\n                                                  )\n\n        # Resize sv_kernel_feature to match the input feature\n        sv_kernel = sv_kernel_feature.reshape(\n                                              1,\n                                              self.input_channels * self.kernel_size * self.kernel_size,\n                                              (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                             )\n\n        # Resize weight to match the input channels and kernel size\n        si_kernel = self.weight.reshape(\n                                        self.weight_output_channels,\n                                        self.input_channels * self.kernel_size * self.kernel_size\n                                       )\n\n        # Apply spatially varying kernels\n        sv_feature = input_feature * sv_kernel\n\n        # Perform matrix multiplication\n        sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                                1, self.weight_output_channels,\n                                                                (x.size(-2) // self.stride),\n                                                                (x.size(-1) // self.stride)\n                                                               )\n        return sa_output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_adaptive_convolution.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, stride=1, padding=1, bias=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>Initializes a spatially adaptive convolution layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Size of the convolution kernel.\n</code></pre> </li> <li> <code>stride</code>           \u2013            <pre><code>          Stride of the convolution.\n</code></pre> </li> <li> <code>padding</code>           \u2013            <pre><code>          Padding added to both sides of the input.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          If True, includes a bias term in the convolution.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Activation function to apply. If None, no activation is applied.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             stride = 1,\n             padding = 1,\n             bias = False,\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    Initializes a spatially adaptive convolution layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Size of the convolution kernel.\n    stride          : int\n                      Stride of the convolution.\n    padding         : int\n                      Padding added to both sides of the input.\n    bias            : bool\n                      If True, includes a bias term in the convolution.\n    activation      : torch.nn.Module\n                      Activation function to apply. If None, no activation is applied.\n    \"\"\"\n    super(spatially_adaptive_convolution, self).__init__()\n    self.kernel_size = kernel_size\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.stride = stride\n    self.padding = padding\n    self.standard_convolution = torch.nn.Conv2d(\n                                                in_channels = input_channels,\n                                                out_channels = self.output_channels,\n                                                kernel_size = kernel_size,\n                                                stride = stride,\n                                                padding = padding,\n                                                bias = bias\n                                               )\n    self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n    self.activation = activation\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_adaptive_convolution.forward","title":"<code>forward(x, sv_kernel_feature)</code>","text":"<p>Forward pass for the spatially adaptive convolution layer.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>            Input data tensor.\n            Dimension: (1, C, H, W)\n</code></pre> </li> <li> <code>sv_kernel_feature</code>           \u2013            <pre><code>            Spatially varying kernel features.\n            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>sa_output</code> (              <code>tensor</code> )          \u2013            <p>Estimated output tensor. Dimension: (1, output_channels, H_out, W_out)</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x, sv_kernel_feature):\n    \"\"\"\n    Forward pass for the spatially adaptive convolution layer.\n\n    Parameters\n    ----------\n    x                  : torch.tensor\n                        Input data tensor.\n                        Dimension: (1, C, H, W)\n    sv_kernel_feature   : torch.tensor\n                        Spatially varying kernel features.\n                        Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n    Returns\n    -------\n    sa_output          : torch.tensor\n                        Estimated output tensor.\n                        Dimension: (1, output_channels, H_out, W_out)\n    \"\"\"\n    # Pad input and sv_kernel_feature if necessary\n    if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n            -2) * self.stride != x.size(-2):\n        diffY = sv_kernel_feature.size(-2) % self.stride\n        diffX = sv_kernel_feature.size(-1) % self.stride\n        sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                        diffY // 2, diffY - diffY // 2))\n        diffY = x.size(-2) % self.stride\n        diffX = x.size(-1) % self.stride\n        x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                        diffY // 2, diffY - diffY // 2))\n\n    # Unfold the input tensor for matrix multiplication\n    input_feature = torch.nn.functional.unfold(\n                                               x,\n                                               kernel_size = (self.kernel_size, self.kernel_size),\n                                               stride = self.stride,\n                                               padding = self.padding\n                                              )\n\n    # Resize sv_kernel_feature to match the input feature\n    sv_kernel = sv_kernel_feature.reshape(\n                                          1,\n                                          self.input_channels * self.kernel_size * self.kernel_size,\n                                          (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                         )\n\n    # Resize weight to match the input channels and kernel size\n    si_kernel = self.weight.reshape(\n                                    self.weight_output_channels,\n                                    self.input_channels * self.kernel_size * self.kernel_size\n                                   )\n\n    # Apply spatially varying kernels\n    sv_feature = input_feature * sv_kernel\n\n    # Perform matrix multiplication\n    sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                            1, self.weight_output_channels,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n    return sa_output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_adaptive_module","title":"<code>spatially_adaptive_module</code>","text":"<p>               Bases: <code>Module</code></p> <p>A spatially adaptive module that combines learned spatially adaptive convolutions.</p> References <p>Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatially_adaptive_module(torch.nn.Module):\n    \"\"\"\n    A spatially adaptive module that combines learned spatially adaptive convolutions.\n\n    References\n    ----------\n\n    Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 stride = 1,\n                 padding = 1,\n                 bias = False,\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        Initializes a spatially adaptive module.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Size of the convolution kernel.\n        stride          : int\n                          Stride of the convolution.\n        padding         : int\n                          Padding added to both sides of the input.\n        bias            : bool\n                          If True, includes a bias term in the convolution.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super(spatially_adaptive_module, self).__init__()\n        self.kernel_size = kernel_size\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.stride = stride\n        self.padding = padding\n        self.weight_output_channels = self.output_channels - 1\n        self.standard_convolution = torch.nn.Conv2d(\n                                                    in_channels = input_channels,\n                                                    out_channels = self.weight_output_channels,\n                                                    kernel_size = kernel_size,\n                                                    stride = stride,\n                                                    padding = padding,\n                                                    bias = bias\n                                                   )\n        self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n        self.activation = activation\n\n\n    def forward(self, x, sv_kernel_feature):\n        \"\"\"\n        Forward pass for the spatially adaptive module.\n\n        Parameters\n        ----------\n        x                  : torch.tensor\n                            Input data tensor.\n                            Dimension: (1, C, H, W)\n        sv_kernel_feature   : torch.tensor\n                            Spatially varying kernel features.\n                            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n        Returns\n        -------\n        output             : torch.tensor\n                            Combined output tensor from standard and spatially adaptive convolutions.\n                            Dimension: (1, output_channels, H_out, W_out)\n        \"\"\"\n        # Pad input and sv_kernel_feature if necessary\n        if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n                -2) * self.stride != x.size(-2):\n            diffY = sv_kernel_feature.size(-2) % self.stride\n            diffX = sv_kernel_feature.size(-1) % self.stride\n            sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                            diffY // 2, diffY - diffY // 2))\n            diffY = x.size(-2) % self.stride\n            diffX = x.size(-1) % self.stride\n            x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                            diffY // 2, diffY - diffY // 2))\n\n        # Unfold the input tensor for matrix multiplication\n        input_feature = torch.nn.functional.unfold(\n                                                   x,\n                                                   kernel_size = (self.kernel_size, self.kernel_size),\n                                                   stride = self.stride,\n                                                   padding = self.padding\n                                                  )\n\n        # Resize sv_kernel_feature to match the input feature\n        sv_kernel = sv_kernel_feature.reshape(\n                                              1,\n                                              self.input_channels * self.kernel_size * self.kernel_size,\n                                              (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                             )\n\n        # Apply sv_kernel to the input_feature\n        sv_feature = input_feature * sv_kernel\n\n        # Original spatially varying convolution output\n        sv_output = torch.sum(sv_feature, dim = 1).reshape(\n                                                           1,\n                                                            1,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n\n        # Reshape weight for spatially adaptive convolution\n        si_kernel = self.weight.reshape(\n                                        self.weight_output_channels,\n                                        self.input_channels * self.kernel_size * self.kernel_size\n                                       )\n\n        # Apply si_kernel on sv convolution output\n        sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                                1, self.weight_output_channels,\n                                                                (x.size(-2) // self.stride),\n                                                                (x.size(-1) // self.stride)\n                                                               )\n\n        # Combine the outputs and apply activation function\n        output = self.activation(torch.cat((sv_output, sa_output), dim = 1))\n        return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_adaptive_module.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, stride=1, padding=1, bias=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>Initializes a spatially adaptive module.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Size of the convolution kernel.\n</code></pre> </li> <li> <code>stride</code>           \u2013            <pre><code>          Stride of the convolution.\n</code></pre> </li> <li> <code>padding</code>           \u2013            <pre><code>          Padding added to both sides of the input.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          If True, includes a bias term in the convolution.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             stride = 1,\n             padding = 1,\n             bias = False,\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    Initializes a spatially adaptive module.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Size of the convolution kernel.\n    stride          : int\n                      Stride of the convolution.\n    padding         : int\n                      Padding added to both sides of the input.\n    bias            : bool\n                      If True, includes a bias term in the convolution.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super(spatially_adaptive_module, self).__init__()\n    self.kernel_size = kernel_size\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.stride = stride\n    self.padding = padding\n    self.weight_output_channels = self.output_channels - 1\n    self.standard_convolution = torch.nn.Conv2d(\n                                                in_channels = input_channels,\n                                                out_channels = self.weight_output_channels,\n                                                kernel_size = kernel_size,\n                                                stride = stride,\n                                                padding = padding,\n                                                bias = bias\n                                               )\n    self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n    self.activation = activation\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_adaptive_module.forward","title":"<code>forward(x, sv_kernel_feature)</code>","text":"<p>Forward pass for the spatially adaptive module.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>            Input data tensor.\n            Dimension: (1, C, H, W)\n</code></pre> </li> <li> <code>sv_kernel_feature</code>           \u2013            <pre><code>            Spatially varying kernel features.\n            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Combined output tensor from standard and spatially adaptive convolutions. Dimension: (1, output_channels, H_out, W_out)</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x, sv_kernel_feature):\n    \"\"\"\n    Forward pass for the spatially adaptive module.\n\n    Parameters\n    ----------\n    x                  : torch.tensor\n                        Input data tensor.\n                        Dimension: (1, C, H, W)\n    sv_kernel_feature   : torch.tensor\n                        Spatially varying kernel features.\n                        Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n    Returns\n    -------\n    output             : torch.tensor\n                        Combined output tensor from standard and spatially adaptive convolutions.\n                        Dimension: (1, output_channels, H_out, W_out)\n    \"\"\"\n    # Pad input and sv_kernel_feature if necessary\n    if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n            -2) * self.stride != x.size(-2):\n        diffY = sv_kernel_feature.size(-2) % self.stride\n        diffX = sv_kernel_feature.size(-1) % self.stride\n        sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                        diffY // 2, diffY - diffY // 2))\n        diffY = x.size(-2) % self.stride\n        diffX = x.size(-1) % self.stride\n        x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                        diffY // 2, diffY - diffY // 2))\n\n    # Unfold the input tensor for matrix multiplication\n    input_feature = torch.nn.functional.unfold(\n                                               x,\n                                               kernel_size = (self.kernel_size, self.kernel_size),\n                                               stride = self.stride,\n                                               padding = self.padding\n                                              )\n\n    # Resize sv_kernel_feature to match the input feature\n    sv_kernel = sv_kernel_feature.reshape(\n                                          1,\n                                          self.input_channels * self.kernel_size * self.kernel_size,\n                                          (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                         )\n\n    # Apply sv_kernel to the input_feature\n    sv_feature = input_feature * sv_kernel\n\n    # Original spatially varying convolution output\n    sv_output = torch.sum(sv_feature, dim = 1).reshape(\n                                                       1,\n                                                        1,\n                                                        (x.size(-2) // self.stride),\n                                                        (x.size(-1) // self.stride)\n                                                       )\n\n    # Reshape weight for spatially adaptive convolution\n    si_kernel = self.weight.reshape(\n                                    self.weight_output_channels,\n                                    self.input_channels * self.kernel_size * self.kernel_size\n                                   )\n\n    # Apply si_kernel on sv convolution output\n    sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                            1, self.weight_output_channels,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n\n    # Combine the outputs and apply activation function\n    output = self.activation(torch.cat((sv_output, sa_output), dim = 1))\n    return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_adaptive_unet","title":"<code>spatially_adaptive_unet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatially varying U-Net model based on spatially adaptive convolution.</p> References <p>Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class spatially_adaptive_unet(torch.nn.Module):\n    \"\"\"\n    Spatially varying U-Net model based on spatially adaptive convolution.\n\n    References\n    ----------\n\n    Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.\n    \"\"\"\n    def __init__(\n                 self,\n                 depth=3,\n                 dimensions=8,\n                 input_channels=6,\n                 out_channels=6,\n                 kernel_size=3,\n                 bias=True,\n                 normalization=False,\n                 activation=torch.nn.LeakyReLU(0.2, inplace=True)\n                ):\n        \"\"\"\n        U-Net model.\n\n        Parameters\n        ----------\n        depth          : int\n                         Number of upsampling and downsampling layers.\n        dimensions     : int\n                         Number of dimensions.\n        input_channels : int\n                         Number of input channels.\n        out_channels   : int\n                         Number of output channels.\n        bias           : bool\n                         Set to True to let convolutional layers learn a bias term.\n        normalization  : bool\n                         If True, adds a Batch Normalization layer after the convolutional layer.\n        activation     : torch.nn\n                         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n        \"\"\"\n        super().__init__()\n        self.depth = depth\n        self.out_channels = out_channels\n        self.inc = convolution_layer(\n                                     input_channels=input_channels,\n                                     output_channels=dimensions,\n                                     kernel_size=kernel_size,\n                                     bias=bias,\n                                     normalization=normalization,\n                                     activation=activation\n                                    )\n\n        self.encoder = torch.nn.ModuleList()\n        for i in range(self.depth + 1):  # Downsampling layers\n            down_in_channels = dimensions * (2 ** i)\n            down_out_channels = 2 * down_in_channels\n            pooling_layer = torch.nn.AvgPool2d(2)\n            double_convolution_layer = double_convolution(\n                                                          input_channels=down_in_channels,\n                                                          mid_channels=down_in_channels,\n                                                          output_channels=down_in_channels,\n                                                          kernel_size=kernel_size,\n                                                          bias=bias,\n                                                          normalization=normalization,\n                                                          activation=activation\n                                                         )\n            sam = spatially_adaptive_module(\n                                            input_channels=down_in_channels,\n                                            output_channels=down_out_channels,\n                                            kernel_size=kernel_size,\n                                            bias=bias,\n                                            activation=activation\n                                           )\n            self.encoder.append(torch.nn.ModuleList([pooling_layer, double_convolution_layer, sam]))\n        self.global_feature_module = torch.nn.ModuleList()\n        double_convolution_layer = double_convolution(\n                                                      input_channels=dimensions * (2 ** (depth + 1)),\n                                                      mid_channels=dimensions * (2 ** (depth + 1)),\n                                                      output_channels=dimensions * (2 ** (depth + 1)),\n                                                      kernel_size=kernel_size,\n                                                      bias=bias,\n                                                      normalization=normalization,\n                                                      activation=activation\n                                                     )\n        global_feature_layer = global_feature_module(\n                                                     input_channels=dimensions * (2 ** (depth + 1)),\n                                                     mid_channels=dimensions * (2 ** (depth + 1)),\n                                                     output_channels=dimensions * (2 ** (depth + 1)),\n                                                     kernel_size=kernel_size,\n                                                     bias=bias,\n                                                     activation=torch.nn.LeakyReLU(0.2, inplace=True)\n                                                    )\n        self.global_feature_module.append(torch.nn.ModuleList([double_convolution_layer, global_feature_layer]))\n        self.decoder = torch.nn.ModuleList()\n        for i in range(depth, -1, -1):\n            up_in_channels = dimensions * (2 ** (i + 1))\n            up_mid_channels = up_in_channels // 2\n            if i == 0:\n                up_out_channels = self.out_channels\n                upsample_layer = upsample_convtranspose2d_layer(\n                                                                input_channels=up_in_channels,\n                                                                output_channels=up_mid_channels,\n                                                                kernel_size=2,\n                                                                stride=2,\n                                                                bias=bias,\n                                                               )\n                conv_layer = torch.nn.Sequential(\n                    convolution_layer(\n                                      input_channels=up_mid_channels,\n                                      output_channels=up_mid_channels,\n                                      kernel_size=kernel_size,\n                                      bias=bias,\n                                      normalization=normalization,\n                                      activation=activation,\n                                     ),\n                    convolution_layer(\n                                      input_channels=up_mid_channels,\n                                      output_channels=up_out_channels,\n                                      kernel_size=1,\n                                      bias=bias,\n                                      normalization=normalization,\n                                      activation=None,\n                                     )\n                )\n                self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n            else:\n                up_out_channels = up_in_channels // 2\n                upsample_layer = upsample_convtranspose2d_layer(\n                                                                input_channels=up_in_channels,\n                                                                output_channels=up_mid_channels,\n                                                                kernel_size=2,\n                                                                stride=2,\n                                                                bias=bias,\n                                                               )\n                conv_layer = double_convolution(\n                                                input_channels=up_mid_channels,\n                                                mid_channels=up_mid_channels,\n                                                output_channels=up_out_channels,\n                                                kernel_size=kernel_size,\n                                                bias=bias,\n                                                normalization=normalization,\n                                                activation=activation,\n                                               )\n                self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n\n\n    def forward(self, sv_kernel, field):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        sv_kernel : list of torch.tensor\n                    Learned spatially varying kernels.\n                    Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                    where C_i, H_i, and W_i represent the channel, height, and width\n                    of each feature at a certain scale.\n\n        field     : torch.tensor\n                    Input field data.\n                    Dimension: (1, 6, H, W)\n\n        Returns\n        -------\n        target_field : torch.tensor\n                       Estimated output.\n                       Dimension: (1, 6, H, W)\n        \"\"\"\n        x = self.inc(field)\n        downsampling_outputs = [x]\n        for i, down_layer in enumerate(self.encoder):\n            x_down = down_layer[0](downsampling_outputs[-1])\n            downsampling_outputs.append(x_down)\n            sam_output = down_layer[2](x_down + down_layer[1](x_down), sv_kernel[self.depth - i])\n            downsampling_outputs.append(sam_output)\n        global_feature = self.global_feature_module[0][0](downsampling_outputs[-1])\n        global_feature = self.global_feature_module[0][1](downsampling_outputs[-1], global_feature)\n        downsampling_outputs.append(global_feature)\n        x_up = downsampling_outputs[-1]\n        for i, up_layer in enumerate(self.decoder):\n            x_up = up_layer[0](x_up, downsampling_outputs[2 * (self.depth - i)])\n            x_up = up_layer[1](x_up)\n        result = x_up\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_adaptive_unet.__init__","title":"<code>__init__(depth=3, dimensions=8, input_channels=6, out_channels=6, kernel_size=3, bias=True, normalization=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>U-Net model.</p> <p>Parameters:</p> <ul> <li> <code>depth</code>           \u2013            <pre><code>         Number of upsampling and downsampling layers.\n</code></pre> </li> <li> <code>dimensions</code>           \u2013            <pre><code>         Number of dimensions.\n</code></pre> </li> <li> <code>input_channels</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <pre><code>         Number of input channels.\n</code></pre> </li> <li> <code>out_channels</code>           \u2013            <pre><code>         Number of output channels.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>         Set to True to let convolutional layers learn a bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>         If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(\n             self,\n             depth=3,\n             dimensions=8,\n             input_channels=6,\n             out_channels=6,\n             kernel_size=3,\n             bias=True,\n             normalization=False,\n             activation=torch.nn.LeakyReLU(0.2, inplace=True)\n            ):\n    \"\"\"\n    U-Net model.\n\n    Parameters\n    ----------\n    depth          : int\n                     Number of upsampling and downsampling layers.\n    dimensions     : int\n                     Number of dimensions.\n    input_channels : int\n                     Number of input channels.\n    out_channels   : int\n                     Number of output channels.\n    bias           : bool\n                     Set to True to let convolutional layers learn a bias term.\n    normalization  : bool\n                     If True, adds a Batch Normalization layer after the convolutional layer.\n    activation     : torch.nn\n                     Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n    \"\"\"\n    super().__init__()\n    self.depth = depth\n    self.out_channels = out_channels\n    self.inc = convolution_layer(\n                                 input_channels=input_channels,\n                                 output_channels=dimensions,\n                                 kernel_size=kernel_size,\n                                 bias=bias,\n                                 normalization=normalization,\n                                 activation=activation\n                                )\n\n    self.encoder = torch.nn.ModuleList()\n    for i in range(self.depth + 1):  # Downsampling layers\n        down_in_channels = dimensions * (2 ** i)\n        down_out_channels = 2 * down_in_channels\n        pooling_layer = torch.nn.AvgPool2d(2)\n        double_convolution_layer = double_convolution(\n                                                      input_channels=down_in_channels,\n                                                      mid_channels=down_in_channels,\n                                                      output_channels=down_in_channels,\n                                                      kernel_size=kernel_size,\n                                                      bias=bias,\n                                                      normalization=normalization,\n                                                      activation=activation\n                                                     )\n        sam = spatially_adaptive_module(\n                                        input_channels=down_in_channels,\n                                        output_channels=down_out_channels,\n                                        kernel_size=kernel_size,\n                                        bias=bias,\n                                        activation=activation\n                                       )\n        self.encoder.append(torch.nn.ModuleList([pooling_layer, double_convolution_layer, sam]))\n    self.global_feature_module = torch.nn.ModuleList()\n    double_convolution_layer = double_convolution(\n                                                  input_channels=dimensions * (2 ** (depth + 1)),\n                                                  mid_channels=dimensions * (2 ** (depth + 1)),\n                                                  output_channels=dimensions * (2 ** (depth + 1)),\n                                                  kernel_size=kernel_size,\n                                                  bias=bias,\n                                                  normalization=normalization,\n                                                  activation=activation\n                                                 )\n    global_feature_layer = global_feature_module(\n                                                 input_channels=dimensions * (2 ** (depth + 1)),\n                                                 mid_channels=dimensions * (2 ** (depth + 1)),\n                                                 output_channels=dimensions * (2 ** (depth + 1)),\n                                                 kernel_size=kernel_size,\n                                                 bias=bias,\n                                                 activation=torch.nn.LeakyReLU(0.2, inplace=True)\n                                                )\n    self.global_feature_module.append(torch.nn.ModuleList([double_convolution_layer, global_feature_layer]))\n    self.decoder = torch.nn.ModuleList()\n    for i in range(depth, -1, -1):\n        up_in_channels = dimensions * (2 ** (i + 1))\n        up_mid_channels = up_in_channels // 2\n        if i == 0:\n            up_out_channels = self.out_channels\n            upsample_layer = upsample_convtranspose2d_layer(\n                                                            input_channels=up_in_channels,\n                                                            output_channels=up_mid_channels,\n                                                            kernel_size=2,\n                                                            stride=2,\n                                                            bias=bias,\n                                                           )\n            conv_layer = torch.nn.Sequential(\n                convolution_layer(\n                                  input_channels=up_mid_channels,\n                                  output_channels=up_mid_channels,\n                                  kernel_size=kernel_size,\n                                  bias=bias,\n                                  normalization=normalization,\n                                  activation=activation,\n                                 ),\n                convolution_layer(\n                                  input_channels=up_mid_channels,\n                                  output_channels=up_out_channels,\n                                  kernel_size=1,\n                                  bias=bias,\n                                  normalization=normalization,\n                                  activation=None,\n                                 )\n            )\n            self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n        else:\n            up_out_channels = up_in_channels // 2\n            upsample_layer = upsample_convtranspose2d_layer(\n                                                            input_channels=up_in_channels,\n                                                            output_channels=up_mid_channels,\n                                                            kernel_size=2,\n                                                            stride=2,\n                                                            bias=bias,\n                                                           )\n            conv_layer = double_convolution(\n                                            input_channels=up_mid_channels,\n                                            mid_channels=up_mid_channels,\n                                            output_channels=up_out_channels,\n                                            kernel_size=kernel_size,\n                                            bias=bias,\n                                            normalization=normalization,\n                                            activation=activation,\n                                           )\n            self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_adaptive_unet.forward","title":"<code>forward(sv_kernel, field)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>sv_kernel</code>               (<code>list of torch.tensor</code>)           \u2013            <pre><code>    Learned spatially varying kernels.\n    Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n    where C_i, H_i, and W_i represent the channel, height, and width\n    of each feature at a certain scale.\n</code></pre> </li> <li> <code>field</code>           \u2013            <pre><code>    Input field data.\n    Dimension: (1, 6, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>target_field</code> (              <code>tensor</code> )          \u2013            <p>Estimated output. Dimension: (1, 6, H, W)</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, sv_kernel, field):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    sv_kernel : list of torch.tensor\n                Learned spatially varying kernels.\n                Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                where C_i, H_i, and W_i represent the channel, height, and width\n                of each feature at a certain scale.\n\n    field     : torch.tensor\n                Input field data.\n                Dimension: (1, 6, H, W)\n\n    Returns\n    -------\n    target_field : torch.tensor\n                   Estimated output.\n                   Dimension: (1, 6, H, W)\n    \"\"\"\n    x = self.inc(field)\n    downsampling_outputs = [x]\n    for i, down_layer in enumerate(self.encoder):\n        x_down = down_layer[0](downsampling_outputs[-1])\n        downsampling_outputs.append(x_down)\n        sam_output = down_layer[2](x_down + down_layer[1](x_down), sv_kernel[self.depth - i])\n        downsampling_outputs.append(sam_output)\n    global_feature = self.global_feature_module[0][0](downsampling_outputs[-1])\n    global_feature = self.global_feature_module[0][1](downsampling_outputs[-1], global_feature)\n    downsampling_outputs.append(global_feature)\n    x_up = downsampling_outputs[-1]\n    for i, up_layer in enumerate(self.decoder):\n        x_up = up_layer[0](x_up, downsampling_outputs[2 * (self.depth - i)])\n        x_up = up_layer[1](x_up)\n    result = x_up\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_varying_kernel_generation_model","title":"<code>spatially_varying_kernel_generation_model</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatially_varying_kernel_generation_model revised from RSGUnet: https://github.com/MTLab/rsgunet_image_enhance.</p> <p>Refer to: J. Huang, P. Zhu, M. Geng et al. Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class spatially_varying_kernel_generation_model(torch.nn.Module):\n    \"\"\"\n    Spatially_varying_kernel_generation_model revised from RSGUnet:\n    https://github.com/MTLab/rsgunet_image_enhance.\n\n    Refer to:\n    J. Huang, P. Zhu, M. Geng et al. Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.\n    \"\"\"\n\n    def __init__(\n                 self,\n                 depth = 3,\n                 dimensions = 8,\n                 input_channels = 7,\n                 kernel_size = 3,\n                 bias = True,\n                 normalization = False,\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        U-Net model.\n\n        Parameters\n        ----------\n        depth          : int\n                         Number of upsampling and downsampling layers.\n        dimensions     : int\n                         Number of dimensions.\n        input_channels : int\n                         Number of input channels.\n        bias           : bool\n                         Set to True to let convolutional layers learn a bias term.\n        normalization  : bool\n                         If True, adds a Batch Normalization layer after the convolutional layer.\n        activation     : torch.nn\n                         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n        \"\"\"\n        super().__init__()\n        self.depth = depth\n        self.inc = convolution_layer(\n                                     input_channels = input_channels,\n                                     output_channels = dimensions,\n                                     kernel_size = kernel_size,\n                                     bias = bias,\n                                     normalization = normalization,\n                                     activation = activation\n                                    )\n        self.encoder = torch.nn.ModuleList()\n        for i in range(depth + 1):  # downsampling layers\n            if i == 0:\n                in_channels = dimensions * (2 ** i)\n                out_channels = dimensions * (2 ** i)\n            elif i == depth:\n                in_channels = dimensions * (2 ** (i - 1))\n                out_channels = dimensions * (2 ** (i - 1))\n            else:\n                in_channels = dimensions * (2 ** (i - 1))\n                out_channels = 2 * in_channels\n            pooling_layer = torch.nn.AvgPool2d(2)\n            double_convolution_layer = double_convolution(\n                                                          input_channels = in_channels,\n                                                          mid_channels = in_channels,\n                                                          output_channels = out_channels,\n                                                          kernel_size = kernel_size,\n                                                          bias = bias,\n                                                          normalization = normalization,\n                                                          activation = activation\n                                                         )\n            self.encoder.append(pooling_layer)\n            self.encoder.append(double_convolution_layer)\n        self.spatially_varying_feature = torch.nn.ModuleList()  # for kernel generation\n        for i in range(depth, -1, -1):\n            if i == 1:\n                svf_in_channels = dimensions + 2 ** (self.depth + i) + 1\n            else:\n                svf_in_channels = 2 ** (self.depth + i) + 1\n            svf_out_channels = (2 ** (self.depth + i)) * (kernel_size * kernel_size)\n            svf_mid_channels = dimensions * (2 ** (self.depth - 1))\n            spatially_varying_kernel_generation = torch.nn.ModuleList()\n            for j in range(i, -1, -1):\n                pooling_layer = torch.nn.AvgPool2d(2 ** (j + 1))\n                spatially_varying_kernel_generation.append(pooling_layer)\n            kernel_generation_block = torch.nn.Sequential(\n                torch.nn.Conv2d(\n                                in_channels = svf_in_channels,\n                                out_channels = svf_mid_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               ),\n                activation,\n                torch.nn.Conv2d(\n                                in_channels = svf_mid_channels,\n                                out_channels = svf_mid_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               ),\n                activation,\n                torch.nn.Conv2d(\n                                in_channels = svf_mid_channels,\n                                out_channels = svf_out_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               ),\n            )\n            spatially_varying_kernel_generation.append(kernel_generation_block)\n            self.spatially_varying_feature.append(spatially_varying_kernel_generation)\n        self.decoder = torch.nn.ModuleList()\n        global_feature_layer = global_feature_module(  # global feature layer\n                                                     input_channels = dimensions * (2 ** (depth - 1)),\n                                                     mid_channels = dimensions * (2 ** (depth - 1)),\n                                                     output_channels = dimensions * (2 ** (depth - 1)),\n                                                     kernel_size = kernel_size,\n                                                     bias = bias,\n                                                     activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                                                    )\n        self.decoder.append(global_feature_layer)\n        for i in range(depth, 0, -1):\n            if i == 2:\n                up_in_channels = (dimensions // 2) * (2 ** i)\n                up_out_channels = up_in_channels\n                up_mid_channels = up_in_channels\n            elif i == 1:\n                up_in_channels = dimensions * 2\n                up_out_channels = dimensions\n                up_mid_channels = up_out_channels\n            else:\n                up_in_channels = (dimensions // 2) * (2 ** i)\n                up_out_channels = up_in_channels // 2\n                up_mid_channels = up_in_channels\n            upsample_layer = upsample_convtranspose2d_layer(\n                                                            input_channels = up_in_channels,\n                                                            output_channels = up_mid_channels,\n                                                            kernel_size = 2,\n                                                            stride = 2,\n                                                            bias = bias,\n                                                           )\n            conv_layer = double_convolution(\n                                            input_channels = up_mid_channels,\n                                            output_channels = up_out_channels,\n                                            kernel_size = kernel_size,\n                                            bias = bias,\n                                            normalization = normalization,\n                                            activation = activation,\n                                           )\n            self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n\n\n    def forward(self, focal_surface, field):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        focal_surface : torch.tensor\n                        Input focal surface data.\n                        Dimension: (1, 1, H, W)\n\n        field         : torch.tensor\n                        Input field data.\n                        Dimension: (1, 6, H, W)\n\n        Returns\n        -------\n        sv_kernel : list of torch.tensor\n                    Learned spatially varying kernels.\n                    Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                    where C_i, H_i, and W_i represent the channel, height, and width\n                    of each feature at a certain scale.\n        \"\"\"\n        x = self.inc(torch.cat((focal_surface, field), dim = 1))\n        downsampling_outputs = [focal_surface]\n        downsampling_outputs.append(x)\n        for i, down_layer in enumerate(self.encoder):\n            x_down = down_layer(downsampling_outputs[-1])\n            downsampling_outputs.append(x_down)\n        sv_kernels = []\n        for i, (up_layer, svf_layer) in enumerate(zip(self.decoder, self.spatially_varying_feature)):\n            if i == 0:\n                global_feature = up_layer(downsampling_outputs[-2], downsampling_outputs[-1])\n                downsampling_outputs[-1] = global_feature\n                sv_feature = [global_feature, downsampling_outputs[0]]\n                for j in range(self.depth - i + 1):\n                    sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                    if j &gt; 0:\n                        sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n                sv_feature = [sv_feature[0], sv_feature[1], sv_feature[4], sv_feature[2],\n                              sv_feature[3]]\n                sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n                sv_kernels.append(sv_kernel)\n            else:\n                x_up = up_layer[0](downsampling_outputs[-1],\n                                   downsampling_outputs[2 * (self.depth + 1 - i) + 1])\n                x_up = up_layer[1](x_up)\n                downsampling_outputs[-1] = x_up\n                sv_feature = [x_up, downsampling_outputs[0]]\n                for j in range(self.depth - i + 1):\n                    sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                    if j &gt; 0:\n                        sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n                if i == 1:\n                    sv_feature = [sv_feature[0], sv_feature[1], sv_feature[3], sv_feature[2]]\n                sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n                sv_kernels.append(sv_kernel)\n        return sv_kernels\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_varying_kernel_generation_model.__init__","title":"<code>__init__(depth=3, dimensions=8, input_channels=7, kernel_size=3, bias=True, normalization=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>U-Net model.</p> <p>Parameters:</p> <ul> <li> <code>depth</code>           \u2013            <pre><code>         Number of upsampling and downsampling layers.\n</code></pre> </li> <li> <code>dimensions</code>           \u2013            <pre><code>         Number of dimensions.\n</code></pre> </li> <li> <code>input_channels</code>               (<code>int</code>, default:                   <code>7</code> )           \u2013            <pre><code>         Number of input channels.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>         Set to True to let convolutional layers learn a bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>         If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(\n             self,\n             depth = 3,\n             dimensions = 8,\n             input_channels = 7,\n             kernel_size = 3,\n             bias = True,\n             normalization = False,\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    U-Net model.\n\n    Parameters\n    ----------\n    depth          : int\n                     Number of upsampling and downsampling layers.\n    dimensions     : int\n                     Number of dimensions.\n    input_channels : int\n                     Number of input channels.\n    bias           : bool\n                     Set to True to let convolutional layers learn a bias term.\n    normalization  : bool\n                     If True, adds a Batch Normalization layer after the convolutional layer.\n    activation     : torch.nn\n                     Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n    \"\"\"\n    super().__init__()\n    self.depth = depth\n    self.inc = convolution_layer(\n                                 input_channels = input_channels,\n                                 output_channels = dimensions,\n                                 kernel_size = kernel_size,\n                                 bias = bias,\n                                 normalization = normalization,\n                                 activation = activation\n                                )\n    self.encoder = torch.nn.ModuleList()\n    for i in range(depth + 1):  # downsampling layers\n        if i == 0:\n            in_channels = dimensions * (2 ** i)\n            out_channels = dimensions * (2 ** i)\n        elif i == depth:\n            in_channels = dimensions * (2 ** (i - 1))\n            out_channels = dimensions * (2 ** (i - 1))\n        else:\n            in_channels = dimensions * (2 ** (i - 1))\n            out_channels = 2 * in_channels\n        pooling_layer = torch.nn.AvgPool2d(2)\n        double_convolution_layer = double_convolution(\n                                                      input_channels = in_channels,\n                                                      mid_channels = in_channels,\n                                                      output_channels = out_channels,\n                                                      kernel_size = kernel_size,\n                                                      bias = bias,\n                                                      normalization = normalization,\n                                                      activation = activation\n                                                     )\n        self.encoder.append(pooling_layer)\n        self.encoder.append(double_convolution_layer)\n    self.spatially_varying_feature = torch.nn.ModuleList()  # for kernel generation\n    for i in range(depth, -1, -1):\n        if i == 1:\n            svf_in_channels = dimensions + 2 ** (self.depth + i) + 1\n        else:\n            svf_in_channels = 2 ** (self.depth + i) + 1\n        svf_out_channels = (2 ** (self.depth + i)) * (kernel_size * kernel_size)\n        svf_mid_channels = dimensions * (2 ** (self.depth - 1))\n        spatially_varying_kernel_generation = torch.nn.ModuleList()\n        for j in range(i, -1, -1):\n            pooling_layer = torch.nn.AvgPool2d(2 ** (j + 1))\n            spatially_varying_kernel_generation.append(pooling_layer)\n        kernel_generation_block = torch.nn.Sequential(\n            torch.nn.Conv2d(\n                            in_channels = svf_in_channels,\n                            out_channels = svf_mid_channels,\n                            kernel_size = kernel_size,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           ),\n            activation,\n            torch.nn.Conv2d(\n                            in_channels = svf_mid_channels,\n                            out_channels = svf_mid_channels,\n                            kernel_size = kernel_size,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           ),\n            activation,\n            torch.nn.Conv2d(\n                            in_channels = svf_mid_channels,\n                            out_channels = svf_out_channels,\n                            kernel_size = kernel_size,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           ),\n        )\n        spatially_varying_kernel_generation.append(kernel_generation_block)\n        self.spatially_varying_feature.append(spatially_varying_kernel_generation)\n    self.decoder = torch.nn.ModuleList()\n    global_feature_layer = global_feature_module(  # global feature layer\n                                                 input_channels = dimensions * (2 ** (depth - 1)),\n                                                 mid_channels = dimensions * (2 ** (depth - 1)),\n                                                 output_channels = dimensions * (2 ** (depth - 1)),\n                                                 kernel_size = kernel_size,\n                                                 bias = bias,\n                                                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                                                )\n    self.decoder.append(global_feature_layer)\n    for i in range(depth, 0, -1):\n        if i == 2:\n            up_in_channels = (dimensions // 2) * (2 ** i)\n            up_out_channels = up_in_channels\n            up_mid_channels = up_in_channels\n        elif i == 1:\n            up_in_channels = dimensions * 2\n            up_out_channels = dimensions\n            up_mid_channels = up_out_channels\n        else:\n            up_in_channels = (dimensions // 2) * (2 ** i)\n            up_out_channels = up_in_channels // 2\n            up_mid_channels = up_in_channels\n        upsample_layer = upsample_convtranspose2d_layer(\n                                                        input_channels = up_in_channels,\n                                                        output_channels = up_mid_channels,\n                                                        kernel_size = 2,\n                                                        stride = 2,\n                                                        bias = bias,\n                                                       )\n        conv_layer = double_convolution(\n                                        input_channels = up_mid_channels,\n                                        output_channels = up_out_channels,\n                                        kernel_size = kernel_size,\n                                        bias = bias,\n                                        normalization = normalization,\n                                        activation = activation,\n                                       )\n        self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.spatially_varying_kernel_generation_model.forward","title":"<code>forward(focal_surface, field)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>focal_surface</code>               (<code>tensor</code>)           \u2013            <pre><code>        Input focal surface data.\n        Dimension: (1, 1, H, W)\n</code></pre> </li> <li> <code>field</code>           \u2013            <pre><code>        Input field data.\n        Dimension: (1, 6, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>sv_kernel</code> (              <code>list of torch.tensor</code> )          \u2013            <p>Learned spatially varying kernels. Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i), where C_i, H_i, and W_i represent the channel, height, and width of each feature at a certain scale.</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, focal_surface, field):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    focal_surface : torch.tensor\n                    Input focal surface data.\n                    Dimension: (1, 1, H, W)\n\n    field         : torch.tensor\n                    Input field data.\n                    Dimension: (1, 6, H, W)\n\n    Returns\n    -------\n    sv_kernel : list of torch.tensor\n                Learned spatially varying kernels.\n                Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                where C_i, H_i, and W_i represent the channel, height, and width\n                of each feature at a certain scale.\n    \"\"\"\n    x = self.inc(torch.cat((focal_surface, field), dim = 1))\n    downsampling_outputs = [focal_surface]\n    downsampling_outputs.append(x)\n    for i, down_layer in enumerate(self.encoder):\n        x_down = down_layer(downsampling_outputs[-1])\n        downsampling_outputs.append(x_down)\n    sv_kernels = []\n    for i, (up_layer, svf_layer) in enumerate(zip(self.decoder, self.spatially_varying_feature)):\n        if i == 0:\n            global_feature = up_layer(downsampling_outputs[-2], downsampling_outputs[-1])\n            downsampling_outputs[-1] = global_feature\n            sv_feature = [global_feature, downsampling_outputs[0]]\n            for j in range(self.depth - i + 1):\n                sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                if j &gt; 0:\n                    sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n            sv_feature = [sv_feature[0], sv_feature[1], sv_feature[4], sv_feature[2],\n                          sv_feature[3]]\n            sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n            sv_kernels.append(sv_kernel)\n        else:\n            x_up = up_layer[0](downsampling_outputs[-1],\n                               downsampling_outputs[2 * (self.depth + 1 - i) + 1])\n            x_up = up_layer[1](x_up)\n            downsampling_outputs[-1] = x_up\n            sv_feature = [x_up, downsampling_outputs[0]]\n            for j in range(self.depth - i + 1):\n                sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                if j &gt; 0:\n                    sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n            if i == 1:\n                sv_feature = [sv_feature[0], sv_feature[1], sv_feature[3], sv_feature[2]]\n            sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n            sv_kernels.append(sv_kernel)\n    return sv_kernels\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.unet","title":"<code>unet</code>","text":"<p>               Bases: <code>Module</code></p> <p>A U-Net model, heavily inspired from <code>https://github.com/milesial/Pytorch-UNet/tree/master/unet</code> and more can be read from Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer International Publishing, 2015.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class unet(torch.nn.Module):\n    \"\"\"\n    A U-Net model, heavily inspired from `https://github.com/milesial/Pytorch-UNet/tree/master/unet` and more can be read from Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer International Publishing, 2015.\n    \"\"\"\n\n    def __init__(\n                 self, \n                 depth = 4,\n                 dimensions = 64, \n                 input_channels = 2, \n                 output_channels = 1, \n                 bilinear = False,\n                 kernel_size = 3,\n                 bias = False,\n                 activation = torch.nn.ReLU(inplace = True),\n                ):\n        \"\"\"\n        U-Net model.\n\n        Parameters\n        ----------\n        depth             : int\n                            Number of upsampling and downsampling\n        dimensions        : int\n                            Number of dimensions.\n        input_channels    : int\n                            Number of input channels.\n        output_channels   : int\n                            Number of output channels.\n        bilinear          : bool\n                            Uses bilinear upsampling in upsampling layers when set True.\n        bias              : bool\n                            Set True to let convolutional layers learn a bias term.\n        activation        : torch.nn\n                            Non-linear activation layer to be used (e.g., torch.nn.ReLU(), torch.nn.Sigmoid().\n        \"\"\"\n        super(unet, self).__init__()\n        self.inc = double_convolution(\n                                      input_channels = input_channels,\n                                      mid_channels = dimensions,\n                                      output_channels = dimensions,\n                                      kernel_size = kernel_size,\n                                      bias = bias,\n                                      activation = activation\n                                     )      \n\n        self.downsampling_layers = torch.nn.ModuleList()\n        self.upsampling_layers = torch.nn.ModuleList()\n        for i in range(depth): # downsampling layers\n            in_channels = dimensions * (2 ** i)\n            out_channels = dimensions * (2 ** (i + 1))\n            down_layer = downsample_layer(in_channels,\n                                            out_channels,\n                                            kernel_size=kernel_size,\n                                            bias=bias,\n                                            activation=activation\n                                            )\n            self.downsampling_layers.append(down_layer)      \n\n        for i in range(depth - 1, -1, -1):  # upsampling layers\n            up_in_channels = dimensions * (2 ** (i + 1))  \n            up_out_channels = dimensions * (2 ** i) \n            up_layer = upsample_layer(up_in_channels, up_out_channels, kernel_size=kernel_size, bias=bias, activation=activation, bilinear=bilinear)\n            self.upsampling_layers.append(up_layer)\n        self.outc = torch.nn.Conv2d(\n                                    dimensions, \n                                    output_channels,\n                                    kernel_size = kernel_size,\n                                    padding = kernel_size // 2,\n                                    bias = bias\n                                   )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        downsampling_outputs = [self.inc(x)]\n        for down_layer in self.downsampling_layers:\n            x_down = down_layer(downsampling_outputs[-1])\n            downsampling_outputs.append(x_down)\n        x_up = downsampling_outputs[-1]\n        for i, up_layer in enumerate((self.upsampling_layers)):\n            x_up = up_layer(x_up, downsampling_outputs[-(i + 2)])       \n        result = self.outc(x_up)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.unet.__init__","title":"<code>__init__(depth=4, dimensions=64, input_channels=2, output_channels=1, bilinear=False, kernel_size=3, bias=False, activation=torch.nn.ReLU(inplace=True))</code>","text":"<p>U-Net model.</p> <p>Parameters:</p> <ul> <li> <code>depth</code>           \u2013            <pre><code>            Number of upsampling and downsampling\n</code></pre> </li> <li> <code>dimensions</code>           \u2013            <pre><code>            Number of dimensions.\n</code></pre> </li> <li> <code>input_channels</code>           \u2013            <pre><code>            Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>           \u2013            <pre><code>            Number of output channels.\n</code></pre> </li> <li> <code>bilinear</code>           \u2013            <pre><code>            Uses bilinear upsampling in upsampling layers when set True.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>            Set True to let convolutional layers learn a bias term.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>            Non-linear activation layer to be used (e.g., torch.nn.ReLU(), torch.nn.Sigmoid().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(\n             self, \n             depth = 4,\n             dimensions = 64, \n             input_channels = 2, \n             output_channels = 1, \n             bilinear = False,\n             kernel_size = 3,\n             bias = False,\n             activation = torch.nn.ReLU(inplace = True),\n            ):\n    \"\"\"\n    U-Net model.\n\n    Parameters\n    ----------\n    depth             : int\n                        Number of upsampling and downsampling\n    dimensions        : int\n                        Number of dimensions.\n    input_channels    : int\n                        Number of input channels.\n    output_channels   : int\n                        Number of output channels.\n    bilinear          : bool\n                        Uses bilinear upsampling in upsampling layers when set True.\n    bias              : bool\n                        Set True to let convolutional layers learn a bias term.\n    activation        : torch.nn\n                        Non-linear activation layer to be used (e.g., torch.nn.ReLU(), torch.nn.Sigmoid().\n    \"\"\"\n    super(unet, self).__init__()\n    self.inc = double_convolution(\n                                  input_channels = input_channels,\n                                  mid_channels = dimensions,\n                                  output_channels = dimensions,\n                                  kernel_size = kernel_size,\n                                  bias = bias,\n                                  activation = activation\n                                 )      \n\n    self.downsampling_layers = torch.nn.ModuleList()\n    self.upsampling_layers = torch.nn.ModuleList()\n    for i in range(depth): # downsampling layers\n        in_channels = dimensions * (2 ** i)\n        out_channels = dimensions * (2 ** (i + 1))\n        down_layer = downsample_layer(in_channels,\n                                        out_channels,\n                                        kernel_size=kernel_size,\n                                        bias=bias,\n                                        activation=activation\n                                        )\n        self.downsampling_layers.append(down_layer)      \n\n    for i in range(depth - 1, -1, -1):  # upsampling layers\n        up_in_channels = dimensions * (2 ** (i + 1))  \n        up_out_channels = dimensions * (2 ** i) \n        up_layer = upsample_layer(up_in_channels, up_out_channels, kernel_size=kernel_size, bias=bias, activation=activation, bilinear=bilinear)\n        self.upsampling_layers.append(up_layer)\n    self.outc = torch.nn.Conv2d(\n                                dimensions, \n                                output_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.unet.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    downsampling_outputs = [self.inc(x)]\n    for down_layer in self.downsampling_layers:\n        x_down = down_layer(downsampling_outputs[-1])\n        downsampling_outputs.append(x_down)\n    x_up = downsampling_outputs[-1]\n    for i, up_layer in enumerate((self.upsampling_layers)):\n        x_up = up_layer(x_up, downsampling_outputs[-(i + 2)])       \n    result = self.outc(x_up)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.upsample_convtranspose2d_layer","title":"<code>upsample_convtranspose2d_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>An upsampling convtranspose2d layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class upsample_convtranspose2d_layer(torch.nn.Module):\n    \"\"\"\n    An upsampling convtranspose2d layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 2,\n                 stride = 2,\n                 bias = False,\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        \"\"\"\n        super().__init__()\n        self.up = torch.nn.ConvTranspose2d(\n                                           in_channels = input_channels,\n                                           out_channels = output_channels,\n                                           bias = bias,\n                                           kernel_size = kernel_size,\n                                           stride = stride\n                                          )\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Result of the forward operation\n        \"\"\"\n        x1 = self.up(x1)\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                          diffY // 2, diffY - diffY // 2])\n        result = x1 + x2\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.upsample_convtranspose2d_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=2, stride=2, bias=False)</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 2,\n             stride = 2,\n             bias = False,\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    \"\"\"\n    super().__init__()\n    self.up = torch.nn.ConvTranspose2d(\n                                       in_channels = input_channels,\n                                       out_channels = output_channels,\n                                       bias = bias,\n                                       kernel_size = kernel_size,\n                                       stride = stride\n                                      )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.upsample_convtranspose2d_layer.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Result of the forward operation\n    \"\"\"\n    x1 = self.up(x1)\n    diffY = x2.size()[2] - x1.size()[2]\n    diffX = x2.size()[3] - x1.size()[3]\n    x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                      diffY // 2, diffY - diffY // 2])\n    result = x1 + x2\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.upsample_layer","title":"<code>upsample_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>An upsampling convolutional layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class upsample_layer(torch.nn.Module):\n    \"\"\"\n    An upsampling convolutional layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU(),\n                 bilinear = True\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        bilinear        : bool\n                          If set to True, bilinear sampling is used.\n        \"\"\"\n        super(upsample_layer, self).__init__()\n        if bilinear:\n            self.up = torch.nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n            self.conv = double_convolution(\n                                           input_channels = input_channels + output_channels,\n                                           mid_channels = input_channels // 2,\n                                           output_channels = output_channels,\n                                           kernel_size = kernel_size,\n                                           normalization = normalization,\n                                           bias = bias,\n                                           activation = activation\n                                          )\n        else:\n            self.up = torch.nn.ConvTranspose2d(input_channels , input_channels // 2, kernel_size = 2, stride = 2)\n            self.conv = double_convolution(\n                                           input_channels = input_channels,\n                                           mid_channels = output_channels,\n                                           output_channels = output_channels,\n                                           kernel_size = kernel_size,\n                                           normalization = normalization,\n                                           bias = bias,\n                                           activation = activation\n                                          )\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Result of the forward operation\n        \"\"\" \n        x1 = self.up(x1)\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                          diffY // 2, diffY - diffY // 2])\n        x = torch.cat([x2, x1], dim = 1)\n        result = self.conv(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.upsample_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU(), bilinear=True)</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> <li> <code>bilinear</code>           \u2013            <pre><code>          If set to True, bilinear sampling is used.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 3,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU(),\n             bilinear = True\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    bilinear        : bool\n                      If set to True, bilinear sampling is used.\n    \"\"\"\n    super(upsample_layer, self).__init__()\n    if bilinear:\n        self.up = torch.nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n        self.conv = double_convolution(\n                                       input_channels = input_channels + output_channels,\n                                       mid_channels = input_channels // 2,\n                                       output_channels = output_channels,\n                                       kernel_size = kernel_size,\n                                       normalization = normalization,\n                                       bias = bias,\n                                       activation = activation\n                                      )\n    else:\n        self.up = torch.nn.ConvTranspose2d(input_channels , input_channels // 2, kernel_size = 2, stride = 2)\n        self.conv = double_convolution(\n                                       input_channels = input_channels,\n                                       mid_channels = output_channels,\n                                       output_channels = output_channels,\n                                       kernel_size = kernel_size,\n                                       normalization = normalization,\n                                       bias = bias,\n                                       activation = activation\n                                      )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.upsample_layer.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Result of the forward operation\n    \"\"\" \n    x1 = self.up(x1)\n    diffY = x2.size()[2] - x1.size()[2]\n    diffX = x2.size()[3] - x1.size()[3]\n    x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                      diffY // 2, diffY - diffY // 2])\n    x = torch.cat([x2, x1], dim = 1)\n    result = self.conv(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.gaussian","title":"<code>gaussian(x, multiplier=1.0)</code>","text":"<p>A Gaussian non-linear activation. For more details: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input data.\n</code></pre> </li> <li> <code>multiplier</code>           \u2013            <pre><code>       Multiplier.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>float or tensor</code> )          \u2013            <p>Ouput data.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def gaussian(x, multiplier = 1.):\n    \"\"\"\n    A Gaussian non-linear activation.\n    For more details: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n\n    Parameters\n    ----------\n    x            : float or torch.tensor\n                   Input data.\n    multiplier   : float or torch.tensor\n                   Multiplier.\n\n    Returns\n    -------\n    result       : float or torch.tensor\n                   Ouput data.\n    \"\"\"\n    result = torch.exp(- (multiplier * x) ** 2)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.swish","title":"<code>swish(x)</code>","text":"<p>A swish non-linear activation. For more details: https://en.wikipedia.org/wiki/Swish_function</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>         Input.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>float or tensor</code> )          \u2013            <p>Output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def swish(x):\n    \"\"\"\n    A swish non-linear activation.\n    For more details: https://en.wikipedia.org/wiki/Swish_function\n\n    Parameters\n    -----------\n    x              : float or torch.tensor\n                     Input.\n\n    Returns\n    -------\n    out            : float or torch.tensor\n                     Output.\n    \"\"\"\n    out = x * torch.sigmoid(x)\n    return out\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.channel_gate","title":"<code>channel_gate</code>","text":"<p>               Bases: <code>Module</code></p> <p>Channel attention module with various pooling strategies. This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class channel_gate(torch.nn.Module):\n    \"\"\"\n    Channel attention module with various pooling strategies.\n    This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).\n    \"\"\"\n    def __init__(\n                 self, \n                 gate_channels, \n                 reduction_ratio = 16, \n                 pool_types = ['avg', 'max']\n                ):\n        \"\"\"\n        Initializes the channel gate module.\n\n        Parameters\n        ----------\n        gate_channels   : int\n                          Number of channels of the input feature map.\n        reduction_ratio : int\n                          Reduction ratio for the intermediate layer.\n        pool_types      : list\n                          List of pooling operations to apply.\n        \"\"\"\n        super().__init__()\n        self.gate_channels = gate_channels\n        hidden_channels = gate_channels // reduction_ratio\n        if hidden_channels == 0:\n            hidden_channels = 1\n        self.mlp = torch.nn.Sequential(\n                                       convolutional_block_attention.Flatten(),\n                                       torch.nn.Linear(gate_channels, hidden_channels),\n                                       torch.nn.ReLU(),\n                                       torch.nn.Linear(hidden_channels, gate_channels)\n                                      )\n        self.pool_types = pool_types\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the ChannelGate module.\n\n        Applies channel-wise attention to the input tensor.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the ChannelGate module.\n\n        Returns\n        -------\n        output       : torch.tensor\n                       Output tensor after applying channel attention.\n        \"\"\"\n        channel_att_sum = None\n        for pool_type in self.pool_types:\n            if pool_type == 'avg':\n                pool = torch.nn.functional.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n            elif pool_type == 'max':\n                pool = torch.nn.functional.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n            channel_att_raw = self.mlp(pool)\n            channel_att_sum = channel_att_raw if channel_att_sum is None else channel_att_sum + channel_att_raw\n        scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n        output = x * scale\n        return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.channel_gate.__init__","title":"<code>__init__(gate_channels, reduction_ratio=16, pool_types=['avg', 'max'])</code>","text":"<p>Initializes the channel gate module.</p> <p>Parameters:</p> <ul> <li> <code>gate_channels</code>           \u2013            <pre><code>          Number of channels of the input feature map.\n</code></pre> </li> <li> <code>reduction_ratio</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <pre><code>          Reduction ratio for the intermediate layer.\n</code></pre> </li> <li> <code>pool_types</code>           \u2013            <pre><code>          List of pooling operations to apply.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self, \n             gate_channels, \n             reduction_ratio = 16, \n             pool_types = ['avg', 'max']\n            ):\n    \"\"\"\n    Initializes the channel gate module.\n\n    Parameters\n    ----------\n    gate_channels   : int\n                      Number of channels of the input feature map.\n    reduction_ratio : int\n                      Reduction ratio for the intermediate layer.\n    pool_types      : list\n                      List of pooling operations to apply.\n    \"\"\"\n    super().__init__()\n    self.gate_channels = gate_channels\n    hidden_channels = gate_channels // reduction_ratio\n    if hidden_channels == 0:\n        hidden_channels = 1\n    self.mlp = torch.nn.Sequential(\n                                   convolutional_block_attention.Flatten(),\n                                   torch.nn.Linear(gate_channels, hidden_channels),\n                                   torch.nn.ReLU(),\n                                   torch.nn.Linear(hidden_channels, gate_channels)\n                                  )\n    self.pool_types = pool_types\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.channel_gate.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the ChannelGate module.</p> <p>Applies channel-wise attention to the input tensor.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the ChannelGate module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying channel attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the ChannelGate module.\n\n    Applies channel-wise attention to the input tensor.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the ChannelGate module.\n\n    Returns\n    -------\n    output       : torch.tensor\n                   Output tensor after applying channel attention.\n    \"\"\"\n    channel_att_sum = None\n    for pool_type in self.pool_types:\n        if pool_type == 'avg':\n            pool = torch.nn.functional.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n        elif pool_type == 'max':\n            pool = torch.nn.functional.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n        channel_att_raw = self.mlp(pool)\n        channel_att_sum = channel_att_raw if channel_att_sum is None else channel_att_sum + channel_att_raw\n    scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n    output = x * scale\n    return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.convolution_layer","title":"<code>convolution_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A convolution layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class convolution_layer(torch.nn.Module):\n    \"\"\"\n    A convolution layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 bias = False,\n                 stride = 1,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A convolutional layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        layers = [\n            torch.nn.Conv2d(\n                            input_channels,\n                            output_channels,\n                            kernel_size = kernel_size,\n                            stride = stride,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           )\n        ]\n        if normalization:\n            layers.append(torch.nn.BatchNorm2d(output_channels))\n        if activation:\n            layers.append(activation)\n        self.model = torch.nn.Sequential(*layers)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        result = self.model(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.convolution_layer.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, bias=False, stride=1, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A convolutional layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             bias = False,\n             stride = 1,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A convolutional layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    layers = [\n        torch.nn.Conv2d(\n                        input_channels,\n                        output_channels,\n                        kernel_size = kernel_size,\n                        stride = stride,\n                        padding = kernel_size // 2,\n                        bias = bias\n                       )\n    ]\n    if normalization:\n        layers.append(torch.nn.BatchNorm2d(output_channels))\n    if activation:\n        layers.append(activation)\n    self.model = torch.nn.Sequential(*layers)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.convolution_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    result = self.model(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.convolutional_block_attention","title":"<code>convolutional_block_attention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convolutional Block Attention Module (CBAM) class.  This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class convolutional_block_attention(torch.nn.Module):\n    \"\"\"\n    Convolutional Block Attention Module (CBAM) class. \n    This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).\n    \"\"\"\n    def __init__(\n                 self, \n                 gate_channels, \n                 reduction_ratio = 16, \n                 pool_types = ['avg', 'max'], \n                 no_spatial = False\n                ):\n        \"\"\"\n        Initializes the convolutional block attention module.\n\n        Parameters\n        ----------\n        gate_channels   : int\n                          Number of channels of the input feature map.\n        reduction_ratio : int\n                          Reduction ratio for the channel attention.\n        pool_types      : list\n                          List of pooling operations to apply for channel attention.\n        no_spatial      : bool\n                          If True, spatial attention is not applied.\n        \"\"\"\n        super(convolutional_block_attention, self).__init__()\n        self.channel_gate = channel_gate(gate_channels, reduction_ratio, pool_types)\n        self.no_spatial = no_spatial\n        if not no_spatial:\n            self.spatial_gate = spatial_gate()\n\n\n    class Flatten(torch.nn.Module):\n        \"\"\"\n        Flattens the input tensor to a 2D matrix.\n        \"\"\"\n        def forward(self, x):\n            return x.view(x.size(0), -1)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the convolutional block attention module.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the CBAM module.\n\n        Returns\n        -------\n        x_out        : torch.tensor\n                       Output tensor after applying channel and spatial attention.\n        \"\"\"\n        x_out = self.channel_gate(x)\n        if not self.no_spatial:\n            x_out = self.spatial_gate(x_out)\n        return x_out\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.convolutional_block_attention.Flatten","title":"<code>Flatten</code>","text":"<p>               Bases: <code>Module</code></p> <p>Flattens the input tensor to a 2D matrix.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class Flatten(torch.nn.Module):\n    \"\"\"\n    Flattens the input tensor to a 2D matrix.\n    \"\"\"\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.convolutional_block_attention.__init__","title":"<code>__init__(gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False)</code>","text":"<p>Initializes the convolutional block attention module.</p> <p>Parameters:</p> <ul> <li> <code>gate_channels</code>           \u2013            <pre><code>          Number of channels of the input feature map.\n</code></pre> </li> <li> <code>reduction_ratio</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <pre><code>          Reduction ratio for the channel attention.\n</code></pre> </li> <li> <code>pool_types</code>           \u2013            <pre><code>          List of pooling operations to apply for channel attention.\n</code></pre> </li> <li> <code>no_spatial</code>           \u2013            <pre><code>          If True, spatial attention is not applied.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self, \n             gate_channels, \n             reduction_ratio = 16, \n             pool_types = ['avg', 'max'], \n             no_spatial = False\n            ):\n    \"\"\"\n    Initializes the convolutional block attention module.\n\n    Parameters\n    ----------\n    gate_channels   : int\n                      Number of channels of the input feature map.\n    reduction_ratio : int\n                      Reduction ratio for the channel attention.\n    pool_types      : list\n                      List of pooling operations to apply for channel attention.\n    no_spatial      : bool\n                      If True, spatial attention is not applied.\n    \"\"\"\n    super(convolutional_block_attention, self).__init__()\n    self.channel_gate = channel_gate(gate_channels, reduction_ratio, pool_types)\n    self.no_spatial = no_spatial\n    if not no_spatial:\n        self.spatial_gate = spatial_gate()\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.convolutional_block_attention.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the convolutional block attention module.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the CBAM module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_out</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying channel and spatial attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the convolutional block attention module.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the CBAM module.\n\n    Returns\n    -------\n    x_out        : torch.tensor\n                   Output tensor after applying channel and spatial attention.\n    \"\"\"\n    x_out = self.channel_gate(x)\n    if not self.no_spatial:\n        x_out = self.spatial_gate(x_out)\n    return x_out\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.double_convolution","title":"<code>double_convolution</code>","text":"<p>               Bases: <code>Module</code></p> <p>A double convolution layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class double_convolution(torch.nn.Module):\n    \"\"\"\n    A double convolution layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 mid_channels = None,\n                 output_channels = 2,\n                 kernel_size = 3, \n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        Double convolution model.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels    : int\n                          Number of channels in the hidden layer between two convolutions.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        if isinstance(mid_channels, type(None)):\n            mid_channels = output_channels\n        self.activation = activation\n        self.model = torch.nn.Sequential(\n                                         convolution_layer(\n                                                           input_channels = input_channels,\n                                                           output_channels = mid_channels,\n                                                           kernel_size = kernel_size,\n                                                           bias = bias,\n                                                           normalization = normalization,\n                                                           activation = self.activation\n                                                          ),\n                                         convolution_layer(\n                                                           input_channels = mid_channels,\n                                                           output_channels = output_channels,\n                                                           kernel_size = kernel_size,\n                                                           bias = bias,\n                                                           normalization = normalization,\n                                                           activation = self.activation\n                                                          )\n                                        )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        result = self.model(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.double_convolution.__init__","title":"<code>__init__(input_channels=2, mid_channels=None, output_channels=2, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>Double convolution model.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of channels in the hidden layer between two convolutions.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             mid_channels = None,\n             output_channels = 2,\n             kernel_size = 3, \n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    Double convolution model.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels    : int\n                      Number of channels in the hidden layer between two convolutions.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    if isinstance(mid_channels, type(None)):\n        mid_channels = output_channels\n    self.activation = activation\n    self.model = torch.nn.Sequential(\n                                     convolution_layer(\n                                                       input_channels = input_channels,\n                                                       output_channels = mid_channels,\n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       normalization = normalization,\n                                                       activation = self.activation\n                                                      ),\n                                     convolution_layer(\n                                                       input_channels = mid_channels,\n                                                       output_channels = output_channels,\n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       normalization = normalization,\n                                                       activation = self.activation\n                                                      )\n                                    )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.double_convolution.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    result = self.model(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.downsample_layer","title":"<code>downsample_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A downscaling component followed by a double convolution.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class downsample_layer(torch.nn.Module):\n    \"\"\"\n    A downscaling component followed by a double convolution.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.maxpool_conv = torch.nn.Sequential(\n                                                torch.nn.MaxPool2d(2),\n                                                double_convolution(\n                                                                   input_channels = input_channels,\n                                                                   mid_channels = output_channels,\n                                                                   output_channels = output_channels,\n                                                                   kernel_size = kernel_size,\n                                                                   bias = bias,\n                                                                   normalization = normalization,\n                                                                   activation = activation\n                                                                  )\n                                               )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x              : torch.tensor\n                         First input data.\n\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        result = self.maxpool_conv(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.downsample_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 3,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.maxpool_conv = torch.nn.Sequential(\n                                            torch.nn.MaxPool2d(2),\n                                            double_convolution(\n                                                               input_channels = input_channels,\n                                                               mid_channels = output_channels,\n                                                               output_channels = output_channels,\n                                                               kernel_size = kernel_size,\n                                                               bias = bias,\n                                                               normalization = normalization,\n                                                               activation = activation\n                                                              )\n                                           )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.downsample_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x              : torch.tensor\n                     First input data.\n\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    result = self.maxpool_conv(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.global_feature_module","title":"<code>global_feature_module</code>","text":"<p>               Bases: <code>Module</code></p> <p>A global feature layer that processes global features from input channels and applies them to another input tensor via learned transformations.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class global_feature_module(torch.nn.Module):\n    \"\"\"\n    A global feature layer that processes global features from input channels and\n    applies them to another input tensor via learned transformations.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 mid_channels,\n                 output_channels,\n                 kernel_size,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A global feature layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels  : int\n                          Number of mid channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.transformations_1 = global_transformations(input_channels, output_channels)\n        self.global_features_1 = double_convolution(\n                                                    input_channels = input_channels,\n                                                    mid_channels = mid_channels,\n                                                    output_channels = output_channels,\n                                                    kernel_size = kernel_size,\n                                                    bias = bias,\n                                                    normalization = normalization,\n                                                    activation = activation\n                                                   )\n        self.global_features_2 = double_convolution(\n                                                    input_channels = input_channels,\n                                                    mid_channels = mid_channels,\n                                                    output_channels = output_channels,\n                                                    kernel_size = kernel_size,\n                                                    bias = bias,\n                                                    normalization = normalization,\n                                                    activation = activation\n                                                   )\n        self.transformations_2 = global_transformations(input_channels, output_channels)\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        global_tensor_1 = self.transformations_1(x1, x2)\n        y1 = self.global_features_1(global_tensor_1)\n        y2 = self.global_features_2(y1)\n        global_tensor_2 = self.transformations_2(y1, y2)\n        return global_tensor_2\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.global_feature_module.__init__","title":"<code>__init__(input_channels, mid_channels, output_channels, kernel_size, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A global feature layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of mid channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             mid_channels,\n             output_channels,\n             kernel_size,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A global feature layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels  : int\n                      Number of mid channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.transformations_1 = global_transformations(input_channels, output_channels)\n    self.global_features_1 = double_convolution(\n                                                input_channels = input_channels,\n                                                mid_channels = mid_channels,\n                                                output_channels = output_channels,\n                                                kernel_size = kernel_size,\n                                                bias = bias,\n                                                normalization = normalization,\n                                                activation = activation\n                                               )\n    self.global_features_2 = double_convolution(\n                                                input_channels = input_channels,\n                                                mid_channels = mid_channels,\n                                                output_channels = output_channels,\n                                                kernel_size = kernel_size,\n                                                bias = bias,\n                                                normalization = normalization,\n                                                activation = activation\n                                               )\n    self.transformations_2 = global_transformations(input_channels, output_channels)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.global_feature_module.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    global_tensor_1 = self.transformations_1(x1, x2)\n    y1 = self.global_features_1(global_tensor_1)\n    y2 = self.global_features_2(y1)\n    global_tensor_2 = self.transformations_2(y1, y2)\n    return global_tensor_2\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.global_transformations","title":"<code>global_transformations</code>","text":"<p>               Bases: <code>Module</code></p> <p>A global feature layer that processes global features from input channels and applies learned transformations to another input tensor.</p> <p>This implementation is adapted from RSGUnet: https://github.com/MTLab/rsgunet_image_enhance.</p> <p>Reference: J. Huang, P. Zhu, M. Geng et al. \"Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.\"</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class global_transformations(torch.nn.Module):\n    \"\"\"\n    A global feature layer that processes global features from input channels and\n    applies learned transformations to another input tensor.\n\n    This implementation is adapted from RSGUnet:\n    https://github.com/MTLab/rsgunet_image_enhance.\n\n    Reference:\n    J. Huang, P. Zhu, M. Geng et al. \"Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.\"\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels\n                ):\n        \"\"\"\n        A global feature layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        \"\"\"\n        super().__init__()\n        self.global_feature_1 = torch.nn.Sequential(\n            torch.nn.Linear(input_channels, output_channels),\n            torch.nn.LeakyReLU(0.2, inplace = True),\n        )\n        self.global_feature_2 = torch.nn.Sequential(\n            torch.nn.Linear(output_channels, output_channels),\n            torch.nn.LeakyReLU(0.2, inplace = True)\n        )\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        y = torch.mean(x2, dim = (2, 3))\n        y1 = self.global_feature_1(y)\n        y2 = self.global_feature_2(y1)\n        y1 = y1.unsqueeze(2).unsqueeze(3)\n        y2 = y2.unsqueeze(2).unsqueeze(3)\n        result = x1 * y1 + y2\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.global_transformations.__init__","title":"<code>__init__(input_channels, output_channels)</code>","text":"<p>A global feature layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels\n            ):\n    \"\"\"\n    A global feature layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    \"\"\"\n    super().__init__()\n    self.global_feature_1 = torch.nn.Sequential(\n        torch.nn.Linear(input_channels, output_channels),\n        torch.nn.LeakyReLU(0.2, inplace = True),\n    )\n    self.global_feature_2 = torch.nn.Sequential(\n        torch.nn.Linear(output_channels, output_channels),\n        torch.nn.LeakyReLU(0.2, inplace = True)\n    )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.global_transformations.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    y = torch.mean(x2, dim = (2, 3))\n    y1 = self.global_feature_1(y)\n    y2 = self.global_feature_2(y1)\n    y1 = y1.unsqueeze(2).unsqueeze(3)\n    y2 = y2.unsqueeze(2).unsqueeze(3)\n    result = x1 * y1 + y2\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.non_local_layer","title":"<code>non_local_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Self-Attention Layer [zi = Wzyi + xi] (non-local block : ref https://arxiv.org/abs/1711.07971)</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class non_local_layer(torch.nn.Module):\n    \"\"\"\n    Self-Attention Layer [zi = Wzyi + xi] (non-local block : ref https://arxiv.org/abs/1711.07971)\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 1024,\n                 bottleneck_channels = 512,\n                 kernel_size = 1,\n                 bias = False,\n                ):\n        \"\"\"\n\n        Parameters\n        ----------\n        input_channels      : int\n                              Number of input channels.\n        bottleneck_channels : int\n                              Number of middle channels.\n        kernel_size         : int\n                              Kernel size.\n        bias                : bool \n                              Set to True to let convolutional layers have bias term.\n        \"\"\"\n        super(non_local_layer, self).__init__()\n        self.input_channels = input_channels\n        self.bottleneck_channels = bottleneck_channels\n        self.g = torch.nn.Conv2d(\n                                 self.input_channels, \n                                 self.bottleneck_channels,\n                                 kernel_size = kernel_size,\n                                 padding = kernel_size // 2,\n                                 bias = bias\n                                )\n        self.W_z = torch.nn.Sequential(\n                                       torch.nn.Conv2d(\n                                                       self.bottleneck_channels,\n                                                       self.input_channels, \n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       padding = kernel_size // 2\n                                                      ),\n                                       torch.nn.BatchNorm2d(self.input_channels)\n                                      )\n        torch.nn.init.constant_(self.W_z[1].weight, 0)   \n        torch.nn.init.constant_(self.W_z[1].bias, 0)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model [zi = Wzyi + xi]\n\n        Parameters\n        ----------\n        x               : torch.tensor\n                          First input data.                       \n\n\n        Returns\n        ----------\n        z               : torch.tensor\n                          Estimated output.\n        \"\"\"\n        batch_size, channels, height, width = x.size()\n        theta = x.view(batch_size, channels, -1).permute(0, 2, 1)\n        phi = x.view(batch_size, channels, -1).permute(0, 2, 1)\n        g = self.g(x).view(batch_size, self.bottleneck_channels, -1).permute(0, 2, 1)\n        attn = torch.bmm(theta, phi.transpose(1, 2)) / (height * width)\n        attn = torch.nn.functional.softmax(attn, dim=-1)\n        y = torch.bmm(attn, g).permute(0, 2, 1).contiguous().view(batch_size, self.bottleneck_channels, height, width)\n        W_y = self.W_z(y)\n        z = W_y + x\n        return z\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.non_local_layer.__init__","title":"<code>__init__(input_channels=1024, bottleneck_channels=512, kernel_size=1, bias=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>              Number of input channels.\n</code></pre> </li> <li> <code>bottleneck_channels</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <pre><code>              Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>              Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>              Set to True to let convolutional layers have bias term.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 1024,\n             bottleneck_channels = 512,\n             kernel_size = 1,\n             bias = False,\n            ):\n    \"\"\"\n\n    Parameters\n    ----------\n    input_channels      : int\n                          Number of input channels.\n    bottleneck_channels : int\n                          Number of middle channels.\n    kernel_size         : int\n                          Kernel size.\n    bias                : bool \n                          Set to True to let convolutional layers have bias term.\n    \"\"\"\n    super(non_local_layer, self).__init__()\n    self.input_channels = input_channels\n    self.bottleneck_channels = bottleneck_channels\n    self.g = torch.nn.Conv2d(\n                             self.input_channels, \n                             self.bottleneck_channels,\n                             kernel_size = kernel_size,\n                             padding = kernel_size // 2,\n                             bias = bias\n                            )\n    self.W_z = torch.nn.Sequential(\n                                   torch.nn.Conv2d(\n                                                   self.bottleneck_channels,\n                                                   self.input_channels, \n                                                   kernel_size = kernel_size,\n                                                   bias = bias,\n                                                   padding = kernel_size // 2\n                                                  ),\n                                   torch.nn.BatchNorm2d(self.input_channels)\n                                  )\n    torch.nn.init.constant_(self.W_z[1].weight, 0)   \n    torch.nn.init.constant_(self.W_z[1].bias, 0)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.non_local_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model [zi = Wzyi + xi]</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>          First input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model [zi = Wzyi + xi]\n\n    Parameters\n    ----------\n    x               : torch.tensor\n                      First input data.                       \n\n\n    Returns\n    ----------\n    z               : torch.tensor\n                      Estimated output.\n    \"\"\"\n    batch_size, channels, height, width = x.size()\n    theta = x.view(batch_size, channels, -1).permute(0, 2, 1)\n    phi = x.view(batch_size, channels, -1).permute(0, 2, 1)\n    g = self.g(x).view(batch_size, self.bottleneck_channels, -1).permute(0, 2, 1)\n    attn = torch.bmm(theta, phi.transpose(1, 2)) / (height * width)\n    attn = torch.nn.functional.softmax(attn, dim=-1)\n    y = torch.bmm(attn, g).permute(0, 2, 1).contiguous().view(batch_size, self.bottleneck_channels, height, width)\n    W_y = self.W_z(y)\n    z = W_y + x\n    return z\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.normalization","title":"<code>normalization</code>","text":"<p>               Bases: <code>Module</code></p> <p>A normalization layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class normalization(torch.nn.Module):\n    \"\"\"\n    A normalization layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 dim = 1,\n                ):\n        \"\"\"\n        Normalization layer.\n\n\n        Parameters\n        ----------\n        dim             : int\n                          Dimension (axis) to normalize.\n        \"\"\"\n        super().__init__()\n        self.k = torch.nn.Parameter(torch.ones(1, dim, 1, 1))\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        result =  (x - mean) * (var + eps).rsqrt() * self.k\n        return result \n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.normalization.__init__","title":"<code>__init__(dim=1)</code>","text":"<p>Normalization layer.</p> <p>Parameters:</p> <ul> <li> <code>dim</code>           \u2013            <pre><code>          Dimension (axis) to normalize.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             dim = 1,\n            ):\n    \"\"\"\n    Normalization layer.\n\n\n    Parameters\n    ----------\n    dim             : int\n                      Dimension (axis) to normalize.\n    \"\"\"\n    super().__init__()\n    self.k = torch.nn.Parameter(torch.ones(1, dim, 1, 1))\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.normalization.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n    var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n    mean = torch.mean(x, dim = 1, keepdim = True)\n    result =  (x - mean) * (var + eps).rsqrt() * self.k\n    return result \n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.positional_encoder","title":"<code>positional_encoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>A positional encoder module. This implementation follows this specific work: <code>Martin-Brualla, Ricardo, Noha Radwan, Mehdi SM Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. \"Nerf in the wild: Neural radiance fields for unconstrained photo collections.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7210-7219. 2021.</code>.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class positional_encoder(torch.nn.Module):\n    \"\"\"\n    A positional encoder module.\n    This implementation follows this specific work: `Martin-Brualla, Ricardo, Noha Radwan, Mehdi SM Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. \"Nerf in the wild: Neural radiance fields for unconstrained photo collections.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7210-7219. 2021.`.\n    \"\"\"\n\n    def __init__(self, L):\n        \"\"\"\n        A positional encoder module.\n\n        Parameters\n        ----------\n        L                   : int\n                              Positional encoding level.\n        \"\"\"\n        super(positional_encoder, self).__init__()\n        self.L = L\n\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x               : torch.tensor\n                          Input data [b x n], where `b` is batch size, `n` is the feature size.\n\n        Returns\n        ----------\n        result          : torch.tensor\n                          Result of the forward operation.\n        \"\"\"\n        freqs = 2 ** torch.arange(self.L, device = x.device)\n        freqs = freqs.view(1, 1, -1)\n        results_cos = torch.cos(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n        results_sin = torch.sin(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n        results = torch.cat((x, results_cos, results_sin), dim = 1)\n        return results\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.positional_encoder.__init__","title":"<code>__init__(L)</code>","text":"<p>A positional encoder module.</p> <p>Parameters:</p> <ul> <li> <code>L</code>           \u2013            <pre><code>              Positional encoding level.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(self, L):\n    \"\"\"\n    A positional encoder module.\n\n    Parameters\n    ----------\n    L                   : int\n                          Positional encoding level.\n    \"\"\"\n    super(positional_encoder, self).__init__()\n    self.L = L\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.positional_encoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>          Input data [b x n], where `b` is batch size, `n` is the feature size.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x               : torch.tensor\n                      Input data [b x n], where `b` is batch size, `n` is the feature size.\n\n    Returns\n    ----------\n    result          : torch.tensor\n                      Result of the forward operation.\n    \"\"\"\n    freqs = 2 ** torch.arange(self.L, device = x.device)\n    freqs = freqs.view(1, 1, -1)\n    results_cos = torch.cos(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n    results_sin = torch.sin(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n    results = torch.cat((x, results_cos, results_sin), dim = 1)\n    return results\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.residual_attention_layer","title":"<code>residual_attention_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A residual block with an attention layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class residual_attention_layer(torch.nn.Module):\n    \"\"\"\n    A residual block with an attention layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 1,\n                 bias = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        An attention layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int or optioal\n                          Number of input channels.\n        output_channels : int or optional\n                          Number of middle channels.\n        kernel_size     : int or optional\n                          Kernel size.\n        bias            : bool or optional\n                          Set to True to let convolutional layers have bias term.\n        activation      : torch.nn or optional\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.activation = activation\n        self.convolution0 = torch.nn.Sequential(\n                                                torch.nn.Conv2d(\n                                                                input_channels,\n                                                                output_channels,\n                                                                kernel_size = kernel_size,\n                                                                padding = kernel_size // 2,\n                                                                bias = bias\n                                                               ),\n                                                torch.nn.BatchNorm2d(output_channels)\n                                               )\n        self.convolution1 = torch.nn.Sequential(\n                                                torch.nn.Conv2d(\n                                                                input_channels,\n                                                                output_channels,\n                                                                kernel_size = kernel_size,\n                                                                padding = kernel_size // 2,\n                                                                bias = bias\n                                                               ),\n                                                torch.nn.BatchNorm2d(output_channels)\n                                               )\n        self.final_layer = torch.nn.Sequential(\n                                               self.activation,\n                                               torch.nn.Conv2d(\n                                                               output_channels,\n                                                               output_channels,\n                                                               kernel_size = kernel_size,\n                                                               padding = kernel_size // 2,\n                                                               bias = bias\n                                                              )\n                                              )\n\n\n    def forward(self, x0, x1):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x0             : torch.tensor\n                         First input data.\n\n        x1             : torch.tensor\n                         Seconnd input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        y0 = self.convolution0(x0)\n        y1 = self.convolution1(x1)\n        y2 = torch.add(y0, y1)\n        result = self.final_layer(y2) * x0\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.residual_attention_layer.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=1, bias=False, activation=torch.nn.ReLU())</code>","text":"<p>An attention layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int or optional</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 1,\n             bias = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    An attention layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int or optioal\n                      Number of input channels.\n    output_channels : int or optional\n                      Number of middle channels.\n    kernel_size     : int or optional\n                      Kernel size.\n    bias            : bool or optional\n                      Set to True to let convolutional layers have bias term.\n    activation      : torch.nn or optional\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.activation = activation\n    self.convolution0 = torch.nn.Sequential(\n                                            torch.nn.Conv2d(\n                                                            input_channels,\n                                                            output_channels,\n                                                            kernel_size = kernel_size,\n                                                            padding = kernel_size // 2,\n                                                            bias = bias\n                                                           ),\n                                            torch.nn.BatchNorm2d(output_channels)\n                                           )\n    self.convolution1 = torch.nn.Sequential(\n                                            torch.nn.Conv2d(\n                                                            input_channels,\n                                                            output_channels,\n                                                            kernel_size = kernel_size,\n                                                            padding = kernel_size // 2,\n                                                            bias = bias\n                                                           ),\n                                            torch.nn.BatchNorm2d(output_channels)\n                                           )\n    self.final_layer = torch.nn.Sequential(\n                                           self.activation,\n                                           torch.nn.Conv2d(\n                                                           output_channels,\n                                                           output_channels,\n                                                           kernel_size = kernel_size,\n                                                           padding = kernel_size // 2,\n                                                           bias = bias\n                                                          )\n                                          )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.residual_attention_layer.forward","title":"<code>forward(x0, x1)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x0</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x1</code>           \u2013            <pre><code>         Seconnd input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x0, x1):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x0             : torch.tensor\n                     First input data.\n\n    x1             : torch.tensor\n                     Seconnd input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    y0 = self.convolution0(x0)\n    y1 = self.convolution1(x1)\n    y2 = torch.add(y0, y1)\n    result = self.final_layer(y2) * x0\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.residual_layer","title":"<code>residual_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A residual layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class residual_layer(torch.nn.Module):\n    \"\"\"\n    A residual layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 mid_channels = 16,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = True,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A convolutional layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels    : int\n                          Number of middle channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.activation = activation\n        self.convolution = double_convolution(\n                                              input_channels,\n                                              mid_channels = mid_channels,\n                                              output_channels = input_channels,\n                                              kernel_size = kernel_size,\n                                              normalization = normalization,\n                                              bias = bias,\n                                              activation = activation\n                                             )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        x0 = self.convolution(x)\n        return x + x0\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.residual_layer.__init__","title":"<code>__init__(input_channels=2, mid_channels=16, kernel_size=3, bias=False, normalization=True, activation=torch.nn.ReLU())</code>","text":"<p>A convolutional layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             mid_channels = 16,\n             kernel_size = 3,\n             bias = False,\n             normalization = True,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A convolutional layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels    : int\n                      Number of middle channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.activation = activation\n    self.convolution = double_convolution(\n                                          input_channels,\n                                          mid_channels = mid_channels,\n                                          output_channels = input_channels,\n                                          kernel_size = kernel_size,\n                                          normalization = normalization,\n                                          bias = bias,\n                                          activation = activation\n                                         )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.residual_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    x0 = self.convolution(x)\n    return x + x0\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.spatial_gate","title":"<code>spatial_gate</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatial attention module that applies a convolution layer after channel pooling. This class is heavily inspired by https://github.com/Jongchan/attention-module/blob/master/MODELS/cbam.py.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatial_gate(torch.nn.Module):\n    \"\"\"\n    Spatial attention module that applies a convolution layer after channel pooling.\n    This class is heavily inspired by https://github.com/Jongchan/attention-module/blob/master/MODELS/cbam.py.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the spatial gate module.\n        \"\"\"\n        super().__init__()\n        kernel_size = 7\n        self.spatial = convolution_layer(2, 1, kernel_size, bias = False, activation = torch.nn.Identity())\n\n\n    def channel_pool(self, x):\n        \"\"\"\n        Applies max and average pooling on the channels.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input tensor.\n\n        Returns\n        -------\n        output        : torch.tensor\n                        Output tensor.\n        \"\"\"\n        max_pool = torch.max(x, 1)[0].unsqueeze(1)\n        avg_pool = torch.mean(x, 1).unsqueeze(1)\n        output = torch.cat((max_pool, avg_pool), dim=1)\n        return output\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the SpatialGate module.\n\n        Applies spatial attention to the input tensor.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the SpatialGate module.\n\n        Returns\n        -------\n        scaled_x     : torch.tensor\n                       Output tensor after applying spatial attention.\n        \"\"\"\n        x_compress = self.channel_pool(x)\n        x_out = self.spatial(x_compress)\n        scale = torch.sigmoid(x_out)\n        scaled_x = x * scale\n        return scaled_x\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.spatial_gate.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the spatial gate module.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the spatial gate module.\n    \"\"\"\n    super().__init__()\n    kernel_size = 7\n    self.spatial = convolution_layer(2, 1, kernel_size, bias = False, activation = torch.nn.Identity())\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.spatial_gate.channel_pool","title":"<code>channel_pool(x)</code>","text":"<p>Applies max and average pooling on the channels.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input tensor.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Output tensor.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def channel_pool(self, x):\n    \"\"\"\n    Applies max and average pooling on the channels.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input tensor.\n\n    Returns\n    -------\n    output        : torch.tensor\n                    Output tensor.\n    \"\"\"\n    max_pool = torch.max(x, 1)[0].unsqueeze(1)\n    avg_pool = torch.mean(x, 1).unsqueeze(1)\n    output = torch.cat((max_pool, avg_pool), dim=1)\n    return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.spatial_gate.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the SpatialGate module.</p> <p>Applies spatial attention to the input tensor.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the SpatialGate module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>scaled_x</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying spatial attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the SpatialGate module.\n\n    Applies spatial attention to the input tensor.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the SpatialGate module.\n\n    Returns\n    -------\n    scaled_x     : torch.tensor\n                   Output tensor after applying spatial attention.\n    \"\"\"\n    x_compress = self.channel_pool(x)\n    x_out = self.spatial(x_compress)\n    scale = torch.sigmoid(x_out)\n    scaled_x = x * scale\n    return scaled_x\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.spatially_adaptive_convolution","title":"<code>spatially_adaptive_convolution</code>","text":"<p>               Bases: <code>Module</code></p> <p>A spatially adaptive convolution layer.</p> References <p>C. Zheng et al. \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions.\" C. Xu et al. \"Squeezesegv3: Spatially-adaptive Convolution for Efficient Point-Cloud Segmentation.\" C. Zheng et al. \"Windowing Decomposition Convolutional Neural Network for Image Enhancement.\"</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatially_adaptive_convolution(torch.nn.Module):\n    \"\"\"\n    A spatially adaptive convolution layer.\n\n    References\n    ----------\n\n    C. Zheng et al. \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions.\"\n    C. Xu et al. \"Squeezesegv3: Spatially-adaptive Convolution for Efficient Point-Cloud Segmentation.\"\n    C. Zheng et al. \"Windowing Decomposition Convolutional Neural Network for Image Enhancement.\"\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 stride = 1,\n                 padding = 1,\n                 bias = False,\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        Initializes a spatially adaptive convolution layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Size of the convolution kernel.\n        stride          : int\n                          Stride of the convolution.\n        padding         : int\n                          Padding added to both sides of the input.\n        bias            : bool\n                          If True, includes a bias term in the convolution.\n        activation      : torch.nn.Module\n                          Activation function to apply. If None, no activation is applied.\n        \"\"\"\n        super(spatially_adaptive_convolution, self).__init__()\n        self.kernel_size = kernel_size\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.stride = stride\n        self.padding = padding\n        self.standard_convolution = torch.nn.Conv2d(\n                                                    in_channels = input_channels,\n                                                    out_channels = self.output_channels,\n                                                    kernel_size = kernel_size,\n                                                    stride = stride,\n                                                    padding = padding,\n                                                    bias = bias\n                                                   )\n        self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n        self.activation = activation\n\n\n    def forward(self, x, sv_kernel_feature):\n        \"\"\"\n        Forward pass for the spatially adaptive convolution layer.\n\n        Parameters\n        ----------\n        x                  : torch.tensor\n                            Input data tensor.\n                            Dimension: (1, C, H, W)\n        sv_kernel_feature   : torch.tensor\n                            Spatially varying kernel features.\n                            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n        Returns\n        -------\n        sa_output          : torch.tensor\n                            Estimated output tensor.\n                            Dimension: (1, output_channels, H_out, W_out)\n        \"\"\"\n        # Pad input and sv_kernel_feature if necessary\n        if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n                -2) * self.stride != x.size(-2):\n            diffY = sv_kernel_feature.size(-2) % self.stride\n            diffX = sv_kernel_feature.size(-1) % self.stride\n            sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                            diffY // 2, diffY - diffY // 2))\n            diffY = x.size(-2) % self.stride\n            diffX = x.size(-1) % self.stride\n            x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                            diffY // 2, diffY - diffY // 2))\n\n        # Unfold the input tensor for matrix multiplication\n        input_feature = torch.nn.functional.unfold(\n                                                   x,\n                                                   kernel_size = (self.kernel_size, self.kernel_size),\n                                                   stride = self.stride,\n                                                   padding = self.padding\n                                                  )\n\n        # Resize sv_kernel_feature to match the input feature\n        sv_kernel = sv_kernel_feature.reshape(\n                                              1,\n                                              self.input_channels * self.kernel_size * self.kernel_size,\n                                              (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                             )\n\n        # Resize weight to match the input channels and kernel size\n        si_kernel = self.weight.reshape(\n                                        self.weight_output_channels,\n                                        self.input_channels * self.kernel_size * self.kernel_size\n                                       )\n\n        # Apply spatially varying kernels\n        sv_feature = input_feature * sv_kernel\n\n        # Perform matrix multiplication\n        sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                                1, self.weight_output_channels,\n                                                                (x.size(-2) // self.stride),\n                                                                (x.size(-1) // self.stride)\n                                                               )\n        return sa_output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.spatially_adaptive_convolution.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, stride=1, padding=1, bias=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>Initializes a spatially adaptive convolution layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Size of the convolution kernel.\n</code></pre> </li> <li> <code>stride</code>           \u2013            <pre><code>          Stride of the convolution.\n</code></pre> </li> <li> <code>padding</code>           \u2013            <pre><code>          Padding added to both sides of the input.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          If True, includes a bias term in the convolution.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Activation function to apply. If None, no activation is applied.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             stride = 1,\n             padding = 1,\n             bias = False,\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    Initializes a spatially adaptive convolution layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Size of the convolution kernel.\n    stride          : int\n                      Stride of the convolution.\n    padding         : int\n                      Padding added to both sides of the input.\n    bias            : bool\n                      If True, includes a bias term in the convolution.\n    activation      : torch.nn.Module\n                      Activation function to apply. If None, no activation is applied.\n    \"\"\"\n    super(spatially_adaptive_convolution, self).__init__()\n    self.kernel_size = kernel_size\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.stride = stride\n    self.padding = padding\n    self.standard_convolution = torch.nn.Conv2d(\n                                                in_channels = input_channels,\n                                                out_channels = self.output_channels,\n                                                kernel_size = kernel_size,\n                                                stride = stride,\n                                                padding = padding,\n                                                bias = bias\n                                               )\n    self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n    self.activation = activation\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.spatially_adaptive_convolution.forward","title":"<code>forward(x, sv_kernel_feature)</code>","text":"<p>Forward pass for the spatially adaptive convolution layer.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>            Input data tensor.\n            Dimension: (1, C, H, W)\n</code></pre> </li> <li> <code>sv_kernel_feature</code>           \u2013            <pre><code>            Spatially varying kernel features.\n            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>sa_output</code> (              <code>tensor</code> )          \u2013            <p>Estimated output tensor. Dimension: (1, output_channels, H_out, W_out)</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x, sv_kernel_feature):\n    \"\"\"\n    Forward pass for the spatially adaptive convolution layer.\n\n    Parameters\n    ----------\n    x                  : torch.tensor\n                        Input data tensor.\n                        Dimension: (1, C, H, W)\n    sv_kernel_feature   : torch.tensor\n                        Spatially varying kernel features.\n                        Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n    Returns\n    -------\n    sa_output          : torch.tensor\n                        Estimated output tensor.\n                        Dimension: (1, output_channels, H_out, W_out)\n    \"\"\"\n    # Pad input and sv_kernel_feature if necessary\n    if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n            -2) * self.stride != x.size(-2):\n        diffY = sv_kernel_feature.size(-2) % self.stride\n        diffX = sv_kernel_feature.size(-1) % self.stride\n        sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                        diffY // 2, diffY - diffY // 2))\n        diffY = x.size(-2) % self.stride\n        diffX = x.size(-1) % self.stride\n        x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                        diffY // 2, diffY - diffY // 2))\n\n    # Unfold the input tensor for matrix multiplication\n    input_feature = torch.nn.functional.unfold(\n                                               x,\n                                               kernel_size = (self.kernel_size, self.kernel_size),\n                                               stride = self.stride,\n                                               padding = self.padding\n                                              )\n\n    # Resize sv_kernel_feature to match the input feature\n    sv_kernel = sv_kernel_feature.reshape(\n                                          1,\n                                          self.input_channels * self.kernel_size * self.kernel_size,\n                                          (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                         )\n\n    # Resize weight to match the input channels and kernel size\n    si_kernel = self.weight.reshape(\n                                    self.weight_output_channels,\n                                    self.input_channels * self.kernel_size * self.kernel_size\n                                   )\n\n    # Apply spatially varying kernels\n    sv_feature = input_feature * sv_kernel\n\n    # Perform matrix multiplication\n    sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                            1, self.weight_output_channels,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n    return sa_output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.spatially_adaptive_module","title":"<code>spatially_adaptive_module</code>","text":"<p>               Bases: <code>Module</code></p> <p>A spatially adaptive module that combines learned spatially adaptive convolutions.</p> References <p>Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatially_adaptive_module(torch.nn.Module):\n    \"\"\"\n    A spatially adaptive module that combines learned spatially adaptive convolutions.\n\n    References\n    ----------\n\n    Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 stride = 1,\n                 padding = 1,\n                 bias = False,\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        Initializes a spatially adaptive module.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Size of the convolution kernel.\n        stride          : int\n                          Stride of the convolution.\n        padding         : int\n                          Padding added to both sides of the input.\n        bias            : bool\n                          If True, includes a bias term in the convolution.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super(spatially_adaptive_module, self).__init__()\n        self.kernel_size = kernel_size\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.stride = stride\n        self.padding = padding\n        self.weight_output_channels = self.output_channels - 1\n        self.standard_convolution = torch.nn.Conv2d(\n                                                    in_channels = input_channels,\n                                                    out_channels = self.weight_output_channels,\n                                                    kernel_size = kernel_size,\n                                                    stride = stride,\n                                                    padding = padding,\n                                                    bias = bias\n                                                   )\n        self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n        self.activation = activation\n\n\n    def forward(self, x, sv_kernel_feature):\n        \"\"\"\n        Forward pass for the spatially adaptive module.\n\n        Parameters\n        ----------\n        x                  : torch.tensor\n                            Input data tensor.\n                            Dimension: (1, C, H, W)\n        sv_kernel_feature   : torch.tensor\n                            Spatially varying kernel features.\n                            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n        Returns\n        -------\n        output             : torch.tensor\n                            Combined output tensor from standard and spatially adaptive convolutions.\n                            Dimension: (1, output_channels, H_out, W_out)\n        \"\"\"\n        # Pad input and sv_kernel_feature if necessary\n        if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n                -2) * self.stride != x.size(-2):\n            diffY = sv_kernel_feature.size(-2) % self.stride\n            diffX = sv_kernel_feature.size(-1) % self.stride\n            sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                            diffY // 2, diffY - diffY // 2))\n            diffY = x.size(-2) % self.stride\n            diffX = x.size(-1) % self.stride\n            x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                            diffY // 2, diffY - diffY // 2))\n\n        # Unfold the input tensor for matrix multiplication\n        input_feature = torch.nn.functional.unfold(\n                                                   x,\n                                                   kernel_size = (self.kernel_size, self.kernel_size),\n                                                   stride = self.stride,\n                                                   padding = self.padding\n                                                  )\n\n        # Resize sv_kernel_feature to match the input feature\n        sv_kernel = sv_kernel_feature.reshape(\n                                              1,\n                                              self.input_channels * self.kernel_size * self.kernel_size,\n                                              (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                             )\n\n        # Apply sv_kernel to the input_feature\n        sv_feature = input_feature * sv_kernel\n\n        # Original spatially varying convolution output\n        sv_output = torch.sum(sv_feature, dim = 1).reshape(\n                                                           1,\n                                                            1,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n\n        # Reshape weight for spatially adaptive convolution\n        si_kernel = self.weight.reshape(\n                                        self.weight_output_channels,\n                                        self.input_channels * self.kernel_size * self.kernel_size\n                                       )\n\n        # Apply si_kernel on sv convolution output\n        sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                                1, self.weight_output_channels,\n                                                                (x.size(-2) // self.stride),\n                                                                (x.size(-1) // self.stride)\n                                                               )\n\n        # Combine the outputs and apply activation function\n        output = self.activation(torch.cat((sv_output, sa_output), dim = 1))\n        return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.spatially_adaptive_module.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, stride=1, padding=1, bias=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>Initializes a spatially adaptive module.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Size of the convolution kernel.\n</code></pre> </li> <li> <code>stride</code>           \u2013            <pre><code>          Stride of the convolution.\n</code></pre> </li> <li> <code>padding</code>           \u2013            <pre><code>          Padding added to both sides of the input.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          If True, includes a bias term in the convolution.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             stride = 1,\n             padding = 1,\n             bias = False,\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    Initializes a spatially adaptive module.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Size of the convolution kernel.\n    stride          : int\n                      Stride of the convolution.\n    padding         : int\n                      Padding added to both sides of the input.\n    bias            : bool\n                      If True, includes a bias term in the convolution.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super(spatially_adaptive_module, self).__init__()\n    self.kernel_size = kernel_size\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.stride = stride\n    self.padding = padding\n    self.weight_output_channels = self.output_channels - 1\n    self.standard_convolution = torch.nn.Conv2d(\n                                                in_channels = input_channels,\n                                                out_channels = self.weight_output_channels,\n                                                kernel_size = kernel_size,\n                                                stride = stride,\n                                                padding = padding,\n                                                bias = bias\n                                               )\n    self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n    self.activation = activation\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.spatially_adaptive_module.forward","title":"<code>forward(x, sv_kernel_feature)</code>","text":"<p>Forward pass for the spatially adaptive module.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>            Input data tensor.\n            Dimension: (1, C, H, W)\n</code></pre> </li> <li> <code>sv_kernel_feature</code>           \u2013            <pre><code>            Spatially varying kernel features.\n            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Combined output tensor from standard and spatially adaptive convolutions. Dimension: (1, output_channels, H_out, W_out)</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x, sv_kernel_feature):\n    \"\"\"\n    Forward pass for the spatially adaptive module.\n\n    Parameters\n    ----------\n    x                  : torch.tensor\n                        Input data tensor.\n                        Dimension: (1, C, H, W)\n    sv_kernel_feature   : torch.tensor\n                        Spatially varying kernel features.\n                        Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n    Returns\n    -------\n    output             : torch.tensor\n                        Combined output tensor from standard and spatially adaptive convolutions.\n                        Dimension: (1, output_channels, H_out, W_out)\n    \"\"\"\n    # Pad input and sv_kernel_feature if necessary\n    if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n            -2) * self.stride != x.size(-2):\n        diffY = sv_kernel_feature.size(-2) % self.stride\n        diffX = sv_kernel_feature.size(-1) % self.stride\n        sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                        diffY // 2, diffY - diffY // 2))\n        diffY = x.size(-2) % self.stride\n        diffX = x.size(-1) % self.stride\n        x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                        diffY // 2, diffY - diffY // 2))\n\n    # Unfold the input tensor for matrix multiplication\n    input_feature = torch.nn.functional.unfold(\n                                               x,\n                                               kernel_size = (self.kernel_size, self.kernel_size),\n                                               stride = self.stride,\n                                               padding = self.padding\n                                              )\n\n    # Resize sv_kernel_feature to match the input feature\n    sv_kernel = sv_kernel_feature.reshape(\n                                          1,\n                                          self.input_channels * self.kernel_size * self.kernel_size,\n                                          (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                         )\n\n    # Apply sv_kernel to the input_feature\n    sv_feature = input_feature * sv_kernel\n\n    # Original spatially varying convolution output\n    sv_output = torch.sum(sv_feature, dim = 1).reshape(\n                                                       1,\n                                                        1,\n                                                        (x.size(-2) // self.stride),\n                                                        (x.size(-1) // self.stride)\n                                                       )\n\n    # Reshape weight for spatially adaptive convolution\n    si_kernel = self.weight.reshape(\n                                    self.weight_output_channels,\n                                    self.input_channels * self.kernel_size * self.kernel_size\n                                   )\n\n    # Apply si_kernel on sv convolution output\n    sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                            1, self.weight_output_channels,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n\n    # Combine the outputs and apply activation function\n    output = self.activation(torch.cat((sv_output, sa_output), dim = 1))\n    return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.upsample_convtranspose2d_layer","title":"<code>upsample_convtranspose2d_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>An upsampling convtranspose2d layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class upsample_convtranspose2d_layer(torch.nn.Module):\n    \"\"\"\n    An upsampling convtranspose2d layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 2,\n                 stride = 2,\n                 bias = False,\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        \"\"\"\n        super().__init__()\n        self.up = torch.nn.ConvTranspose2d(\n                                           in_channels = input_channels,\n                                           out_channels = output_channels,\n                                           bias = bias,\n                                           kernel_size = kernel_size,\n                                           stride = stride\n                                          )\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Result of the forward operation\n        \"\"\"\n        x1 = self.up(x1)\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                          diffY // 2, diffY - diffY // 2])\n        result = x1 + x2\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.upsample_convtranspose2d_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=2, stride=2, bias=False)</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 2,\n             stride = 2,\n             bias = False,\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    \"\"\"\n    super().__init__()\n    self.up = torch.nn.ConvTranspose2d(\n                                       in_channels = input_channels,\n                                       out_channels = output_channels,\n                                       bias = bias,\n                                       kernel_size = kernel_size,\n                                       stride = stride\n                                      )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.upsample_convtranspose2d_layer.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Result of the forward operation\n    \"\"\"\n    x1 = self.up(x1)\n    diffY = x2.size()[2] - x1.size()[2]\n    diffX = x2.size()[3] - x1.size()[3]\n    x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                      diffY // 2, diffY - diffY // 2])\n    result = x1 + x2\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.upsample_layer","title":"<code>upsample_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>An upsampling convolutional layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class upsample_layer(torch.nn.Module):\n    \"\"\"\n    An upsampling convolutional layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU(),\n                 bilinear = True\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        bilinear        : bool\n                          If set to True, bilinear sampling is used.\n        \"\"\"\n        super(upsample_layer, self).__init__()\n        if bilinear:\n            self.up = torch.nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n            self.conv = double_convolution(\n                                           input_channels = input_channels + output_channels,\n                                           mid_channels = input_channels // 2,\n                                           output_channels = output_channels,\n                                           kernel_size = kernel_size,\n                                           normalization = normalization,\n                                           bias = bias,\n                                           activation = activation\n                                          )\n        else:\n            self.up = torch.nn.ConvTranspose2d(input_channels , input_channels // 2, kernel_size = 2, stride = 2)\n            self.conv = double_convolution(\n                                           input_channels = input_channels,\n                                           mid_channels = output_channels,\n                                           output_channels = output_channels,\n                                           kernel_size = kernel_size,\n                                           normalization = normalization,\n                                           bias = bias,\n                                           activation = activation\n                                          )\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Result of the forward operation\n        \"\"\" \n        x1 = self.up(x1)\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                          diffY // 2, diffY - diffY // 2])\n        x = torch.cat([x2, x1], dim = 1)\n        result = self.conv(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.upsample_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU(), bilinear=True)</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> <li> <code>bilinear</code>           \u2013            <pre><code>          If set to True, bilinear sampling is used.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 3,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU(),\n             bilinear = True\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    bilinear        : bool\n                      If set to True, bilinear sampling is used.\n    \"\"\"\n    super(upsample_layer, self).__init__()\n    if bilinear:\n        self.up = torch.nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n        self.conv = double_convolution(\n                                       input_channels = input_channels + output_channels,\n                                       mid_channels = input_channels // 2,\n                                       output_channels = output_channels,\n                                       kernel_size = kernel_size,\n                                       normalization = normalization,\n                                       bias = bias,\n                                       activation = activation\n                                      )\n    else:\n        self.up = torch.nn.ConvTranspose2d(input_channels , input_channels // 2, kernel_size = 2, stride = 2)\n        self.conv = double_convolution(\n                                       input_channels = input_channels,\n                                       mid_channels = output_channels,\n                                       output_channels = output_channels,\n                                       kernel_size = kernel_size,\n                                       normalization = normalization,\n                                       bias = bias,\n                                       activation = activation\n                                      )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.upsample_layer.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Result of the forward operation\n    \"\"\" \n    x1 = self.up(x1)\n    diffY = x2.size()[2] - x1.size()[2]\n    diffX = x2.size()[3] - x1.size()[3]\n    x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                      diffY // 2, diffY - diffY // 2])\n    x = torch.cat([x2, x1], dim = 1)\n    result = self.conv(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.gaussian","title":"<code>gaussian(x, multiplier=1.0)</code>","text":"<p>A Gaussian non-linear activation. For more details: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input data.\n</code></pre> </li> <li> <code>multiplier</code>           \u2013            <pre><code>       Multiplier.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>float or tensor</code> )          \u2013            <p>Ouput data.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def gaussian(x, multiplier = 1.):\n    \"\"\"\n    A Gaussian non-linear activation.\n    For more details: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n\n    Parameters\n    ----------\n    x            : float or torch.tensor\n                   Input data.\n    multiplier   : float or torch.tensor\n                   Multiplier.\n\n    Returns\n    -------\n    result       : float or torch.tensor\n                   Ouput data.\n    \"\"\"\n    result = torch.exp(- (multiplier * x) ** 2)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.components.swish","title":"<code>swish(x)</code>","text":"<p>A swish non-linear activation. For more details: https://en.wikipedia.org/wiki/Swish_function</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>         Input.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>float or tensor</code> )          \u2013            <p>Output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def swish(x):\n    \"\"\"\n    A swish non-linear activation.\n    For more details: https://en.wikipedia.org/wiki/Swish_function\n\n    Parameters\n    -----------\n    x              : float or torch.tensor\n                     Input.\n\n    Returns\n    -------\n    out            : float or torch.tensor\n                     Output.\n    \"\"\"\n    out = x * torch.sigmoid(x)\n    return out\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.channel_gate","title":"<code>channel_gate</code>","text":"<p>               Bases: <code>Module</code></p> <p>Channel attention module with various pooling strategies. This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class channel_gate(torch.nn.Module):\n    \"\"\"\n    Channel attention module with various pooling strategies.\n    This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).\n    \"\"\"\n    def __init__(\n                 self, \n                 gate_channels, \n                 reduction_ratio = 16, \n                 pool_types = ['avg', 'max']\n                ):\n        \"\"\"\n        Initializes the channel gate module.\n\n        Parameters\n        ----------\n        gate_channels   : int\n                          Number of channels of the input feature map.\n        reduction_ratio : int\n                          Reduction ratio for the intermediate layer.\n        pool_types      : list\n                          List of pooling operations to apply.\n        \"\"\"\n        super().__init__()\n        self.gate_channels = gate_channels\n        hidden_channels = gate_channels // reduction_ratio\n        if hidden_channels == 0:\n            hidden_channels = 1\n        self.mlp = torch.nn.Sequential(\n                                       convolutional_block_attention.Flatten(),\n                                       torch.nn.Linear(gate_channels, hidden_channels),\n                                       torch.nn.ReLU(),\n                                       torch.nn.Linear(hidden_channels, gate_channels)\n                                      )\n        self.pool_types = pool_types\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the ChannelGate module.\n\n        Applies channel-wise attention to the input tensor.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the ChannelGate module.\n\n        Returns\n        -------\n        output       : torch.tensor\n                       Output tensor after applying channel attention.\n        \"\"\"\n        channel_att_sum = None\n        for pool_type in self.pool_types:\n            if pool_type == 'avg':\n                pool = torch.nn.functional.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n            elif pool_type == 'max':\n                pool = torch.nn.functional.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n            channel_att_raw = self.mlp(pool)\n            channel_att_sum = channel_att_raw if channel_att_sum is None else channel_att_sum + channel_att_raw\n        scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n        output = x * scale\n        return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.channel_gate.__init__","title":"<code>__init__(gate_channels, reduction_ratio=16, pool_types=['avg', 'max'])</code>","text":"<p>Initializes the channel gate module.</p> <p>Parameters:</p> <ul> <li> <code>gate_channels</code>           \u2013            <pre><code>          Number of channels of the input feature map.\n</code></pre> </li> <li> <code>reduction_ratio</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <pre><code>          Reduction ratio for the intermediate layer.\n</code></pre> </li> <li> <code>pool_types</code>           \u2013            <pre><code>          List of pooling operations to apply.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self, \n             gate_channels, \n             reduction_ratio = 16, \n             pool_types = ['avg', 'max']\n            ):\n    \"\"\"\n    Initializes the channel gate module.\n\n    Parameters\n    ----------\n    gate_channels   : int\n                      Number of channels of the input feature map.\n    reduction_ratio : int\n                      Reduction ratio for the intermediate layer.\n    pool_types      : list\n                      List of pooling operations to apply.\n    \"\"\"\n    super().__init__()\n    self.gate_channels = gate_channels\n    hidden_channels = gate_channels // reduction_ratio\n    if hidden_channels == 0:\n        hidden_channels = 1\n    self.mlp = torch.nn.Sequential(\n                                   convolutional_block_attention.Flatten(),\n                                   torch.nn.Linear(gate_channels, hidden_channels),\n                                   torch.nn.ReLU(),\n                                   torch.nn.Linear(hidden_channels, gate_channels)\n                                  )\n    self.pool_types = pool_types\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.channel_gate.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the ChannelGate module.</p> <p>Applies channel-wise attention to the input tensor.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the ChannelGate module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying channel attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the ChannelGate module.\n\n    Applies channel-wise attention to the input tensor.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the ChannelGate module.\n\n    Returns\n    -------\n    output       : torch.tensor\n                   Output tensor after applying channel attention.\n    \"\"\"\n    channel_att_sum = None\n    for pool_type in self.pool_types:\n        if pool_type == 'avg':\n            pool = torch.nn.functional.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n        elif pool_type == 'max':\n            pool = torch.nn.functional.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n        channel_att_raw = self.mlp(pool)\n        channel_att_sum = channel_att_raw if channel_att_sum is None else channel_att_sum + channel_att_raw\n    scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n    output = x * scale\n    return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.convolution_layer","title":"<code>convolution_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A convolution layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class convolution_layer(torch.nn.Module):\n    \"\"\"\n    A convolution layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 bias = False,\n                 stride = 1,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A convolutional layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        layers = [\n            torch.nn.Conv2d(\n                            input_channels,\n                            output_channels,\n                            kernel_size = kernel_size,\n                            stride = stride,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           )\n        ]\n        if normalization:\n            layers.append(torch.nn.BatchNorm2d(output_channels))\n        if activation:\n            layers.append(activation)\n        self.model = torch.nn.Sequential(*layers)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        result = self.model(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.convolution_layer.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, bias=False, stride=1, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A convolutional layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             bias = False,\n             stride = 1,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A convolutional layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    layers = [\n        torch.nn.Conv2d(\n                        input_channels,\n                        output_channels,\n                        kernel_size = kernel_size,\n                        stride = stride,\n                        padding = kernel_size // 2,\n                        bias = bias\n                       )\n    ]\n    if normalization:\n        layers.append(torch.nn.BatchNorm2d(output_channels))\n    if activation:\n        layers.append(activation)\n    self.model = torch.nn.Sequential(*layers)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.convolution_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    result = self.model(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.convolutional_block_attention","title":"<code>convolutional_block_attention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convolutional Block Attention Module (CBAM) class.  This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class convolutional_block_attention(torch.nn.Module):\n    \"\"\"\n    Convolutional Block Attention Module (CBAM) class. \n    This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).\n    \"\"\"\n    def __init__(\n                 self, \n                 gate_channels, \n                 reduction_ratio = 16, \n                 pool_types = ['avg', 'max'], \n                 no_spatial = False\n                ):\n        \"\"\"\n        Initializes the convolutional block attention module.\n\n        Parameters\n        ----------\n        gate_channels   : int\n                          Number of channels of the input feature map.\n        reduction_ratio : int\n                          Reduction ratio for the channel attention.\n        pool_types      : list\n                          List of pooling operations to apply for channel attention.\n        no_spatial      : bool\n                          If True, spatial attention is not applied.\n        \"\"\"\n        super(convolutional_block_attention, self).__init__()\n        self.channel_gate = channel_gate(gate_channels, reduction_ratio, pool_types)\n        self.no_spatial = no_spatial\n        if not no_spatial:\n            self.spatial_gate = spatial_gate()\n\n\n    class Flatten(torch.nn.Module):\n        \"\"\"\n        Flattens the input tensor to a 2D matrix.\n        \"\"\"\n        def forward(self, x):\n            return x.view(x.size(0), -1)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the convolutional block attention module.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the CBAM module.\n\n        Returns\n        -------\n        x_out        : torch.tensor\n                       Output tensor after applying channel and spatial attention.\n        \"\"\"\n        x_out = self.channel_gate(x)\n        if not self.no_spatial:\n            x_out = self.spatial_gate(x_out)\n        return x_out\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.convolutional_block_attention.Flatten","title":"<code>Flatten</code>","text":"<p>               Bases: <code>Module</code></p> <p>Flattens the input tensor to a 2D matrix.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class Flatten(torch.nn.Module):\n    \"\"\"\n    Flattens the input tensor to a 2D matrix.\n    \"\"\"\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.convolutional_block_attention.__init__","title":"<code>__init__(gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False)</code>","text":"<p>Initializes the convolutional block attention module.</p> <p>Parameters:</p> <ul> <li> <code>gate_channels</code>           \u2013            <pre><code>          Number of channels of the input feature map.\n</code></pre> </li> <li> <code>reduction_ratio</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <pre><code>          Reduction ratio for the channel attention.\n</code></pre> </li> <li> <code>pool_types</code>           \u2013            <pre><code>          List of pooling operations to apply for channel attention.\n</code></pre> </li> <li> <code>no_spatial</code>           \u2013            <pre><code>          If True, spatial attention is not applied.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self, \n             gate_channels, \n             reduction_ratio = 16, \n             pool_types = ['avg', 'max'], \n             no_spatial = False\n            ):\n    \"\"\"\n    Initializes the convolutional block attention module.\n\n    Parameters\n    ----------\n    gate_channels   : int\n                      Number of channels of the input feature map.\n    reduction_ratio : int\n                      Reduction ratio for the channel attention.\n    pool_types      : list\n                      List of pooling operations to apply for channel attention.\n    no_spatial      : bool\n                      If True, spatial attention is not applied.\n    \"\"\"\n    super(convolutional_block_attention, self).__init__()\n    self.channel_gate = channel_gate(gate_channels, reduction_ratio, pool_types)\n    self.no_spatial = no_spatial\n    if not no_spatial:\n        self.spatial_gate = spatial_gate()\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.convolutional_block_attention.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the convolutional block attention module.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the CBAM module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_out</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying channel and spatial attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the convolutional block attention module.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the CBAM module.\n\n    Returns\n    -------\n    x_out        : torch.tensor\n                   Output tensor after applying channel and spatial attention.\n    \"\"\"\n    x_out = self.channel_gate(x)\n    if not self.no_spatial:\n        x_out = self.spatial_gate(x_out)\n    return x_out\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.double_convolution","title":"<code>double_convolution</code>","text":"<p>               Bases: <code>Module</code></p> <p>A double convolution layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class double_convolution(torch.nn.Module):\n    \"\"\"\n    A double convolution layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 mid_channels = None,\n                 output_channels = 2,\n                 kernel_size = 3, \n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        Double convolution model.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels    : int\n                          Number of channels in the hidden layer between two convolutions.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        if isinstance(mid_channels, type(None)):\n            mid_channels = output_channels\n        self.activation = activation\n        self.model = torch.nn.Sequential(\n                                         convolution_layer(\n                                                           input_channels = input_channels,\n                                                           output_channels = mid_channels,\n                                                           kernel_size = kernel_size,\n                                                           bias = bias,\n                                                           normalization = normalization,\n                                                           activation = self.activation\n                                                          ),\n                                         convolution_layer(\n                                                           input_channels = mid_channels,\n                                                           output_channels = output_channels,\n                                                           kernel_size = kernel_size,\n                                                           bias = bias,\n                                                           normalization = normalization,\n                                                           activation = self.activation\n                                                          )\n                                        )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        result = self.model(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.double_convolution.__init__","title":"<code>__init__(input_channels=2, mid_channels=None, output_channels=2, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>Double convolution model.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of channels in the hidden layer between two convolutions.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             mid_channels = None,\n             output_channels = 2,\n             kernel_size = 3, \n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    Double convolution model.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels    : int\n                      Number of channels in the hidden layer between two convolutions.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    if isinstance(mid_channels, type(None)):\n        mid_channels = output_channels\n    self.activation = activation\n    self.model = torch.nn.Sequential(\n                                     convolution_layer(\n                                                       input_channels = input_channels,\n                                                       output_channels = mid_channels,\n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       normalization = normalization,\n                                                       activation = self.activation\n                                                      ),\n                                     convolution_layer(\n                                                       input_channels = mid_channels,\n                                                       output_channels = output_channels,\n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       normalization = normalization,\n                                                       activation = self.activation\n                                                      )\n                                    )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.double_convolution.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    result = self.model(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.downsample_layer","title":"<code>downsample_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A downscaling component followed by a double convolution.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class downsample_layer(torch.nn.Module):\n    \"\"\"\n    A downscaling component followed by a double convolution.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.maxpool_conv = torch.nn.Sequential(\n                                                torch.nn.MaxPool2d(2),\n                                                double_convolution(\n                                                                   input_channels = input_channels,\n                                                                   mid_channels = output_channels,\n                                                                   output_channels = output_channels,\n                                                                   kernel_size = kernel_size,\n                                                                   bias = bias,\n                                                                   normalization = normalization,\n                                                                   activation = activation\n                                                                  )\n                                               )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x              : torch.tensor\n                         First input data.\n\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        result = self.maxpool_conv(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.downsample_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 3,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.maxpool_conv = torch.nn.Sequential(\n                                            torch.nn.MaxPool2d(2),\n                                            double_convolution(\n                                                               input_channels = input_channels,\n                                                               mid_channels = output_channels,\n                                                               output_channels = output_channels,\n                                                               kernel_size = kernel_size,\n                                                               bias = bias,\n                                                               normalization = normalization,\n                                                               activation = activation\n                                                              )\n                                           )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.downsample_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x              : torch.tensor\n                     First input data.\n\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    result = self.maxpool_conv(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.global_feature_module","title":"<code>global_feature_module</code>","text":"<p>               Bases: <code>Module</code></p> <p>A global feature layer that processes global features from input channels and applies them to another input tensor via learned transformations.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class global_feature_module(torch.nn.Module):\n    \"\"\"\n    A global feature layer that processes global features from input channels and\n    applies them to another input tensor via learned transformations.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 mid_channels,\n                 output_channels,\n                 kernel_size,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A global feature layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels  : int\n                          Number of mid channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.transformations_1 = global_transformations(input_channels, output_channels)\n        self.global_features_1 = double_convolution(\n                                                    input_channels = input_channels,\n                                                    mid_channels = mid_channels,\n                                                    output_channels = output_channels,\n                                                    kernel_size = kernel_size,\n                                                    bias = bias,\n                                                    normalization = normalization,\n                                                    activation = activation\n                                                   )\n        self.global_features_2 = double_convolution(\n                                                    input_channels = input_channels,\n                                                    mid_channels = mid_channels,\n                                                    output_channels = output_channels,\n                                                    kernel_size = kernel_size,\n                                                    bias = bias,\n                                                    normalization = normalization,\n                                                    activation = activation\n                                                   )\n        self.transformations_2 = global_transformations(input_channels, output_channels)\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        global_tensor_1 = self.transformations_1(x1, x2)\n        y1 = self.global_features_1(global_tensor_1)\n        y2 = self.global_features_2(y1)\n        global_tensor_2 = self.transformations_2(y1, y2)\n        return global_tensor_2\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.global_feature_module.__init__","title":"<code>__init__(input_channels, mid_channels, output_channels, kernel_size, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A global feature layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of mid channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             mid_channels,\n             output_channels,\n             kernel_size,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A global feature layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels  : int\n                      Number of mid channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.transformations_1 = global_transformations(input_channels, output_channels)\n    self.global_features_1 = double_convolution(\n                                                input_channels = input_channels,\n                                                mid_channels = mid_channels,\n                                                output_channels = output_channels,\n                                                kernel_size = kernel_size,\n                                                bias = bias,\n                                                normalization = normalization,\n                                                activation = activation\n                                               )\n    self.global_features_2 = double_convolution(\n                                                input_channels = input_channels,\n                                                mid_channels = mid_channels,\n                                                output_channels = output_channels,\n                                                kernel_size = kernel_size,\n                                                bias = bias,\n                                                normalization = normalization,\n                                                activation = activation\n                                               )\n    self.transformations_2 = global_transformations(input_channels, output_channels)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.global_feature_module.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    global_tensor_1 = self.transformations_1(x1, x2)\n    y1 = self.global_features_1(global_tensor_1)\n    y2 = self.global_features_2(y1)\n    global_tensor_2 = self.transformations_2(y1, y2)\n    return global_tensor_2\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.global_transformations","title":"<code>global_transformations</code>","text":"<p>               Bases: <code>Module</code></p> <p>A global feature layer that processes global features from input channels and applies learned transformations to another input tensor.</p> <p>This implementation is adapted from RSGUnet: https://github.com/MTLab/rsgunet_image_enhance.</p> <p>Reference: J. Huang, P. Zhu, M. Geng et al. \"Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.\"</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class global_transformations(torch.nn.Module):\n    \"\"\"\n    A global feature layer that processes global features from input channels and\n    applies learned transformations to another input tensor.\n\n    This implementation is adapted from RSGUnet:\n    https://github.com/MTLab/rsgunet_image_enhance.\n\n    Reference:\n    J. Huang, P. Zhu, M. Geng et al. \"Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.\"\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels\n                ):\n        \"\"\"\n        A global feature layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        \"\"\"\n        super().__init__()\n        self.global_feature_1 = torch.nn.Sequential(\n            torch.nn.Linear(input_channels, output_channels),\n            torch.nn.LeakyReLU(0.2, inplace = True),\n        )\n        self.global_feature_2 = torch.nn.Sequential(\n            torch.nn.Linear(output_channels, output_channels),\n            torch.nn.LeakyReLU(0.2, inplace = True)\n        )\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        y = torch.mean(x2, dim = (2, 3))\n        y1 = self.global_feature_1(y)\n        y2 = self.global_feature_2(y1)\n        y1 = y1.unsqueeze(2).unsqueeze(3)\n        y2 = y2.unsqueeze(2).unsqueeze(3)\n        result = x1 * y1 + y2\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.global_transformations.__init__","title":"<code>__init__(input_channels, output_channels)</code>","text":"<p>A global feature layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels\n            ):\n    \"\"\"\n    A global feature layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    \"\"\"\n    super().__init__()\n    self.global_feature_1 = torch.nn.Sequential(\n        torch.nn.Linear(input_channels, output_channels),\n        torch.nn.LeakyReLU(0.2, inplace = True),\n    )\n    self.global_feature_2 = torch.nn.Sequential(\n        torch.nn.Linear(output_channels, output_channels),\n        torch.nn.LeakyReLU(0.2, inplace = True)\n    )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.global_transformations.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    y = torch.mean(x2, dim = (2, 3))\n    y1 = self.global_feature_1(y)\n    y2 = self.global_feature_2(y1)\n    y1 = y1.unsqueeze(2).unsqueeze(3)\n    y2 = y2.unsqueeze(2).unsqueeze(3)\n    result = x1 * y1 + y2\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.multi_layer_perceptron","title":"<code>multi_layer_perceptron</code>","text":"<p>               Bases: <code>Module</code></p> <p>A multi-layer perceptron model.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class multi_layer_perceptron(torch.nn.Module):\n    \"\"\"\n    A multi-layer perceptron model.\n    \"\"\"\n\n    def __init__(self,\n                 dimensions,\n                 activation = torch.nn.ReLU(),\n                 bias = False,\n                 model_type = 'conventional',\n                 siren_multiplier = 1.,\n                 input_multiplier = None\n                ):\n        \"\"\"\n        Parameters\n        ----------\n        dimensions        : list\n                            List of integers representing the dimensions of each layer (e.g., [2, 10, 1], where the first layer has two channels and last one has one channel.).\n        activation        : torch.nn\n                            Nonlinear activation function.\n                            Default is `torch.nn.ReLU()`.\n        bias              : bool\n                            If set to True, linear layers will include biases.\n        siren_multiplier  : float\n                            When using `SIREN` model type, this parameter functions as a hyperparameter.\n                            The original SIREN work uses 30.\n                            You can bypass this parameter by providing input that are not normalized and larger then one.\n        input_multiplier  : float\n                            Initial value of the input multiplier before the very first layer.\n        model_type        : str\n                            Model type: `conventional`, `swish`, `SIREN`, `FILM SIREN`, `Gaussian`.\n                            `conventional` refers to a standard multi layer perceptron.\n                            For `SIREN,` see: Sitzmann, Vincent, et al. \"Implicit neural representations with periodic activation functions.\" Advances in neural information processing systems 33 (2020): 7462-7473.\n                            For `Swish,` see: Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for activation functions.\" arXiv preprint arXiv:1710.05941 (2017). \n                            For `FILM SIREN,` see: Chan, Eric R., et al. \"pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n                            For `Gaussian,` see: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n        \"\"\"\n        super(multi_layer_perceptron, self).__init__()\n        self.activation = activation\n        self.bias = bias\n        self.model_type = model_type\n        self.layers = torch.nn.ModuleList()\n        self.siren_multiplier = siren_multiplier\n        self.dimensions = dimensions\n        for i in range(len(self.dimensions) - 1):\n            self.layers.append(torch.nn.Linear(self.dimensions[i], self.dimensions[i + 1], bias = self.bias))\n        if not isinstance(input_multiplier, type(None)):\n            self.input_multiplier = torch.nn.ParameterList()\n            self.input_multiplier.append(torch.nn.Parameter(torch.ones(1, self.dimensions[0]) * input_multiplier))\n        if self.model_type == 'FILM SIREN':\n            self.alpha = torch.nn.ParameterList()\n            for j in self.dimensions[1::]:\n                self.alpha.append(torch.nn.Parameter(torch.randn(2, 1, j)))\n        if self.model_type == 'Gaussian':\n            self.alpha = torch.nn.ParameterList()\n            for j in self.dimensions[1::]:\n                self.alpha.append(torch.nn.Parameter(torch.randn(1, 1, j)))\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        if hasattr(self, 'input_multiplier'):\n            result = x * self.input_multiplier[0]\n        else:\n            result = x\n        for layer_id, layer in enumerate(self.layers):\n            result = layer(result)\n            if self.model_type == 'conventional' and layer_id != len(self.layers) -1:\n                result = self.activation(result)\n            elif self.model_type == 'swish' and layer_id != len(self.layers) - 1:\n                result = swish(result)\n            elif self.model_type == 'SIREN' and layer_id != len(self.layers) - 1:\n                result = torch.sin(result * self.siren_multiplier)\n            elif self.model_type == 'FILM SIREN' and layer_id != len(self.layers) - 1:\n                result = torch.sin(self.alpha[layer_id][0] * result + self.alpha[layer_id][1])\n            elif self.model_type == 'Gaussian' and layer_id != len(self.layers) - 1: \n                result = gaussian(result, self.alpha[layer_id][0])\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.multi_layer_perceptron.__init__","title":"<code>__init__(dimensions, activation=torch.nn.ReLU(), bias=False, model_type='conventional', siren_multiplier=1.0, input_multiplier=None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>dimensions</code>           \u2013            <pre><code>            List of integers representing the dimensions of each layer (e.g., [2, 10, 1], where the first layer has two channels and last one has one channel.).\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>            Nonlinear activation function.\n            Default is `torch.nn.ReLU()`.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>            If set to True, linear layers will include biases.\n</code></pre> </li> <li> <code>siren_multiplier</code>           \u2013            <pre><code>            When using `SIREN` model type, this parameter functions as a hyperparameter.\n            The original SIREN work uses 30.\n            You can bypass this parameter by providing input that are not normalized and larger then one.\n</code></pre> </li> <li> <code>input_multiplier</code>           \u2013            <pre><code>            Initial value of the input multiplier before the very first layer.\n</code></pre> </li> <li> <code>model_type</code>           \u2013            <pre><code>            Model type: `conventional`, `swish`, `SIREN`, `FILM SIREN`, `Gaussian`.\n            `conventional` refers to a standard multi layer perceptron.\n            For `SIREN,` see: Sitzmann, Vincent, et al. \"Implicit neural representations with periodic activation functions.\" Advances in neural information processing systems 33 (2020): 7462-7473.\n            For `Swish,` see: Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for activation functions.\" arXiv preprint arXiv:1710.05941 (2017). \n            For `FILM SIREN,` see: Chan, Eric R., et al. \"pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n            For `Gaussian,` see: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(self,\n             dimensions,\n             activation = torch.nn.ReLU(),\n             bias = False,\n             model_type = 'conventional',\n             siren_multiplier = 1.,\n             input_multiplier = None\n            ):\n    \"\"\"\n    Parameters\n    ----------\n    dimensions        : list\n                        List of integers representing the dimensions of each layer (e.g., [2, 10, 1], where the first layer has two channels and last one has one channel.).\n    activation        : torch.nn\n                        Nonlinear activation function.\n                        Default is `torch.nn.ReLU()`.\n    bias              : bool\n                        If set to True, linear layers will include biases.\n    siren_multiplier  : float\n                        When using `SIREN` model type, this parameter functions as a hyperparameter.\n                        The original SIREN work uses 30.\n                        You can bypass this parameter by providing input that are not normalized and larger then one.\n    input_multiplier  : float\n                        Initial value of the input multiplier before the very first layer.\n    model_type        : str\n                        Model type: `conventional`, `swish`, `SIREN`, `FILM SIREN`, `Gaussian`.\n                        `conventional` refers to a standard multi layer perceptron.\n                        For `SIREN,` see: Sitzmann, Vincent, et al. \"Implicit neural representations with periodic activation functions.\" Advances in neural information processing systems 33 (2020): 7462-7473.\n                        For `Swish,` see: Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for activation functions.\" arXiv preprint arXiv:1710.05941 (2017). \n                        For `FILM SIREN,` see: Chan, Eric R., et al. \"pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n                        For `Gaussian,` see: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n    \"\"\"\n    super(multi_layer_perceptron, self).__init__()\n    self.activation = activation\n    self.bias = bias\n    self.model_type = model_type\n    self.layers = torch.nn.ModuleList()\n    self.siren_multiplier = siren_multiplier\n    self.dimensions = dimensions\n    for i in range(len(self.dimensions) - 1):\n        self.layers.append(torch.nn.Linear(self.dimensions[i], self.dimensions[i + 1], bias = self.bias))\n    if not isinstance(input_multiplier, type(None)):\n        self.input_multiplier = torch.nn.ParameterList()\n        self.input_multiplier.append(torch.nn.Parameter(torch.ones(1, self.dimensions[0]) * input_multiplier))\n    if self.model_type == 'FILM SIREN':\n        self.alpha = torch.nn.ParameterList()\n        for j in self.dimensions[1::]:\n            self.alpha.append(torch.nn.Parameter(torch.randn(2, 1, j)))\n    if self.model_type == 'Gaussian':\n        self.alpha = torch.nn.ParameterList()\n        for j in self.dimensions[1::]:\n            self.alpha.append(torch.nn.Parameter(torch.randn(1, 1, j)))\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.multi_layer_perceptron.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    if hasattr(self, 'input_multiplier'):\n        result = x * self.input_multiplier[0]\n    else:\n        result = x\n    for layer_id, layer in enumerate(self.layers):\n        result = layer(result)\n        if self.model_type == 'conventional' and layer_id != len(self.layers) -1:\n            result = self.activation(result)\n        elif self.model_type == 'swish' and layer_id != len(self.layers) - 1:\n            result = swish(result)\n        elif self.model_type == 'SIREN' and layer_id != len(self.layers) - 1:\n            result = torch.sin(result * self.siren_multiplier)\n        elif self.model_type == 'FILM SIREN' and layer_id != len(self.layers) - 1:\n            result = torch.sin(self.alpha[layer_id][0] * result + self.alpha[layer_id][1])\n        elif self.model_type == 'Gaussian' and layer_id != len(self.layers) - 1: \n            result = gaussian(result, self.alpha[layer_id][0])\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.non_local_layer","title":"<code>non_local_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Self-Attention Layer [zi = Wzyi + xi] (non-local block : ref https://arxiv.org/abs/1711.07971)</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class non_local_layer(torch.nn.Module):\n    \"\"\"\n    Self-Attention Layer [zi = Wzyi + xi] (non-local block : ref https://arxiv.org/abs/1711.07971)\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 1024,\n                 bottleneck_channels = 512,\n                 kernel_size = 1,\n                 bias = False,\n                ):\n        \"\"\"\n\n        Parameters\n        ----------\n        input_channels      : int\n                              Number of input channels.\n        bottleneck_channels : int\n                              Number of middle channels.\n        kernel_size         : int\n                              Kernel size.\n        bias                : bool \n                              Set to True to let convolutional layers have bias term.\n        \"\"\"\n        super(non_local_layer, self).__init__()\n        self.input_channels = input_channels\n        self.bottleneck_channels = bottleneck_channels\n        self.g = torch.nn.Conv2d(\n                                 self.input_channels, \n                                 self.bottleneck_channels,\n                                 kernel_size = kernel_size,\n                                 padding = kernel_size // 2,\n                                 bias = bias\n                                )\n        self.W_z = torch.nn.Sequential(\n                                       torch.nn.Conv2d(\n                                                       self.bottleneck_channels,\n                                                       self.input_channels, \n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       padding = kernel_size // 2\n                                                      ),\n                                       torch.nn.BatchNorm2d(self.input_channels)\n                                      )\n        torch.nn.init.constant_(self.W_z[1].weight, 0)   \n        torch.nn.init.constant_(self.W_z[1].bias, 0)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model [zi = Wzyi + xi]\n\n        Parameters\n        ----------\n        x               : torch.tensor\n                          First input data.                       \n\n\n        Returns\n        ----------\n        z               : torch.tensor\n                          Estimated output.\n        \"\"\"\n        batch_size, channels, height, width = x.size()\n        theta = x.view(batch_size, channels, -1).permute(0, 2, 1)\n        phi = x.view(batch_size, channels, -1).permute(0, 2, 1)\n        g = self.g(x).view(batch_size, self.bottleneck_channels, -1).permute(0, 2, 1)\n        attn = torch.bmm(theta, phi.transpose(1, 2)) / (height * width)\n        attn = torch.nn.functional.softmax(attn, dim=-1)\n        y = torch.bmm(attn, g).permute(0, 2, 1).contiguous().view(batch_size, self.bottleneck_channels, height, width)\n        W_y = self.W_z(y)\n        z = W_y + x\n        return z\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.non_local_layer.__init__","title":"<code>__init__(input_channels=1024, bottleneck_channels=512, kernel_size=1, bias=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>              Number of input channels.\n</code></pre> </li> <li> <code>bottleneck_channels</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <pre><code>              Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>              Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>              Set to True to let convolutional layers have bias term.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 1024,\n             bottleneck_channels = 512,\n             kernel_size = 1,\n             bias = False,\n            ):\n    \"\"\"\n\n    Parameters\n    ----------\n    input_channels      : int\n                          Number of input channels.\n    bottleneck_channels : int\n                          Number of middle channels.\n    kernel_size         : int\n                          Kernel size.\n    bias                : bool \n                          Set to True to let convolutional layers have bias term.\n    \"\"\"\n    super(non_local_layer, self).__init__()\n    self.input_channels = input_channels\n    self.bottleneck_channels = bottleneck_channels\n    self.g = torch.nn.Conv2d(\n                             self.input_channels, \n                             self.bottleneck_channels,\n                             kernel_size = kernel_size,\n                             padding = kernel_size // 2,\n                             bias = bias\n                            )\n    self.W_z = torch.nn.Sequential(\n                                   torch.nn.Conv2d(\n                                                   self.bottleneck_channels,\n                                                   self.input_channels, \n                                                   kernel_size = kernel_size,\n                                                   bias = bias,\n                                                   padding = kernel_size // 2\n                                                  ),\n                                   torch.nn.BatchNorm2d(self.input_channels)\n                                  )\n    torch.nn.init.constant_(self.W_z[1].weight, 0)   \n    torch.nn.init.constant_(self.W_z[1].bias, 0)\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.non_local_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model [zi = Wzyi + xi]</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>          First input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model [zi = Wzyi + xi]\n\n    Parameters\n    ----------\n    x               : torch.tensor\n                      First input data.                       \n\n\n    Returns\n    ----------\n    z               : torch.tensor\n                      Estimated output.\n    \"\"\"\n    batch_size, channels, height, width = x.size()\n    theta = x.view(batch_size, channels, -1).permute(0, 2, 1)\n    phi = x.view(batch_size, channels, -1).permute(0, 2, 1)\n    g = self.g(x).view(batch_size, self.bottleneck_channels, -1).permute(0, 2, 1)\n    attn = torch.bmm(theta, phi.transpose(1, 2)) / (height * width)\n    attn = torch.nn.functional.softmax(attn, dim=-1)\n    y = torch.bmm(attn, g).permute(0, 2, 1).contiguous().view(batch_size, self.bottleneck_channels, height, width)\n    W_y = self.W_z(y)\n    z = W_y + x\n    return z\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.normalization","title":"<code>normalization</code>","text":"<p>               Bases: <code>Module</code></p> <p>A normalization layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class normalization(torch.nn.Module):\n    \"\"\"\n    A normalization layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 dim = 1,\n                ):\n        \"\"\"\n        Normalization layer.\n\n\n        Parameters\n        ----------\n        dim             : int\n                          Dimension (axis) to normalize.\n        \"\"\"\n        super().__init__()\n        self.k = torch.nn.Parameter(torch.ones(1, dim, 1, 1))\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        result =  (x - mean) * (var + eps).rsqrt() * self.k\n        return result \n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.normalization.__init__","title":"<code>__init__(dim=1)</code>","text":"<p>Normalization layer.</p> <p>Parameters:</p> <ul> <li> <code>dim</code>           \u2013            <pre><code>          Dimension (axis) to normalize.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             dim = 1,\n            ):\n    \"\"\"\n    Normalization layer.\n\n\n    Parameters\n    ----------\n    dim             : int\n                      Dimension (axis) to normalize.\n    \"\"\"\n    super().__init__()\n    self.k = torch.nn.Parameter(torch.ones(1, dim, 1, 1))\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.normalization.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n    var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n    mean = torch.mean(x, dim = 1, keepdim = True)\n    result =  (x - mean) * (var + eps).rsqrt() * self.k\n    return result \n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.positional_encoder","title":"<code>positional_encoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>A positional encoder module. This implementation follows this specific work: <code>Martin-Brualla, Ricardo, Noha Radwan, Mehdi SM Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. \"Nerf in the wild: Neural radiance fields for unconstrained photo collections.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7210-7219. 2021.</code>.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class positional_encoder(torch.nn.Module):\n    \"\"\"\n    A positional encoder module.\n    This implementation follows this specific work: `Martin-Brualla, Ricardo, Noha Radwan, Mehdi SM Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. \"Nerf in the wild: Neural radiance fields for unconstrained photo collections.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7210-7219. 2021.`.\n    \"\"\"\n\n    def __init__(self, L):\n        \"\"\"\n        A positional encoder module.\n\n        Parameters\n        ----------\n        L                   : int\n                              Positional encoding level.\n        \"\"\"\n        super(positional_encoder, self).__init__()\n        self.L = L\n\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x               : torch.tensor\n                          Input data [b x n], where `b` is batch size, `n` is the feature size.\n\n        Returns\n        ----------\n        result          : torch.tensor\n                          Result of the forward operation.\n        \"\"\"\n        freqs = 2 ** torch.arange(self.L, device = x.device)\n        freqs = freqs.view(1, 1, -1)\n        results_cos = torch.cos(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n        results_sin = torch.sin(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n        results = torch.cat((x, results_cos, results_sin), dim = 1)\n        return results\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.positional_encoder.__init__","title":"<code>__init__(L)</code>","text":"<p>A positional encoder module.</p> <p>Parameters:</p> <ul> <li> <code>L</code>           \u2013            <pre><code>              Positional encoding level.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(self, L):\n    \"\"\"\n    A positional encoder module.\n\n    Parameters\n    ----------\n    L                   : int\n                          Positional encoding level.\n    \"\"\"\n    super(positional_encoder, self).__init__()\n    self.L = L\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.positional_encoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>          Input data [b x n], where `b` is batch size, `n` is the feature size.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x               : torch.tensor\n                      Input data [b x n], where `b` is batch size, `n` is the feature size.\n\n    Returns\n    ----------\n    result          : torch.tensor\n                      Result of the forward operation.\n    \"\"\"\n    freqs = 2 ** torch.arange(self.L, device = x.device)\n    freqs = freqs.view(1, 1, -1)\n    results_cos = torch.cos(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n    results_sin = torch.sin(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n    results = torch.cat((x, results_cos, results_sin), dim = 1)\n    return results\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.residual_attention_layer","title":"<code>residual_attention_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A residual block with an attention layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class residual_attention_layer(torch.nn.Module):\n    \"\"\"\n    A residual block with an attention layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 1,\n                 bias = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        An attention layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int or optioal\n                          Number of input channels.\n        output_channels : int or optional\n                          Number of middle channels.\n        kernel_size     : int or optional\n                          Kernel size.\n        bias            : bool or optional\n                          Set to True to let convolutional layers have bias term.\n        activation      : torch.nn or optional\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.activation = activation\n        self.convolution0 = torch.nn.Sequential(\n                                                torch.nn.Conv2d(\n                                                                input_channels,\n                                                                output_channels,\n                                                                kernel_size = kernel_size,\n                                                                padding = kernel_size // 2,\n                                                                bias = bias\n                                                               ),\n                                                torch.nn.BatchNorm2d(output_channels)\n                                               )\n        self.convolution1 = torch.nn.Sequential(\n                                                torch.nn.Conv2d(\n                                                                input_channels,\n                                                                output_channels,\n                                                                kernel_size = kernel_size,\n                                                                padding = kernel_size // 2,\n                                                                bias = bias\n                                                               ),\n                                                torch.nn.BatchNorm2d(output_channels)\n                                               )\n        self.final_layer = torch.nn.Sequential(\n                                               self.activation,\n                                               torch.nn.Conv2d(\n                                                               output_channels,\n                                                               output_channels,\n                                                               kernel_size = kernel_size,\n                                                               padding = kernel_size // 2,\n                                                               bias = bias\n                                                              )\n                                              )\n\n\n    def forward(self, x0, x1):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x0             : torch.tensor\n                         First input data.\n\n        x1             : torch.tensor\n                         Seconnd input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        y0 = self.convolution0(x0)\n        y1 = self.convolution1(x1)\n        y2 = torch.add(y0, y1)\n        result = self.final_layer(y2) * x0\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.residual_attention_layer.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=1, bias=False, activation=torch.nn.ReLU())</code>","text":"<p>An attention layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int or optional</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 1,\n             bias = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    An attention layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int or optioal\n                      Number of input channels.\n    output_channels : int or optional\n                      Number of middle channels.\n    kernel_size     : int or optional\n                      Kernel size.\n    bias            : bool or optional\n                      Set to True to let convolutional layers have bias term.\n    activation      : torch.nn or optional\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.activation = activation\n    self.convolution0 = torch.nn.Sequential(\n                                            torch.nn.Conv2d(\n                                                            input_channels,\n                                                            output_channels,\n                                                            kernel_size = kernel_size,\n                                                            padding = kernel_size // 2,\n                                                            bias = bias\n                                                           ),\n                                            torch.nn.BatchNorm2d(output_channels)\n                                           )\n    self.convolution1 = torch.nn.Sequential(\n                                            torch.nn.Conv2d(\n                                                            input_channels,\n                                                            output_channels,\n                                                            kernel_size = kernel_size,\n                                                            padding = kernel_size // 2,\n                                                            bias = bias\n                                                           ),\n                                            torch.nn.BatchNorm2d(output_channels)\n                                           )\n    self.final_layer = torch.nn.Sequential(\n                                           self.activation,\n                                           torch.nn.Conv2d(\n                                                           output_channels,\n                                                           output_channels,\n                                                           kernel_size = kernel_size,\n                                                           padding = kernel_size // 2,\n                                                           bias = bias\n                                                          )\n                                          )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.residual_attention_layer.forward","title":"<code>forward(x0, x1)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x0</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x1</code>           \u2013            <pre><code>         Seconnd input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x0, x1):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x0             : torch.tensor\n                     First input data.\n\n    x1             : torch.tensor\n                     Seconnd input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    y0 = self.convolution0(x0)\n    y1 = self.convolution1(x1)\n    y2 = torch.add(y0, y1)\n    result = self.final_layer(y2) * x0\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.residual_layer","title":"<code>residual_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A residual layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class residual_layer(torch.nn.Module):\n    \"\"\"\n    A residual layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 mid_channels = 16,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = True,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A convolutional layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels    : int\n                          Number of middle channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.activation = activation\n        self.convolution = double_convolution(\n                                              input_channels,\n                                              mid_channels = mid_channels,\n                                              output_channels = input_channels,\n                                              kernel_size = kernel_size,\n                                              normalization = normalization,\n                                              bias = bias,\n                                              activation = activation\n                                             )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        x0 = self.convolution(x)\n        return x + x0\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.residual_layer.__init__","title":"<code>__init__(input_channels=2, mid_channels=16, kernel_size=3, bias=False, normalization=True, activation=torch.nn.ReLU())</code>","text":"<p>A convolutional layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             mid_channels = 16,\n             kernel_size = 3,\n             bias = False,\n             normalization = True,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A convolutional layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels    : int\n                      Number of middle channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.activation = activation\n    self.convolution = double_convolution(\n                                          input_channels,\n                                          mid_channels = mid_channels,\n                                          output_channels = input_channels,\n                                          kernel_size = kernel_size,\n                                          normalization = normalization,\n                                          bias = bias,\n                                          activation = activation\n                                         )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.residual_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    x0 = self.convolution(x)\n    return x + x0\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatial_gate","title":"<code>spatial_gate</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatial attention module that applies a convolution layer after channel pooling. This class is heavily inspired by https://github.com/Jongchan/attention-module/blob/master/MODELS/cbam.py.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatial_gate(torch.nn.Module):\n    \"\"\"\n    Spatial attention module that applies a convolution layer after channel pooling.\n    This class is heavily inspired by https://github.com/Jongchan/attention-module/blob/master/MODELS/cbam.py.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the spatial gate module.\n        \"\"\"\n        super().__init__()\n        kernel_size = 7\n        self.spatial = convolution_layer(2, 1, kernel_size, bias = False, activation = torch.nn.Identity())\n\n\n    def channel_pool(self, x):\n        \"\"\"\n        Applies max and average pooling on the channels.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input tensor.\n\n        Returns\n        -------\n        output        : torch.tensor\n                        Output tensor.\n        \"\"\"\n        max_pool = torch.max(x, 1)[0].unsqueeze(1)\n        avg_pool = torch.mean(x, 1).unsqueeze(1)\n        output = torch.cat((max_pool, avg_pool), dim=1)\n        return output\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the SpatialGate module.\n\n        Applies spatial attention to the input tensor.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the SpatialGate module.\n\n        Returns\n        -------\n        scaled_x     : torch.tensor\n                       Output tensor after applying spatial attention.\n        \"\"\"\n        x_compress = self.channel_pool(x)\n        x_out = self.spatial(x_compress)\n        scale = torch.sigmoid(x_out)\n        scaled_x = x * scale\n        return scaled_x\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatial_gate.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the spatial gate module.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the spatial gate module.\n    \"\"\"\n    super().__init__()\n    kernel_size = 7\n    self.spatial = convolution_layer(2, 1, kernel_size, bias = False, activation = torch.nn.Identity())\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatial_gate.channel_pool","title":"<code>channel_pool(x)</code>","text":"<p>Applies max and average pooling on the channels.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input tensor.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Output tensor.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def channel_pool(self, x):\n    \"\"\"\n    Applies max and average pooling on the channels.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input tensor.\n\n    Returns\n    -------\n    output        : torch.tensor\n                    Output tensor.\n    \"\"\"\n    max_pool = torch.max(x, 1)[0].unsqueeze(1)\n    avg_pool = torch.mean(x, 1).unsqueeze(1)\n    output = torch.cat((max_pool, avg_pool), dim=1)\n    return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatial_gate.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the SpatialGate module.</p> <p>Applies spatial attention to the input tensor.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the SpatialGate module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>scaled_x</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying spatial attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the SpatialGate module.\n\n    Applies spatial attention to the input tensor.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the SpatialGate module.\n\n    Returns\n    -------\n    scaled_x     : torch.tensor\n                   Output tensor after applying spatial attention.\n    \"\"\"\n    x_compress = self.channel_pool(x)\n    x_out = self.spatial(x_compress)\n    scale = torch.sigmoid(x_out)\n    scaled_x = x * scale\n    return scaled_x\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_adaptive_convolution","title":"<code>spatially_adaptive_convolution</code>","text":"<p>               Bases: <code>Module</code></p> <p>A spatially adaptive convolution layer.</p> References <p>C. Zheng et al. \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions.\" C. Xu et al. \"Squeezesegv3: Spatially-adaptive Convolution for Efficient Point-Cloud Segmentation.\" C. Zheng et al. \"Windowing Decomposition Convolutional Neural Network for Image Enhancement.\"</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatially_adaptive_convolution(torch.nn.Module):\n    \"\"\"\n    A spatially adaptive convolution layer.\n\n    References\n    ----------\n\n    C. Zheng et al. \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions.\"\n    C. Xu et al. \"Squeezesegv3: Spatially-adaptive Convolution for Efficient Point-Cloud Segmentation.\"\n    C. Zheng et al. \"Windowing Decomposition Convolutional Neural Network for Image Enhancement.\"\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 stride = 1,\n                 padding = 1,\n                 bias = False,\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        Initializes a spatially adaptive convolution layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Size of the convolution kernel.\n        stride          : int\n                          Stride of the convolution.\n        padding         : int\n                          Padding added to both sides of the input.\n        bias            : bool\n                          If True, includes a bias term in the convolution.\n        activation      : torch.nn.Module\n                          Activation function to apply. If None, no activation is applied.\n        \"\"\"\n        super(spatially_adaptive_convolution, self).__init__()\n        self.kernel_size = kernel_size\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.stride = stride\n        self.padding = padding\n        self.standard_convolution = torch.nn.Conv2d(\n                                                    in_channels = input_channels,\n                                                    out_channels = self.output_channels,\n                                                    kernel_size = kernel_size,\n                                                    stride = stride,\n                                                    padding = padding,\n                                                    bias = bias\n                                                   )\n        self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n        self.activation = activation\n\n\n    def forward(self, x, sv_kernel_feature):\n        \"\"\"\n        Forward pass for the spatially adaptive convolution layer.\n\n        Parameters\n        ----------\n        x                  : torch.tensor\n                            Input data tensor.\n                            Dimension: (1, C, H, W)\n        sv_kernel_feature   : torch.tensor\n                            Spatially varying kernel features.\n                            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n        Returns\n        -------\n        sa_output          : torch.tensor\n                            Estimated output tensor.\n                            Dimension: (1, output_channels, H_out, W_out)\n        \"\"\"\n        # Pad input and sv_kernel_feature if necessary\n        if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n                -2) * self.stride != x.size(-2):\n            diffY = sv_kernel_feature.size(-2) % self.stride\n            diffX = sv_kernel_feature.size(-1) % self.stride\n            sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                            diffY // 2, diffY - diffY // 2))\n            diffY = x.size(-2) % self.stride\n            diffX = x.size(-1) % self.stride\n            x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                            diffY // 2, diffY - diffY // 2))\n\n        # Unfold the input tensor for matrix multiplication\n        input_feature = torch.nn.functional.unfold(\n                                                   x,\n                                                   kernel_size = (self.kernel_size, self.kernel_size),\n                                                   stride = self.stride,\n                                                   padding = self.padding\n                                                  )\n\n        # Resize sv_kernel_feature to match the input feature\n        sv_kernel = sv_kernel_feature.reshape(\n                                              1,\n                                              self.input_channels * self.kernel_size * self.kernel_size,\n                                              (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                             )\n\n        # Resize weight to match the input channels and kernel size\n        si_kernel = self.weight.reshape(\n                                        self.weight_output_channels,\n                                        self.input_channels * self.kernel_size * self.kernel_size\n                                       )\n\n        # Apply spatially varying kernels\n        sv_feature = input_feature * sv_kernel\n\n        # Perform matrix multiplication\n        sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                                1, self.weight_output_channels,\n                                                                (x.size(-2) // self.stride),\n                                                                (x.size(-1) // self.stride)\n                                                               )\n        return sa_output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_adaptive_convolution.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, stride=1, padding=1, bias=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>Initializes a spatially adaptive convolution layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Size of the convolution kernel.\n</code></pre> </li> <li> <code>stride</code>           \u2013            <pre><code>          Stride of the convolution.\n</code></pre> </li> <li> <code>padding</code>           \u2013            <pre><code>          Padding added to both sides of the input.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          If True, includes a bias term in the convolution.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Activation function to apply. If None, no activation is applied.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             stride = 1,\n             padding = 1,\n             bias = False,\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    Initializes a spatially adaptive convolution layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Size of the convolution kernel.\n    stride          : int\n                      Stride of the convolution.\n    padding         : int\n                      Padding added to both sides of the input.\n    bias            : bool\n                      If True, includes a bias term in the convolution.\n    activation      : torch.nn.Module\n                      Activation function to apply. If None, no activation is applied.\n    \"\"\"\n    super(spatially_adaptive_convolution, self).__init__()\n    self.kernel_size = kernel_size\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.stride = stride\n    self.padding = padding\n    self.standard_convolution = torch.nn.Conv2d(\n                                                in_channels = input_channels,\n                                                out_channels = self.output_channels,\n                                                kernel_size = kernel_size,\n                                                stride = stride,\n                                                padding = padding,\n                                                bias = bias\n                                               )\n    self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n    self.activation = activation\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_adaptive_convolution.forward","title":"<code>forward(x, sv_kernel_feature)</code>","text":"<p>Forward pass for the spatially adaptive convolution layer.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>            Input data tensor.\n            Dimension: (1, C, H, W)\n</code></pre> </li> <li> <code>sv_kernel_feature</code>           \u2013            <pre><code>            Spatially varying kernel features.\n            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>sa_output</code> (              <code>tensor</code> )          \u2013            <p>Estimated output tensor. Dimension: (1, output_channels, H_out, W_out)</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x, sv_kernel_feature):\n    \"\"\"\n    Forward pass for the spatially adaptive convolution layer.\n\n    Parameters\n    ----------\n    x                  : torch.tensor\n                        Input data tensor.\n                        Dimension: (1, C, H, W)\n    sv_kernel_feature   : torch.tensor\n                        Spatially varying kernel features.\n                        Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n    Returns\n    -------\n    sa_output          : torch.tensor\n                        Estimated output tensor.\n                        Dimension: (1, output_channels, H_out, W_out)\n    \"\"\"\n    # Pad input and sv_kernel_feature if necessary\n    if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n            -2) * self.stride != x.size(-2):\n        diffY = sv_kernel_feature.size(-2) % self.stride\n        diffX = sv_kernel_feature.size(-1) % self.stride\n        sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                        diffY // 2, diffY - diffY // 2))\n        diffY = x.size(-2) % self.stride\n        diffX = x.size(-1) % self.stride\n        x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                        diffY // 2, diffY - diffY // 2))\n\n    # Unfold the input tensor for matrix multiplication\n    input_feature = torch.nn.functional.unfold(\n                                               x,\n                                               kernel_size = (self.kernel_size, self.kernel_size),\n                                               stride = self.stride,\n                                               padding = self.padding\n                                              )\n\n    # Resize sv_kernel_feature to match the input feature\n    sv_kernel = sv_kernel_feature.reshape(\n                                          1,\n                                          self.input_channels * self.kernel_size * self.kernel_size,\n                                          (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                         )\n\n    # Resize weight to match the input channels and kernel size\n    si_kernel = self.weight.reshape(\n                                    self.weight_output_channels,\n                                    self.input_channels * self.kernel_size * self.kernel_size\n                                   )\n\n    # Apply spatially varying kernels\n    sv_feature = input_feature * sv_kernel\n\n    # Perform matrix multiplication\n    sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                            1, self.weight_output_channels,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n    return sa_output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_adaptive_module","title":"<code>spatially_adaptive_module</code>","text":"<p>               Bases: <code>Module</code></p> <p>A spatially adaptive module that combines learned spatially adaptive convolutions.</p> References <p>Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatially_adaptive_module(torch.nn.Module):\n    \"\"\"\n    A spatially adaptive module that combines learned spatially adaptive convolutions.\n\n    References\n    ----------\n\n    Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 stride = 1,\n                 padding = 1,\n                 bias = False,\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        Initializes a spatially adaptive module.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Size of the convolution kernel.\n        stride          : int\n                          Stride of the convolution.\n        padding         : int\n                          Padding added to both sides of the input.\n        bias            : bool\n                          If True, includes a bias term in the convolution.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super(spatially_adaptive_module, self).__init__()\n        self.kernel_size = kernel_size\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.stride = stride\n        self.padding = padding\n        self.weight_output_channels = self.output_channels - 1\n        self.standard_convolution = torch.nn.Conv2d(\n                                                    in_channels = input_channels,\n                                                    out_channels = self.weight_output_channels,\n                                                    kernel_size = kernel_size,\n                                                    stride = stride,\n                                                    padding = padding,\n                                                    bias = bias\n                                                   )\n        self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n        self.activation = activation\n\n\n    def forward(self, x, sv_kernel_feature):\n        \"\"\"\n        Forward pass for the spatially adaptive module.\n\n        Parameters\n        ----------\n        x                  : torch.tensor\n                            Input data tensor.\n                            Dimension: (1, C, H, W)\n        sv_kernel_feature   : torch.tensor\n                            Spatially varying kernel features.\n                            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n        Returns\n        -------\n        output             : torch.tensor\n                            Combined output tensor from standard and spatially adaptive convolutions.\n                            Dimension: (1, output_channels, H_out, W_out)\n        \"\"\"\n        # Pad input and sv_kernel_feature if necessary\n        if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n                -2) * self.stride != x.size(-2):\n            diffY = sv_kernel_feature.size(-2) % self.stride\n            diffX = sv_kernel_feature.size(-1) % self.stride\n            sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                            diffY // 2, diffY - diffY // 2))\n            diffY = x.size(-2) % self.stride\n            diffX = x.size(-1) % self.stride\n            x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                            diffY // 2, diffY - diffY // 2))\n\n        # Unfold the input tensor for matrix multiplication\n        input_feature = torch.nn.functional.unfold(\n                                                   x,\n                                                   kernel_size = (self.kernel_size, self.kernel_size),\n                                                   stride = self.stride,\n                                                   padding = self.padding\n                                                  )\n\n        # Resize sv_kernel_feature to match the input feature\n        sv_kernel = sv_kernel_feature.reshape(\n                                              1,\n                                              self.input_channels * self.kernel_size * self.kernel_size,\n                                              (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                             )\n\n        # Apply sv_kernel to the input_feature\n        sv_feature = input_feature * sv_kernel\n\n        # Original spatially varying convolution output\n        sv_output = torch.sum(sv_feature, dim = 1).reshape(\n                                                           1,\n                                                            1,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n\n        # Reshape weight for spatially adaptive convolution\n        si_kernel = self.weight.reshape(\n                                        self.weight_output_channels,\n                                        self.input_channels * self.kernel_size * self.kernel_size\n                                       )\n\n        # Apply si_kernel on sv convolution output\n        sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                                1, self.weight_output_channels,\n                                                                (x.size(-2) // self.stride),\n                                                                (x.size(-1) // self.stride)\n                                                               )\n\n        # Combine the outputs and apply activation function\n        output = self.activation(torch.cat((sv_output, sa_output), dim = 1))\n        return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_adaptive_module.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, stride=1, padding=1, bias=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>Initializes a spatially adaptive module.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Size of the convolution kernel.\n</code></pre> </li> <li> <code>stride</code>           \u2013            <pre><code>          Stride of the convolution.\n</code></pre> </li> <li> <code>padding</code>           \u2013            <pre><code>          Padding added to both sides of the input.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          If True, includes a bias term in the convolution.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             stride = 1,\n             padding = 1,\n             bias = False,\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    Initializes a spatially adaptive module.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Size of the convolution kernel.\n    stride          : int\n                      Stride of the convolution.\n    padding         : int\n                      Padding added to both sides of the input.\n    bias            : bool\n                      If True, includes a bias term in the convolution.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super(spatially_adaptive_module, self).__init__()\n    self.kernel_size = kernel_size\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.stride = stride\n    self.padding = padding\n    self.weight_output_channels = self.output_channels - 1\n    self.standard_convolution = torch.nn.Conv2d(\n                                                in_channels = input_channels,\n                                                out_channels = self.weight_output_channels,\n                                                kernel_size = kernel_size,\n                                                stride = stride,\n                                                padding = padding,\n                                                bias = bias\n                                               )\n    self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n    self.activation = activation\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_adaptive_module.forward","title":"<code>forward(x, sv_kernel_feature)</code>","text":"<p>Forward pass for the spatially adaptive module.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>            Input data tensor.\n            Dimension: (1, C, H, W)\n</code></pre> </li> <li> <code>sv_kernel_feature</code>           \u2013            <pre><code>            Spatially varying kernel features.\n            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Combined output tensor from standard and spatially adaptive convolutions. Dimension: (1, output_channels, H_out, W_out)</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x, sv_kernel_feature):\n    \"\"\"\n    Forward pass for the spatially adaptive module.\n\n    Parameters\n    ----------\n    x                  : torch.tensor\n                        Input data tensor.\n                        Dimension: (1, C, H, W)\n    sv_kernel_feature   : torch.tensor\n                        Spatially varying kernel features.\n                        Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n    Returns\n    -------\n    output             : torch.tensor\n                        Combined output tensor from standard and spatially adaptive convolutions.\n                        Dimension: (1, output_channels, H_out, W_out)\n    \"\"\"\n    # Pad input and sv_kernel_feature if necessary\n    if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n            -2) * self.stride != x.size(-2):\n        diffY = sv_kernel_feature.size(-2) % self.stride\n        diffX = sv_kernel_feature.size(-1) % self.stride\n        sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                        diffY // 2, diffY - diffY // 2))\n        diffY = x.size(-2) % self.stride\n        diffX = x.size(-1) % self.stride\n        x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                        diffY // 2, diffY - diffY // 2))\n\n    # Unfold the input tensor for matrix multiplication\n    input_feature = torch.nn.functional.unfold(\n                                               x,\n                                               kernel_size = (self.kernel_size, self.kernel_size),\n                                               stride = self.stride,\n                                               padding = self.padding\n                                              )\n\n    # Resize sv_kernel_feature to match the input feature\n    sv_kernel = sv_kernel_feature.reshape(\n                                          1,\n                                          self.input_channels * self.kernel_size * self.kernel_size,\n                                          (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                         )\n\n    # Apply sv_kernel to the input_feature\n    sv_feature = input_feature * sv_kernel\n\n    # Original spatially varying convolution output\n    sv_output = torch.sum(sv_feature, dim = 1).reshape(\n                                                       1,\n                                                        1,\n                                                        (x.size(-2) // self.stride),\n                                                        (x.size(-1) // self.stride)\n                                                       )\n\n    # Reshape weight for spatially adaptive convolution\n    si_kernel = self.weight.reshape(\n                                    self.weight_output_channels,\n                                    self.input_channels * self.kernel_size * self.kernel_size\n                                   )\n\n    # Apply si_kernel on sv convolution output\n    sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                            1, self.weight_output_channels,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n\n    # Combine the outputs and apply activation function\n    output = self.activation(torch.cat((sv_output, sa_output), dim = 1))\n    return output\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_adaptive_unet","title":"<code>spatially_adaptive_unet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatially varying U-Net model based on spatially adaptive convolution.</p> References <p>Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class spatially_adaptive_unet(torch.nn.Module):\n    \"\"\"\n    Spatially varying U-Net model based on spatially adaptive convolution.\n\n    References\n    ----------\n\n    Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.\n    \"\"\"\n    def __init__(\n                 self,\n                 depth=3,\n                 dimensions=8,\n                 input_channels=6,\n                 out_channels=6,\n                 kernel_size=3,\n                 bias=True,\n                 normalization=False,\n                 activation=torch.nn.LeakyReLU(0.2, inplace=True)\n                ):\n        \"\"\"\n        U-Net model.\n\n        Parameters\n        ----------\n        depth          : int\n                         Number of upsampling and downsampling layers.\n        dimensions     : int\n                         Number of dimensions.\n        input_channels : int\n                         Number of input channels.\n        out_channels   : int\n                         Number of output channels.\n        bias           : bool\n                         Set to True to let convolutional layers learn a bias term.\n        normalization  : bool\n                         If True, adds a Batch Normalization layer after the convolutional layer.\n        activation     : torch.nn\n                         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n        \"\"\"\n        super().__init__()\n        self.depth = depth\n        self.out_channels = out_channels\n        self.inc = convolution_layer(\n                                     input_channels=input_channels,\n                                     output_channels=dimensions,\n                                     kernel_size=kernel_size,\n                                     bias=bias,\n                                     normalization=normalization,\n                                     activation=activation\n                                    )\n\n        self.encoder = torch.nn.ModuleList()\n        for i in range(self.depth + 1):  # Downsampling layers\n            down_in_channels = dimensions * (2 ** i)\n            down_out_channels = 2 * down_in_channels\n            pooling_layer = torch.nn.AvgPool2d(2)\n            double_convolution_layer = double_convolution(\n                                                          input_channels=down_in_channels,\n                                                          mid_channels=down_in_channels,\n                                                          output_channels=down_in_channels,\n                                                          kernel_size=kernel_size,\n                                                          bias=bias,\n                                                          normalization=normalization,\n                                                          activation=activation\n                                                         )\n            sam = spatially_adaptive_module(\n                                            input_channels=down_in_channels,\n                                            output_channels=down_out_channels,\n                                            kernel_size=kernel_size,\n                                            bias=bias,\n                                            activation=activation\n                                           )\n            self.encoder.append(torch.nn.ModuleList([pooling_layer, double_convolution_layer, sam]))\n        self.global_feature_module = torch.nn.ModuleList()\n        double_convolution_layer = double_convolution(\n                                                      input_channels=dimensions * (2 ** (depth + 1)),\n                                                      mid_channels=dimensions * (2 ** (depth + 1)),\n                                                      output_channels=dimensions * (2 ** (depth + 1)),\n                                                      kernel_size=kernel_size,\n                                                      bias=bias,\n                                                      normalization=normalization,\n                                                      activation=activation\n                                                     )\n        global_feature_layer = global_feature_module(\n                                                     input_channels=dimensions * (2 ** (depth + 1)),\n                                                     mid_channels=dimensions * (2 ** (depth + 1)),\n                                                     output_channels=dimensions * (2 ** (depth + 1)),\n                                                     kernel_size=kernel_size,\n                                                     bias=bias,\n                                                     activation=torch.nn.LeakyReLU(0.2, inplace=True)\n                                                    )\n        self.global_feature_module.append(torch.nn.ModuleList([double_convolution_layer, global_feature_layer]))\n        self.decoder = torch.nn.ModuleList()\n        for i in range(depth, -1, -1):\n            up_in_channels = dimensions * (2 ** (i + 1))\n            up_mid_channels = up_in_channels // 2\n            if i == 0:\n                up_out_channels = self.out_channels\n                upsample_layer = upsample_convtranspose2d_layer(\n                                                                input_channels=up_in_channels,\n                                                                output_channels=up_mid_channels,\n                                                                kernel_size=2,\n                                                                stride=2,\n                                                                bias=bias,\n                                                               )\n                conv_layer = torch.nn.Sequential(\n                    convolution_layer(\n                                      input_channels=up_mid_channels,\n                                      output_channels=up_mid_channels,\n                                      kernel_size=kernel_size,\n                                      bias=bias,\n                                      normalization=normalization,\n                                      activation=activation,\n                                     ),\n                    convolution_layer(\n                                      input_channels=up_mid_channels,\n                                      output_channels=up_out_channels,\n                                      kernel_size=1,\n                                      bias=bias,\n                                      normalization=normalization,\n                                      activation=None,\n                                     )\n                )\n                self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n            else:\n                up_out_channels = up_in_channels // 2\n                upsample_layer = upsample_convtranspose2d_layer(\n                                                                input_channels=up_in_channels,\n                                                                output_channels=up_mid_channels,\n                                                                kernel_size=2,\n                                                                stride=2,\n                                                                bias=bias,\n                                                               )\n                conv_layer = double_convolution(\n                                                input_channels=up_mid_channels,\n                                                mid_channels=up_mid_channels,\n                                                output_channels=up_out_channels,\n                                                kernel_size=kernel_size,\n                                                bias=bias,\n                                                normalization=normalization,\n                                                activation=activation,\n                                               )\n                self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n\n\n    def forward(self, sv_kernel, field):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        sv_kernel : list of torch.tensor\n                    Learned spatially varying kernels.\n                    Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                    where C_i, H_i, and W_i represent the channel, height, and width\n                    of each feature at a certain scale.\n\n        field     : torch.tensor\n                    Input field data.\n                    Dimension: (1, 6, H, W)\n\n        Returns\n        -------\n        target_field : torch.tensor\n                       Estimated output.\n                       Dimension: (1, 6, H, W)\n        \"\"\"\n        x = self.inc(field)\n        downsampling_outputs = [x]\n        for i, down_layer in enumerate(self.encoder):\n            x_down = down_layer[0](downsampling_outputs[-1])\n            downsampling_outputs.append(x_down)\n            sam_output = down_layer[2](x_down + down_layer[1](x_down), sv_kernel[self.depth - i])\n            downsampling_outputs.append(sam_output)\n        global_feature = self.global_feature_module[0][0](downsampling_outputs[-1])\n        global_feature = self.global_feature_module[0][1](downsampling_outputs[-1], global_feature)\n        downsampling_outputs.append(global_feature)\n        x_up = downsampling_outputs[-1]\n        for i, up_layer in enumerate(self.decoder):\n            x_up = up_layer[0](x_up, downsampling_outputs[2 * (self.depth - i)])\n            x_up = up_layer[1](x_up)\n        result = x_up\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_adaptive_unet.__init__","title":"<code>__init__(depth=3, dimensions=8, input_channels=6, out_channels=6, kernel_size=3, bias=True, normalization=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>U-Net model.</p> <p>Parameters:</p> <ul> <li> <code>depth</code>           \u2013            <pre><code>         Number of upsampling and downsampling layers.\n</code></pre> </li> <li> <code>dimensions</code>           \u2013            <pre><code>         Number of dimensions.\n</code></pre> </li> <li> <code>input_channels</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <pre><code>         Number of input channels.\n</code></pre> </li> <li> <code>out_channels</code>           \u2013            <pre><code>         Number of output channels.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>         Set to True to let convolutional layers learn a bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>         If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(\n             self,\n             depth=3,\n             dimensions=8,\n             input_channels=6,\n             out_channels=6,\n             kernel_size=3,\n             bias=True,\n             normalization=False,\n             activation=torch.nn.LeakyReLU(0.2, inplace=True)\n            ):\n    \"\"\"\n    U-Net model.\n\n    Parameters\n    ----------\n    depth          : int\n                     Number of upsampling and downsampling layers.\n    dimensions     : int\n                     Number of dimensions.\n    input_channels : int\n                     Number of input channels.\n    out_channels   : int\n                     Number of output channels.\n    bias           : bool\n                     Set to True to let convolutional layers learn a bias term.\n    normalization  : bool\n                     If True, adds a Batch Normalization layer after the convolutional layer.\n    activation     : torch.nn\n                     Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n    \"\"\"\n    super().__init__()\n    self.depth = depth\n    self.out_channels = out_channels\n    self.inc = convolution_layer(\n                                 input_channels=input_channels,\n                                 output_channels=dimensions,\n                                 kernel_size=kernel_size,\n                                 bias=bias,\n                                 normalization=normalization,\n                                 activation=activation\n                                )\n\n    self.encoder = torch.nn.ModuleList()\n    for i in range(self.depth + 1):  # Downsampling layers\n        down_in_channels = dimensions * (2 ** i)\n        down_out_channels = 2 * down_in_channels\n        pooling_layer = torch.nn.AvgPool2d(2)\n        double_convolution_layer = double_convolution(\n                                                      input_channels=down_in_channels,\n                                                      mid_channels=down_in_channels,\n                                                      output_channels=down_in_channels,\n                                                      kernel_size=kernel_size,\n                                                      bias=bias,\n                                                      normalization=normalization,\n                                                      activation=activation\n                                                     )\n        sam = spatially_adaptive_module(\n                                        input_channels=down_in_channels,\n                                        output_channels=down_out_channels,\n                                        kernel_size=kernel_size,\n                                        bias=bias,\n                                        activation=activation\n                                       )\n        self.encoder.append(torch.nn.ModuleList([pooling_layer, double_convolution_layer, sam]))\n    self.global_feature_module = torch.nn.ModuleList()\n    double_convolution_layer = double_convolution(\n                                                  input_channels=dimensions * (2 ** (depth + 1)),\n                                                  mid_channels=dimensions * (2 ** (depth + 1)),\n                                                  output_channels=dimensions * (2 ** (depth + 1)),\n                                                  kernel_size=kernel_size,\n                                                  bias=bias,\n                                                  normalization=normalization,\n                                                  activation=activation\n                                                 )\n    global_feature_layer = global_feature_module(\n                                                 input_channels=dimensions * (2 ** (depth + 1)),\n                                                 mid_channels=dimensions * (2 ** (depth + 1)),\n                                                 output_channels=dimensions * (2 ** (depth + 1)),\n                                                 kernel_size=kernel_size,\n                                                 bias=bias,\n                                                 activation=torch.nn.LeakyReLU(0.2, inplace=True)\n                                                )\n    self.global_feature_module.append(torch.nn.ModuleList([double_convolution_layer, global_feature_layer]))\n    self.decoder = torch.nn.ModuleList()\n    for i in range(depth, -1, -1):\n        up_in_channels = dimensions * (2 ** (i + 1))\n        up_mid_channels = up_in_channels // 2\n        if i == 0:\n            up_out_channels = self.out_channels\n            upsample_layer = upsample_convtranspose2d_layer(\n                                                            input_channels=up_in_channels,\n                                                            output_channels=up_mid_channels,\n                                                            kernel_size=2,\n                                                            stride=2,\n                                                            bias=bias,\n                                                           )\n            conv_layer = torch.nn.Sequential(\n                convolution_layer(\n                                  input_channels=up_mid_channels,\n                                  output_channels=up_mid_channels,\n                                  kernel_size=kernel_size,\n                                  bias=bias,\n                                  normalization=normalization,\n                                  activation=activation,\n                                 ),\n                convolution_layer(\n                                  input_channels=up_mid_channels,\n                                  output_channels=up_out_channels,\n                                  kernel_size=1,\n                                  bias=bias,\n                                  normalization=normalization,\n                                  activation=None,\n                                 )\n            )\n            self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n        else:\n            up_out_channels = up_in_channels // 2\n            upsample_layer = upsample_convtranspose2d_layer(\n                                                            input_channels=up_in_channels,\n                                                            output_channels=up_mid_channels,\n                                                            kernel_size=2,\n                                                            stride=2,\n                                                            bias=bias,\n                                                           )\n            conv_layer = double_convolution(\n                                            input_channels=up_mid_channels,\n                                            mid_channels=up_mid_channels,\n                                            output_channels=up_out_channels,\n                                            kernel_size=kernel_size,\n                                            bias=bias,\n                                            normalization=normalization,\n                                            activation=activation,\n                                           )\n            self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_adaptive_unet.forward","title":"<code>forward(sv_kernel, field)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>sv_kernel</code>               (<code>list of torch.tensor</code>)           \u2013            <pre><code>    Learned spatially varying kernels.\n    Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n    where C_i, H_i, and W_i represent the channel, height, and width\n    of each feature at a certain scale.\n</code></pre> </li> <li> <code>field</code>           \u2013            <pre><code>    Input field data.\n    Dimension: (1, 6, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>target_field</code> (              <code>tensor</code> )          \u2013            <p>Estimated output. Dimension: (1, 6, H, W)</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, sv_kernel, field):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    sv_kernel : list of torch.tensor\n                Learned spatially varying kernels.\n                Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                where C_i, H_i, and W_i represent the channel, height, and width\n                of each feature at a certain scale.\n\n    field     : torch.tensor\n                Input field data.\n                Dimension: (1, 6, H, W)\n\n    Returns\n    -------\n    target_field : torch.tensor\n                   Estimated output.\n                   Dimension: (1, 6, H, W)\n    \"\"\"\n    x = self.inc(field)\n    downsampling_outputs = [x]\n    for i, down_layer in enumerate(self.encoder):\n        x_down = down_layer[0](downsampling_outputs[-1])\n        downsampling_outputs.append(x_down)\n        sam_output = down_layer[2](x_down + down_layer[1](x_down), sv_kernel[self.depth - i])\n        downsampling_outputs.append(sam_output)\n    global_feature = self.global_feature_module[0][0](downsampling_outputs[-1])\n    global_feature = self.global_feature_module[0][1](downsampling_outputs[-1], global_feature)\n    downsampling_outputs.append(global_feature)\n    x_up = downsampling_outputs[-1]\n    for i, up_layer in enumerate(self.decoder):\n        x_up = up_layer[0](x_up, downsampling_outputs[2 * (self.depth - i)])\n        x_up = up_layer[1](x_up)\n    result = x_up\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_varying_kernel_generation_model","title":"<code>spatially_varying_kernel_generation_model</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatially_varying_kernel_generation_model revised from RSGUnet: https://github.com/MTLab/rsgunet_image_enhance.</p> <p>Refer to: J. Huang, P. Zhu, M. Geng et al. Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class spatially_varying_kernel_generation_model(torch.nn.Module):\n    \"\"\"\n    Spatially_varying_kernel_generation_model revised from RSGUnet:\n    https://github.com/MTLab/rsgunet_image_enhance.\n\n    Refer to:\n    J. Huang, P. Zhu, M. Geng et al. Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.\n    \"\"\"\n\n    def __init__(\n                 self,\n                 depth = 3,\n                 dimensions = 8,\n                 input_channels = 7,\n                 kernel_size = 3,\n                 bias = True,\n                 normalization = False,\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        U-Net model.\n\n        Parameters\n        ----------\n        depth          : int\n                         Number of upsampling and downsampling layers.\n        dimensions     : int\n                         Number of dimensions.\n        input_channels : int\n                         Number of input channels.\n        bias           : bool\n                         Set to True to let convolutional layers learn a bias term.\n        normalization  : bool\n                         If True, adds a Batch Normalization layer after the convolutional layer.\n        activation     : torch.nn\n                         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n        \"\"\"\n        super().__init__()\n        self.depth = depth\n        self.inc = convolution_layer(\n                                     input_channels = input_channels,\n                                     output_channels = dimensions,\n                                     kernel_size = kernel_size,\n                                     bias = bias,\n                                     normalization = normalization,\n                                     activation = activation\n                                    )\n        self.encoder = torch.nn.ModuleList()\n        for i in range(depth + 1):  # downsampling layers\n            if i == 0:\n                in_channels = dimensions * (2 ** i)\n                out_channels = dimensions * (2 ** i)\n            elif i == depth:\n                in_channels = dimensions * (2 ** (i - 1))\n                out_channels = dimensions * (2 ** (i - 1))\n            else:\n                in_channels = dimensions * (2 ** (i - 1))\n                out_channels = 2 * in_channels\n            pooling_layer = torch.nn.AvgPool2d(2)\n            double_convolution_layer = double_convolution(\n                                                          input_channels = in_channels,\n                                                          mid_channels = in_channels,\n                                                          output_channels = out_channels,\n                                                          kernel_size = kernel_size,\n                                                          bias = bias,\n                                                          normalization = normalization,\n                                                          activation = activation\n                                                         )\n            self.encoder.append(pooling_layer)\n            self.encoder.append(double_convolution_layer)\n        self.spatially_varying_feature = torch.nn.ModuleList()  # for kernel generation\n        for i in range(depth, -1, -1):\n            if i == 1:\n                svf_in_channels = dimensions + 2 ** (self.depth + i) + 1\n            else:\n                svf_in_channels = 2 ** (self.depth + i) + 1\n            svf_out_channels = (2 ** (self.depth + i)) * (kernel_size * kernel_size)\n            svf_mid_channels = dimensions * (2 ** (self.depth - 1))\n            spatially_varying_kernel_generation = torch.nn.ModuleList()\n            for j in range(i, -1, -1):\n                pooling_layer = torch.nn.AvgPool2d(2 ** (j + 1))\n                spatially_varying_kernel_generation.append(pooling_layer)\n            kernel_generation_block = torch.nn.Sequential(\n                torch.nn.Conv2d(\n                                in_channels = svf_in_channels,\n                                out_channels = svf_mid_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               ),\n                activation,\n                torch.nn.Conv2d(\n                                in_channels = svf_mid_channels,\n                                out_channels = svf_mid_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               ),\n                activation,\n                torch.nn.Conv2d(\n                                in_channels = svf_mid_channels,\n                                out_channels = svf_out_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               ),\n            )\n            spatially_varying_kernel_generation.append(kernel_generation_block)\n            self.spatially_varying_feature.append(spatially_varying_kernel_generation)\n        self.decoder = torch.nn.ModuleList()\n        global_feature_layer = global_feature_module(  # global feature layer\n                                                     input_channels = dimensions * (2 ** (depth - 1)),\n                                                     mid_channels = dimensions * (2 ** (depth - 1)),\n                                                     output_channels = dimensions * (2 ** (depth - 1)),\n                                                     kernel_size = kernel_size,\n                                                     bias = bias,\n                                                     activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                                                    )\n        self.decoder.append(global_feature_layer)\n        for i in range(depth, 0, -1):\n            if i == 2:\n                up_in_channels = (dimensions // 2) * (2 ** i)\n                up_out_channels = up_in_channels\n                up_mid_channels = up_in_channels\n            elif i == 1:\n                up_in_channels = dimensions * 2\n                up_out_channels = dimensions\n                up_mid_channels = up_out_channels\n            else:\n                up_in_channels = (dimensions // 2) * (2 ** i)\n                up_out_channels = up_in_channels // 2\n                up_mid_channels = up_in_channels\n            upsample_layer = upsample_convtranspose2d_layer(\n                                                            input_channels = up_in_channels,\n                                                            output_channels = up_mid_channels,\n                                                            kernel_size = 2,\n                                                            stride = 2,\n                                                            bias = bias,\n                                                           )\n            conv_layer = double_convolution(\n                                            input_channels = up_mid_channels,\n                                            output_channels = up_out_channels,\n                                            kernel_size = kernel_size,\n                                            bias = bias,\n                                            normalization = normalization,\n                                            activation = activation,\n                                           )\n            self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n\n\n    def forward(self, focal_surface, field):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        focal_surface : torch.tensor\n                        Input focal surface data.\n                        Dimension: (1, 1, H, W)\n\n        field         : torch.tensor\n                        Input field data.\n                        Dimension: (1, 6, H, W)\n\n        Returns\n        -------\n        sv_kernel : list of torch.tensor\n                    Learned spatially varying kernels.\n                    Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                    where C_i, H_i, and W_i represent the channel, height, and width\n                    of each feature at a certain scale.\n        \"\"\"\n        x = self.inc(torch.cat((focal_surface, field), dim = 1))\n        downsampling_outputs = [focal_surface]\n        downsampling_outputs.append(x)\n        for i, down_layer in enumerate(self.encoder):\n            x_down = down_layer(downsampling_outputs[-1])\n            downsampling_outputs.append(x_down)\n        sv_kernels = []\n        for i, (up_layer, svf_layer) in enumerate(zip(self.decoder, self.spatially_varying_feature)):\n            if i == 0:\n                global_feature = up_layer(downsampling_outputs[-2], downsampling_outputs[-1])\n                downsampling_outputs[-1] = global_feature\n                sv_feature = [global_feature, downsampling_outputs[0]]\n                for j in range(self.depth - i + 1):\n                    sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                    if j &gt; 0:\n                        sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n                sv_feature = [sv_feature[0], sv_feature[1], sv_feature[4], sv_feature[2],\n                              sv_feature[3]]\n                sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n                sv_kernels.append(sv_kernel)\n            else:\n                x_up = up_layer[0](downsampling_outputs[-1],\n                                   downsampling_outputs[2 * (self.depth + 1 - i) + 1])\n                x_up = up_layer[1](x_up)\n                downsampling_outputs[-1] = x_up\n                sv_feature = [x_up, downsampling_outputs[0]]\n                for j in range(self.depth - i + 1):\n                    sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                    if j &gt; 0:\n                        sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n                if i == 1:\n                    sv_feature = [sv_feature[0], sv_feature[1], sv_feature[3], sv_feature[2]]\n                sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n                sv_kernels.append(sv_kernel)\n        return sv_kernels\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_varying_kernel_generation_model.__init__","title":"<code>__init__(depth=3, dimensions=8, input_channels=7, kernel_size=3, bias=True, normalization=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>U-Net model.</p> <p>Parameters:</p> <ul> <li> <code>depth</code>           \u2013            <pre><code>         Number of upsampling and downsampling layers.\n</code></pre> </li> <li> <code>dimensions</code>           \u2013            <pre><code>         Number of dimensions.\n</code></pre> </li> <li> <code>input_channels</code>               (<code>int</code>, default:                   <code>7</code> )           \u2013            <pre><code>         Number of input channels.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>         Set to True to let convolutional layers learn a bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>         If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(\n             self,\n             depth = 3,\n             dimensions = 8,\n             input_channels = 7,\n             kernel_size = 3,\n             bias = True,\n             normalization = False,\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    U-Net model.\n\n    Parameters\n    ----------\n    depth          : int\n                     Number of upsampling and downsampling layers.\n    dimensions     : int\n                     Number of dimensions.\n    input_channels : int\n                     Number of input channels.\n    bias           : bool\n                     Set to True to let convolutional layers learn a bias term.\n    normalization  : bool\n                     If True, adds a Batch Normalization layer after the convolutional layer.\n    activation     : torch.nn\n                     Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n    \"\"\"\n    super().__init__()\n    self.depth = depth\n    self.inc = convolution_layer(\n                                 input_channels = input_channels,\n                                 output_channels = dimensions,\n                                 kernel_size = kernel_size,\n                                 bias = bias,\n                                 normalization = normalization,\n                                 activation = activation\n                                )\n    self.encoder = torch.nn.ModuleList()\n    for i in range(depth + 1):  # downsampling layers\n        if i == 0:\n            in_channels = dimensions * (2 ** i)\n            out_channels = dimensions * (2 ** i)\n        elif i == depth:\n            in_channels = dimensions * (2 ** (i - 1))\n            out_channels = dimensions * (2 ** (i - 1))\n        else:\n            in_channels = dimensions * (2 ** (i - 1))\n            out_channels = 2 * in_channels\n        pooling_layer = torch.nn.AvgPool2d(2)\n        double_convolution_layer = double_convolution(\n                                                      input_channels = in_channels,\n                                                      mid_channels = in_channels,\n                                                      output_channels = out_channels,\n                                                      kernel_size = kernel_size,\n                                                      bias = bias,\n                                                      normalization = normalization,\n                                                      activation = activation\n                                                     )\n        self.encoder.append(pooling_layer)\n        self.encoder.append(double_convolution_layer)\n    self.spatially_varying_feature = torch.nn.ModuleList()  # for kernel generation\n    for i in range(depth, -1, -1):\n        if i == 1:\n            svf_in_channels = dimensions + 2 ** (self.depth + i) + 1\n        else:\n            svf_in_channels = 2 ** (self.depth + i) + 1\n        svf_out_channels = (2 ** (self.depth + i)) * (kernel_size * kernel_size)\n        svf_mid_channels = dimensions * (2 ** (self.depth - 1))\n        spatially_varying_kernel_generation = torch.nn.ModuleList()\n        for j in range(i, -1, -1):\n            pooling_layer = torch.nn.AvgPool2d(2 ** (j + 1))\n            spatially_varying_kernel_generation.append(pooling_layer)\n        kernel_generation_block = torch.nn.Sequential(\n            torch.nn.Conv2d(\n                            in_channels = svf_in_channels,\n                            out_channels = svf_mid_channels,\n                            kernel_size = kernel_size,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           ),\n            activation,\n            torch.nn.Conv2d(\n                            in_channels = svf_mid_channels,\n                            out_channels = svf_mid_channels,\n                            kernel_size = kernel_size,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           ),\n            activation,\n            torch.nn.Conv2d(\n                            in_channels = svf_mid_channels,\n                            out_channels = svf_out_channels,\n                            kernel_size = kernel_size,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           ),\n        )\n        spatially_varying_kernel_generation.append(kernel_generation_block)\n        self.spatially_varying_feature.append(spatially_varying_kernel_generation)\n    self.decoder = torch.nn.ModuleList()\n    global_feature_layer = global_feature_module(  # global feature layer\n                                                 input_channels = dimensions * (2 ** (depth - 1)),\n                                                 mid_channels = dimensions * (2 ** (depth - 1)),\n                                                 output_channels = dimensions * (2 ** (depth - 1)),\n                                                 kernel_size = kernel_size,\n                                                 bias = bias,\n                                                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                                                )\n    self.decoder.append(global_feature_layer)\n    for i in range(depth, 0, -1):\n        if i == 2:\n            up_in_channels = (dimensions // 2) * (2 ** i)\n            up_out_channels = up_in_channels\n            up_mid_channels = up_in_channels\n        elif i == 1:\n            up_in_channels = dimensions * 2\n            up_out_channels = dimensions\n            up_mid_channels = up_out_channels\n        else:\n            up_in_channels = (dimensions // 2) * (2 ** i)\n            up_out_channels = up_in_channels // 2\n            up_mid_channels = up_in_channels\n        upsample_layer = upsample_convtranspose2d_layer(\n                                                        input_channels = up_in_channels,\n                                                        output_channels = up_mid_channels,\n                                                        kernel_size = 2,\n                                                        stride = 2,\n                                                        bias = bias,\n                                                       )\n        conv_layer = double_convolution(\n                                        input_channels = up_mid_channels,\n                                        output_channels = up_out_channels,\n                                        kernel_size = kernel_size,\n                                        bias = bias,\n                                        normalization = normalization,\n                                        activation = activation,\n                                       )\n        self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.spatially_varying_kernel_generation_model.forward","title":"<code>forward(focal_surface, field)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>focal_surface</code>               (<code>tensor</code>)           \u2013            <pre><code>        Input focal surface data.\n        Dimension: (1, 1, H, W)\n</code></pre> </li> <li> <code>field</code>           \u2013            <pre><code>        Input field data.\n        Dimension: (1, 6, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>sv_kernel</code> (              <code>list of torch.tensor</code> )          \u2013            <p>Learned spatially varying kernels. Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i), where C_i, H_i, and W_i represent the channel, height, and width of each feature at a certain scale.</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, focal_surface, field):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    focal_surface : torch.tensor\n                    Input focal surface data.\n                    Dimension: (1, 1, H, W)\n\n    field         : torch.tensor\n                    Input field data.\n                    Dimension: (1, 6, H, W)\n\n    Returns\n    -------\n    sv_kernel : list of torch.tensor\n                Learned spatially varying kernels.\n                Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                where C_i, H_i, and W_i represent the channel, height, and width\n                of each feature at a certain scale.\n    \"\"\"\n    x = self.inc(torch.cat((focal_surface, field), dim = 1))\n    downsampling_outputs = [focal_surface]\n    downsampling_outputs.append(x)\n    for i, down_layer in enumerate(self.encoder):\n        x_down = down_layer(downsampling_outputs[-1])\n        downsampling_outputs.append(x_down)\n    sv_kernels = []\n    for i, (up_layer, svf_layer) in enumerate(zip(self.decoder, self.spatially_varying_feature)):\n        if i == 0:\n            global_feature = up_layer(downsampling_outputs[-2], downsampling_outputs[-1])\n            downsampling_outputs[-1] = global_feature\n            sv_feature = [global_feature, downsampling_outputs[0]]\n            for j in range(self.depth - i + 1):\n                sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                if j &gt; 0:\n                    sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n            sv_feature = [sv_feature[0], sv_feature[1], sv_feature[4], sv_feature[2],\n                          sv_feature[3]]\n            sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n            sv_kernels.append(sv_kernel)\n        else:\n            x_up = up_layer[0](downsampling_outputs[-1],\n                               downsampling_outputs[2 * (self.depth + 1 - i) + 1])\n            x_up = up_layer[1](x_up)\n            downsampling_outputs[-1] = x_up\n            sv_feature = [x_up, downsampling_outputs[0]]\n            for j in range(self.depth - i + 1):\n                sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                if j &gt; 0:\n                    sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n            if i == 1:\n                sv_feature = [sv_feature[0], sv_feature[1], sv_feature[3], sv_feature[2]]\n            sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n            sv_kernels.append(sv_kernel)\n    return sv_kernels\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.unet","title":"<code>unet</code>","text":"<p>               Bases: <code>Module</code></p> <p>A U-Net model, heavily inspired from <code>https://github.com/milesial/Pytorch-UNet/tree/master/unet</code> and more can be read from Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer International Publishing, 2015.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class unet(torch.nn.Module):\n    \"\"\"\n    A U-Net model, heavily inspired from `https://github.com/milesial/Pytorch-UNet/tree/master/unet` and more can be read from Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer International Publishing, 2015.\n    \"\"\"\n\n    def __init__(\n                 self, \n                 depth = 4,\n                 dimensions = 64, \n                 input_channels = 2, \n                 output_channels = 1, \n                 bilinear = False,\n                 kernel_size = 3,\n                 bias = False,\n                 activation = torch.nn.ReLU(inplace = True),\n                ):\n        \"\"\"\n        U-Net model.\n\n        Parameters\n        ----------\n        depth             : int\n                            Number of upsampling and downsampling\n        dimensions        : int\n                            Number of dimensions.\n        input_channels    : int\n                            Number of input channels.\n        output_channels   : int\n                            Number of output channels.\n        bilinear          : bool\n                            Uses bilinear upsampling in upsampling layers when set True.\n        bias              : bool\n                            Set True to let convolutional layers learn a bias term.\n        activation        : torch.nn\n                            Non-linear activation layer to be used (e.g., torch.nn.ReLU(), torch.nn.Sigmoid().\n        \"\"\"\n        super(unet, self).__init__()\n        self.inc = double_convolution(\n                                      input_channels = input_channels,\n                                      mid_channels = dimensions,\n                                      output_channels = dimensions,\n                                      kernel_size = kernel_size,\n                                      bias = bias,\n                                      activation = activation\n                                     )      \n\n        self.downsampling_layers = torch.nn.ModuleList()\n        self.upsampling_layers = torch.nn.ModuleList()\n        for i in range(depth): # downsampling layers\n            in_channels = dimensions * (2 ** i)\n            out_channels = dimensions * (2 ** (i + 1))\n            down_layer = downsample_layer(in_channels,\n                                            out_channels,\n                                            kernel_size=kernel_size,\n                                            bias=bias,\n                                            activation=activation\n                                            )\n            self.downsampling_layers.append(down_layer)      \n\n        for i in range(depth - 1, -1, -1):  # upsampling layers\n            up_in_channels = dimensions * (2 ** (i + 1))  \n            up_out_channels = dimensions * (2 ** i) \n            up_layer = upsample_layer(up_in_channels, up_out_channels, kernel_size=kernel_size, bias=bias, activation=activation, bilinear=bilinear)\n            self.upsampling_layers.append(up_layer)\n        self.outc = torch.nn.Conv2d(\n                                    dimensions, \n                                    output_channels,\n                                    kernel_size = kernel_size,\n                                    padding = kernel_size // 2,\n                                    bias = bias\n                                   )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        downsampling_outputs = [self.inc(x)]\n        for down_layer in self.downsampling_layers:\n            x_down = down_layer(downsampling_outputs[-1])\n            downsampling_outputs.append(x_down)\n        x_up = downsampling_outputs[-1]\n        for i, up_layer in enumerate((self.upsampling_layers)):\n            x_up = up_layer(x_up, downsampling_outputs[-(i + 2)])       \n        result = self.outc(x_up)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.unet.__init__","title":"<code>__init__(depth=4, dimensions=64, input_channels=2, output_channels=1, bilinear=False, kernel_size=3, bias=False, activation=torch.nn.ReLU(inplace=True))</code>","text":"<p>U-Net model.</p> <p>Parameters:</p> <ul> <li> <code>depth</code>           \u2013            <pre><code>            Number of upsampling and downsampling\n</code></pre> </li> <li> <code>dimensions</code>           \u2013            <pre><code>            Number of dimensions.\n</code></pre> </li> <li> <code>input_channels</code>           \u2013            <pre><code>            Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>           \u2013            <pre><code>            Number of output channels.\n</code></pre> </li> <li> <code>bilinear</code>           \u2013            <pre><code>            Uses bilinear upsampling in upsampling layers when set True.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>            Set True to let convolutional layers learn a bias term.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>            Non-linear activation layer to be used (e.g., torch.nn.ReLU(), torch.nn.Sigmoid().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(\n             self, \n             depth = 4,\n             dimensions = 64, \n             input_channels = 2, \n             output_channels = 1, \n             bilinear = False,\n             kernel_size = 3,\n             bias = False,\n             activation = torch.nn.ReLU(inplace = True),\n            ):\n    \"\"\"\n    U-Net model.\n\n    Parameters\n    ----------\n    depth             : int\n                        Number of upsampling and downsampling\n    dimensions        : int\n                        Number of dimensions.\n    input_channels    : int\n                        Number of input channels.\n    output_channels   : int\n                        Number of output channels.\n    bilinear          : bool\n                        Uses bilinear upsampling in upsampling layers when set True.\n    bias              : bool\n                        Set True to let convolutional layers learn a bias term.\n    activation        : torch.nn\n                        Non-linear activation layer to be used (e.g., torch.nn.ReLU(), torch.nn.Sigmoid().\n    \"\"\"\n    super(unet, self).__init__()\n    self.inc = double_convolution(\n                                  input_channels = input_channels,\n                                  mid_channels = dimensions,\n                                  output_channels = dimensions,\n                                  kernel_size = kernel_size,\n                                  bias = bias,\n                                  activation = activation\n                                 )      \n\n    self.downsampling_layers = torch.nn.ModuleList()\n    self.upsampling_layers = torch.nn.ModuleList()\n    for i in range(depth): # downsampling layers\n        in_channels = dimensions * (2 ** i)\n        out_channels = dimensions * (2 ** (i + 1))\n        down_layer = downsample_layer(in_channels,\n                                        out_channels,\n                                        kernel_size=kernel_size,\n                                        bias=bias,\n                                        activation=activation\n                                        )\n        self.downsampling_layers.append(down_layer)      \n\n    for i in range(depth - 1, -1, -1):  # upsampling layers\n        up_in_channels = dimensions * (2 ** (i + 1))  \n        up_out_channels = dimensions * (2 ** i) \n        up_layer = upsample_layer(up_in_channels, up_out_channels, kernel_size=kernel_size, bias=bias, activation=activation, bilinear=bilinear)\n        self.upsampling_layers.append(up_layer)\n    self.outc = torch.nn.Conv2d(\n                                dimensions, \n                                output_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.unet.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    downsampling_outputs = [self.inc(x)]\n    for down_layer in self.downsampling_layers:\n        x_down = down_layer(downsampling_outputs[-1])\n        downsampling_outputs.append(x_down)\n    x_up = downsampling_outputs[-1]\n    for i, up_layer in enumerate((self.upsampling_layers)):\n        x_up = up_layer(x_up, downsampling_outputs[-(i + 2)])       \n    result = self.outc(x_up)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.upsample_convtranspose2d_layer","title":"<code>upsample_convtranspose2d_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>An upsampling convtranspose2d layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class upsample_convtranspose2d_layer(torch.nn.Module):\n    \"\"\"\n    An upsampling convtranspose2d layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 2,\n                 stride = 2,\n                 bias = False,\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        \"\"\"\n        super().__init__()\n        self.up = torch.nn.ConvTranspose2d(\n                                           in_channels = input_channels,\n                                           out_channels = output_channels,\n                                           bias = bias,\n                                           kernel_size = kernel_size,\n                                           stride = stride\n                                          )\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Result of the forward operation\n        \"\"\"\n        x1 = self.up(x1)\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                          diffY // 2, diffY - diffY // 2])\n        result = x1 + x2\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.upsample_convtranspose2d_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=2, stride=2, bias=False)</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 2,\n             stride = 2,\n             bias = False,\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    \"\"\"\n    super().__init__()\n    self.up = torch.nn.ConvTranspose2d(\n                                       in_channels = input_channels,\n                                       out_channels = output_channels,\n                                       bias = bias,\n                                       kernel_size = kernel_size,\n                                       stride = stride\n                                      )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.upsample_convtranspose2d_layer.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Result of the forward operation\n    \"\"\"\n    x1 = self.up(x1)\n    diffY = x2.size()[2] - x1.size()[2]\n    diffX = x2.size()[3] - x1.size()[3]\n    x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                      diffY // 2, diffY - diffY // 2])\n    result = x1 + x2\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.upsample_layer","title":"<code>upsample_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>An upsampling convolutional layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class upsample_layer(torch.nn.Module):\n    \"\"\"\n    An upsampling convolutional layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU(),\n                 bilinear = True\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        bilinear        : bool\n                          If set to True, bilinear sampling is used.\n        \"\"\"\n        super(upsample_layer, self).__init__()\n        if bilinear:\n            self.up = torch.nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n            self.conv = double_convolution(\n                                           input_channels = input_channels + output_channels,\n                                           mid_channels = input_channels // 2,\n                                           output_channels = output_channels,\n                                           kernel_size = kernel_size,\n                                           normalization = normalization,\n                                           bias = bias,\n                                           activation = activation\n                                          )\n        else:\n            self.up = torch.nn.ConvTranspose2d(input_channels , input_channels // 2, kernel_size = 2, stride = 2)\n            self.conv = double_convolution(\n                                           input_channels = input_channels,\n                                           mid_channels = output_channels,\n                                           output_channels = output_channels,\n                                           kernel_size = kernel_size,\n                                           normalization = normalization,\n                                           bias = bias,\n                                           activation = activation\n                                          )\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Result of the forward operation\n        \"\"\" \n        x1 = self.up(x1)\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                          diffY // 2, diffY - diffY // 2])\n        x = torch.cat([x2, x1], dim = 1)\n        result = self.conv(x)\n        return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.upsample_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU(), bilinear=True)</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> <li> <code>bilinear</code>           \u2013            <pre><code>          If set to True, bilinear sampling is used.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 3,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU(),\n             bilinear = True\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    bilinear        : bool\n                      If set to True, bilinear sampling is used.\n    \"\"\"\n    super(upsample_layer, self).__init__()\n    if bilinear:\n        self.up = torch.nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n        self.conv = double_convolution(\n                                       input_channels = input_channels + output_channels,\n                                       mid_channels = input_channels // 2,\n                                       output_channels = output_channels,\n                                       kernel_size = kernel_size,\n                                       normalization = normalization,\n                                       bias = bias,\n                                       activation = activation\n                                      )\n    else:\n        self.up = torch.nn.ConvTranspose2d(input_channels , input_channels // 2, kernel_size = 2, stride = 2)\n        self.conv = double_convolution(\n                                       input_channels = input_channels,\n                                       mid_channels = output_channels,\n                                       output_channels = output_channels,\n                                       kernel_size = kernel_size,\n                                       normalization = normalization,\n                                       bias = bias,\n                                       activation = activation\n                                      )\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.upsample_layer.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Result of the forward operation\n    \"\"\" \n    x1 = self.up(x1)\n    diffY = x2.size()[2] - x1.size()[2]\n    diffX = x2.size()[3] - x1.size()[3]\n    x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                      diffY // 2, diffY - diffY // 2])\n    x = torch.cat([x2, x1], dim = 1)\n    result = self.conv(x)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.gaussian","title":"<code>gaussian(x, multiplier=1.0)</code>","text":"<p>A Gaussian non-linear activation. For more details: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input data.\n</code></pre> </li> <li> <code>multiplier</code>           \u2013            <pre><code>       Multiplier.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>float or tensor</code> )          \u2013            <p>Ouput data.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def gaussian(x, multiplier = 1.):\n    \"\"\"\n    A Gaussian non-linear activation.\n    For more details: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n\n    Parameters\n    ----------\n    x            : float or torch.tensor\n                   Input data.\n    multiplier   : float or torch.tensor\n                   Multiplier.\n\n    Returns\n    -------\n    result       : float or torch.tensor\n                   Ouput data.\n    \"\"\"\n    result = torch.exp(- (multiplier * x) ** 2)\n    return result\n</code></pre>"},{"location":"odak/learn_models/#odak.learn.models.models.swish","title":"<code>swish(x)</code>","text":"<p>A swish non-linear activation. For more details: https://en.wikipedia.org/wiki/Swish_function</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>         Input.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>float or tensor</code> )          \u2013            <p>Output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def swish(x):\n    \"\"\"\n    A swish non-linear activation.\n    For more details: https://en.wikipedia.org/wiki/Swish_function\n\n    Parameters\n    -----------\n    x              : float or torch.tensor\n                     Input.\n\n    Returns\n    -------\n    out            : float or torch.tensor\n                     Output.\n    \"\"\"\n    out = x * torch.sigmoid(x)\n    return out\n</code></pre>"},{"location":"odak/learn_perception/","title":"odak.learn.perception","text":"<p><code>odak.learn.perception</code></p> <p>Defines a number of different perceptual loss functions, which can be used to optimise images where gaze location is known.</p>"},{"location":"odak/learn_perception/#odak.learn.perception.BlurLoss","title":"<code>BlurLoss</code>","text":"<p><code>BlurLoss</code> implements two different blur losses. When <code>blur_source</code> is set to <code>False</code>, it implements blur_match, trying to match the input image to the blurred target image. This tries to match the source input image to a blurred version of the target.</p> <p>When <code>blur_source</code> is set to <code>True</code>, it implements blur_lowpass, matching the blurred version of the input image to the blurred target image. This tries to match only the low frequencies of the source input image to the low frequencies of the target.</p> <p>The interface is similar to other <code>pytorch</code> loss functions, but note that the gaze location must be provided in addition to the source and target images.</p> Source code in <code>odak/learn/perception/blur_loss.py</code> <pre><code>class BlurLoss():\n    \"\"\" \n\n    `BlurLoss` implements two different blur losses. When `blur_source` is set to `False`, it implements blur_match, trying to match the input image to the blurred target image. This tries to match the source input image to a blurred version of the target.\n\n    When `blur_source` is set to `True`, it implements blur_lowpass, matching the blurred version of the input image to the blurred target image. This tries to match only the low frequencies of the source input image to the low frequencies of the target.\n\n    The interface is similar to other `pytorch` loss functions, but note that the gaze location must be provided in addition to the source and target images.\n    \"\"\"\n\n\n    def __init__(self, device=torch.device(\"cpu\"),\n                 alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode=\"quadratic\", blur_source=False, equi=False):\n        \"\"\"\n        Parameters\n        ----------\n\n        alpha                   : float\n                                    parameter controlling foveation - larger values mean bigger pooling regions.\n        real_image_width        : float \n                                    The real width of the image as displayed to the user.\n                                    Units don't matter as long as they are the same as for real_viewing_distance.\n        real_viewing_distance   : float \n                                    The real distance of the observer's eyes to the image plane.\n                                    Units don't matter as long as they are the same as for real_image_width.\n        mode                    : str \n                                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                    as you move away from the fovea. We got best results with \"quadratic\".\n        blur_source             : bool\n                                    If true, blurs the source image as well as the target before computing the loss.\n        equi                    : bool\n                                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                    [-pi,pi]x[-pi/2,pi]\n        \"\"\"\n        self.target = None\n        self.device = device\n        self.alpha = alpha\n        self.real_image_width = real_image_width\n        self.real_viewing_distance = real_viewing_distance\n        self.mode = mode\n        self.blur = None\n        self.loss_func = torch.nn.MSELoss()\n        self.blur_source = blur_source\n        self.equi = equi\n\n    def blur_image(self, image, gaze):\n        if self.blur is None:\n            self.blur = RadiallyVaryingBlur()\n        return self.blur.blur(image, self.alpha, self.real_image_width, self.real_viewing_distance, gaze, self.mode, self.equi)\n\n    def __call__(self, image, target, gaze=[0.5, 0.5]):\n        \"\"\" \n        Calculates the Blur Loss.\n\n        Parameters\n        ----------\n        image               : torch.tensor\n                                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        target              : torch.tensor\n                                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        gaze                : list\n                                Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n        Returns\n        -------\n\n        loss                : torch.tensor\n                                The computed loss.\n        \"\"\"\n        check_loss_inputs(\"BlurLoss\", image, target)\n        blurred_target = self.blur_image(target, gaze)\n        if self.blur_source:\n            blurred_image = self.blur_image(image, gaze)\n            return self.loss_func(blurred_image, blurred_target)\n        else:\n            return self.loss_func(image, blurred_target)\n\n    def to(self, device):\n        self.device = device\n        return self\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.BlurLoss.__call__","title":"<code>__call__(image, target, gaze=[0.5, 0.5])</code>","text":"<p>Calculates the Blur Loss.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>target</code>           \u2013            <pre><code>                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>gaze</code>           \u2013            <pre><code>                Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>tensor</code> )          \u2013            <p>The computed loss.</p> </li> </ul> Source code in <code>odak/learn/perception/blur_loss.py</code> <pre><code>def __call__(self, image, target, gaze=[0.5, 0.5]):\n    \"\"\" \n    Calculates the Blur Loss.\n\n    Parameters\n    ----------\n    image               : torch.tensor\n                            Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    target              : torch.tensor\n                            Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    gaze                : list\n                            Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n    Returns\n    -------\n\n    loss                : torch.tensor\n                            The computed loss.\n    \"\"\"\n    check_loss_inputs(\"BlurLoss\", image, target)\n    blurred_target = self.blur_image(target, gaze)\n    if self.blur_source:\n        blurred_image = self.blur_image(image, gaze)\n        return self.loss_func(blurred_image, blurred_target)\n    else:\n        return self.loss_func(image, blurred_target)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.BlurLoss.__init__","title":"<code>__init__(device=torch.device('cpu'), alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode='quadratic', blur_source=False, equi=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>           \u2013            <pre><code>                    parameter controlling foveation - larger values mean bigger pooling regions.\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed to the user.\n                    Units don't matter as long as they are the same as for real_viewing_distance.\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance of the observer's eyes to the image plane.\n                    Units don't matter as long as they are the same as for real_image_width.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                    as you move away from the fovea. We got best results with \"quadratic\".\n</code></pre> </li> <li> <code>blur_source</code>           \u2013            <pre><code>                    If true, blurs the source image as well as the target before computing the loss.\n</code></pre> </li> <li> <code>equi</code>           \u2013            <pre><code>                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                    [-pi,pi]x[-pi/2,pi]\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/blur_loss.py</code> <pre><code>def __init__(self, device=torch.device(\"cpu\"),\n             alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode=\"quadratic\", blur_source=False, equi=False):\n    \"\"\"\n    Parameters\n    ----------\n\n    alpha                   : float\n                                parameter controlling foveation - larger values mean bigger pooling regions.\n    real_image_width        : float \n                                The real width of the image as displayed to the user.\n                                Units don't matter as long as they are the same as for real_viewing_distance.\n    real_viewing_distance   : float \n                                The real distance of the observer's eyes to the image plane.\n                                Units don't matter as long as they are the same as for real_image_width.\n    mode                    : str \n                                Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                as you move away from the fovea. We got best results with \"quadratic\".\n    blur_source             : bool\n                                If true, blurs the source image as well as the target before computing the loss.\n    equi                    : bool\n                                If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                [-pi,pi]x[-pi/2,pi]\n    \"\"\"\n    self.target = None\n    self.device = device\n    self.alpha = alpha\n    self.real_image_width = real_image_width\n    self.real_viewing_distance = real_viewing_distance\n    self.mode = mode\n    self.blur = None\n    self.loss_func = torch.nn.MSELoss()\n    self.blur_source = blur_source\n    self.equi = equi\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.CVVDP","title":"<code>CVVDP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>class CVVDP(nn.Module):\n    def __init__(self, device = torch.device('cpu')):\n        \"\"\"\n        Initializes the CVVDP model with a specified device.\n\n        Parameters\n        ----------\n        device   : torch.device\n                    The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n        \"\"\"\n        super(CVVDP, self).__init__()\n        try:\n            import pycvvdp\n            self.cvvdp = pycvvdp.cvvdp(display_name = 'standard_4k', device = device)\n        except Exception as e:\n            logging.warning('ColorVideoVDP is missing, consider installing by running \"pip install -U git+https://github.com/gfxdisp/ColorVideoVDP\"')\n            logging.warning(e)\n\n\n    def forward(self, predictions, targets, dim_order = 'BCHW'):\n        \"\"\"\n        Parameters\n        ----------\n        predictions   : torch.tensor\n                        The predicted images.\n        targets    h  : torch.tensor\n                        The ground truth images.\n        dim_order     : str\n                        The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n\n        Returns\n        -------\n        result        : torch.tensor\n                        The computed loss if successful, otherwise 0.0.\n        \"\"\"\n        try:\n            if len(predictions.shape) == 3:\n                predictions = predictions.unsqueeze(0)\n                targets = targets.unsqueeze(0)\n            l_ColorVideoVDP = self.cvvdp.predict(predictions, targets, dim_order = dim_order)[0]\n            return l_ColorVideoVDP\n        except Exception as e:\n            logging.warning('ColorVideoVDP failed to compute.')\n            logging.warning(e)\n            return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.CVVDP.__init__","title":"<code>__init__(device=torch.device('cpu'))</code>","text":"<p>Initializes the CVVDP model with a specified device.</p> <p>Parameters:</p> <ul> <li> <code>device</code>           \u2013            <pre><code>    The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def __init__(self, device = torch.device('cpu')):\n    \"\"\"\n    Initializes the CVVDP model with a specified device.\n\n    Parameters\n    ----------\n    device   : torch.device\n                The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n    \"\"\"\n    super(CVVDP, self).__init__()\n    try:\n        import pycvvdp\n        self.cvvdp = pycvvdp.cvvdp(display_name = 'standard_4k', device = device)\n    except Exception as e:\n        logging.warning('ColorVideoVDP is missing, consider installing by running \"pip install -U git+https://github.com/gfxdisp/ColorVideoVDP\"')\n        logging.warning(e)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.CVVDP.forward","title":"<code>forward(predictions, targets, dim_order='BCHW')</code>","text":"<p>Parameters:</p> <ul> <li> <code>predictions</code>           \u2013            <pre><code>        The predicted images.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>        The ground truth images.\n</code></pre> </li> <li> <code>dim_order</code>           \u2013            <pre><code>        The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>The computed loss if successful, otherwise 0.0.</p> </li> </ul> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def forward(self, predictions, targets, dim_order = 'BCHW'):\n    \"\"\"\n    Parameters\n    ----------\n    predictions   : torch.tensor\n                    The predicted images.\n    targets    h  : torch.tensor\n                    The ground truth images.\n    dim_order     : str\n                    The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n\n    Returns\n    -------\n    result        : torch.tensor\n                    The computed loss if successful, otherwise 0.0.\n    \"\"\"\n    try:\n        if len(predictions.shape) == 3:\n            predictions = predictions.unsqueeze(0)\n            targets = targets.unsqueeze(0)\n        l_ColorVideoVDP = self.cvvdp.predict(predictions, targets, dim_order = dim_order)[0]\n        return l_ColorVideoVDP\n    except Exception as e:\n        logging.warning('ColorVideoVDP failed to compute.')\n        logging.warning(e)\n        return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.FVVDP","title":"<code>FVVDP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>class FVVDP(nn.Module):\n    def __init__(self, device = torch.device('cpu')):\n        \"\"\"\n        Initializes the FVVDP model with a specified device.\n\n        Parameters\n        ----------\n        device   : torch.device\n                    The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n        \"\"\"\n        super(FVVDP, self).__init__()\n        try:\n            import pyfvvdp\n            self.fvvdp = pyfvvdp.fvvdp(display_name = 'standard_4k', heatmap = 'none', device = device)\n        except Exception as e:\n            logging.warning('FovVideoVDP is missing, consider installing by running \"pip install pyfvvdp\"')\n            logging.warning(e)\n\n\n    def forward(self, predictions, targets, dim_order = 'BCHW'):\n        \"\"\"\n        Parameters\n        ----------\n        predictions   : torch.tensor\n                        The predicted images.\n        targets       : torch.tensor\n                        The ground truth images.\n        dim_order     : str\n                        The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n\n        Returns\n        -------\n        result        : torch.tensor\n                          The computed loss if successful, otherwise 0.0.\n        \"\"\"\n        try:\n            if len(predictions.shape) == 3:\n                predictions = predictions.unsqueeze(0)\n                targets = targets.unsqueeze(0)\n            l_FovVideoVDP = self.fvvdp.predict(predictions, targets, dim_order = dim_order)[0]\n            return l_FovVideoVDP\n        except Exception as e:\n            logging.warning('FovVideoVDP failed to compute.')\n            logging.warning(e)\n            return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.FVVDP.__init__","title":"<code>__init__(device=torch.device('cpu'))</code>","text":"<p>Initializes the FVVDP model with a specified device.</p> <p>Parameters:</p> <ul> <li> <code>device</code>           \u2013            <pre><code>    The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def __init__(self, device = torch.device('cpu')):\n    \"\"\"\n    Initializes the FVVDP model with a specified device.\n\n    Parameters\n    ----------\n    device   : torch.device\n                The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n    \"\"\"\n    super(FVVDP, self).__init__()\n    try:\n        import pyfvvdp\n        self.fvvdp = pyfvvdp.fvvdp(display_name = 'standard_4k', heatmap = 'none', device = device)\n    except Exception as e:\n        logging.warning('FovVideoVDP is missing, consider installing by running \"pip install pyfvvdp\"')\n        logging.warning(e)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.FVVDP.forward","title":"<code>forward(predictions, targets, dim_order='BCHW')</code>","text":"<p>Parameters:</p> <ul> <li> <code>predictions</code>           \u2013            <pre><code>        The predicted images.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>        The ground truth images.\n</code></pre> </li> <li> <code>dim_order</code>           \u2013            <pre><code>        The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>The computed loss if successful, otherwise 0.0.</p> </li> </ul> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def forward(self, predictions, targets, dim_order = 'BCHW'):\n    \"\"\"\n    Parameters\n    ----------\n    predictions   : torch.tensor\n                    The predicted images.\n    targets       : torch.tensor\n                    The ground truth images.\n    dim_order     : str\n                    The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n\n    Returns\n    -------\n    result        : torch.tensor\n                      The computed loss if successful, otherwise 0.0.\n    \"\"\"\n    try:\n        if len(predictions.shape) == 3:\n            predictions = predictions.unsqueeze(0)\n            targets = targets.unsqueeze(0)\n        l_FovVideoVDP = self.fvvdp.predict(predictions, targets, dim_order = dim_order)[0]\n        return l_FovVideoVDP\n    except Exception as e:\n        logging.warning('FovVideoVDP failed to compute.')\n        logging.warning(e)\n        return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.LPIPS","title":"<code>LPIPS</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>class LPIPS(nn.Module):\n\n    def __init__(self):\n        \"\"\"\n        Initializes the LPIPS (Learned Perceptual Image Patch Similarity) model.\n\n        \"\"\"\n        super(LPIPS, self).__init__()\n        try:\n            import torchmetrics\n            self.lpips = torchmetrics.image.lpip.LearnedPerceptualImagePatchSimilarity(net_type = 'squeeze')\n        except Exception as e:\n            logging.warning('torchmetrics is missing, consider installing by running \"pip install torchmetrics\"')\n            logging.warning(e)\n\n\n    def forward(self, predictions, targets):\n        \"\"\"\n        Parameters\n        ----------\n        predictions   : torch.tensor\n                        The predicted images.\n        targets       : torch.tensor\n                        The ground truth images.\n\n        Returns\n        -------\n        result        : torch.tensor\n                        The computed loss if successful, otherwise 0.0.\n        \"\"\"\n        try:\n            if len(predictions.shape) == 3:\n                predictions = predictions.unsqueeze(0)\n                targets = targets.unsqueeze(0)\n            lpips_image = predictions\n            lpips_target = targets\n            if len(lpips_image.shape) == 3:\n                lpips_image = lpips_image.unsqueeze(0)\n                lpips_target = lpips_target.unsqueeze(0)\n            if lpips_image.shape[1] == 1:\n                lpips_image = lpips_image.repeat(1, 3, 1, 1)\n                lpips_target = lpips_target.repeat(1, 3, 1, 1)\n            lpips_image = (lpips_image * 2 - 1).clamp(-1, 1)\n            lpips_target = (lpips_target * 2 - 1).clamp(-1, 1)\n            l_LPIPS = self.lpips(lpips_image, lpips_target)\n            return l_LPIPS\n        except Exception as e:\n            logging.warning('LPIPS failed to compute.')\n            logging.warning(e)\n            return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.LPIPS.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the LPIPS (Learned Perceptual Image Patch Similarity) model.</p> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the LPIPS (Learned Perceptual Image Patch Similarity) model.\n\n    \"\"\"\n    super(LPIPS, self).__init__()\n    try:\n        import torchmetrics\n        self.lpips = torchmetrics.image.lpip.LearnedPerceptualImagePatchSimilarity(net_type = 'squeeze')\n    except Exception as e:\n        logging.warning('torchmetrics is missing, consider installing by running \"pip install torchmetrics\"')\n        logging.warning(e)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.LPIPS.forward","title":"<code>forward(predictions, targets)</code>","text":"<p>Parameters:</p> <ul> <li> <code>predictions</code>           \u2013            <pre><code>        The predicted images.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>        The ground truth images.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>The computed loss if successful, otherwise 0.0.</p> </li> </ul> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def forward(self, predictions, targets):\n    \"\"\"\n    Parameters\n    ----------\n    predictions   : torch.tensor\n                    The predicted images.\n    targets       : torch.tensor\n                    The ground truth images.\n\n    Returns\n    -------\n    result        : torch.tensor\n                    The computed loss if successful, otherwise 0.0.\n    \"\"\"\n    try:\n        if len(predictions.shape) == 3:\n            predictions = predictions.unsqueeze(0)\n            targets = targets.unsqueeze(0)\n        lpips_image = predictions\n        lpips_target = targets\n        if len(lpips_image.shape) == 3:\n            lpips_image = lpips_image.unsqueeze(0)\n            lpips_target = lpips_target.unsqueeze(0)\n        if lpips_image.shape[1] == 1:\n            lpips_image = lpips_image.repeat(1, 3, 1, 1)\n            lpips_target = lpips_target.repeat(1, 3, 1, 1)\n        lpips_image = (lpips_image * 2 - 1).clamp(-1, 1)\n        lpips_target = (lpips_target * 2 - 1).clamp(-1, 1)\n        l_LPIPS = self.lpips(lpips_image, lpips_target)\n        return l_LPIPS\n    except Exception as e:\n        logging.warning('LPIPS failed to compute.')\n        logging.warning(e)\n        return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MSSSIM","title":"<code>MSSSIM</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class to calculate multi-scale structural similarity index of an image with respect to a ground truth image.</p> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>class MSSSIM(nn.Module):\n    '''\n    A class to calculate multi-scale structural similarity index of an image with respect to a ground truth image.\n    '''\n\n    def __init__(self):\n        super(MSSSIM, self).__init__()\n\n    def forward(self, predictions, targets):\n        \"\"\"\n        Parameters\n        ----------\n        predictions : torch.tensor\n                      The predicted images.\n        targets     : torch.tensor\n                      The ground truth images.\n\n        Returns\n        -------\n        result      : torch.tensor \n                      The computed MS-SSIM value if successful, otherwise 0.0.\n        \"\"\"\n        try:\n            from torchmetrics.functional.image import multiscale_structural_similarity_index_measure\n            if len(predictions.shape) == 3:\n                predictions = predictions.unsqueeze(0)\n                targets = targets.unsqueeze(0)\n            l_MSSSIM = multiscale_structural_similarity_index_measure(predictions, targets, data_range = 1.0)\n            return l_MSSSIM  \n        except Exception as e:\n            logging.warning('MS-SSIM failed to compute.')\n            logging.warning(e)\n            return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MSSSIM.forward","title":"<code>forward(predictions, targets)</code>","text":"<p>Parameters:</p> <ul> <li> <code>predictions</code>               (<code>tensor</code>)           \u2013            <pre><code>      The predicted images.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>      The ground truth images.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>The computed MS-SSIM value if successful, otherwise 0.0.</p> </li> </ul> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>def forward(self, predictions, targets):\n    \"\"\"\n    Parameters\n    ----------\n    predictions : torch.tensor\n                  The predicted images.\n    targets     : torch.tensor\n                  The ground truth images.\n\n    Returns\n    -------\n    result      : torch.tensor \n                  The computed MS-SSIM value if successful, otherwise 0.0.\n    \"\"\"\n    try:\n        from torchmetrics.functional.image import multiscale_structural_similarity_index_measure\n        if len(predictions.shape) == 3:\n            predictions = predictions.unsqueeze(0)\n            targets = targets.unsqueeze(0)\n        l_MSSSIM = multiscale_structural_similarity_index_measure(predictions, targets, data_range = 1.0)\n        return l_MSSSIM  \n    except Exception as e:\n        logging.warning('MS-SSIM failed to compute.')\n        logging.warning(e)\n        return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MetamerMSELoss","title":"<code>MetamerMSELoss</code>","text":"<p>The <code>MetamerMSELoss</code> class provides a perceptual loss function. This generates a metamer for the target image, and then optimises the source image to be the same as this target image metamer.</p> <p>Please note this is different to <code>MetamericLoss</code> which optimises the source image to be any metamer of the target image.</p> <p>Its interface is similar to other <code>pytorch</code> loss functions, but note that the gaze location must be provided in addition to the source and target images.</p> Source code in <code>odak/learn/perception/metamer_mse_loss.py</code> <pre><code>class MetamerMSELoss():\n    \"\"\" \n    The `MetamerMSELoss` class provides a perceptual loss function. This generates a metamer for the target image, and then optimises the source image to be the same as this target image metamer.\n\n    Please note this is different to `MetamericLoss` which optimises the source image to be any metamer of the target image.\n\n    Its interface is similar to other `pytorch` loss functions, but note that the gaze location must be provided in addition to the source and target images.\n    \"\"\"\n\n\n    def __init__(self, device=torch.device(\"cpu\"),\n                 alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode=\"quadratic\",\n                 n_pyramid_levels=5, n_orientations=2, equi=False):\n        \"\"\"\n        Parameters\n        ----------\n        alpha                   : float\n                                    parameter controlling foveation - larger values mean bigger pooling regions.\n        real_image_width        : float \n                                    The real width of the image as displayed to the user.\n                                    Units don't matter as long as they are the same as for real_viewing_distance.\n        real_viewing_distance   : float \n                                    The real distance of the observer's eyes to the image plane.\n                                    Units don't matter as long as they are the same as for real_image_width.\n        n_pyramid_levels        : int \n                                    Number of levels of the steerable pyramid. Note that the image is padded\n                                    so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                                    too high will slow down the calculation a lot.\n        mode                    : str \n                                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                    as you move away from the fovea. We got best results with \"quadratic\".\n        n_orientations          : int \n                                    Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                                    Increasing this will increase runtime.\n        equi                    : bool\n                                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                    [-pi,pi]x[-pi/2,pi]\n        \"\"\"\n        self.target = None\n        self.target_metamer = None\n        self.metameric_loss = MetamericLoss(device=device, alpha=alpha, real_image_width=real_image_width,\n                                            real_viewing_distance=real_viewing_distance,\n                                            n_pyramid_levels=n_pyramid_levels, n_orientations=n_orientations, use_l2_foveal_loss=False, equi=equi)\n        self.loss_func = torch.nn.MSELoss()\n        self.noise = None\n\n\n    def gen_metamer(self, image, gaze):\n        \"\"\" \n        Generates a metamer for an image, following the method in [this paper](https://dl.acm.org/doi/abs/10.1145/3450626.3459943)\n        This function can be used on its own to generate a metamer for a desired image.\n\n        Parameters\n        ----------\n        image   : torch.tensor\n                Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n        gaze    : list\n                Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n        Returns\n        -------\n\n        metamer : torch.tensor\n                The generated metamer image\n        \"\"\"\n        image = rgb_2_ycrcb(image)\n        image_size = image.size()\n        image = pad_image_for_pyramid(image, self.metameric_loss.n_pyramid_levels)\n\n        target_stats = self.metameric_loss.calc_statsmaps(\n            image, gaze=gaze, alpha=self.metameric_loss.alpha)\n        target_means = target_stats[::2]\n        target_stdevs = target_stats[1::2]\n        if self.noise is None or self.noise.size() != image.size():\n            torch.manual_seed(0)\n            noise_image = torch.rand_like(image)\n        noise_pyramid = self.metameric_loss.pyramid_maker.construct_pyramid(\n            noise_image, self.metameric_loss.n_pyramid_levels)\n        input_pyramid = self.metameric_loss.pyramid_maker.construct_pyramid(\n            image, self.metameric_loss.n_pyramid_levels)\n\n\n        def match_level(input_level, target_mean, target_std):\n            level = input_level.clone()\n            level -= torch.mean(level)\n            input_std = torch.sqrt(torch.mean(level * level))\n            eps = 1e-6\n            # Safeguard against divide by zero\n            input_std[input_std &lt; eps] = eps\n            level /= input_std\n            level *= target_std\n            level += target_mean\n            return level\n\n        nbands = len(noise_pyramid[0][\"b\"])\n        noise_pyramid[0][\"h\"] = match_level(\n            noise_pyramid[0][\"h\"], target_means[0], target_stdevs[0])\n        for l in range(len(noise_pyramid)-1):\n            for b in range(nbands):\n                noise_pyramid[l][\"b\"][b] = match_level(\n                    noise_pyramid[l][\"b\"][b], target_means[1 + l * nbands + b], target_stdevs[1 + l * nbands + b])\n        noise_pyramid[-1][\"l\"] = input_pyramid[-1][\"l\"]\n\n        metamer = self.metameric_loss.pyramid_maker.reconstruct_from_pyramid(\n            noise_pyramid)\n        metamer = ycrcb_2_rgb(metamer)\n        # Crop to remove any padding\n        metamer = metamer[:image_size[0], :image_size[1], :image_size[2], :image_size[3]]\n        return metamer\n\n\n    def __call__(self, image, target, gaze = [0.5, 0.5]):\n        \"\"\" \n        Calculates the Metamer MSE Loss.\n\n        Parameters\n        ----------\n        image   : torch.tensor\n                  Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        target  : torch.tensor\n                  Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        gaze    : list\n                   Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n        Returns\n        -------\n\n        loss    : torch.tensor\n                  The computed loss.\n        \"\"\"\n        check_loss_inputs(\"MetamerMSELoss\", image, target)\n        # Pad image and target if necessary\n        image = pad_image_for_pyramid(image, self.metameric_loss.n_pyramid_levels)\n        target = pad_image_for_pyramid(target, self.metameric_loss.n_pyramid_levels)\n\n        if target is not self.target or self.target is None:\n            self.target_metamer = self.gen_metamer(target, gaze)\n            self.target = target\n\n        return self.loss_func(image, self.target_metamer)\n\n\n    def to(self, device):\n        self.metameric_loss = self.metameric_loss.to(device)\n        return self\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MetamerMSELoss.__call__","title":"<code>__call__(image, target, gaze=[0.5, 0.5])</code>","text":"<p>Calculates the Metamer MSE Loss.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>  Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>target</code>           \u2013            <pre><code>  Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>gaze</code>           \u2013            <pre><code>   Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>tensor</code> )          \u2013            <p>The computed loss.</p> </li> </ul> Source code in <code>odak/learn/perception/metamer_mse_loss.py</code> <pre><code>def __call__(self, image, target, gaze = [0.5, 0.5]):\n    \"\"\" \n    Calculates the Metamer MSE Loss.\n\n    Parameters\n    ----------\n    image   : torch.tensor\n              Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    target  : torch.tensor\n              Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    gaze    : list\n               Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n    Returns\n    -------\n\n    loss    : torch.tensor\n              The computed loss.\n    \"\"\"\n    check_loss_inputs(\"MetamerMSELoss\", image, target)\n    # Pad image and target if necessary\n    image = pad_image_for_pyramid(image, self.metameric_loss.n_pyramid_levels)\n    target = pad_image_for_pyramid(target, self.metameric_loss.n_pyramid_levels)\n\n    if target is not self.target or self.target is None:\n        self.target_metamer = self.gen_metamer(target, gaze)\n        self.target = target\n\n    return self.loss_func(image, self.target_metamer)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MetamerMSELoss.__init__","title":"<code>__init__(device=torch.device('cpu'), alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode='quadratic', n_pyramid_levels=5, n_orientations=2, equi=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>           \u2013            <pre><code>                    parameter controlling foveation - larger values mean bigger pooling regions.\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed to the user.\n                    Units don't matter as long as they are the same as for real_viewing_distance.\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance of the observer's eyes to the image plane.\n                    Units don't matter as long as they are the same as for real_image_width.\n</code></pre> </li> <li> <code>n_pyramid_levels</code>           \u2013            <pre><code>                    Number of levels of the steerable pyramid. Note that the image is padded\n                    so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                    too high will slow down the calculation a lot.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                    as you move away from the fovea. We got best results with \"quadratic\".\n</code></pre> </li> <li> <code>n_orientations</code>           \u2013            <pre><code>                    Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                    Increasing this will increase runtime.\n</code></pre> </li> <li> <code>equi</code>           \u2013            <pre><code>                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                    [-pi,pi]x[-pi/2,pi]\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/metamer_mse_loss.py</code> <pre><code>def __init__(self, device=torch.device(\"cpu\"),\n             alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode=\"quadratic\",\n             n_pyramid_levels=5, n_orientations=2, equi=False):\n    \"\"\"\n    Parameters\n    ----------\n    alpha                   : float\n                                parameter controlling foveation - larger values mean bigger pooling regions.\n    real_image_width        : float \n                                The real width of the image as displayed to the user.\n                                Units don't matter as long as they are the same as for real_viewing_distance.\n    real_viewing_distance   : float \n                                The real distance of the observer's eyes to the image plane.\n                                Units don't matter as long as they are the same as for real_image_width.\n    n_pyramid_levels        : int \n                                Number of levels of the steerable pyramid. Note that the image is padded\n                                so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                                too high will slow down the calculation a lot.\n    mode                    : str \n                                Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                as you move away from the fovea. We got best results with \"quadratic\".\n    n_orientations          : int \n                                Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                                Increasing this will increase runtime.\n    equi                    : bool\n                                If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                [-pi,pi]x[-pi/2,pi]\n    \"\"\"\n    self.target = None\n    self.target_metamer = None\n    self.metameric_loss = MetamericLoss(device=device, alpha=alpha, real_image_width=real_image_width,\n                                        real_viewing_distance=real_viewing_distance,\n                                        n_pyramid_levels=n_pyramid_levels, n_orientations=n_orientations, use_l2_foveal_loss=False, equi=equi)\n    self.loss_func = torch.nn.MSELoss()\n    self.noise = None\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MetamerMSELoss.gen_metamer","title":"<code>gen_metamer(image, gaze)</code>","text":"<p>Generates a metamer for an image, following the method in this paper This function can be used on its own to generate a metamer for a desired image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>gaze</code>           \u2013            <pre><code>Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>metamer</code> (              <code>tensor</code> )          \u2013            <p>The generated metamer image</p> </li> </ul> Source code in <code>odak/learn/perception/metamer_mse_loss.py</code> <pre><code>def gen_metamer(self, image, gaze):\n    \"\"\" \n    Generates a metamer for an image, following the method in [this paper](https://dl.acm.org/doi/abs/10.1145/3450626.3459943)\n    This function can be used on its own to generate a metamer for a desired image.\n\n    Parameters\n    ----------\n    image   : torch.tensor\n            Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n    gaze    : list\n            Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n    Returns\n    -------\n\n    metamer : torch.tensor\n            The generated metamer image\n    \"\"\"\n    image = rgb_2_ycrcb(image)\n    image_size = image.size()\n    image = pad_image_for_pyramid(image, self.metameric_loss.n_pyramid_levels)\n\n    target_stats = self.metameric_loss.calc_statsmaps(\n        image, gaze=gaze, alpha=self.metameric_loss.alpha)\n    target_means = target_stats[::2]\n    target_stdevs = target_stats[1::2]\n    if self.noise is None or self.noise.size() != image.size():\n        torch.manual_seed(0)\n        noise_image = torch.rand_like(image)\n    noise_pyramid = self.metameric_loss.pyramid_maker.construct_pyramid(\n        noise_image, self.metameric_loss.n_pyramid_levels)\n    input_pyramid = self.metameric_loss.pyramid_maker.construct_pyramid(\n        image, self.metameric_loss.n_pyramid_levels)\n\n\n    def match_level(input_level, target_mean, target_std):\n        level = input_level.clone()\n        level -= torch.mean(level)\n        input_std = torch.sqrt(torch.mean(level * level))\n        eps = 1e-6\n        # Safeguard against divide by zero\n        input_std[input_std &lt; eps] = eps\n        level /= input_std\n        level *= target_std\n        level += target_mean\n        return level\n\n    nbands = len(noise_pyramid[0][\"b\"])\n    noise_pyramid[0][\"h\"] = match_level(\n        noise_pyramid[0][\"h\"], target_means[0], target_stdevs[0])\n    for l in range(len(noise_pyramid)-1):\n        for b in range(nbands):\n            noise_pyramid[l][\"b\"][b] = match_level(\n                noise_pyramid[l][\"b\"][b], target_means[1 + l * nbands + b], target_stdevs[1 + l * nbands + b])\n    noise_pyramid[-1][\"l\"] = input_pyramid[-1][\"l\"]\n\n    metamer = self.metameric_loss.pyramid_maker.reconstruct_from_pyramid(\n        noise_pyramid)\n    metamer = ycrcb_2_rgb(metamer)\n    # Crop to remove any padding\n    metamer = metamer[:image_size[0], :image_size[1], :image_size[2], :image_size[3]]\n    return metamer\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MetamericLoss","title":"<code>MetamericLoss</code>","text":"<p>The <code>MetamericLoss</code> class provides a perceptual loss function.</p> <p>Rather than exactly match the source image to the target, it tries to ensure the source is a metamer to the target image.</p> <p>Its interface is similar to other <code>pytorch</code> loss functions, but note that the gaze location must be provided in addition to the source and target images.</p> Source code in <code>odak/learn/perception/metameric_loss.py</code> <pre><code>class MetamericLoss():\n    \"\"\"\n    The `MetamericLoss` class provides a perceptual loss function.\n\n    Rather than exactly match the source image to the target, it tries to ensure the source is a *metamer* to the target image.\n\n    Its interface is similar to other `pytorch` loss functions, but note that the gaze location must be provided in addition to the source and target images.\n    \"\"\"\n\n\n    def __init__(self, device=torch.device('cpu'), alpha=0.2, real_image_width=0.2,\n                 real_viewing_distance=0.7, n_pyramid_levels=5, mode=\"quadratic\",\n                 n_orientations=2, use_l2_foveal_loss=True, fovea_weight=20.0, use_radial_weight=False,\n                 use_fullres_l0=False, equi=False):\n        \"\"\"\n        Parameters\n        ----------\n\n        alpha                   : float\n                                    parameter controlling foveation - larger values mean bigger pooling regions.\n        real_image_width        : float \n                                    The real width of the image as displayed to the user.\n                                    Units don't matter as long as they are the same as for real_viewing_distance.\n        real_viewing_distance   : float \n                                    The real distance of the observer's eyes to the image plane.\n                                    Units don't matter as long as they are the same as for real_image_width.\n        n_pyramid_levels        : int \n                                    Number of levels of the steerable pyramid. Note that the image is padded\n                                    so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                                    too high will slow down the calculation a lot.\n        mode                    : str \n                                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                    as you move away from the fovea. We got best results with \"quadratic\".\n        n_orientations          : int \n                                    Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                                    Increasing this will increase runtime.\n        use_l2_foveal_loss      : bool \n                                    If true, for all the pixels that have pooling size 1 pixel in the \n                                    largest scale will use direct L2 against target rather than pooling over pyramid levels.\n                                    In practice this gives better results when the loss is used for holography.\n        fovea_weight            : float \n                                    A weight to apply to the foveal region if use_l2_foveal_loss is set to True.\n        use_radial_weight       : bool \n                                    If True, will apply a radial weighting when calculating the difference between\n                                    the source and target stats maps. This weights stats closer to the fovea more than those\n                                    further away.\n        use_fullres_l0          : bool \n                                    If true, stats for the lowpass residual are replaced with blurred versions\n                                    of the full-resolution source and target images.\n        equi                    : bool\n                                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                    [-pi,pi]x[-pi/2,pi]\n        \"\"\"\n        self.device = device\n        self.pyramid_maker = None\n        self.alpha = alpha\n        self.real_image_width = real_image_width\n        self.real_viewing_distance = real_viewing_distance\n        self.blurs = None\n        self.n_pyramid_levels = n_pyramid_levels\n        self.n_orientations = n_orientations\n        self.mode = mode\n        self.use_l2_foveal_loss = use_l2_foveal_loss\n        self.fovea_weight = fovea_weight\n        self.use_radial_weight = use_radial_weight\n        self.use_fullres_l0 = use_fullres_l0\n        self.equi = equi\n        if self.use_fullres_l0 and self.use_l2_foveal_loss:\n            raise Exception(\n                \"Can't use use_fullres_l0 and use_l2_foveal_loss options together in MetamericLoss!\")\n\n    def calc_statsmaps(self, image, gaze=None, alpha=0.01, real_image_width=0.3,\n                       real_viewing_distance=0.6, mode=\"quadratic\", equi=False):\n\n        if self.pyramid_maker is None or \\\n                self.pyramid_maker.device != self.device or \\\n                len(self.pyramid_maker.band_filters) != self.n_orientations or\\\n                self.pyramid_maker.filt_h0.size(0) != image.size(1):\n            self.pyramid_maker = SpatialSteerablePyramid(\n                use_bilinear_downup=False, n_channels=image.size(1),\n                device=self.device, n_orientations=self.n_orientations, filter_type=\"cropped\", filter_size=5)\n\n        if self.blurs is None or len(self.blurs) != self.n_pyramid_levels:\n            self.blurs = [RadiallyVaryingBlur()\n                          for i in range(self.n_pyramid_levels)]\n\n        def find_stats(image_pyr_level, blur):\n            image_means = blur.blur(\n                image_pyr_level, alpha, real_image_width, real_viewing_distance, centre=gaze, mode=mode, equi=self.equi)\n            image_meansq = blur.blur(image_pyr_level*image_pyr_level, alpha,\n                                     real_image_width, real_viewing_distance, centre=gaze, mode=mode, equi=self.equi)\n\n            image_vars = image_meansq - (image_means*image_means)\n            image_vars[image_vars &lt; 1e-7] = 1e-7\n            image_std = torch.sqrt(image_vars)\n            if torch.any(torch.isnan(image_means)):\n                print(image_means)\n                raise Exception(\"NaN in image means!\")\n            if torch.any(torch.isnan(image_std)):\n                print(image_std)\n                raise Exception(\"NaN in image stdevs!\")\n            if self.use_fullres_l0:\n                mask = blur.lod_map &gt; 1e-6\n                mask = mask[None, None, ...]\n                if image_means.size(1) &gt; 1:\n                    mask = mask.repeat(image_means.size(0), image_means.size(1), 1, 1)\n                matte = torch.zeros_like(image_means)\n                matte[mask] = 1.0\n                return image_means * matte, image_std * matte\n            return image_means, image_std\n        output_stats = []\n        image_pyramid = self.pyramid_maker.construct_pyramid(\n            image, self.n_pyramid_levels)\n        means, variances = find_stats(image_pyramid[0]['h'], self.blurs[0])\n        if self.use_l2_foveal_loss:\n            base_mask = 1.0 - (self.blurs[0].lod_map / torch.max(self.blurs[0].lod_map))\n            base_mask[self.blurs[0].lod_map &lt; 1e-6] = 1.0\n            self.fovea_mask = base_mask[None, None, ...].expand(image.size()).clone()\n            self.fovea_mask = torch.pow(self.fovea_mask, 10.0)\n            #self.fovea_mask     = torch.nn.functional.interpolate(self.fovea_mask, scale_factor=0.125, mode=\"area\")\n            #self.fovea_mask     = torch.nn.functional.interpolate(self.fovea_mask, size=(image.size(-2), image.size(-1)), mode=\"bilinear\")\n            periphery_mask = 1.0 - self.fovea_mask\n            self.periphery_mask = periphery_mask.clone()\n            output_stats.append(means * periphery_mask)\n            output_stats.append(variances * periphery_mask)\n        else:\n            output_stats.append(means)\n            output_stats.append(variances)\n\n        for l in range(0, len(image_pyramid)-1):\n            for o in range(len(image_pyramid[l]['b'])):\n                means, variances = find_stats(\n                    image_pyramid[l]['b'][o], self.blurs[l])\n                if self.use_l2_foveal_loss:\n                    output_stats.append(means * periphery_mask)\n                    output_stats.append(variances * periphery_mask)\n                else:\n                    output_stats.append(means)\n                    output_stats.append(variances)\n            if self.use_l2_foveal_loss:\n                periphery_mask = torch.nn.functional.interpolate(\n                    periphery_mask, scale_factor=0.5, mode=\"area\", recompute_scale_factor=False)\n\n        if self.use_l2_foveal_loss:\n            output_stats.append(image_pyramid[-1][\"l\"] * periphery_mask)\n        elif self.use_fullres_l0:\n            output_stats.append(self.blurs[0].blur(\n                image, alpha, real_image_width, real_viewing_distance, gaze, mode))\n        else:\n            output_stats.append(image_pyramid[-1][\"l\"])\n        return output_stats\n\n    def metameric_loss_stats(self, statsmap_a, statsmap_b, gaze):\n        loss = 0.0\n        for a, b in zip(statsmap_a, statsmap_b):\n            if self.use_radial_weight:\n                radii = make_radial_map(\n                    [a.size(-2), a.size(-1)], gaze).to(a.device)\n                weights = 1.1 - (radii * radii * radii * radii)\n                weights = weights[None, None, ...].repeat(a.size(0), a.size(1), 1, 1)\n                loss += torch.nn.MSELoss()(weights*a, weights*b)\n            else:\n                loss += torch.nn.MSELoss()(a, b)\n        loss /= len(statsmap_a)\n        return loss\n\n    def visualise_loss_map(self, image_stats):\n        batch_size = image_stats[0].size(0)\n        loss_map = torch.zeros((batch_size,) + image_stats[0].size()[-2:])\n        for i in range(len(image_stats)):\n            stats = image_stats[i]\n            target_stats = self.target_stats[i]\n            stat_mse_map = torch.sqrt(torch.pow(stats - target_stats, 2))\n            stat_mse_map = torch.nn.functional.interpolate(stat_mse_map, size=loss_map.size()[1:],\n                mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n            loss_map += stat_mse_map[:, 0, ...]\n        self.loss_map = loss_map\n\n    def __call__(self, image, target, gaze=[0.5, 0.5], image_colorspace=\"RGB\", visualise_loss=False):\n        \"\"\" \n        Calculates the Metameric Loss.\n\n        Parameters\n        ----------\n        image               : torch.tensor\n                                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        target              : torch.tensor\n                                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        image_colorspace    : str\n                                The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                                accepted values: RGB, YCrCb.\n        gaze                : list\n                                Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n        visualise_loss      : bool\n                                Shows a heatmap indicating which parts of the image contributed most to the loss. \n\n        Returns\n        -------\n\n        loss                : torch.tensor\n                                The computed loss.\n        \"\"\"\n        check_loss_inputs(\"MetamericLoss\", image, target)\n        # Pad image and target if necessary\n        image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n        target = pad_image_for_pyramid(target, self.n_pyramid_levels)\n        # If input is RGB, convert to YCrCb.\n        if image.size(1) == 3 and image_colorspace == \"RGB\":\n            image = rgb_2_ycrcb(image)\n            target = rgb_2_ycrcb(target)\n\n        self.target_stats = self.calc_statsmaps(\n            target,\n            gaze=gaze,\n            alpha=self.alpha,\n            real_image_width=self.real_image_width,\n            real_viewing_distance=self.real_viewing_distance,\n            mode=self.mode\n        )\n\n        image_stats = self.calc_statsmaps(\n            image,\n            gaze=gaze,\n            alpha=self.alpha,\n            real_image_width=self.real_image_width,\n            real_viewing_distance=self.real_viewing_distance,\n            mode=self.mode\n        )\n\n        if visualise_loss:\n            self.visualise_loss_map(image_stats)\n\n        if self.use_l2_foveal_loss:\n            peripheral_loss = self.metameric_loss_stats(\n                image_stats, self.target_stats, gaze)\n            foveal_loss = torch.nn.MSELoss()(self.fovea_mask*image, self.fovea_mask*target)\n            # New weighting - evenly weight fovea and periphery.\n            loss = peripheral_loss + self.fovea_weight * foveal_loss\n        else:\n            loss = self.metameric_loss_stats(\n                image_stats, self.target_stats, gaze)\n        return loss\n\n    def to(self, device):\n        self.device = device\n        return self\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MetamericLoss.__call__","title":"<code>__call__(image, target, gaze=[0.5, 0.5], image_colorspace='RGB', visualise_loss=False)</code>","text":"<p>Calculates the Metameric Loss.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>target</code>           \u2013            <pre><code>                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>image_colorspace</code>           \u2013            <pre><code>                The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                accepted values: RGB, YCrCb.\n</code></pre> </li> <li> <code>gaze</code>           \u2013            <pre><code>                Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n</code></pre> </li> <li> <code>visualise_loss</code>           \u2013            <pre><code>                Shows a heatmap indicating which parts of the image contributed most to the loss.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>tensor</code> )          \u2013            <p>The computed loss.</p> </li> </ul> Source code in <code>odak/learn/perception/metameric_loss.py</code> <pre><code>def __call__(self, image, target, gaze=[0.5, 0.5], image_colorspace=\"RGB\", visualise_loss=False):\n    \"\"\" \n    Calculates the Metameric Loss.\n\n    Parameters\n    ----------\n    image               : torch.tensor\n                            Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    target              : torch.tensor\n                            Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    image_colorspace    : str\n                            The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                            accepted values: RGB, YCrCb.\n    gaze                : list\n                            Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n    visualise_loss      : bool\n                            Shows a heatmap indicating which parts of the image contributed most to the loss. \n\n    Returns\n    -------\n\n    loss                : torch.tensor\n                            The computed loss.\n    \"\"\"\n    check_loss_inputs(\"MetamericLoss\", image, target)\n    # Pad image and target if necessary\n    image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n    target = pad_image_for_pyramid(target, self.n_pyramid_levels)\n    # If input is RGB, convert to YCrCb.\n    if image.size(1) == 3 and image_colorspace == \"RGB\":\n        image = rgb_2_ycrcb(image)\n        target = rgb_2_ycrcb(target)\n\n    self.target_stats = self.calc_statsmaps(\n        target,\n        gaze=gaze,\n        alpha=self.alpha,\n        real_image_width=self.real_image_width,\n        real_viewing_distance=self.real_viewing_distance,\n        mode=self.mode\n    )\n\n    image_stats = self.calc_statsmaps(\n        image,\n        gaze=gaze,\n        alpha=self.alpha,\n        real_image_width=self.real_image_width,\n        real_viewing_distance=self.real_viewing_distance,\n        mode=self.mode\n    )\n\n    if visualise_loss:\n        self.visualise_loss_map(image_stats)\n\n    if self.use_l2_foveal_loss:\n        peripheral_loss = self.metameric_loss_stats(\n            image_stats, self.target_stats, gaze)\n        foveal_loss = torch.nn.MSELoss()(self.fovea_mask*image, self.fovea_mask*target)\n        # New weighting - evenly weight fovea and periphery.\n        loss = peripheral_loss + self.fovea_weight * foveal_loss\n    else:\n        loss = self.metameric_loss_stats(\n            image_stats, self.target_stats, gaze)\n    return loss\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MetamericLoss.__init__","title":"<code>__init__(device=torch.device('cpu'), alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, n_pyramid_levels=5, mode='quadratic', n_orientations=2, use_l2_foveal_loss=True, fovea_weight=20.0, use_radial_weight=False, use_fullres_l0=False, equi=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>           \u2013            <pre><code>                    parameter controlling foveation - larger values mean bigger pooling regions.\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed to the user.\n                    Units don't matter as long as they are the same as for real_viewing_distance.\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance of the observer's eyes to the image plane.\n                    Units don't matter as long as they are the same as for real_image_width.\n</code></pre> </li> <li> <code>n_pyramid_levels</code>           \u2013            <pre><code>                    Number of levels of the steerable pyramid. Note that the image is padded\n                    so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                    too high will slow down the calculation a lot.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                    as you move away from the fovea. We got best results with \"quadratic\".\n</code></pre> </li> <li> <code>n_orientations</code>           \u2013            <pre><code>                    Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                    Increasing this will increase runtime.\n</code></pre> </li> <li> <code>use_l2_foveal_loss</code>           \u2013            <pre><code>                    If true, for all the pixels that have pooling size 1 pixel in the \n                    largest scale will use direct L2 against target rather than pooling over pyramid levels.\n                    In practice this gives better results when the loss is used for holography.\n</code></pre> </li> <li> <code>fovea_weight</code>           \u2013            <pre><code>                    A weight to apply to the foveal region if use_l2_foveal_loss is set to True.\n</code></pre> </li> <li> <code>use_radial_weight</code>           \u2013            <pre><code>                    If True, will apply a radial weighting when calculating the difference between\n                    the source and target stats maps. This weights stats closer to the fovea more than those\n                    further away.\n</code></pre> </li> <li> <code>use_fullres_l0</code>           \u2013            <pre><code>                    If true, stats for the lowpass residual are replaced with blurred versions\n                    of the full-resolution source and target images.\n</code></pre> </li> <li> <code>equi</code>           \u2013            <pre><code>                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                    [-pi,pi]x[-pi/2,pi]\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/metameric_loss.py</code> <pre><code>def __init__(self, device=torch.device('cpu'), alpha=0.2, real_image_width=0.2,\n             real_viewing_distance=0.7, n_pyramid_levels=5, mode=\"quadratic\",\n             n_orientations=2, use_l2_foveal_loss=True, fovea_weight=20.0, use_radial_weight=False,\n             use_fullres_l0=False, equi=False):\n    \"\"\"\n    Parameters\n    ----------\n\n    alpha                   : float\n                                parameter controlling foveation - larger values mean bigger pooling regions.\n    real_image_width        : float \n                                The real width of the image as displayed to the user.\n                                Units don't matter as long as they are the same as for real_viewing_distance.\n    real_viewing_distance   : float \n                                The real distance of the observer's eyes to the image plane.\n                                Units don't matter as long as they are the same as for real_image_width.\n    n_pyramid_levels        : int \n                                Number of levels of the steerable pyramid. Note that the image is padded\n                                so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                                too high will slow down the calculation a lot.\n    mode                    : str \n                                Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                as you move away from the fovea. We got best results with \"quadratic\".\n    n_orientations          : int \n                                Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                                Increasing this will increase runtime.\n    use_l2_foveal_loss      : bool \n                                If true, for all the pixels that have pooling size 1 pixel in the \n                                largest scale will use direct L2 against target rather than pooling over pyramid levels.\n                                In practice this gives better results when the loss is used for holography.\n    fovea_weight            : float \n                                A weight to apply to the foveal region if use_l2_foveal_loss is set to True.\n    use_radial_weight       : bool \n                                If True, will apply a radial weighting when calculating the difference between\n                                the source and target stats maps. This weights stats closer to the fovea more than those\n                                further away.\n    use_fullres_l0          : bool \n                                If true, stats for the lowpass residual are replaced with blurred versions\n                                of the full-resolution source and target images.\n    equi                    : bool\n                                If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                [-pi,pi]x[-pi/2,pi]\n    \"\"\"\n    self.device = device\n    self.pyramid_maker = None\n    self.alpha = alpha\n    self.real_image_width = real_image_width\n    self.real_viewing_distance = real_viewing_distance\n    self.blurs = None\n    self.n_pyramid_levels = n_pyramid_levels\n    self.n_orientations = n_orientations\n    self.mode = mode\n    self.use_l2_foveal_loss = use_l2_foveal_loss\n    self.fovea_weight = fovea_weight\n    self.use_radial_weight = use_radial_weight\n    self.use_fullres_l0 = use_fullres_l0\n    self.equi = equi\n    if self.use_fullres_l0 and self.use_l2_foveal_loss:\n        raise Exception(\n            \"Can't use use_fullres_l0 and use_l2_foveal_loss options together in MetamericLoss!\")\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MetamericLossUniform","title":"<code>MetamericLossUniform</code>","text":"<p>Measures metameric loss between a given image and a metamer of the given target image. This variant of the metameric loss is not foveated - it applies uniform pooling sizes to the whole input image.</p> Source code in <code>odak/learn/perception/metameric_loss_uniform.py</code> <pre><code>class MetamericLossUniform():\n    \"\"\"\n    Measures metameric loss between a given image and a metamer of the given target image.\n    This variant of the metameric loss is not foveated - it applies uniform pooling sizes to the whole input image.\n    \"\"\"\n\n    def __init__(self, device=torch.device('cpu'), pooling_size=32, n_pyramid_levels=5, n_orientations=2):\n        \"\"\"\n\n        Parameters\n        ----------\n        pooling_size            : int\n                                  Pooling size, in pixels. For example 32 will pool over 32x32 blocks of the image.\n        n_pyramid_levels        : int \n                                  Number of levels of the steerable pyramid. Note that the image is padded\n                                  so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                                  too high will slow down the calculation a lot.\n        n_orientations          : int \n                                  Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                                  Increasing this will increase runtime.\n\n        \"\"\"\n        self.target = None\n        self.device = device\n        self.pyramid_maker = None\n        self.pooling_size = pooling_size\n        self.n_pyramid_levels = n_pyramid_levels\n        self.n_orientations = n_orientations\n\n    def calc_statsmaps(self, image, pooling_size):\n\n        if self.pyramid_maker is None or \\\n                self.pyramid_maker.device != self.device or \\\n                len(self.pyramid_maker.band_filters) != self.n_orientations or\\\n                self.pyramid_maker.filt_h0.size(0) != image.size(1):\n            self.pyramid_maker = SpatialSteerablePyramid(\n                use_bilinear_downup=False, n_channels=image.size(1),\n                device=self.device, n_orientations=self.n_orientations, filter_type=\"cropped\", filter_size=5)\n\n\n        def find_stats(image_pyr_level, pooling_size):\n            image_means = uniform_blur(image_pyr_level, pooling_size)\n            image_meansq = uniform_blur(image_pyr_level*image_pyr_level, pooling_size)\n            image_vars = image_meansq - (image_means*image_means)\n            image_vars[image_vars &lt; 1e-7] = 1e-7\n            image_std = torch.sqrt(image_vars)\n            if torch.any(torch.isnan(image_means)):\n                print(image_means)\n                raise Exception(\"NaN in image means!\")\n            if torch.any(torch.isnan(image_std)):\n                print(image_std)\n                raise Exception(\"NaN in image stdevs!\")\n            return image_means, image_std\n\n        output_stats = []\n        image_pyramid = self.pyramid_maker.construct_pyramid(\n            image, self.n_pyramid_levels)\n        curr_pooling_size = pooling_size\n        means, variances = find_stats(image_pyramid[0]['h'], curr_pooling_size)\n        output_stats.append(means)\n        output_stats.append(variances)\n\n        for l in range(0, len(image_pyramid)-1):\n            for o in range(len(image_pyramid[l]['b'])):\n                means, variances = find_stats(\n                    image_pyramid[l]['b'][o], curr_pooling_size)\n                output_stats.append(means)\n                output_stats.append(variances)\n            curr_pooling_size /= 2\n\n        output_stats.append(image_pyramid[-1][\"l\"])\n        return output_stats\n\n    def metameric_loss_stats(self, statsmap_a, statsmap_b):\n        loss = 0.0\n        for a, b in zip(statsmap_a, statsmap_b):\n            loss += torch.nn.MSELoss()(a, b)\n        loss /= len(statsmap_a)\n        return loss\n\n    def visualise_loss_map(self, image_stats):\n        loss_map = torch.zeros(image_stats[0].size()[-2:])\n        for i in range(len(image_stats)):\n            stats = image_stats[i]\n            target_stats = self.target_stats[i]\n            stat_mse_map = torch.sqrt(torch.pow(stats - target_stats, 2))\n            stat_mse_map = torch.nn.functional.interpolate(stat_mse_map, size=loss_map.size(\n            ), mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n            loss_map += stat_mse_map[0, 0, ...]\n        self.loss_map = loss_map\n\n    def __call__(self, image, target, image_colorspace=\"RGB\", visualise_loss=False):\n        \"\"\" \n        Calculates the Metameric Loss.\n\n        Parameters\n        ----------\n        image               : torch.tensor\n                                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        target              : torch.tensor\n                                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        image_colorspace    : str\n                                The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                                accepted values: RGB, YCrCb.\n        visualise_loss      : bool\n                                Shows a heatmap indicating which parts of the image contributed most to the loss. \n\n        Returns\n        -------\n\n        loss                : torch.tensor\n                                The computed loss.\n        \"\"\"\n        check_loss_inputs(\"MetamericLossUniform\", image, target)\n        # Pad image and target if necessary\n        image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n        target = pad_image_for_pyramid(target, self.n_pyramid_levels)\n        # If input is RGB, convert to YCrCb.\n        if image.size(1) == 3 and image_colorspace == \"RGB\":\n            image = rgb_2_ycrcb(image)\n            target = rgb_2_ycrcb(target)\n        if self.target is None:\n            self.target = torch.zeros(target.shape).to(target.device)\n        if type(target) == type(self.target):\n            if not torch.all(torch.eq(target, self.target)):\n                self.target = target.detach().clone()\n                self.target_stats = self.calc_statsmaps(self.target, self.pooling_size)\n                self.target = target.detach().clone()\n            image_stats = self.calc_statsmaps(image, self.pooling_size)\n\n            if visualise_loss:\n                self.visualise_loss_map(image_stats)\n            loss = self.metameric_loss_stats(\n                image_stats, self.target_stats)\n            return loss\n        else:\n            raise Exception(\"Target of incorrect type\")\n\n    def gen_metamer(self, image):\n        \"\"\" \n        Generates a metamer for an image, following the method in [this paper](https://dl.acm.org/doi/abs/10.1145/3450626.3459943)\n        This function can be used on its own to generate a metamer for a desired image.\n\n        Parameters\n        ----------\n        image   : torch.tensor\n                  Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n\n        Returns\n        -------\n        metamer : torch.tensor\n                  The generated metamer image\n        \"\"\"\n        image = rgb_2_ycrcb(image)\n        image_size = image.size()\n        image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n\n        target_stats = self.calc_statsmaps(\n            image, self.pooling_size)\n        target_means = target_stats[::2]\n        target_stdevs = target_stats[1::2]\n        torch.manual_seed(0)\n        noise_image = torch.rand_like(image)\n        noise_pyramid = self.pyramid_maker.construct_pyramid(\n            noise_image, self.n_pyramid_levels)\n        input_pyramid = self.pyramid_maker.construct_pyramid(\n            image, self.n_pyramid_levels)\n\n        def match_level(input_level, target_mean, target_std):\n            level = input_level.clone()\n            level -= torch.mean(level)\n            input_std = torch.sqrt(torch.mean(level * level))\n            eps = 1e-6\n            # Safeguard against divide by zero\n            input_std[input_std &lt; eps] = eps\n            level /= input_std\n            level *= target_std\n            level += target_mean\n            return level\n\n        nbands = len(noise_pyramid[0][\"b\"])\n        noise_pyramid[0][\"h\"] = match_level(\n            noise_pyramid[0][\"h\"], target_means[0], target_stdevs[0])\n        for l in range(len(noise_pyramid)-1):\n            for b in range(nbands):\n                noise_pyramid[l][\"b\"][b] = match_level(\n                    noise_pyramid[l][\"b\"][b], target_means[1 + l * nbands + b], target_stdevs[1 + l * nbands + b])\n        noise_pyramid[-1][\"l\"] = input_pyramid[-1][\"l\"]\n\n        metamer = self.pyramid_maker.reconstruct_from_pyramid(\n            noise_pyramid)\n        metamer = ycrcb_2_rgb(metamer)\n        # Crop to remove any padding\n        metamer = metamer[:image_size[0], :image_size[1], :image_size[2], :image_size[3]]\n        return metamer\n\n    def to(self, device):\n        self.device = device\n        return self\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MetamericLossUniform.__call__","title":"<code>__call__(image, target, image_colorspace='RGB', visualise_loss=False)</code>","text":"<p>Calculates the Metameric Loss.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>target</code>           \u2013            <pre><code>                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>image_colorspace</code>           \u2013            <pre><code>                The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                accepted values: RGB, YCrCb.\n</code></pre> </li> <li> <code>visualise_loss</code>           \u2013            <pre><code>                Shows a heatmap indicating which parts of the image contributed most to the loss.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>tensor</code> )          \u2013            <p>The computed loss.</p> </li> </ul> Source code in <code>odak/learn/perception/metameric_loss_uniform.py</code> <pre><code>def __call__(self, image, target, image_colorspace=\"RGB\", visualise_loss=False):\n    \"\"\" \n    Calculates the Metameric Loss.\n\n    Parameters\n    ----------\n    image               : torch.tensor\n                            Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    target              : torch.tensor\n                            Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    image_colorspace    : str\n                            The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                            accepted values: RGB, YCrCb.\n    visualise_loss      : bool\n                            Shows a heatmap indicating which parts of the image contributed most to the loss. \n\n    Returns\n    -------\n\n    loss                : torch.tensor\n                            The computed loss.\n    \"\"\"\n    check_loss_inputs(\"MetamericLossUniform\", image, target)\n    # Pad image and target if necessary\n    image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n    target = pad_image_for_pyramid(target, self.n_pyramid_levels)\n    # If input is RGB, convert to YCrCb.\n    if image.size(1) == 3 and image_colorspace == \"RGB\":\n        image = rgb_2_ycrcb(image)\n        target = rgb_2_ycrcb(target)\n    if self.target is None:\n        self.target = torch.zeros(target.shape).to(target.device)\n    if type(target) == type(self.target):\n        if not torch.all(torch.eq(target, self.target)):\n            self.target = target.detach().clone()\n            self.target_stats = self.calc_statsmaps(self.target, self.pooling_size)\n            self.target = target.detach().clone()\n        image_stats = self.calc_statsmaps(image, self.pooling_size)\n\n        if visualise_loss:\n            self.visualise_loss_map(image_stats)\n        loss = self.metameric_loss_stats(\n            image_stats, self.target_stats)\n        return loss\n    else:\n        raise Exception(\"Target of incorrect type\")\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MetamericLossUniform.__init__","title":"<code>__init__(device=torch.device('cpu'), pooling_size=32, n_pyramid_levels=5, n_orientations=2)</code>","text":"<p>Parameters:</p> <ul> <li> <code>pooling_size</code>           \u2013            <pre><code>                  Pooling size, in pixels. For example 32 will pool over 32x32 blocks of the image.\n</code></pre> </li> <li> <code>n_pyramid_levels</code>           \u2013            <pre><code>                  Number of levels of the steerable pyramid. Note that the image is padded\n                  so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                  too high will slow down the calculation a lot.\n</code></pre> </li> <li> <code>n_orientations</code>           \u2013            <pre><code>                  Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                  Increasing this will increase runtime.\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/metameric_loss_uniform.py</code> <pre><code>def __init__(self, device=torch.device('cpu'), pooling_size=32, n_pyramid_levels=5, n_orientations=2):\n    \"\"\"\n\n    Parameters\n    ----------\n    pooling_size            : int\n                              Pooling size, in pixels. For example 32 will pool over 32x32 blocks of the image.\n    n_pyramid_levels        : int \n                              Number of levels of the steerable pyramid. Note that the image is padded\n                              so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                              too high will slow down the calculation a lot.\n    n_orientations          : int \n                              Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                              Increasing this will increase runtime.\n\n    \"\"\"\n    self.target = None\n    self.device = device\n    self.pyramid_maker = None\n    self.pooling_size = pooling_size\n    self.n_pyramid_levels = n_pyramid_levels\n    self.n_orientations = n_orientations\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.MetamericLossUniform.gen_metamer","title":"<code>gen_metamer(image)</code>","text":"<p>Generates a metamer for an image, following the method in this paper This function can be used on its own to generate a metamer for a desired image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>  Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>metamer</code> (              <code>tensor</code> )          \u2013            <p>The generated metamer image</p> </li> </ul> Source code in <code>odak/learn/perception/metameric_loss_uniform.py</code> <pre><code>def gen_metamer(self, image):\n    \"\"\" \n    Generates a metamer for an image, following the method in [this paper](https://dl.acm.org/doi/abs/10.1145/3450626.3459943)\n    This function can be used on its own to generate a metamer for a desired image.\n\n    Parameters\n    ----------\n    image   : torch.tensor\n              Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n\n    Returns\n    -------\n    metamer : torch.tensor\n              The generated metamer image\n    \"\"\"\n    image = rgb_2_ycrcb(image)\n    image_size = image.size()\n    image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n\n    target_stats = self.calc_statsmaps(\n        image, self.pooling_size)\n    target_means = target_stats[::2]\n    target_stdevs = target_stats[1::2]\n    torch.manual_seed(0)\n    noise_image = torch.rand_like(image)\n    noise_pyramid = self.pyramid_maker.construct_pyramid(\n        noise_image, self.n_pyramid_levels)\n    input_pyramid = self.pyramid_maker.construct_pyramid(\n        image, self.n_pyramid_levels)\n\n    def match_level(input_level, target_mean, target_std):\n        level = input_level.clone()\n        level -= torch.mean(level)\n        input_std = torch.sqrt(torch.mean(level * level))\n        eps = 1e-6\n        # Safeguard against divide by zero\n        input_std[input_std &lt; eps] = eps\n        level /= input_std\n        level *= target_std\n        level += target_mean\n        return level\n\n    nbands = len(noise_pyramid[0][\"b\"])\n    noise_pyramid[0][\"h\"] = match_level(\n        noise_pyramid[0][\"h\"], target_means[0], target_stdevs[0])\n    for l in range(len(noise_pyramid)-1):\n        for b in range(nbands):\n            noise_pyramid[l][\"b\"][b] = match_level(\n                noise_pyramid[l][\"b\"][b], target_means[1 + l * nbands + b], target_stdevs[1 + l * nbands + b])\n    noise_pyramid[-1][\"l\"] = input_pyramid[-1][\"l\"]\n\n    metamer = self.pyramid_maker.reconstruct_from_pyramid(\n        noise_pyramid)\n    metamer = ycrcb_2_rgb(metamer)\n    # Crop to remove any padding\n    metamer = metamer[:image_size[0], :image_size[1], :image_size[2], :image_size[3]]\n    return metamer\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.PSNR","title":"<code>PSNR</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class to calculate peak-signal-to-noise ratio of an image with respect to a ground truth image.</p> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>class PSNR(nn.Module):\n    '''\n    A class to calculate peak-signal-to-noise ratio of an image with respect to a ground truth image.\n    '''\n\n    def __init__(self):\n        super(PSNR, self).__init__()\n\n    def forward(self, predictions, targets, peak_value = 1.0):\n        \"\"\"\n        A function to calculate peak-signal-to-noise ratio of an image with respect to a ground truth image.\n\n        Parameters\n        ----------\n        predictions   : torch.tensor\n                        Image to be tested.\n        targets       : torch.tensor\n                        Ground truth image.\n        peak_value    : float\n                        Peak value that given tensors could have.\n\n        Returns\n        -------\n        result        : torch.tensor\n                        Peak-signal-to-noise ratio.\n        \"\"\"\n        mse = torch.mean((targets - predictions) ** 2)\n        result = 20 * torch.log10(peak_value / torch.sqrt(mse))\n        return result\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.PSNR.forward","title":"<code>forward(predictions, targets, peak_value=1.0)</code>","text":"<p>A function to calculate peak-signal-to-noise ratio of an image with respect to a ground truth image.</p> <p>Parameters:</p> <ul> <li> <code>predictions</code>           \u2013            <pre><code>        Image to be tested.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>        Ground truth image.\n</code></pre> </li> <li> <code>peak_value</code>           \u2013            <pre><code>        Peak value that given tensors could have.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Peak-signal-to-noise ratio.</p> </li> </ul> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>def forward(self, predictions, targets, peak_value = 1.0):\n    \"\"\"\n    A function to calculate peak-signal-to-noise ratio of an image with respect to a ground truth image.\n\n    Parameters\n    ----------\n    predictions   : torch.tensor\n                    Image to be tested.\n    targets       : torch.tensor\n                    Ground truth image.\n    peak_value    : float\n                    Peak value that given tensors could have.\n\n    Returns\n    -------\n    result        : torch.tensor\n                    Peak-signal-to-noise ratio.\n    \"\"\"\n    mse = torch.mean((targets - predictions) ** 2)\n    result = 20 * torch.log10(peak_value / torch.sqrt(mse))\n    return result\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.RadiallyVaryingBlur","title":"<code>RadiallyVaryingBlur</code>","text":"<p>The <code>RadiallyVaryingBlur</code> class provides a way to apply a radially varying blur to an image. Given a gaze location and information about the image and foveation, it applies a blur that will achieve the proper pooling size. The pooling size is chosen to appear the same at a range of display sizes and viewing distances, for a given <code>alpha</code> parameter value. For more information on how the pooling sizes are computed, please see link coming soon.</p> <p>The blur is accelerated by generating and sampling from MIP maps of the input image.</p> <p>This class caches the foveation information. This means that if it is run repeatedly with the same foveation parameters, gaze location and image size (e.g. in an optimisation loop) it won't recalculate the pooling maps.</p> <p>If you are repeatedly applying blur to images of different sizes (e.g. a pyramid) for best performance use one instance of this class per image size.</p> Source code in <code>odak/learn/perception/radially_varying_blur.py</code> <pre><code>class RadiallyVaryingBlur():\n    \"\"\" \n\n    The `RadiallyVaryingBlur` class provides a way to apply a radially varying blur to an image. Given a gaze location and information about the image and foveation, it applies a blur that will achieve the proper pooling size. The pooling size is chosen to appear the same at a range of display sizes and viewing distances, for a given `alpha` parameter value. For more information on how the pooling sizes are computed, please see [link coming soon]().\n\n    The blur is accelerated by generating and sampling from MIP maps of the input image.\n\n    This class caches the foveation information. This means that if it is run repeatedly with the same foveation parameters, gaze location and image size (e.g. in an optimisation loop) it won't recalculate the pooling maps.\n\n    If you are repeatedly applying blur to images of different sizes (e.g. a pyramid) for best performance use one instance of this class per image size.\n\n    \"\"\"\n\n    def __init__(self):\n        self.lod_map = None\n        self.equi = None\n\n    def blur(self, image, alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, centre=None, mode=\"quadratic\", equi=False):\n        \"\"\"\n        Apply the radially varying blur to an image.\n\n        Parameters\n        ----------\n\n        image                   : torch.tensor\n                                    The image to blur, in NCHW format.\n        alpha                   : float\n                                    parameter controlling foveation - larger values mean bigger pooling regions.\n        real_image_width        : float \n                                    The real width of the image as displayed to the user.\n                                    Units don't matter as long as they are the same as for real_viewing_distance.\n                                    Ignored in equirectangular mode (equi==True)\n        real_viewing_distance   : float \n                                    The real distance of the observer's eyes to the image plane.\n                                    Units don't matter as long as they are the same as for real_image_width.\n                                    Ignored in equirectangular mode (equi==True)\n        centre                  : tuple of floats\n                                    The centre of the radially varying blur (the gaze location).\n                                    Should be a tuple of floats containing normalised image coordinates in range [0,1]\n                                    In equirectangular mode this should be yaw &amp; pitch angles in [-pi,pi]x[-pi/2,pi/2]\n        mode                    : str \n                                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                    as you move away from the fovea. We got best results with \"quadratic\".\n        equi                    : bool\n                                    If true, run the blur function in equirectangular mode. The input is assumed to be an equirectangular\n                                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                    The centre argument is instead interpreted as gaze angles, and should be in the range\n                                    [-pi,pi]x[-pi/2,pi]\n\n        Returns\n        -------\n\n        output                  : torch.tensor\n                                    The blurred image\n        \"\"\"\n        size = (image.size(-2), image.size(-1))\n\n        # LOD map caching\n        if self.lod_map is None or\\\n                self.size != size or\\\n                self.n_channels != image.size(1) or\\\n                self.alpha != alpha or\\\n                self.real_image_width != real_image_width or\\\n                self.real_viewing_distance != real_viewing_distance or\\\n                self.centre != centre or\\\n                self.mode != mode or\\\n                self.equi != equi:\n            if not equi:\n                self.lod_map = make_pooling_size_map_lod(\n                    centre, (image.size(-2), image.size(-1)), alpha, real_image_width, real_viewing_distance, mode)\n            else:\n                self.lod_map = make_equi_pooling_size_map_lod(\n                    centre, (image.size(-2), image.size(-1)), alpha, mode)\n            self.size = size\n            self.n_channels = image.size(1)\n            self.alpha = alpha\n            self.real_image_width = real_image_width\n            self.real_viewing_distance = real_viewing_distance\n            self.centre = centre\n            self.lod_map = self.lod_map.to(image.device)\n            self.lod_fraction = torch.fmod(self.lod_map, 1.0)\n            self.lod_fraction = self.lod_fraction[None, None, ...].repeat(\n                1, image.size(1), 1, 1)\n            self.mode = mode\n            self.equi = equi\n\n        if self.lod_map.device != image.device:\n            self.lod_map = self.lod_map.to(image.device)\n        if self.lod_fraction.device != image.device:\n            self.lod_fraction = self.lod_fraction.to(image.device)\n\n        mipmap = [image]\n        while mipmap[-1].size(-1) &gt; 1 and mipmap[-1].size(-2) &gt; 1:\n            mipmap.append(torch.nn.functional.interpolate(\n                mipmap[-1], scale_factor=0.5, mode=\"area\", recompute_scale_factor=False))\n        if mipmap[-1].size(-1) == 2:\n            final_mip = torch.mean(mipmap[-1], axis=-1)[..., None]\n            mipmap.append(final_mip)\n        if mipmap[-1].size(-2) == 2:\n            final_mip = torch.mean(mipmap[-2], axis=-2)[..., None, :]\n            mipmap.append(final_mip)\n\n        for l in range(len(mipmap)):\n            if l == len(mipmap)-1:\n                mipmap[l] = mipmap[l] * \\\n                    torch.ones(image.size(), device=image.device)\n            else:\n                for l2 in range(l-1, -1, -1):\n                    mipmap[l] = torch.nn.functional.interpolate(mipmap[l], size=(\n                        image.size(-2), image.size(-1)), mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n\n        output = torch.zeros(image.size(), device=image.device)\n        for l in range(len(mipmap)):\n            if l == 0:\n                mask = self.lod_map &lt; (l+1)\n            elif l == len(mipmap)-1:\n                mask = self.lod_map &gt;= l\n            else:\n                mask = torch.logical_and(\n                    self.lod_map &gt;= l, self.lod_map &lt; (l+1))\n\n            if l == len(mipmap)-1:\n                blended_levels = mipmap[l]\n            else:\n                blended_levels = (1 - self.lod_fraction) * \\\n                    mipmap[l] + self.lod_fraction*mipmap[l+1]\n            mask = mask[None, None, ...]\n            mask = mask.repeat(image.size(0), image.size(1), 1, 1)\n            output[mask] = blended_levels[mask]\n\n        return output\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.RadiallyVaryingBlur.blur","title":"<code>blur(image, alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, centre=None, mode='quadratic', equi=False)</code>","text":"<p>Apply the radially varying blur to an image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>                    The image to blur, in NCHW format.\n</code></pre> </li> <li> <code>alpha</code>           \u2013            <pre><code>                    parameter controlling foveation - larger values mean bigger pooling regions.\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed to the user.\n                    Units don't matter as long as they are the same as for real_viewing_distance.\n                    Ignored in equirectangular mode (equi==True)\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance of the observer's eyes to the image plane.\n                    Units don't matter as long as they are the same as for real_image_width.\n                    Ignored in equirectangular mode (equi==True)\n</code></pre> </li> <li> <code>centre</code>           \u2013            <pre><code>                    The centre of the radially varying blur (the gaze location).\n                    Should be a tuple of floats containing normalised image coordinates in range [0,1]\n                    In equirectangular mode this should be yaw &amp; pitch angles in [-pi,pi]x[-pi/2,pi/2]\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                    as you move away from the fovea. We got best results with \"quadratic\".\n</code></pre> </li> <li> <code>equi</code>           \u2013            <pre><code>                    If true, run the blur function in equirectangular mode. The input is assumed to be an equirectangular\n                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                    The centre argument is instead interpreted as gaze angles, and should be in the range\n                    [-pi,pi]x[-pi/2,pi]\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>The blurred image</p> </li> </ul> Source code in <code>odak/learn/perception/radially_varying_blur.py</code> <pre><code>def blur(self, image, alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, centre=None, mode=\"quadratic\", equi=False):\n    \"\"\"\n    Apply the radially varying blur to an image.\n\n    Parameters\n    ----------\n\n    image                   : torch.tensor\n                                The image to blur, in NCHW format.\n    alpha                   : float\n                                parameter controlling foveation - larger values mean bigger pooling regions.\n    real_image_width        : float \n                                The real width of the image as displayed to the user.\n                                Units don't matter as long as they are the same as for real_viewing_distance.\n                                Ignored in equirectangular mode (equi==True)\n    real_viewing_distance   : float \n                                The real distance of the observer's eyes to the image plane.\n                                Units don't matter as long as they are the same as for real_image_width.\n                                Ignored in equirectangular mode (equi==True)\n    centre                  : tuple of floats\n                                The centre of the radially varying blur (the gaze location).\n                                Should be a tuple of floats containing normalised image coordinates in range [0,1]\n                                In equirectangular mode this should be yaw &amp; pitch angles in [-pi,pi]x[-pi/2,pi/2]\n    mode                    : str \n                                Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                as you move away from the fovea. We got best results with \"quadratic\".\n    equi                    : bool\n                                If true, run the blur function in equirectangular mode. The input is assumed to be an equirectangular\n                                format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                The centre argument is instead interpreted as gaze angles, and should be in the range\n                                [-pi,pi]x[-pi/2,pi]\n\n    Returns\n    -------\n\n    output                  : torch.tensor\n                                The blurred image\n    \"\"\"\n    size = (image.size(-2), image.size(-1))\n\n    # LOD map caching\n    if self.lod_map is None or\\\n            self.size != size or\\\n            self.n_channels != image.size(1) or\\\n            self.alpha != alpha or\\\n            self.real_image_width != real_image_width or\\\n            self.real_viewing_distance != real_viewing_distance or\\\n            self.centre != centre or\\\n            self.mode != mode or\\\n            self.equi != equi:\n        if not equi:\n            self.lod_map = make_pooling_size_map_lod(\n                centre, (image.size(-2), image.size(-1)), alpha, real_image_width, real_viewing_distance, mode)\n        else:\n            self.lod_map = make_equi_pooling_size_map_lod(\n                centre, (image.size(-2), image.size(-1)), alpha, mode)\n        self.size = size\n        self.n_channels = image.size(1)\n        self.alpha = alpha\n        self.real_image_width = real_image_width\n        self.real_viewing_distance = real_viewing_distance\n        self.centre = centre\n        self.lod_map = self.lod_map.to(image.device)\n        self.lod_fraction = torch.fmod(self.lod_map, 1.0)\n        self.lod_fraction = self.lod_fraction[None, None, ...].repeat(\n            1, image.size(1), 1, 1)\n        self.mode = mode\n        self.equi = equi\n\n    if self.lod_map.device != image.device:\n        self.lod_map = self.lod_map.to(image.device)\n    if self.lod_fraction.device != image.device:\n        self.lod_fraction = self.lod_fraction.to(image.device)\n\n    mipmap = [image]\n    while mipmap[-1].size(-1) &gt; 1 and mipmap[-1].size(-2) &gt; 1:\n        mipmap.append(torch.nn.functional.interpolate(\n            mipmap[-1], scale_factor=0.5, mode=\"area\", recompute_scale_factor=False))\n    if mipmap[-1].size(-1) == 2:\n        final_mip = torch.mean(mipmap[-1], axis=-1)[..., None]\n        mipmap.append(final_mip)\n    if mipmap[-1].size(-2) == 2:\n        final_mip = torch.mean(mipmap[-2], axis=-2)[..., None, :]\n        mipmap.append(final_mip)\n\n    for l in range(len(mipmap)):\n        if l == len(mipmap)-1:\n            mipmap[l] = mipmap[l] * \\\n                torch.ones(image.size(), device=image.device)\n        else:\n            for l2 in range(l-1, -1, -1):\n                mipmap[l] = torch.nn.functional.interpolate(mipmap[l], size=(\n                    image.size(-2), image.size(-1)), mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n\n    output = torch.zeros(image.size(), device=image.device)\n    for l in range(len(mipmap)):\n        if l == 0:\n            mask = self.lod_map &lt; (l+1)\n        elif l == len(mipmap)-1:\n            mask = self.lod_map &gt;= l\n        else:\n            mask = torch.logical_and(\n                self.lod_map &gt;= l, self.lod_map &lt; (l+1))\n\n        if l == len(mipmap)-1:\n            blended_levels = mipmap[l]\n        else:\n            blended_levels = (1 - self.lod_fraction) * \\\n                mipmap[l] + self.lod_fraction*mipmap[l+1]\n        mask = mask[None, None, ...]\n        mask = mask.repeat(image.size(0), image.size(1), 1, 1)\n        output[mask] = blended_levels[mask]\n\n    return output\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.SSIM","title":"<code>SSIM</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class to calculate structural similarity index of an image with respect to a ground truth image.</p> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>class SSIM(nn.Module):\n    '''\n    A class to calculate structural similarity index of an image with respect to a ground truth image.\n    '''\n\n    def __init__(self):\n        super(SSIM, self).__init__()\n\n    def forward(self, predictions, targets):\n        \"\"\"\n        Parameters\n        ----------\n        predictions : torch.tensor\n                      The predicted images.\n        targets     : torch.tensor\n                      The ground truth images.\n\n        Returns\n        -------\n        result      : torch.tensor \n                      The computed SSIM value if successful, otherwise 0.0.\n        \"\"\"\n        try:\n            from torchmetrics.functional.image import structural_similarity_index_measure\n            if len(predictions.shape) == 3:\n                predictions = predictions.unsqueeze(0)\n                targets = targets.unsqueeze(0)\n            l_SSIM = structural_similarity_index_measure(predictions, targets)\n            return l_SSIM\n        except Exception as e:\n            logging.warning('SSIM failed to compute.')\n            logging.warning(e)\n            return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.SSIM.forward","title":"<code>forward(predictions, targets)</code>","text":"<p>Parameters:</p> <ul> <li> <code>predictions</code>               (<code>tensor</code>)           \u2013            <pre><code>      The predicted images.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>      The ground truth images.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>The computed SSIM value if successful, otherwise 0.0.</p> </li> </ul> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>def forward(self, predictions, targets):\n    \"\"\"\n    Parameters\n    ----------\n    predictions : torch.tensor\n                  The predicted images.\n    targets     : torch.tensor\n                  The ground truth images.\n\n    Returns\n    -------\n    result      : torch.tensor \n                  The computed SSIM value if successful, otherwise 0.0.\n    \"\"\"\n    try:\n        from torchmetrics.functional.image import structural_similarity_index_measure\n        if len(predictions.shape) == 3:\n            predictions = predictions.unsqueeze(0)\n            targets = targets.unsqueeze(0)\n        l_SSIM = structural_similarity_index_measure(predictions, targets)\n        return l_SSIM\n    except Exception as e:\n        logging.warning('SSIM failed to compute.')\n        logging.warning(e)\n        return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.SpatialSteerablePyramid","title":"<code>SpatialSteerablePyramid</code>","text":"<p>This implements a real-valued steerable pyramid where the filtering is carried out spatially (using convolution) as opposed to multiplication in the Fourier domain. This has a number of optimisations over previous implementations that increase efficiency, but introduce some reconstruction error.</p> Source code in <code>odak/learn/perception/spatial_steerable_pyramid.py</code> <pre><code>class SpatialSteerablePyramid():\n    \"\"\"\n    This implements a real-valued steerable pyramid where the filtering is carried out spatially (using convolution)\n    as opposed to multiplication in the Fourier domain.\n    This has a number of optimisations over previous implementations that increase efficiency, but introduce some\n    reconstruction error.\n    \"\"\"\n\n\n    def __init__(self, use_bilinear_downup=True, n_channels=1,\n                 filter_size=9, n_orientations=6, filter_type=\"full\",\n                 device=torch.device('cpu')):\n        \"\"\"\n        Parameters\n        ----------\n\n        use_bilinear_downup     : bool\n                                    This uses bilinear filtering when upsampling/downsampling, rather than the original approach\n                                    of applying a large lowpass kernel and sampling even rows/columns\n        n_channels              : int\n                                    Number of channels in the input images (e.g. 3 for RGB input)\n        filter_size             : int\n                                    Desired size of filters (e.g. 3 will use 3x3 filters).\n        n_orientations          : int\n                                    Number of oriented bands in each level of the pyramid.\n        filter_type             : str\n                                    This can be used to select smaller filters than the original ones if desired.\n                                    full: Original filter sizes\n                                    cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate.\n                                    trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels.\n        device                  : torch.device\n                                    torch device the input images will be supplied from.\n        \"\"\"\n        self.use_bilinear_downup = use_bilinear_downup\n        self.device = device\n\n        filters = get_steerable_pyramid_filters(\n            filter_size, n_orientations, filter_type)\n\n        def make_pad(filter):\n            filter_size = filter.size(-1)\n            pad_amt = (filter_size-1) // 2\n            return torch.nn.ReflectionPad2d((pad_amt, pad_amt, pad_amt, pad_amt))\n\n        if not self.use_bilinear_downup:\n            self.filt_l = filters[\"l\"].to(device)\n            self.pad_l = make_pad(self.filt_l)\n        self.filt_l0 = filters[\"l0\"].to(device)\n        self.pad_l0 = make_pad(self.filt_l0)\n        self.filt_h0 = filters[\"h0\"].to(device)\n        self.pad_h0 = make_pad(self.filt_h0)\n        for b in range(len(filters[\"b\"])):\n            filters[\"b\"][b] = filters[\"b\"][b].to(device)\n        self.band_filters = filters[\"b\"]\n        self.pad_b = make_pad(self.band_filters[0])\n\n        if n_channels != 1:\n            def add_channels_to_filter(filter):\n                padded = torch.zeros(n_channels, n_channels, filter.size()[\n                                     2], filter.size()[3]).to(device)\n                for channel in range(n_channels):\n                    padded[channel, channel, :, :] = filter\n                return padded\n            self.filt_h0 = add_channels_to_filter(self.filt_h0)\n            for b in range(len(self.band_filters)):\n                self.band_filters[b] = add_channels_to_filter(\n                    self.band_filters[b])\n            self.filt_l0 = add_channels_to_filter(self.filt_l0)\n            if not self.use_bilinear_downup:\n                self.filt_l = add_channels_to_filter(self.filt_l)\n\n    def construct_pyramid(self, image, n_levels, multiple_highpass=False):\n        \"\"\"\n        Constructs and returns a steerable pyramid for the provided image.\n\n        Parameters\n        ----------\n\n        image               : torch.tensor\n                                The input image, in NCHW format. The number of channels C should match num_channels\n                                when the pyramid maker was created.\n        n_levels            : int\n                                Number of levels in the constructed steerable pyramid.\n        multiple_highpass   : bool\n                                If true, computes a highpass for each level of the pyramid.\n                                These extra levels are redundant (not used for reconstruction).\n\n        Returns\n        -------\n\n        pyramid             : list of dicts of torch.tensor\n                                The computed steerable pyramid.\n                                Each level is an entry in a list. The pyramid is ordered from largest levels to smallest levels.\n                                Each level is stored as a dict, with the following keys:\n                                \"h\" Highpass residual\n                                \"l\" Lowpass residual\n                                \"b\" Oriented bands (a list of torch.tensor)\n        \"\"\"\n        pyramid = []\n\n        # Make level 0, containing highpass, lowpass and the bands\n        level0 = {}\n        level0['h'] = torch.nn.functional.conv2d(\n            self.pad_h0(image), self.filt_h0)\n        lowpass = torch.nn.functional.conv2d(self.pad_l0(image), self.filt_l0)\n        level0['l'] = lowpass.clone()\n        bands = []\n        for filt_b in self.band_filters:\n            bands.append(torch.nn.functional.conv2d(\n                self.pad_b(lowpass), filt_b))\n        level0['b'] = bands\n        pyramid.append(level0)\n\n        # Make intermediate levels\n        for l in range(n_levels-2):\n            level = {}\n            if self.use_bilinear_downup:\n                lowpass = torch.nn.functional.interpolate(\n                    lowpass, scale_factor=0.5, mode=\"area\", recompute_scale_factor=False)\n            else:\n                lowpass = torch.nn.functional.conv2d(\n                    self.pad_l(lowpass), self.filt_l)\n                lowpass = lowpass[:, :, ::2, ::2]\n            level['l'] = lowpass.clone()\n            bands = []\n            for filt_b in self.band_filters:\n                bands.append(torch.nn.functional.conv2d(\n                    self.pad_b(lowpass), filt_b))\n            level['b'] = bands\n            if multiple_highpass:\n                level['h'] = torch.nn.functional.conv2d(\n                    self.pad_h0(lowpass), self.filt_h0)\n            pyramid.append(level)\n\n        # Make final level (lowpass residual)\n        level = {}\n        if self.use_bilinear_downup:\n            lowpass = torch.nn.functional.interpolate(\n                lowpass, scale_factor=0.5, mode=\"area\", recompute_scale_factor=False)\n        else:\n            lowpass = torch.nn.functional.conv2d(\n                self.pad_l(lowpass), self.filt_l)\n            lowpass = lowpass[:, :, ::2, ::2]\n        level['l'] = lowpass\n        pyramid.append(level)\n\n        return pyramid\n\n    def reconstruct_from_pyramid(self, pyramid):\n        \"\"\"\n        Reconstructs an input image from a steerable pyramid.\n\n        Parameters\n        ----------\n\n        pyramid : list of dicts of torch.tensor\n                    The steerable pyramid.\n                    Should be in the same format as output by construct_steerable_pyramid().\n                    The number of channels should match num_channels when the pyramid maker was created.\n\n        Returns\n        -------\n\n        image   : torch.tensor\n                    The reconstructed image, in NCHW format.         \n        \"\"\"\n        def upsample(image, size):\n            if self.use_bilinear_downup:\n                return torch.nn.functional.interpolate(image, size=size, mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n            else:\n                zeros = torch.zeros((image.size()[0], image.size()[1], image.size()[\n                                    2]*2, image.size()[3]*2)).to(self.device)\n                zeros[:, :, ::2, ::2] = image\n                zeros = torch.nn.functional.conv2d(\n                    self.pad_l(zeros), self.filt_l)\n                return zeros\n\n        image = pyramid[-1]['l']\n        for level in reversed(pyramid[:-1]):\n            image = upsample(image, level['b'][0].size()[2:])\n            for b in range(len(level['b'])):\n                b_filtered = torch.nn.functional.conv2d(\n                    self.pad_b(level['b'][b]), -self.band_filters[b])\n                image += b_filtered\n\n        image = torch.nn.functional.conv2d(self.pad_l0(image), self.filt_l0)\n        image += torch.nn.functional.conv2d(\n            self.pad_h0(pyramid[0]['h']), self.filt_h0)\n\n        return image\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.SpatialSteerablePyramid.__init__","title":"<code>__init__(use_bilinear_downup=True, n_channels=1, filter_size=9, n_orientations=6, filter_type='full', device=torch.device('cpu'))</code>","text":"<p>Parameters:</p> <ul> <li> <code>use_bilinear_downup</code>           \u2013            <pre><code>                    This uses bilinear filtering when upsampling/downsampling, rather than the original approach\n                    of applying a large lowpass kernel and sampling even rows/columns\n</code></pre> </li> <li> <code>n_channels</code>           \u2013            <pre><code>                    Number of channels in the input images (e.g. 3 for RGB input)\n</code></pre> </li> <li> <code>filter_size</code>           \u2013            <pre><code>                    Desired size of filters (e.g. 3 will use 3x3 filters).\n</code></pre> </li> <li> <code>n_orientations</code>           \u2013            <pre><code>                    Number of oriented bands in each level of the pyramid.\n</code></pre> </li> <li> <code>filter_type</code>           \u2013            <pre><code>                    This can be used to select smaller filters than the original ones if desired.\n                    full: Original filter sizes\n                    cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate.\n                    trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>                    torch device the input images will be supplied from.\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/spatial_steerable_pyramid.py</code> <pre><code>def __init__(self, use_bilinear_downup=True, n_channels=1,\n             filter_size=9, n_orientations=6, filter_type=\"full\",\n             device=torch.device('cpu')):\n    \"\"\"\n    Parameters\n    ----------\n\n    use_bilinear_downup     : bool\n                                This uses bilinear filtering when upsampling/downsampling, rather than the original approach\n                                of applying a large lowpass kernel and sampling even rows/columns\n    n_channels              : int\n                                Number of channels in the input images (e.g. 3 for RGB input)\n    filter_size             : int\n                                Desired size of filters (e.g. 3 will use 3x3 filters).\n    n_orientations          : int\n                                Number of oriented bands in each level of the pyramid.\n    filter_type             : str\n                                This can be used to select smaller filters than the original ones if desired.\n                                full: Original filter sizes\n                                cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate.\n                                trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels.\n    device                  : torch.device\n                                torch device the input images will be supplied from.\n    \"\"\"\n    self.use_bilinear_downup = use_bilinear_downup\n    self.device = device\n\n    filters = get_steerable_pyramid_filters(\n        filter_size, n_orientations, filter_type)\n\n    def make_pad(filter):\n        filter_size = filter.size(-1)\n        pad_amt = (filter_size-1) // 2\n        return torch.nn.ReflectionPad2d((pad_amt, pad_amt, pad_amt, pad_amt))\n\n    if not self.use_bilinear_downup:\n        self.filt_l = filters[\"l\"].to(device)\n        self.pad_l = make_pad(self.filt_l)\n    self.filt_l0 = filters[\"l0\"].to(device)\n    self.pad_l0 = make_pad(self.filt_l0)\n    self.filt_h0 = filters[\"h0\"].to(device)\n    self.pad_h0 = make_pad(self.filt_h0)\n    for b in range(len(filters[\"b\"])):\n        filters[\"b\"][b] = filters[\"b\"][b].to(device)\n    self.band_filters = filters[\"b\"]\n    self.pad_b = make_pad(self.band_filters[0])\n\n    if n_channels != 1:\n        def add_channels_to_filter(filter):\n            padded = torch.zeros(n_channels, n_channels, filter.size()[\n                                 2], filter.size()[3]).to(device)\n            for channel in range(n_channels):\n                padded[channel, channel, :, :] = filter\n            return padded\n        self.filt_h0 = add_channels_to_filter(self.filt_h0)\n        for b in range(len(self.band_filters)):\n            self.band_filters[b] = add_channels_to_filter(\n                self.band_filters[b])\n        self.filt_l0 = add_channels_to_filter(self.filt_l0)\n        if not self.use_bilinear_downup:\n            self.filt_l = add_channels_to_filter(self.filt_l)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.SpatialSteerablePyramid.construct_pyramid","title":"<code>construct_pyramid(image, n_levels, multiple_highpass=False)</code>","text":"<p>Constructs and returns a steerable pyramid for the provided image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>                The input image, in NCHW format. The number of channels C should match num_channels\n                when the pyramid maker was created.\n</code></pre> </li> <li> <code>n_levels</code>           \u2013            <pre><code>                Number of levels in the constructed steerable pyramid.\n</code></pre> </li> <li> <code>multiple_highpass</code>           \u2013            <pre><code>                If true, computes a highpass for each level of the pyramid.\n                These extra levels are redundant (not used for reconstruction).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>pyramid</code> (              <code>list of dicts of torch.tensor</code> )          \u2013            <p>The computed steerable pyramid. Each level is an entry in a list. The pyramid is ordered from largest levels to smallest levels. Each level is stored as a dict, with the following keys: \"h\" Highpass residual \"l\" Lowpass residual \"b\" Oriented bands (a list of torch.tensor)</p> </li> </ul> Source code in <code>odak/learn/perception/spatial_steerable_pyramid.py</code> <pre><code>def construct_pyramid(self, image, n_levels, multiple_highpass=False):\n    \"\"\"\n    Constructs and returns a steerable pyramid for the provided image.\n\n    Parameters\n    ----------\n\n    image               : torch.tensor\n                            The input image, in NCHW format. The number of channels C should match num_channels\n                            when the pyramid maker was created.\n    n_levels            : int\n                            Number of levels in the constructed steerable pyramid.\n    multiple_highpass   : bool\n                            If true, computes a highpass for each level of the pyramid.\n                            These extra levels are redundant (not used for reconstruction).\n\n    Returns\n    -------\n\n    pyramid             : list of dicts of torch.tensor\n                            The computed steerable pyramid.\n                            Each level is an entry in a list. The pyramid is ordered from largest levels to smallest levels.\n                            Each level is stored as a dict, with the following keys:\n                            \"h\" Highpass residual\n                            \"l\" Lowpass residual\n                            \"b\" Oriented bands (a list of torch.tensor)\n    \"\"\"\n    pyramid = []\n\n    # Make level 0, containing highpass, lowpass and the bands\n    level0 = {}\n    level0['h'] = torch.nn.functional.conv2d(\n        self.pad_h0(image), self.filt_h0)\n    lowpass = torch.nn.functional.conv2d(self.pad_l0(image), self.filt_l0)\n    level0['l'] = lowpass.clone()\n    bands = []\n    for filt_b in self.band_filters:\n        bands.append(torch.nn.functional.conv2d(\n            self.pad_b(lowpass), filt_b))\n    level0['b'] = bands\n    pyramid.append(level0)\n\n    # Make intermediate levels\n    for l in range(n_levels-2):\n        level = {}\n        if self.use_bilinear_downup:\n            lowpass = torch.nn.functional.interpolate(\n                lowpass, scale_factor=0.5, mode=\"area\", recompute_scale_factor=False)\n        else:\n            lowpass = torch.nn.functional.conv2d(\n                self.pad_l(lowpass), self.filt_l)\n            lowpass = lowpass[:, :, ::2, ::2]\n        level['l'] = lowpass.clone()\n        bands = []\n        for filt_b in self.band_filters:\n            bands.append(torch.nn.functional.conv2d(\n                self.pad_b(lowpass), filt_b))\n        level['b'] = bands\n        if multiple_highpass:\n            level['h'] = torch.nn.functional.conv2d(\n                self.pad_h0(lowpass), self.filt_h0)\n        pyramid.append(level)\n\n    # Make final level (lowpass residual)\n    level = {}\n    if self.use_bilinear_downup:\n        lowpass = torch.nn.functional.interpolate(\n            lowpass, scale_factor=0.5, mode=\"area\", recompute_scale_factor=False)\n    else:\n        lowpass = torch.nn.functional.conv2d(\n            self.pad_l(lowpass), self.filt_l)\n        lowpass = lowpass[:, :, ::2, ::2]\n    level['l'] = lowpass\n    pyramid.append(level)\n\n    return pyramid\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.SpatialSteerablePyramid.reconstruct_from_pyramid","title":"<code>reconstruct_from_pyramid(pyramid)</code>","text":"<p>Reconstructs an input image from a steerable pyramid.</p> <p>Parameters:</p> <ul> <li> <code>pyramid</code>               (<code>list of dicts of torch.tensor</code>)           \u2013            <pre><code>    The steerable pyramid.\n    Should be in the same format as output by construct_steerable_pyramid().\n    The number of channels should match num_channels when the pyramid maker was created.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image</code> (              <code>tensor</code> )          \u2013            <p>The reconstructed image, in NCHW format.</p> </li> </ul> Source code in <code>odak/learn/perception/spatial_steerable_pyramid.py</code> <pre><code>def reconstruct_from_pyramid(self, pyramid):\n    \"\"\"\n    Reconstructs an input image from a steerable pyramid.\n\n    Parameters\n    ----------\n\n    pyramid : list of dicts of torch.tensor\n                The steerable pyramid.\n                Should be in the same format as output by construct_steerable_pyramid().\n                The number of channels should match num_channels when the pyramid maker was created.\n\n    Returns\n    -------\n\n    image   : torch.tensor\n                The reconstructed image, in NCHW format.         \n    \"\"\"\n    def upsample(image, size):\n        if self.use_bilinear_downup:\n            return torch.nn.functional.interpolate(image, size=size, mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n        else:\n            zeros = torch.zeros((image.size()[0], image.size()[1], image.size()[\n                                2]*2, image.size()[3]*2)).to(self.device)\n            zeros[:, :, ::2, ::2] = image\n            zeros = torch.nn.functional.conv2d(\n                self.pad_l(zeros), self.filt_l)\n            return zeros\n\n    image = pyramid[-1]['l']\n    for level in reversed(pyramid[:-1]):\n        image = upsample(image, level['b'][0].size()[2:])\n        for b in range(len(level['b'])):\n            b_filtered = torch.nn.functional.conv2d(\n                self.pad_b(level['b'][b]), -self.band_filters[b])\n            image += b_filtered\n\n    image = torch.nn.functional.conv2d(self.pad_l0(image), self.filt_l0)\n    image += torch.nn.functional.conv2d(\n        self.pad_h0(pyramid[0]['h']), self.filt_h0)\n\n    return image\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs","title":"<code>display_color_hvs</code>","text":"Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>class display_color_hvs():\n\n\n    def __init__(\n                 self,\n                 resolution = [1920, 1080],\n                 distance_from_screen = 800,\n                 pixel_pitch = 0.311,\n                 read_spectrum = 'tensor',\n                 primaries_spectrum = torch.rand(3, 301),\n                 device = torch.device('cpu')\n                ):\n        '''\n        Parameters\n        ----------\n        resolution                  : list\n                                      Resolution of the display in pixels.\n        distance_from_screen        : int\n                                      Distance from the screen in mm.\n        pixel_pitch                 : float\n                                      Pixel pitch of the display in mm.\n        read_spectrum               : str\n                                      Spectrum of the display. Default is 'default' which is the spectrum of the Dell U2415 display [3 x 301].\n        device                      : torch.device\n                                      Device to run the code on. Default is None which means the code will run on CPU.\n\n        '''\n        self.device = device\n        self.read_spectrum = read_spectrum\n        self.primaries_spectrum = primaries_spectrum.to(self.device)\n        self.resolution = resolution\n        self.distance_from_screen = distance_from_screen\n        self.pixel_pitch = pixel_pitch\n        self.l_normalized, self.m_normalized, self.s_normalized = self.initialize_cones_normalized()\n        self.lms_tensor = self.construct_matrix_lms(\n                                                    self.l_normalized,\n                                                    self.m_normalized,\n                                                    self.s_normalized\n                                                   )   \n        self.primaries_tensor = self.construct_matrix_primaries(\n                                                                self.l_normalized,\n                                                                self.m_normalized,\n                                                                self.s_normalized\n                                                               )   \n        return\n\n\n    def __call__(self, input_image, ground_truth, gaze=None):\n        \"\"\"\n        Evaluating an input image against a target ground truth image for a given gaze of a viewer.\n        \"\"\"\n        lms_image_second = self.primaries_to_lms(input_image.to(self.device))\n        lms_ground_truth_second = self.primaries_to_lms(ground_truth.to(self.device))\n        lms_image_third = self.second_to_third_stage(lms_image_second)\n        lms_ground_truth_third = self.second_to_third_stage(lms_ground_truth_second)\n        loss_metamer_color = torch.mean((lms_ground_truth_third - lms_image_third) ** 2)\n        return loss_metamer_color\n\n\n    def initialize_cones_normalized(self):\n        \"\"\"\n        Internal function to initialize normalized L,M,S cones as normal distribution with given sigma, and mu values. \n\n        Returns\n        -------\n        l_cone_n                     : torch.tensor\n                                       Normalised L cone distribution.\n        m_cone_n                     : torch.tensor\n                                       Normalised M cone distribution.\n        s_cone_n                     : torch.tensor\n                                       Normalised S cone distribution.\n        \"\"\"\n        wavelength_range = torch.linspace(400, 700, steps = self.primaries_spectrum.shape[-1], device = self.device)\n        dist_l = 1 / (32.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 567.5) ** 2 / (2 * 32.5 ** 2))\n        dist_m = 1 / (27.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 545.0) ** 2 / (2 * 27.5 ** 2))\n        dist_s = 1 / (17.0 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 447.5) ** 2 / (2 * 17.0 ** 2))\n\n        l_cone_n = dist_l / dist_l.max()\n        m_cone_n = dist_m / dist_m.max()\n        s_cone_n = dist_s / dist_s.max()\n        return l_cone_n, m_cone_n, s_cone_n\n\n\n    def initialize_rgb_backlight_spectrum(self):\n        \"\"\"\n        Internal function to initialize baclight spectrum for color primaries. \n\n        Returns\n        -------\n        red_spectrum                 : torch.tensor\n                                       Normalised backlight spectrum for red color primary.\n        green_spectrum               : torch.tensor\n                                       Normalised backlight spectrum for green color primary.\n        blue_spectrum                : torch.tensor\n                                       Normalised backlight spectrum for blue color primary.\n        \"\"\"\n        wavelength_range = torch.linspace(400, 700, steps = self.primaries_spectrum.shape[-1], device = self.device)\n        red_spectrum = 1 / (14.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 650) ** 2 / (2 * 14.5 ** 2))\n        green_spectrum = 1 / (12 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 550) ** 2 / (2 * 12.0 ** 2))\n        blue_spectrum = 1 / (12 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 450) ** 2 / (2 * 12.0 ** 2))\n\n        red_spectrum = red_spectrum / red_spectrum.max()\n        green_spectrum = green_spectrum / green_spectrum.max()\n        blue_spectrum = blue_spectrum / blue_spectrum.max()\n\n        return red_spectrum, green_spectrum, blue_spectrum\n\n\n    def initialize_random_spectrum_normalized(self, dataset):\n        \"\"\"\n        Initialize normalized light spectrum via combination of 3 gaussian distribution curve fitting [L-BFGS]. \n\n        Parameters\n        ----------\n        dataset                                : torch.tensor \n                                                 spectrum value against wavelength \n        \"\"\"\n        dataset = torch.swapaxes(dataset, 0, 1)\n        x_spectrum = torch.linspace(400, 700, steps = 301) - 550\n        y_spectrum = torch.from_numpy(np_cpu.interp(x_spectrum, dataset[0].numpy(), dataset[1].numpy()))\n        max_spectrum = torch.max(y_spectrum)\n        y_spectrum /= max_spectrum\n\n        def gaussian(x, A = 1, sigma = 1, centre = 0): return A * \\\n            torch.exp(-(x - centre) ** 2 / (2 * sigma ** 2))\n\n        def function(x, weights): \n            return gaussian(x, *weights[:3]) + gaussian(x, *weights[3:6]) + gaussian(x, *weights[6:9])\n\n        weights = torch.tensor([1.0, 1.0, -0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2], requires_grad = True)\n        optimizer = torch.optim.LBFGS([weights], max_iter = 1000, lr = 0.1, line_search_fn = None)\n\n        def closure():\n            optimizer.zero_grad()\n            output = function(x_spectrum, weights)\n            loss = F.mse_loss(output, y_spectrum)\n            loss.backward()\n            return loss\n        optimizer.step(closure)\n        spectrum = function(x_spectrum, weights)\n        return spectrum.detach().to(self.device)\n\n\n    def display_spectrum_response(wavelength, function):\n        \"\"\"\n        Internal function to provide light spectrum response at particular wavelength\n\n        Parameters\n        ----------\n        wavelength                          : torch.tensor\n                                              Wavelength in nm [400...700]\n        function                            : torch.tensor\n                                              Display light spectrum distribution function\n\n        Returns\n        -------\n        ligth_response_dict                  : float\n                                               Display light spectrum response value\n        \"\"\"\n        wavelength = int(round(wavelength, 0))\n        if wavelength &gt;= 400 and wavelength &lt;= 700:\n            return function[wavelength - 400].item()\n        elif wavelength &lt; 400:\n            return function[0].item()\n        else:\n            return function[300].item()\n\n\n    def cone_response_to_spectrum(self, cone_spectrum, light_spectrum):\n        \"\"\"\n        Internal function to calculate cone response at particular light spectrum. \n\n        Parameters\n        ----------\n        cone_spectrum                         : torch.tensor\n                                                Spectrum, Wavelength [2,300] tensor \n        light_spectrum                        : torch.tensor\n                                                Spectrum, Wavelength [2,300] tensor \n\n\n        Returns\n        -------\n        response_to_spectrum                  : float\n                                                Response of cone to light spectrum [1x1] \n        \"\"\"\n        response_to_spectrum = torch.mul(cone_spectrum, light_spectrum)\n        response_to_spectrum = torch.sum(response_to_spectrum)\n        return response_to_spectrum.item()\n\n\n    def construct_matrix_lms(self, l_response, m_response, s_response):\n        '''\n        Internal function to calculate cone  response at particular light spectrum. \n\n        Parameters\n        ----------\n        l_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n        m_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n        s_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n\n\n\n        Returns\n        -------\n        lms_image_tensor                      : torch.tensor\n                                                3x3 LMSrgb tensor\n\n        '''\n        if self.read_spectrum == 'tensor':\n            logging.warning('Tensor primary spectrum is used')\n            logging.warning('The number of primaries used is {}'.format(self.primaries_spectrum.shape[0]))\n        else:\n            logging.warning(\"No Spectrum data is provided\")\n\n        self.lms_tensor = torch.zeros(self.primaries_spectrum.shape[0], 3).to(self.device)\n        for i in range(self.primaries_spectrum.shape[0]):\n            self.lms_tensor[i, 0] = self.cone_response_to_spectrum(l_response, self.primaries_spectrum[i])\n            self.lms_tensor[i, 1] = self.cone_response_to_spectrum(m_response, self.primaries_spectrum[i])\n            self.lms_tensor[i, 2] = self.cone_response_to_spectrum(s_response, self.primaries_spectrum[i]) \n        return self.lms_tensor    \n\n\n    def construct_matrix_primaries(self, l_response, m_response, s_response):\n        '''\n        Internal function to calculate cone  response at particular light spectrum. \n\n        Parameters\n        ----------\n        l_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n        m_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n        s_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n\n\n\n        Returns\n        -------\n        lms_image_tensor                      : torch.tensor\n                                                3x3 LMSrgb tensor\n\n        '''\n        if self.read_spectrum == 'tensor':\n            logging.warning('Tensor primary spectrum is used')\n            logging.warning('The number of primaries used is {}'.format(self.primaries_spectrum.shape[0]))\n        else:\n            logging.warning(\"No Spectrum data is provided\")\n\n        self.primaries_tensor = torch.zeros(3, self.primaries_spectrum.shape[0]).to(self.device)\n        for i in range(self.primaries_spectrum.shape[0]):\n            self.primaries_tensor[0, i] = self.cone_response_to_spectrum(\n                                                                         l_response,\n                                                                         self.primaries_spectrum[i]\n                                                                        )\n            self.primaries_tensor[1, i] = self.cone_response_to_spectrum(\n                                                                         m_response,\n                                                                         self.primaries_spectrum[i]\n                                                                        )\n            self.primaries_tensor[2, i] = self.cone_response_to_spectrum(\n                                                                         s_response,\n                                                                         self.primaries_spectrum[i]\n                                                                        ) \n        return self.primaries_tensor    \n\n\n    def primaries_to_lms(self, primaries):\n        \"\"\"\n        Internal function to convert primaries space to LMS space \n\n        Parameters\n        ----------\n        primaries                              : torch.tensor\n                                                 Primaries data to be transformed to LMS space [BxPHxW]\n\n\n        Returns\n        -------\n        lms_color                              : torch.tensor\n                                                 LMS data transformed from Primaries space [BxPxHxW]\n        \"\"\"                \n        primaries_flatten = primaries.reshape(primaries.shape[0], primaries.shape[1], 1, -1)\n        lms = self.lms_tensor.unsqueeze(0).unsqueeze(-1)\n        lms_color = torch.sum(primaries_flatten * lms, axis = 1).reshape(primaries.shape)\n        return lms_color\n\n\n    def lms_to_primaries(self, lms_color_tensor):\n        \"\"\"\n        Internal function to convert LMS image to primaries space\n\n        Parameters\n        ----------\n        lms_color_tensor                        : torch.tensor\n                                                  LMS data to be transformed to primaries space [Bx3xHxW]\n\n\n        Returns\n        -------\n        primaries                              : torch.tensor\n                                               : Primaries data transformed from LMS space [BxPxHxW]\n        \"\"\"\n        lms_color_tensor = lms_color_tensor.permute(0, 2, 3, 1).to(self.device)\n        lms_color_flatten = torch.flatten(lms_color_tensor, start_dim=0, end_dim=1)\n        unflatten = torch.nn.Unflatten(0, (lms_color_tensor.size(0), lms_color_tensor.size(1)))\n        converted_unflatten = torch.matmul(lms_color_flatten.double(), self.lms_tensor.pinverse().double())\n        primaries = unflatten(converted_unflatten)     \n        primaries = primaries.permute(0, 3, 1, 2)   \n        return primaries\n\n\n    def second_to_third_stage(self, lms_image):\n        '''\n        This function turns second stage [L,M,S] values into third stage [(M+S)-L, (L+S)-M, L+M+S], \n        See table 1 from Schmidt et al. \"Neurobiological hypothesis of color appearance and hue perception,\" Optics Express 2014.\n\n        Parameters\n        ----------\n        lms_image                             : torch.tensor\n                                                 Image data at LMS space (second stage)\n\n        Returns\n        -------\n        third_stage                            : torch.tensor\n                                                 Image data at LMS space (third stage)\n\n        '''\n        third_stage = torch.zeros_like(lms_image)\n        third_stage[:, 0] = (lms_image[:, 1] + lms_image[:, 2]) - lms_image[:, 1]\n        third_stage[:, 1] = (lms_image[:, 0] + lms_image[:, 2]) - lms_image[:, 1]\n        third_stage[:, 2] = lms_image[:, 0] + lms_image[:, 1]  + lms_image[:, 2]\n        return third_stage\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.__call__","title":"<code>__call__(input_image, ground_truth, gaze=None)</code>","text":"<p>Evaluating an input image against a target ground truth image for a given gaze of a viewer.</p> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def __call__(self, input_image, ground_truth, gaze=None):\n    \"\"\"\n    Evaluating an input image against a target ground truth image for a given gaze of a viewer.\n    \"\"\"\n    lms_image_second = self.primaries_to_lms(input_image.to(self.device))\n    lms_ground_truth_second = self.primaries_to_lms(ground_truth.to(self.device))\n    lms_image_third = self.second_to_third_stage(lms_image_second)\n    lms_ground_truth_third = self.second_to_third_stage(lms_ground_truth_second)\n    loss_metamer_color = torch.mean((lms_ground_truth_third - lms_image_third) ** 2)\n    return loss_metamer_color\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.__init__","title":"<code>__init__(resolution=[1920, 1080], distance_from_screen=800, pixel_pitch=0.311, read_spectrum='tensor', primaries_spectrum=torch.rand(3, 301), device=torch.device('cpu'))</code>","text":"<p>Parameters:</p> <ul> <li> <code>resolution</code>           \u2013            <pre><code>                      Resolution of the display in pixels.\n</code></pre> </li> <li> <code>distance_from_screen</code>           \u2013            <pre><code>                      Distance from the screen in mm.\n</code></pre> </li> <li> <code>pixel_pitch</code>           \u2013            <pre><code>                      Pixel pitch of the display in mm.\n</code></pre> </li> <li> <code>read_spectrum</code>           \u2013            <pre><code>                      Spectrum of the display. Default is 'default' which is the spectrum of the Dell U2415 display [3 x 301].\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>                      Device to run the code on. Default is None which means the code will run on CPU.\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def __init__(\n             self,\n             resolution = [1920, 1080],\n             distance_from_screen = 800,\n             pixel_pitch = 0.311,\n             read_spectrum = 'tensor',\n             primaries_spectrum = torch.rand(3, 301),\n             device = torch.device('cpu')\n            ):\n    '''\n    Parameters\n    ----------\n    resolution                  : list\n                                  Resolution of the display in pixels.\n    distance_from_screen        : int\n                                  Distance from the screen in mm.\n    pixel_pitch                 : float\n                                  Pixel pitch of the display in mm.\n    read_spectrum               : str\n                                  Spectrum of the display. Default is 'default' which is the spectrum of the Dell U2415 display [3 x 301].\n    device                      : torch.device\n                                  Device to run the code on. Default is None which means the code will run on CPU.\n\n    '''\n    self.device = device\n    self.read_spectrum = read_spectrum\n    self.primaries_spectrum = primaries_spectrum.to(self.device)\n    self.resolution = resolution\n    self.distance_from_screen = distance_from_screen\n    self.pixel_pitch = pixel_pitch\n    self.l_normalized, self.m_normalized, self.s_normalized = self.initialize_cones_normalized()\n    self.lms_tensor = self.construct_matrix_lms(\n                                                self.l_normalized,\n                                                self.m_normalized,\n                                                self.s_normalized\n                                               )   \n    self.primaries_tensor = self.construct_matrix_primaries(\n                                                            self.l_normalized,\n                                                            self.m_normalized,\n                                                            self.s_normalized\n                                                           )   \n    return\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.cone_response_to_spectrum","title":"<code>cone_response_to_spectrum(cone_spectrum, light_spectrum)</code>","text":"<p>Internal function to calculate cone response at particular light spectrum. </p> <p>Parameters:</p> <ul> <li> <code>cone_spectrum</code>           \u2013            <pre><code>                                Spectrum, Wavelength [2,300] tensor\n</code></pre> </li> <li> <code>light_spectrum</code>           \u2013            <pre><code>                                Spectrum, Wavelength [2,300] tensor\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>response_to_spectrum</code> (              <code>float</code> )          \u2013            <p>Response of cone to light spectrum [1x1]</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def cone_response_to_spectrum(self, cone_spectrum, light_spectrum):\n    \"\"\"\n    Internal function to calculate cone response at particular light spectrum. \n\n    Parameters\n    ----------\n    cone_spectrum                         : torch.tensor\n                                            Spectrum, Wavelength [2,300] tensor \n    light_spectrum                        : torch.tensor\n                                            Spectrum, Wavelength [2,300] tensor \n\n\n    Returns\n    -------\n    response_to_spectrum                  : float\n                                            Response of cone to light spectrum [1x1] \n    \"\"\"\n    response_to_spectrum = torch.mul(cone_spectrum, light_spectrum)\n    response_to_spectrum = torch.sum(response_to_spectrum)\n    return response_to_spectrum.item()\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.construct_matrix_lms","title":"<code>construct_matrix_lms(l_response, m_response, s_response)</code>","text":"<p>Internal function to calculate cone  response at particular light spectrum. </p> <p>Parameters:</p> <ul> <li> <code>l_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> <li> <code>m_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> <li> <code>s_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>lms_image_tensor</code> (              <code>tensor</code> )          \u2013            <p>3x3 LMSrgb tensor</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def construct_matrix_lms(self, l_response, m_response, s_response):\n    '''\n    Internal function to calculate cone  response at particular light spectrum. \n\n    Parameters\n    ----------\n    l_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n    m_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n    s_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n\n\n\n    Returns\n    -------\n    lms_image_tensor                      : torch.tensor\n                                            3x3 LMSrgb tensor\n\n    '''\n    if self.read_spectrum == 'tensor':\n        logging.warning('Tensor primary spectrum is used')\n        logging.warning('The number of primaries used is {}'.format(self.primaries_spectrum.shape[0]))\n    else:\n        logging.warning(\"No Spectrum data is provided\")\n\n    self.lms_tensor = torch.zeros(self.primaries_spectrum.shape[0], 3).to(self.device)\n    for i in range(self.primaries_spectrum.shape[0]):\n        self.lms_tensor[i, 0] = self.cone_response_to_spectrum(l_response, self.primaries_spectrum[i])\n        self.lms_tensor[i, 1] = self.cone_response_to_spectrum(m_response, self.primaries_spectrum[i])\n        self.lms_tensor[i, 2] = self.cone_response_to_spectrum(s_response, self.primaries_spectrum[i]) \n    return self.lms_tensor    \n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.construct_matrix_primaries","title":"<code>construct_matrix_primaries(l_response, m_response, s_response)</code>","text":"<p>Internal function to calculate cone  response at particular light spectrum. </p> <p>Parameters:</p> <ul> <li> <code>l_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> <li> <code>m_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> <li> <code>s_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>lms_image_tensor</code> (              <code>tensor</code> )          \u2013            <p>3x3 LMSrgb tensor</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def construct_matrix_primaries(self, l_response, m_response, s_response):\n    '''\n    Internal function to calculate cone  response at particular light spectrum. \n\n    Parameters\n    ----------\n    l_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n    m_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n    s_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n\n\n\n    Returns\n    -------\n    lms_image_tensor                      : torch.tensor\n                                            3x3 LMSrgb tensor\n\n    '''\n    if self.read_spectrum == 'tensor':\n        logging.warning('Tensor primary spectrum is used')\n        logging.warning('The number of primaries used is {}'.format(self.primaries_spectrum.shape[0]))\n    else:\n        logging.warning(\"No Spectrum data is provided\")\n\n    self.primaries_tensor = torch.zeros(3, self.primaries_spectrum.shape[0]).to(self.device)\n    for i in range(self.primaries_spectrum.shape[0]):\n        self.primaries_tensor[0, i] = self.cone_response_to_spectrum(\n                                                                     l_response,\n                                                                     self.primaries_spectrum[i]\n                                                                    )\n        self.primaries_tensor[1, i] = self.cone_response_to_spectrum(\n                                                                     m_response,\n                                                                     self.primaries_spectrum[i]\n                                                                    )\n        self.primaries_tensor[2, i] = self.cone_response_to_spectrum(\n                                                                     s_response,\n                                                                     self.primaries_spectrum[i]\n                                                                    ) \n    return self.primaries_tensor    \n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.display_spectrum_response","title":"<code>display_spectrum_response(wavelength, function)</code>","text":"<p>Internal function to provide light spectrum response at particular wavelength</p> <p>Parameters:</p> <ul> <li> <code>wavelength</code>           \u2013            <pre><code>                              Wavelength in nm [400...700]\n</code></pre> </li> <li> <code>function</code>           \u2013            <pre><code>                              Display light spectrum distribution function\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ligth_response_dict</code> (              <code>float</code> )          \u2013            <p>Display light spectrum response value</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def display_spectrum_response(wavelength, function):\n    \"\"\"\n    Internal function to provide light spectrum response at particular wavelength\n\n    Parameters\n    ----------\n    wavelength                          : torch.tensor\n                                          Wavelength in nm [400...700]\n    function                            : torch.tensor\n                                          Display light spectrum distribution function\n\n    Returns\n    -------\n    ligth_response_dict                  : float\n                                           Display light spectrum response value\n    \"\"\"\n    wavelength = int(round(wavelength, 0))\n    if wavelength &gt;= 400 and wavelength &lt;= 700:\n        return function[wavelength - 400].item()\n    elif wavelength &lt; 400:\n        return function[0].item()\n    else:\n        return function[300].item()\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.initialize_cones_normalized","title":"<code>initialize_cones_normalized()</code>","text":"<p>Internal function to initialize normalized L,M,S cones as normal distribution with given sigma, and mu values. </p> <p>Returns:</p> <ul> <li> <code>l_cone_n</code> (              <code>tensor</code> )          \u2013            <p>Normalised L cone distribution.</p> </li> <li> <code>m_cone_n</code> (              <code>tensor</code> )          \u2013            <p>Normalised M cone distribution.</p> </li> <li> <code>s_cone_n</code> (              <code>tensor</code> )          \u2013            <p>Normalised S cone distribution.</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def initialize_cones_normalized(self):\n    \"\"\"\n    Internal function to initialize normalized L,M,S cones as normal distribution with given sigma, and mu values. \n\n    Returns\n    -------\n    l_cone_n                     : torch.tensor\n                                   Normalised L cone distribution.\n    m_cone_n                     : torch.tensor\n                                   Normalised M cone distribution.\n    s_cone_n                     : torch.tensor\n                                   Normalised S cone distribution.\n    \"\"\"\n    wavelength_range = torch.linspace(400, 700, steps = self.primaries_spectrum.shape[-1], device = self.device)\n    dist_l = 1 / (32.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 567.5) ** 2 / (2 * 32.5 ** 2))\n    dist_m = 1 / (27.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 545.0) ** 2 / (2 * 27.5 ** 2))\n    dist_s = 1 / (17.0 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 447.5) ** 2 / (2 * 17.0 ** 2))\n\n    l_cone_n = dist_l / dist_l.max()\n    m_cone_n = dist_m / dist_m.max()\n    s_cone_n = dist_s / dist_s.max()\n    return l_cone_n, m_cone_n, s_cone_n\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.initialize_random_spectrum_normalized","title":"<code>initialize_random_spectrum_normalized(dataset)</code>","text":"<p>Initialize normalized light spectrum via combination of 3 gaussian distribution curve fitting [L-BFGS]. </p> <p>Parameters:</p> <ul> <li> <code>dataset</code>           \u2013            <pre><code>                                 spectrum value against wavelength\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def initialize_random_spectrum_normalized(self, dataset):\n    \"\"\"\n    Initialize normalized light spectrum via combination of 3 gaussian distribution curve fitting [L-BFGS]. \n\n    Parameters\n    ----------\n    dataset                                : torch.tensor \n                                             spectrum value against wavelength \n    \"\"\"\n    dataset = torch.swapaxes(dataset, 0, 1)\n    x_spectrum = torch.linspace(400, 700, steps = 301) - 550\n    y_spectrum = torch.from_numpy(np_cpu.interp(x_spectrum, dataset[0].numpy(), dataset[1].numpy()))\n    max_spectrum = torch.max(y_spectrum)\n    y_spectrum /= max_spectrum\n\n    def gaussian(x, A = 1, sigma = 1, centre = 0): return A * \\\n        torch.exp(-(x - centre) ** 2 / (2 * sigma ** 2))\n\n    def function(x, weights): \n        return gaussian(x, *weights[:3]) + gaussian(x, *weights[3:6]) + gaussian(x, *weights[6:9])\n\n    weights = torch.tensor([1.0, 1.0, -0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2], requires_grad = True)\n    optimizer = torch.optim.LBFGS([weights], max_iter = 1000, lr = 0.1, line_search_fn = None)\n\n    def closure():\n        optimizer.zero_grad()\n        output = function(x_spectrum, weights)\n        loss = F.mse_loss(output, y_spectrum)\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n    spectrum = function(x_spectrum, weights)\n    return spectrum.detach().to(self.device)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.initialize_rgb_backlight_spectrum","title":"<code>initialize_rgb_backlight_spectrum()</code>","text":"<p>Internal function to initialize baclight spectrum for color primaries. </p> <p>Returns:</p> <ul> <li> <code>red_spectrum</code> (              <code>tensor</code> )          \u2013            <p>Normalised backlight spectrum for red color primary.</p> </li> <li> <code>green_spectrum</code> (              <code>tensor</code> )          \u2013            <p>Normalised backlight spectrum for green color primary.</p> </li> <li> <code>blue_spectrum</code> (              <code>tensor</code> )          \u2013            <p>Normalised backlight spectrum for blue color primary.</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def initialize_rgb_backlight_spectrum(self):\n    \"\"\"\n    Internal function to initialize baclight spectrum for color primaries. \n\n    Returns\n    -------\n    red_spectrum                 : torch.tensor\n                                   Normalised backlight spectrum for red color primary.\n    green_spectrum               : torch.tensor\n                                   Normalised backlight spectrum for green color primary.\n    blue_spectrum                : torch.tensor\n                                   Normalised backlight spectrum for blue color primary.\n    \"\"\"\n    wavelength_range = torch.linspace(400, 700, steps = self.primaries_spectrum.shape[-1], device = self.device)\n    red_spectrum = 1 / (14.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 650) ** 2 / (2 * 14.5 ** 2))\n    green_spectrum = 1 / (12 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 550) ** 2 / (2 * 12.0 ** 2))\n    blue_spectrum = 1 / (12 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 450) ** 2 / (2 * 12.0 ** 2))\n\n    red_spectrum = red_spectrum / red_spectrum.max()\n    green_spectrum = green_spectrum / green_spectrum.max()\n    blue_spectrum = blue_spectrum / blue_spectrum.max()\n\n    return red_spectrum, green_spectrum, blue_spectrum\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.lms_to_primaries","title":"<code>lms_to_primaries(lms_color_tensor)</code>","text":"<p>Internal function to convert LMS image to primaries space</p> <p>Parameters:</p> <ul> <li> <code>lms_color_tensor</code>           \u2013            <pre><code>                                  LMS data to be transformed to primaries space [Bx3xHxW]\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>primaries</code> (              <code>tensor</code> )          \u2013            <p>: Primaries data transformed from LMS space [BxPxHxW]</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def lms_to_primaries(self, lms_color_tensor):\n    \"\"\"\n    Internal function to convert LMS image to primaries space\n\n    Parameters\n    ----------\n    lms_color_tensor                        : torch.tensor\n                                              LMS data to be transformed to primaries space [Bx3xHxW]\n\n\n    Returns\n    -------\n    primaries                              : torch.tensor\n                                           : Primaries data transformed from LMS space [BxPxHxW]\n    \"\"\"\n    lms_color_tensor = lms_color_tensor.permute(0, 2, 3, 1).to(self.device)\n    lms_color_flatten = torch.flatten(lms_color_tensor, start_dim=0, end_dim=1)\n    unflatten = torch.nn.Unflatten(0, (lms_color_tensor.size(0), lms_color_tensor.size(1)))\n    converted_unflatten = torch.matmul(lms_color_flatten.double(), self.lms_tensor.pinverse().double())\n    primaries = unflatten(converted_unflatten)     \n    primaries = primaries.permute(0, 3, 1, 2)   \n    return primaries\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.primaries_to_lms","title":"<code>primaries_to_lms(primaries)</code>","text":"<p>Internal function to convert primaries space to LMS space </p> <p>Parameters:</p> <ul> <li> <code>primaries</code>           \u2013            <pre><code>                                 Primaries data to be transformed to LMS space [BxPHxW]\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>lms_color</code> (              <code>tensor</code> )          \u2013            <p>LMS data transformed from Primaries space [BxPxHxW]</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def primaries_to_lms(self, primaries):\n    \"\"\"\n    Internal function to convert primaries space to LMS space \n\n    Parameters\n    ----------\n    primaries                              : torch.tensor\n                                             Primaries data to be transformed to LMS space [BxPHxW]\n\n\n    Returns\n    -------\n    lms_color                              : torch.tensor\n                                             LMS data transformed from Primaries space [BxPxHxW]\n    \"\"\"                \n    primaries_flatten = primaries.reshape(primaries.shape[0], primaries.shape[1], 1, -1)\n    lms = self.lms_tensor.unsqueeze(0).unsqueeze(-1)\n    lms_color = torch.sum(primaries_flatten * lms, axis = 1).reshape(primaries.shape)\n    return lms_color\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.display_color_hvs.second_to_third_stage","title":"<code>second_to_third_stage(lms_image)</code>","text":"<p>This function turns second stage [L,M,S] values into third stage [(M+S)-L, (L+S)-M, L+M+S],  See table 1 from Schmidt et al. \"Neurobiological hypothesis of color appearance and hue perception,\" Optics Express 2014.</p> <p>Parameters:</p> <ul> <li> <code>lms_image</code>           \u2013            <pre><code>                                 Image data at LMS space (second stage)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>third_stage</code> (              <code>tensor</code> )          \u2013            <p>Image data at LMS space (third stage)</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def second_to_third_stage(self, lms_image):\n    '''\n    This function turns second stage [L,M,S] values into third stage [(M+S)-L, (L+S)-M, L+M+S], \n    See table 1 from Schmidt et al. \"Neurobiological hypothesis of color appearance and hue perception,\" Optics Express 2014.\n\n    Parameters\n    ----------\n    lms_image                             : torch.tensor\n                                             Image data at LMS space (second stage)\n\n    Returns\n    -------\n    third_stage                            : torch.tensor\n                                             Image data at LMS space (third stage)\n\n    '''\n    third_stage = torch.zeros_like(lms_image)\n    third_stage[:, 0] = (lms_image[:, 1] + lms_image[:, 2]) - lms_image[:, 1]\n    third_stage[:, 1] = (lms_image[:, 0] + lms_image[:, 2]) - lms_image[:, 1]\n    third_stage[:, 2] = lms_image[:, 0] + lms_image[:, 1]  + lms_image[:, 2]\n    return third_stage\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_map","title":"<code>color_map(input_image, target_image, model='Lab Stats')</code>","text":"<p>Internal function to map the color of an image to another image. Reference: Color transfer between images, Reinhard et al., 2001.</p> <p>Parameters:</p> <ul> <li> <code>input_image</code>           \u2013            <pre><code>              Input image in RGB color space [3 x m x n].\n</code></pre> </li> <li> <code>target_image</code>           \u2013            </li> </ul> <p>Returns:</p> <ul> <li> <code>mapped_image</code> (              <code>Tensor</code> )          \u2013            <p>Input image with the color the distribution of the target image [3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def color_map(input_image, target_image, model = 'Lab Stats'):\n    \"\"\"\n    Internal function to map the color of an image to another image.\n    Reference: Color transfer between images, Reinhard et al., 2001.\n\n    Parameters\n    ----------\n    input_image         : torch.Tensor\n                          Input image in RGB color space [3 x m x n].\n    target_image        : torch.Tensor\n\n    Returns\n    -------\n    mapped_image           : torch.Tensor\n                             Input image with the color the distribution of the target image [3 x m x n].\n    \"\"\"\n    if model == 'Lab Stats':\n        lab_input = srgb_to_lab(input_image)\n        lab_target = srgb_to_lab(target_image)\n        input_mean_L = torch.mean(lab_input[0, :, :])\n        input_mean_a = torch.mean(lab_input[1, :, :])\n        input_mean_b = torch.mean(lab_input[2, :, :])\n        input_std_L = torch.std(lab_input[0, :, :])\n        input_std_a = torch.std(lab_input[1, :, :])\n        input_std_b = torch.std(lab_input[2, :, :])\n        target_mean_L = torch.mean(lab_target[0, :, :])\n        target_mean_a = torch.mean(lab_target[1, :, :])\n        target_mean_b = torch.mean(lab_target[2, :, :])\n        target_std_L = torch.std(lab_target[0, :, :])\n        target_std_a = torch.std(lab_target[1, :, :])\n        target_std_b = torch.std(lab_target[2, :, :])\n        lab_input[0, :, :] = (lab_input[0, :, :] - input_mean_L) * (target_std_L / input_std_L) + target_mean_L\n        lab_input[1, :, :] = (lab_input[1, :, :] - input_mean_a) * (target_std_a / input_std_a) + target_mean_a\n        lab_input[2, :, :] = (lab_input[2, :, :] - input_mean_b) * (target_std_b / input_std_b) + target_mean_b\n        mapped_image = lab_to_srgb(lab_input.permute(1, 2, 0))\n        return mapped_image\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.crop_steerable_pyramid_filters","title":"<code>crop_steerable_pyramid_filters(filters, size)</code>","text":"<p>Given original 9x9 NYU filters, this crops them to the desired size. The size must be an odd number &gt;= 3 Note this only crops the h0, l0 and band filters (not the l downsampling filter)</p> <p>Parameters:</p> <ul> <li> <code>filters</code>           \u2013            <pre><code>        Filters to crop (should in format used by get_steerable_pyramid_filters.)\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>        Size to crop to. For example, an input of 3 will crop the filters to a size of 3x3.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>filters</code> (              <code>dict of torch.tensor</code> )          \u2013            <p>The cropped filters.</p> </li> </ul> Source code in <code>odak/learn/perception/steerable_pyramid_filters.py</code> <pre><code>def crop_steerable_pyramid_filters(filters, size):\n    \"\"\"\n    Given original 9x9 NYU filters, this crops them to the desired size.\n    The size must be an odd number &gt;= 3\n    Note this only crops the h0, l0 and band filters (not the l downsampling filter)\n\n    Parameters\n    ----------\n    filters     : dict of torch.tensor\n                    Filters to crop (should in format used by get_steerable_pyramid_filters.)\n    size        : int\n                    Size to crop to. For example, an input of 3 will crop the filters to a size of 3x3.\n\n    Returns\n    -------\n    filters     : dict of torch.tensor\n                    The cropped filters.\n    \"\"\"\n    assert(size &gt;= 3)\n    assert(size % 2 == 1)\n    r = (size-1) // 2\n\n    def crop_filter(filter, r, normalise=True):\n        r2 = (filter.size(-1)-1)//2\n        filter = filter[:, :, r2-r:r2+r+1, r2-r:r2+r+1]\n        if normalise:\n            filter -= torch.sum(filter)\n        return filter\n\n    filters[\"h0\"] = crop_filter(filters[\"h0\"], r, normalise=False)\n    sum_l = torch.sum(filters[\"l\"])\n    filters[\"l\"] = crop_filter(filters[\"l\"], 6, normalise=False)\n    filters[\"l\"] *= sum_l / torch.sum(filters[\"l\"])\n    sum_l0 = torch.sum(filters[\"l0\"])\n    filters[\"l0\"] = crop_filter(filters[\"l0\"], 2, normalise=False)\n    filters[\"l0\"] *= sum_l0 / torch.sum(filters[\"l0\"])\n    for b in range(len(filters[\"b\"])):\n        filters[\"b\"][b] = crop_filter(filters[\"b\"][b], r, normalise=True)\n    return filters\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.get_steerable_pyramid_filters","title":"<code>get_steerable_pyramid_filters(size, n_orientations, filter_type)</code>","text":"<p>This returns filters for a real-valued steerable pyramid.</p> <p>Parameters:</p> <ul> <li> <code>size</code>           \u2013            <pre><code>            Width of the filters (e.g. 3 will return 3x3 filters)\n</code></pre> </li> <li> <code>n_orientations</code>           \u2013            <pre><code>            Number of oriented band filters\n</code></pre> </li> <li> <code>filter_type</code>           \u2013            <pre><code>            This can be used to select between the original NYU filters and cropped or trained alternatives.\n            full: Original NYU filters from https://github.com/LabForComputationalVision/pyrtools/blob/master/pyrtools/pyramids/filters.py\n            cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate.\n            trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>filters</code> (              <code>dict of torch.tensor</code> )          \u2013            <p>The steerable pyramid filters. Returned as a dict with the following keys: \"l\" The lowpass downsampling filter \"l0\" The lowpass residual filter \"h0\" The highpass residual filter \"b\" The band filters (a list of torch.tensor filters, one for each orientation).</p> </li> </ul> Source code in <code>odak/learn/perception/steerable_pyramid_filters.py</code> <pre><code>def get_steerable_pyramid_filters(size, n_orientations, filter_type):\n    \"\"\"\n    This returns filters for a real-valued steerable pyramid.\n\n    Parameters\n    ----------\n\n    size            : int\n                        Width of the filters (e.g. 3 will return 3x3 filters)\n    n_orientations  : int\n                        Number of oriented band filters\n    filter_type     :  str\n                        This can be used to select between the original NYU filters and cropped or trained alternatives.\n                        full: Original NYU filters from https://github.com/LabForComputationalVision/pyrtools/blob/master/pyrtools/pyramids/filters.py\n                        cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate.\n                        trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels.\n\n    Returns\n    -------\n    filters         : dict of torch.tensor\n                        The steerable pyramid filters. Returned as a dict with the following keys:\n                        \"l\" The lowpass downsampling filter\n                        \"l0\" The lowpass residual filter\n                        \"h0\" The highpass residual filter\n                        \"b\" The band filters (a list of torch.tensor filters, one for each orientation).\n    \"\"\"\n\n    if filter_type != \"full\" and filter_type != \"cropped\" and filter_type != \"trained\":\n        raise Exception(\n            \"Unknown filter type %s! Only filter types are full, cropped or trained.\" % filter_type)\n\n    filters = {}\n    if n_orientations == 1:\n        filters[\"l\"] = torch.tensor([\n            [-2.257000e-04, -8.064400e-04, -5.686000e-05, 8.741400e-04, -1.862800e-04, -1.031640e-03, -\n                1.871920e-03, -1.031640e-03, -1.862800e-04, 8.741400e-04, -5.686000e-05, -8.064400e-04, -2.257000e-04],\n            [-8.064400e-04, 1.417620e-03, -1.903800e-04, -2.449060e-03, -4.596420e-03, -7.006740e-03, -\n                6.948900e-03, -7.006740e-03, -4.596420e-03, -2.449060e-03, -1.903800e-04, 1.417620e-03, -8.064400e-04],\n            [-5.686000e-05, -1.903800e-04, -3.059760e-03, -6.401000e-03, -6.720800e-03, -5.236180e-03, -\n                3.781600e-03, -5.236180e-03, -6.720800e-03, -6.401000e-03, -3.059760e-03, -1.903800e-04, -5.686000e-05],\n            [8.741400e-04, -2.449060e-03, -6.401000e-03, -5.260020e-03, 3.938620e-03, 1.722078e-02, 2.449600e-02,\n                1.722078e-02, 3.938620e-03, -5.260020e-03, -6.401000e-03, -2.449060e-03, 8.741400e-04],\n            [-1.862800e-04, -4.596420e-03, -6.720800e-03, 3.938620e-03, 3.220744e-02, 6.306262e-02, 7.624674e-02,\n                6.306262e-02, 3.220744e-02, 3.938620e-03, -6.720800e-03, -4.596420e-03, -1.862800e-04],\n            [-1.031640e-03, -7.006740e-03, -5.236180e-03, 1.722078e-02, 6.306262e-02, 1.116388e-01, 1.348999e-01,\n                1.116388e-01, 6.306262e-02, 1.722078e-02, -5.236180e-03, -7.006740e-03, -1.031640e-03],\n            [-1.871920e-03, -6.948900e-03, -3.781600e-03, 2.449600e-02, 7.624674e-02, 1.348999e-01, 1.576508e-01,\n                1.348999e-01, 7.624674e-02, 2.449600e-02, -3.781600e-03, -6.948900e-03, -1.871920e-03],\n            [-1.031640e-03, -7.006740e-03, -5.236180e-03, 1.722078e-02, 6.306262e-02, 1.116388e-01, 1.348999e-01,\n                1.116388e-01, 6.306262e-02, 1.722078e-02, -5.236180e-03, -7.006740e-03, -1.031640e-03],\n            [-1.862800e-04, -4.596420e-03, -6.720800e-03, 3.938620e-03, 3.220744e-02, 6.306262e-02, 7.624674e-02,\n                6.306262e-02, 3.220744e-02, 3.938620e-03, -6.720800e-03, -4.596420e-03, -1.862800e-04],\n            [8.741400e-04, -2.449060e-03, -6.401000e-03, -5.260020e-03, 3.938620e-03, 1.722078e-02, 2.449600e-02,\n                1.722078e-02, 3.938620e-03, -5.260020e-03, -6.401000e-03, -2.449060e-03, 8.741400e-04],\n            [-5.686000e-05, -1.903800e-04, -3.059760e-03, -6.401000e-03, -6.720800e-03, -5.236180e-03, -\n                3.781600e-03, -5.236180e-03, -6.720800e-03, -6.401000e-03, -3.059760e-03, -1.903800e-04, -5.686000e-05],\n            [-8.064400e-04, 1.417620e-03, -1.903800e-04, -2.449060e-03, -4.596420e-03, -7.006740e-03, -\n                6.948900e-03, -7.006740e-03, -4.596420e-03, -2.449060e-03, -1.903800e-04, 1.417620e-03, -8.064400e-04],\n            [-2.257000e-04, -8.064400e-04, -5.686000e-05, 8.741400e-04, -1.862800e-04, -1.031640e-03, -1.871920e-03, -1.031640e-03, -1.862800e-04, 8.741400e-04, -5.686000e-05, -8.064400e-04, -2.257000e-04]]\n        ).reshape(1, 1, 13, 13)\n        filters[\"l0\"] = torch.tensor([\n            [-4.514000e-04, -1.137100e-04, -3.725800e-04, -\n                3.743860e-03, -3.725800e-04, -1.137100e-04, -4.514000e-04],\n            [-1.137100e-04, -6.119520e-03, -1.344160e-02, -\n                7.563200e-03, -1.344160e-02, -6.119520e-03, -1.137100e-04],\n            [-3.725800e-04, -1.344160e-02, 6.441488e-02, 1.524935e-01,\n                6.441488e-02, -1.344160e-02, -3.725800e-04],\n            [-3.743860e-03, -7.563200e-03, 1.524935e-01, 3.153017e-01,\n                1.524935e-01, -7.563200e-03, -3.743860e-03],\n            [-3.725800e-04, -1.344160e-02, 6.441488e-02, 1.524935e-01,\n                6.441488e-02, -1.344160e-02, -3.725800e-04],\n            [-1.137100e-04, -6.119520e-03, -1.344160e-02, -\n                7.563200e-03, -1.344160e-02, -6.119520e-03, -1.137100e-04],\n            [-4.514000e-04, -1.137100e-04, -3.725800e-04, -3.743860e-03, -3.725800e-04, -1.137100e-04, -4.514000e-04]]\n        ).reshape(1, 1, 7, 7)\n        filters[\"h0\"] = torch.tensor([\n            [5.997200e-04, -6.068000e-05, -3.324900e-04, -3.325600e-04, -\n                2.406600e-04, -3.325600e-04, -3.324900e-04, -6.068000e-05, 5.997200e-04],\n            [-6.068000e-05, 1.263100e-04, 4.927100e-04, 1.459700e-04, -\n                3.732100e-04, 1.459700e-04, 4.927100e-04, 1.263100e-04, -6.068000e-05],\n            [-3.324900e-04, 4.927100e-04, -1.616650e-03, -1.437358e-02, -\n                2.420138e-02, -1.437358e-02, -1.616650e-03, 4.927100e-04, -3.324900e-04],\n            [-3.325600e-04, 1.459700e-04, -1.437358e-02, -6.300923e-02, -\n                9.623594e-02, -6.300923e-02, -1.437358e-02, 1.459700e-04, -3.325600e-04],\n            [-2.406600e-04, -3.732100e-04, -2.420138e-02, -9.623594e-02,\n                8.554893e-01, -9.623594e-02, -2.420138e-02, -3.732100e-04, -2.406600e-04],\n            [-3.325600e-04, 1.459700e-04, -1.437358e-02, -6.300923e-02, -\n                9.623594e-02, -6.300923e-02, -1.437358e-02, 1.459700e-04, -3.325600e-04],\n            [-3.324900e-04, 4.927100e-04, -1.616650e-03, -1.437358e-02, -\n                2.420138e-02, -1.437358e-02, -1.616650e-03, 4.927100e-04, -3.324900e-04],\n            [-6.068000e-05, 1.263100e-04, 4.927100e-04, 1.459700e-04, -\n                3.732100e-04, 1.459700e-04, 4.927100e-04, 1.263100e-04, -6.068000e-05],\n            [5.997200e-04, -6.068000e-05, -3.324900e-04, -3.325600e-04, -2.406600e-04, -3.325600e-04, -3.324900e-04, -6.068000e-05, 5.997200e-04]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"b\"] = []\n        filters[\"b\"].append(torch.tensor([\n            -9.066000e-05, -1.738640e-03, -4.942500e-03, -7.889390e-03, -\n            1.009473e-02, -7.889390e-03, -4.942500e-03, -1.738640e-03, -9.066000e-05,\n            -1.738640e-03, -4.625150e-03, -7.272540e-03, -7.623410e-03, -\n            9.091950e-03, -7.623410e-03, -7.272540e-03, -4.625150e-03, -1.738640e-03,\n            -4.942500e-03, -7.272540e-03, -2.129540e-02, -2.435662e-02, -\n            3.487008e-02, -2.435662e-02, -2.129540e-02, -7.272540e-03, -4.942500e-03,\n            -7.889390e-03, -7.623410e-03, -2.435662e-02, -1.730466e-02, -\n            3.158605e-02, -1.730466e-02, -2.435662e-02, -7.623410e-03, -7.889390e-03,\n            -1.009473e-02, -9.091950e-03, -3.487008e-02, -3.158605e-02, 9.464195e-01, -\n            3.158605e-02, -3.487008e-02, -9.091950e-03, -1.009473e-02,\n            -7.889390e-03, -7.623410e-03, -2.435662e-02, -1.730466e-02, -\n            3.158605e-02, -1.730466e-02, -2.435662e-02, -7.623410e-03, -7.889390e-03,\n            -4.942500e-03, -7.272540e-03, -2.129540e-02, -2.435662e-02, -\n            3.487008e-02, -2.435662e-02, -2.129540e-02, -7.272540e-03, -4.942500e-03,\n            -1.738640e-03, -4.625150e-03, -7.272540e-03, -7.623410e-03, -\n            9.091950e-03, -7.623410e-03, -7.272540e-03, -4.625150e-03, -1.738640e-03,\n            -9.066000e-05, -1.738640e-03, -4.942500e-03, -7.889390e-03, -1.009473e-02, -7.889390e-03, -4.942500e-03, -1.738640e-03, -9.066000e-05]\n        ).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n\n    elif n_orientations == 2:\n        filters[\"l\"] = torch.tensor(\n            [[-4.350000e-05, 1.207800e-04, -6.771400e-04, -1.243400e-04, -8.006400e-04, -1.597040e-03, -2.516800e-04, -4.202000e-04, 1.262000e-03, -4.202000e-04, -2.516800e-04, -1.597040e-03, -8.006400e-04, -1.243400e-04, -6.771400e-04, 1.207800e-04, -4.350000e-05],\n             [1.207800e-04, 4.460600e-04, -5.814600e-04, 5.621600e-04, -1.368800e-04, 2.325540e-03, 2.889860e-03, 4.287280e-03, 5.589400e-03,\n                 4.287280e-03, 2.889860e-03, 2.325540e-03, -1.368800e-04, 5.621600e-04, -5.814600e-04, 4.460600e-04, 1.207800e-04],\n             [-6.771400e-04, -5.814600e-04, 1.460780e-03, 2.160540e-03, 3.761360e-03, 3.080980e-03, 4.112200e-03, 2.221220e-03, 5.538200e-04,\n                 2.221220e-03, 4.112200e-03, 3.080980e-03, 3.761360e-03, 2.160540e-03, 1.460780e-03, -5.814600e-04, -6.771400e-04],\n             [-1.243400e-04, 5.621600e-04, 2.160540e-03, 3.175780e-03, 3.184680e-03, -1.777480e-03, -7.431700e-03, -9.056920e-03, -\n                 9.637220e-03, -9.056920e-03, -7.431700e-03, -1.777480e-03, 3.184680e-03, 3.175780e-03, 2.160540e-03, 5.621600e-04, -1.243400e-04],\n             [-8.006400e-04, -1.368800e-04, 3.761360e-03, 3.184680e-03, -3.530640e-03, -1.260420e-02, -1.884744e-02, -1.750818e-02, -\n                 1.648568e-02, -1.750818e-02, -1.884744e-02, -1.260420e-02, -3.530640e-03, 3.184680e-03, 3.761360e-03, -1.368800e-04, -8.006400e-04],\n             [-1.597040e-03, 2.325540e-03, 3.080980e-03, -1.777480e-03, -1.260420e-02, -2.022938e-02, -1.109170e-02, 3.955660e-03, 1.438512e-02,\n                 3.955660e-03, -1.109170e-02, -2.022938e-02, -1.260420e-02, -1.777480e-03, 3.080980e-03, 2.325540e-03, -1.597040e-03],\n             [-2.516800e-04, 2.889860e-03, 4.112200e-03, -7.431700e-03, -1.884744e-02, -1.109170e-02, 2.190660e-02, 6.806584e-02, 9.058014e-02,\n                 6.806584e-02, 2.190660e-02, -1.109170e-02, -1.884744e-02, -7.431700e-03, 4.112200e-03, 2.889860e-03, -2.516800e-04],\n             [-4.202000e-04, 4.287280e-03, 2.221220e-03, -9.056920e-03, -1.750818e-02, 3.955660e-03, 6.806584e-02, 1.445500e-01, 1.773651e-01,\n                 1.445500e-01, 6.806584e-02, 3.955660e-03, -1.750818e-02, -9.056920e-03, 2.221220e-03, 4.287280e-03, -4.202000e-04],\n             [1.262000e-03, 5.589400e-03, 5.538200e-04, -9.637220e-03, -1.648568e-02, 1.438512e-02, 9.058014e-02, 1.773651e-01, 2.120374e-01,\n                 1.773651e-01, 9.058014e-02, 1.438512e-02, -1.648568e-02, -9.637220e-03, 5.538200e-04, 5.589400e-03, 1.262000e-03],\n             [-4.202000e-04, 4.287280e-03, 2.221220e-03, -9.056920e-03, -1.750818e-02, 3.955660e-03, 6.806584e-02, 1.445500e-01, 1.773651e-01,\n                 1.445500e-01, 6.806584e-02, 3.955660e-03, -1.750818e-02, -9.056920e-03, 2.221220e-03, 4.287280e-03, -4.202000e-04],\n             [-2.516800e-04, 2.889860e-03, 4.112200e-03, -7.431700e-03, -1.884744e-02, -1.109170e-02, 2.190660e-02, 6.806584e-02, 9.058014e-02,\n                 6.806584e-02, 2.190660e-02, -1.109170e-02, -1.884744e-02, -7.431700e-03, 4.112200e-03, 2.889860e-03, -2.516800e-04],\n             [-1.597040e-03, 2.325540e-03, 3.080980e-03, -1.777480e-03, -1.260420e-02, -2.022938e-02, -1.109170e-02, 3.955660e-03, 1.438512e-02,\n                 3.955660e-03, -1.109170e-02, -2.022938e-02, -1.260420e-02, -1.777480e-03, 3.080980e-03, 2.325540e-03, -1.597040e-03],\n             [-8.006400e-04, -1.368800e-04, 3.761360e-03, 3.184680e-03, -3.530640e-03, -1.260420e-02, -1.884744e-02, -1.750818e-02, -\n                 1.648568e-02, -1.750818e-02, -1.884744e-02, -1.260420e-02, -3.530640e-03, 3.184680e-03, 3.761360e-03, -1.368800e-04, -8.006400e-04],\n             [-1.243400e-04, 5.621600e-04, 2.160540e-03, 3.175780e-03, 3.184680e-03, -1.777480e-03, -7.431700e-03, -9.056920e-03, -\n                 9.637220e-03, -9.056920e-03, -7.431700e-03, -1.777480e-03, 3.184680e-03, 3.175780e-03, 2.160540e-03, 5.621600e-04, -1.243400e-04],\n             [-6.771400e-04, -5.814600e-04, 1.460780e-03, 2.160540e-03, 3.761360e-03, 3.080980e-03, 4.112200e-03, 2.221220e-03, 5.538200e-04,\n                 2.221220e-03, 4.112200e-03, 3.080980e-03, 3.761360e-03, 2.160540e-03, 1.460780e-03, -5.814600e-04, -6.771400e-04],\n             [1.207800e-04, 4.460600e-04, -5.814600e-04, 5.621600e-04, -1.368800e-04, 2.325540e-03, 2.889860e-03, 4.287280e-03, 5.589400e-03,\n                 4.287280e-03, 2.889860e-03, 2.325540e-03, -1.368800e-04, 5.621600e-04, -5.814600e-04, 4.460600e-04, 1.207800e-04],\n             [-4.350000e-05, 1.207800e-04, -6.771400e-04, -1.243400e-04, -8.006400e-04, -1.597040e-03, -2.516800e-04, -4.202000e-04, 1.262000e-03, -4.202000e-04, -2.516800e-04, -1.597040e-03, -8.006400e-04, -1.243400e-04, -6.771400e-04, 1.207800e-04, -4.350000e-05]]\n        ).reshape(1, 1, 17, 17)\n        filters[\"l0\"] = torch.tensor(\n            [[-8.701000e-05, -1.354280e-03, -1.601260e-03, -5.033700e-04, 2.524010e-03, -5.033700e-04, -1.601260e-03, -1.354280e-03, -8.701000e-05],\n             [-1.354280e-03, 2.921580e-03, 7.522720e-03, 8.224420e-03, 1.107620e-03,\n                 8.224420e-03, 7.522720e-03, 2.921580e-03, -1.354280e-03],\n             [-1.601260e-03, 7.522720e-03, -7.061290e-03, -3.769487e-02, -\n                 3.297137e-02, -3.769487e-02, -7.061290e-03, 7.522720e-03, -1.601260e-03],\n             [-5.033700e-04, 8.224420e-03, -3.769487e-02, 4.381320e-02, 1.811603e-01,\n                 4.381320e-02, -3.769487e-02, 8.224420e-03, -5.033700e-04],\n             [2.524010e-03, 1.107620e-03, -3.297137e-02, 1.811603e-01, 4.376250e-01,\n                 1.811603e-01, -3.297137e-02, 1.107620e-03, 2.524010e-03],\n             [-5.033700e-04, 8.224420e-03, -3.769487e-02, 4.381320e-02, 1.811603e-01,\n                 4.381320e-02, -3.769487e-02, 8.224420e-03, -5.033700e-04],\n             [-1.601260e-03, 7.522720e-03, -7.061290e-03, -3.769487e-02, -\n                 3.297137e-02, -3.769487e-02, -7.061290e-03, 7.522720e-03, -1.601260e-03],\n             [-1.354280e-03, 2.921580e-03, 7.522720e-03, 8.224420e-03, 1.107620e-03,\n                 8.224420e-03, 7.522720e-03, 2.921580e-03, -1.354280e-03],\n             [-8.701000e-05, -1.354280e-03, -1.601260e-03, -5.033700e-04, 2.524010e-03, -5.033700e-04, -1.601260e-03, -1.354280e-03, -8.701000e-05]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"h0\"] = torch.tensor(\n            [[-9.570000e-04, -2.424100e-04, -1.424720e-03, -8.742600e-04, -1.166810e-03, -8.742600e-04, -1.424720e-03, -2.424100e-04, -9.570000e-04],\n             [-2.424100e-04, -4.317530e-03, 8.998600e-04, 9.156420e-03, 1.098012e-02,\n                 9.156420e-03, 8.998600e-04, -4.317530e-03, -2.424100e-04],\n             [-1.424720e-03, 8.998600e-04, 1.706347e-02, 1.094866e-02, -\n                 5.897780e-03, 1.094866e-02, 1.706347e-02, 8.998600e-04, -1.424720e-03],\n             [-8.742600e-04, 9.156420e-03, 1.094866e-02, -7.841370e-02, -\n                 1.562827e-01, -7.841370e-02, 1.094866e-02, 9.156420e-03, -8.742600e-04],\n             [-1.166810e-03, 1.098012e-02, -5.897780e-03, -1.562827e-01,\n                 7.282593e-01, -1.562827e-01, -5.897780e-03, 1.098012e-02, -1.166810e-03],\n             [-8.742600e-04, 9.156420e-03, 1.094866e-02, -7.841370e-02, -\n                 1.562827e-01, -7.841370e-02, 1.094866e-02, 9.156420e-03, -8.742600e-04],\n             [-1.424720e-03, 8.998600e-04, 1.706347e-02, 1.094866e-02, -\n                 5.897780e-03, 1.094866e-02, 1.706347e-02, 8.998600e-04, -1.424720e-03],\n             [-2.424100e-04, -4.317530e-03, 8.998600e-04, 9.156420e-03, 1.098012e-02,\n                 9.156420e-03, 8.998600e-04, -4.317530e-03, -2.424100e-04],\n             [-9.570000e-04, -2.424100e-04, -1.424720e-03, -8.742600e-04, -1.166810e-03, -8.742600e-04, -1.424720e-03, -2.424100e-04, -9.570000e-04]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"b\"] = []\n        filters[\"b\"].append(torch.tensor(\n            [6.125880e-03, -8.052600e-03, -2.103714e-02, -1.536890e-02, -1.851466e-02, -1.536890e-02, -2.103714e-02, -8.052600e-03, 6.125880e-03,\n             -1.287416e-02, -9.611520e-03, 1.023569e-02, 6.009450e-03, 1.872620e-03, 6.009450e-03, 1.023569e-02, -\n             9.611520e-03, -1.287416e-02,\n             -5.641530e-03, 4.168400e-03, -2.382180e-02, -5.375324e-02, -\n             2.076086e-02, -5.375324e-02, -2.382180e-02, 4.168400e-03, -5.641530e-03,\n             -8.957260e-03, -1.751170e-03, -1.836909e-02, 1.265655e-01, 2.996168e-01, 1.265655e-01, -\n             1.836909e-02, -1.751170e-03, -8.957260e-03,\n             0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n             8.957260e-03, 1.751170e-03, 1.836909e-02, -1.265655e-01, -\n             2.996168e-01, -1.265655e-01, 1.836909e-02, 1.751170e-03, 8.957260e-03,\n             5.641530e-03, -4.168400e-03, 2.382180e-02, 5.375324e-02, 2.076086e-02, 5.375324e-02, 2.382180e-02, -\n             4.168400e-03, 5.641530e-03,\n             1.287416e-02, 9.611520e-03, -1.023569e-02, -6.009450e-03, -\n             1.872620e-03, -6.009450e-03, -1.023569e-02, 9.611520e-03, 1.287416e-02,\n             -6.125880e-03, 8.052600e-03, 2.103714e-02, 1.536890e-02, 1.851466e-02, 1.536890e-02, 2.103714e-02, 8.052600e-03, -6.125880e-03]).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor(\n            [-6.125880e-03, 1.287416e-02, 5.641530e-03, 8.957260e-03, 0.000000e+00, -8.957260e-03, -5.641530e-03, -1.287416e-02, 6.125880e-03,\n             8.052600e-03, 9.611520e-03, -4.168400e-03, 1.751170e-03, 0.000000e+00, -\n             1.751170e-03, 4.168400e-03, -9.611520e-03, -8.052600e-03,\n             2.103714e-02, -1.023569e-02, 2.382180e-02, 1.836909e-02, 0.000000e+00, -\n             1.836909e-02, -2.382180e-02, 1.023569e-02, -2.103714e-02,\n             1.536890e-02, -6.009450e-03, 5.375324e-02, -\n             1.265655e-01, 0.000000e+00, 1.265655e-01, -\n             5.375324e-02, 6.009450e-03, -1.536890e-02,\n             1.851466e-02, -1.872620e-03, 2.076086e-02, -\n             2.996168e-01, 0.000000e+00, 2.996168e-01, -\n             2.076086e-02, 1.872620e-03, -1.851466e-02,\n             1.536890e-02, -6.009450e-03, 5.375324e-02, -\n             1.265655e-01, 0.000000e+00, 1.265655e-01, -\n             5.375324e-02, 6.009450e-03, -1.536890e-02,\n             2.103714e-02, -1.023569e-02, 2.382180e-02, 1.836909e-02, 0.000000e+00, -\n             1.836909e-02, -2.382180e-02, 1.023569e-02, -2.103714e-02,\n             8.052600e-03, 9.611520e-03, -4.168400e-03, 1.751170e-03, 0.000000e+00, -\n             1.751170e-03, 4.168400e-03, -9.611520e-03, -8.052600e-03,\n             -6.125880e-03, 1.287416e-02, 5.641530e-03, 8.957260e-03, 0.000000e+00, -8.957260e-03, -5.641530e-03, -1.287416e-02, 6.125880e-03]).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n\n    elif n_orientations == 4:\n        filters[\"l\"] = torch.tensor([\n            [-4.3500000174E-5, 1.2078000145E-4, -6.7714002216E-4, -1.2434000382E-4, -8.0063997302E-4, -1.5970399836E-3, -2.5168000138E-4, -4.2019999819E-4,\n                1.2619999470E-3, -4.2019999819E-4, -2.5168000138E-4, -1.5970399836E-3, -8.0063997302E-4, -1.2434000382E-4, -6.7714002216E-4, 1.2078000145E-4, -4.3500000174E-5],\n            [1.2078000145E-4, 4.4606000301E-4, -5.8146001538E-4, 5.6215998484E-4, -1.3688000035E-4, 2.3255399428E-3, 2.8898599558E-3, 4.2872801423E-3, 5.5893999524E-3,\n                4.2872801423E-3, 2.8898599558E-3, 2.3255399428E-3, -1.3688000035E-4, 5.6215998484E-4, -5.8146001538E-4, 4.4606000301E-4, 1.2078000145E-4],\n            [-6.7714002216E-4, -5.8146001538E-4, 1.4607800404E-3, 2.1605400834E-3, 3.7613599561E-3, 3.0809799209E-3, 4.1121998802E-3, 2.2212199401E-3, 5.5381999118E-4,\n                2.2212199401E-3, 4.1121998802E-3, 3.0809799209E-3, 3.7613599561E-3, 2.1605400834E-3, 1.4607800404E-3, -5.8146001538E-4, -6.7714002216E-4],\n            [-1.2434000382E-4, 5.6215998484E-4, 2.1605400834E-3, 3.1757799443E-3, 3.1846798956E-3, -1.7774800071E-3, -7.4316998944E-3, -9.0569201857E-3, -\n                9.6372198313E-3, -9.0569201857E-3, -7.4316998944E-3, -1.7774800071E-3, 3.1846798956E-3, 3.1757799443E-3, 2.1605400834E-3, 5.6215998484E-4, -1.2434000382E-4],\n            [-8.0063997302E-4, -1.3688000035E-4, 3.7613599561E-3, 3.1846798956E-3, -3.5306399222E-3, -1.2604200281E-2, -1.8847439438E-2, -1.7508180812E-2, -\n                1.6485679895E-2, -1.7508180812E-2, -1.8847439438E-2, -1.2604200281E-2, -3.5306399222E-3, 3.1846798956E-3, 3.7613599561E-3, -1.3688000035E-4, -8.0063997302E-4],\n            [-1.5970399836E-3, 2.3255399428E-3, 3.0809799209E-3, -1.7774800071E-3, -1.2604200281E-2, -2.0229380578E-2, -1.1091699824E-2, 3.9556599222E-3, 1.4385120012E-2,\n                3.9556599222E-3, -1.1091699824E-2, -2.0229380578E-2, -1.2604200281E-2, -1.7774800071E-3, 3.0809799209E-3, 2.3255399428E-3, -1.5970399836E-3],\n            [-2.5168000138E-4, 2.8898599558E-3, 4.1121998802E-3, -7.4316998944E-3, -1.8847439438E-2, -1.1091699824E-2, 2.1906599402E-2, 6.8065837026E-2, 9.0580143034E-2,\n                6.8065837026E-2, 2.1906599402E-2, -1.1091699824E-2, -1.8847439438E-2, -7.4316998944E-3, 4.1121998802E-3, 2.8898599558E-3, -2.5168000138E-4],\n            [-4.2019999819E-4, 4.2872801423E-3, 2.2212199401E-3, -9.0569201857E-3, -1.7508180812E-2, 3.9556599222E-3, 6.8065837026E-2, 0.1445499808, 0.1773651242,\n                0.1445499808, 6.8065837026E-2, 3.9556599222E-3, -1.7508180812E-2, -9.0569201857E-3, 2.2212199401E-3, 4.2872801423E-3, -4.2019999819E-4],\n            [1.2619999470E-3, 5.5893999524E-3, 5.5381999118E-4, -9.6372198313E-3, -1.6485679895E-2, 1.4385120012E-2, 9.0580143034E-2, 0.1773651242, 0.2120374441,\n                0.1773651242, 9.0580143034E-2, 1.4385120012E-2, -1.6485679895E-2, -9.6372198313E-3, 5.5381999118E-4, 5.5893999524E-3, 1.2619999470E-3],\n            [-4.2019999819E-4, 4.2872801423E-3, 2.2212199401E-3, -9.0569201857E-3, -1.7508180812E-2, 3.9556599222E-3, 6.8065837026E-2, 0.1445499808, 0.1773651242,\n                0.1445499808, 6.8065837026E-2, 3.9556599222E-3, -1.7508180812E-2, -9.0569201857E-3, 2.2212199401E-3, 4.2872801423E-3, -4.2019999819E-4],\n            [-2.5168000138E-4, 2.8898599558E-3, 4.1121998802E-3, -7.4316998944E-3, -1.8847439438E-2, -1.1091699824E-2, 2.1906599402E-2, 6.8065837026E-2, 9.0580143034E-2,\n                6.8065837026E-2, 2.1906599402E-2, -1.1091699824E-2, -1.8847439438E-2, -7.4316998944E-3, 4.1121998802E-3, 2.8898599558E-3, -2.5168000138E-4],\n            [-1.5970399836E-3, 2.3255399428E-3, 3.0809799209E-3, -1.7774800071E-3, -1.2604200281E-2, -2.0229380578E-2, -1.1091699824E-2, 3.9556599222E-3, 1.4385120012E-2,\n                3.9556599222E-3, -1.1091699824E-2, -2.0229380578E-2, -1.2604200281E-2, -1.7774800071E-3, 3.0809799209E-3, 2.3255399428E-3, -1.5970399836E-3],\n            [-8.0063997302E-4, -1.3688000035E-4, 3.7613599561E-3, 3.1846798956E-3, -3.5306399222E-3, -1.2604200281E-2, -1.8847439438E-2, -1.7508180812E-2, -\n                1.6485679895E-2, -1.7508180812E-2, -1.8847439438E-2, -1.2604200281E-2, -3.5306399222E-3, 3.1846798956E-3, 3.7613599561E-3, -1.3688000035E-4, -8.0063997302E-4],\n            [-1.2434000382E-4, 5.6215998484E-4, 2.1605400834E-3, 3.1757799443E-3, 3.1846798956E-3, -1.7774800071E-3, -7.4316998944E-3, -9.0569201857E-3, -\n                9.6372198313E-3, -9.0569201857E-3, -7.4316998944E-3, -1.7774800071E-3, 3.1846798956E-3, 3.1757799443E-3, 2.1605400834E-3, 5.6215998484E-4, -1.2434000382E-4],\n            [-6.7714002216E-4, -5.8146001538E-4, 1.4607800404E-3, 2.1605400834E-3, 3.7613599561E-3, 3.0809799209E-3, 4.1121998802E-3, 2.2212199401E-3, 5.5381999118E-4,\n                2.2212199401E-3, 4.1121998802E-3, 3.0809799209E-3, 3.7613599561E-3, 2.1605400834E-3, 1.4607800404E-3, -5.8146001538E-4, -6.7714002216E-4],\n            [1.2078000145E-4, 4.4606000301E-4, -5.8146001538E-4, 5.6215998484E-4, -1.3688000035E-4, 2.3255399428E-3, 2.8898599558E-3, 4.2872801423E-3, 5.5893999524E-3,\n                4.2872801423E-3, 2.8898599558E-3, 2.3255399428E-3, -1.3688000035E-4, 5.6215998484E-4, -5.8146001538E-4, 4.4606000301E-4, 1.2078000145E-4],\n            [-4.3500000174E-5, 1.2078000145E-4, -6.7714002216E-4, -1.2434000382E-4, -8.0063997302E-4, -1.5970399836E-3, -2.5168000138E-4, -4.2019999819E-4, 1.2619999470E-3, -4.2019999819E-4, -2.5168000138E-4, -1.5970399836E-3, -8.0063997302E-4, -1.2434000382E-4, -6.7714002216E-4, 1.2078000145E-4, -4.3500000174E-5]]\n        ).reshape(1, 1, 17, 17)\n        filters[\"l0\"] = torch.tensor([\n            [-8.7009997515E-5, -1.3542800443E-3, -1.6012600390E-3, -5.0337001448E-4,\n                2.5240099058E-3, -5.0337001448E-4, -1.6012600390E-3, -1.3542800443E-3, -8.7009997515E-5],\n            [-1.3542800443E-3, 2.9215801042E-3, 7.5227199122E-3, 8.2244202495E-3, 1.1076199589E-3,\n                8.2244202495E-3, 7.5227199122E-3, 2.9215801042E-3, -1.3542800443E-3],\n            [-1.6012600390E-3, 7.5227199122E-3, -7.0612900890E-3, -3.7694871426E-2, -\n                3.2971370965E-2, -3.7694871426E-2, -7.0612900890E-3, 7.5227199122E-3, -1.6012600390E-3],\n            [-5.0337001448E-4, 8.2244202495E-3, -3.7694871426E-2, 4.3813198805E-2, 0.1811603010,\n                4.3813198805E-2, -3.7694871426E-2, 8.2244202495E-3, -5.0337001448E-4],\n            [2.5240099058E-3, 1.1076199589E-3, -3.2971370965E-2, 0.1811603010, 0.4376249909,\n                0.1811603010, -3.2971370965E-2, 1.1076199589E-3, 2.5240099058E-3],\n            [-5.0337001448E-4, 8.2244202495E-3, -3.7694871426E-2, 4.3813198805E-2, 0.1811603010,\n                4.3813198805E-2, -3.7694871426E-2, 8.2244202495E-3, -5.0337001448E-4],\n            [-1.6012600390E-3, 7.5227199122E-3, -7.0612900890E-3, -3.7694871426E-2, -\n                3.2971370965E-2, -3.7694871426E-2, -7.0612900890E-3, 7.5227199122E-3, -1.6012600390E-3],\n            [-1.3542800443E-3, 2.9215801042E-3, 7.5227199122E-3, 8.2244202495E-3, 1.1076199589E-3,\n                8.2244202495E-3, 7.5227199122E-3, 2.9215801042E-3, -1.3542800443E-3],\n            [-8.7009997515E-5, -1.3542800443E-3, -1.6012600390E-3, -5.0337001448E-4, 2.5240099058E-3, -5.0337001448E-4, -1.6012600390E-3, -1.3542800443E-3, -8.7009997515E-5]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"h0\"] = torch.tensor([\n            [-4.0483998600E-4, -6.2596000498E-4, -3.7829999201E-5, 8.8387000142E-4, 1.5450799838E-3, 1.9235999789E-3, 2.0687500946E-3, 2.0898699295E-3,\n                2.0687500946E-3, 1.9235999789E-3, 1.5450799838E-3, 8.8387000142E-4, -3.7829999201E-5, -6.2596000498E-4, -4.0483998600E-4],\n            [-6.2596000498E-4, -3.2734998967E-4, 7.7435001731E-4, 1.5874400269E-3, 2.1750701126E-3, 2.5626500137E-3, 2.2892199922E-3, 1.9755100366E-3,\n                2.2892199922E-3, 2.5626500137E-3, 2.1750701126E-3, 1.5874400269E-3, 7.7435001731E-4, -3.2734998967E-4, -6.2596000498E-4],\n            [-3.7829999201E-5, 7.7435001731E-4, 1.1793200392E-3, 1.4050999889E-3, 2.2253401112E-3, 2.1145299543E-3, 3.3578000148E-4, -\n                8.3368999185E-4, 3.3578000148E-4, 2.1145299543E-3, 2.2253401112E-3, 1.4050999889E-3, 1.1793200392E-3, 7.7435001731E-4, -3.7829999201E-5],\n            [8.8387000142E-4, 1.5874400269E-3, 1.4050999889E-3, 1.2960999738E-3, -4.9274001503E-4, -3.1295299996E-3, -4.5751798898E-3, -\n                5.1014497876E-3, -4.5751798898E-3, -3.1295299996E-3, -4.9274001503E-4, 1.2960999738E-3, 1.4050999889E-3, 1.5874400269E-3, 8.8387000142E-4],\n            [1.5450799838E-3, 2.1750701126E-3, 2.2253401112E-3, -4.9274001503E-4, -6.3222697936E-3, -2.7556000277E-3, 5.3632198833E-3, 7.3032598011E-3,\n                5.3632198833E-3, -2.7556000277E-3, -6.3222697936E-3, -4.9274001503E-4, 2.2253401112E-3, 2.1750701126E-3, 1.5450799838E-3],\n            [1.9235999789E-3, 2.5626500137E-3, 2.1145299543E-3, -3.1295299996E-3, -2.7556000277E-3, 1.3962360099E-2, 7.8046298586E-3, -\n                9.3812197447E-3, 7.8046298586E-3, 1.3962360099E-2, -2.7556000277E-3, -3.1295299996E-3, 2.1145299543E-3, 2.5626500137E-3, 1.9235999789E-3],\n            [2.0687500946E-3, 2.2892199922E-3, 3.3578000148E-4, -4.5751798898E-3, 5.3632198833E-3, 7.8046298586E-3, -7.9501636326E-2, -\n                0.1554141641, -7.9501636326E-2, 7.8046298586E-3, 5.3632198833E-3, -4.5751798898E-3, 3.3578000148E-4, 2.2892199922E-3, 2.0687500946E-3],\n            [2.0898699295E-3, 1.9755100366E-3, -8.3368999185E-4, -5.1014497876E-3, 7.3032598011E-3, -9.3812197447E-3, -0.1554141641,\n                0.7303866148, -0.1554141641, -9.3812197447E-3, 7.3032598011E-3, -5.1014497876E-3, -8.3368999185E-4, 1.9755100366E-3, 2.0898699295E-3],\n            [2.0687500946E-3, 2.2892199922E-3, 3.3578000148E-4, -4.5751798898E-3, 5.3632198833E-3, 7.8046298586E-3, -7.9501636326E-2, -\n                0.1554141641, -7.9501636326E-2, 7.8046298586E-3, 5.3632198833E-3, -4.5751798898E-3, 3.3578000148E-4, 2.2892199922E-3, 2.0687500946E-3],\n            [1.9235999789E-3, 2.5626500137E-3, 2.1145299543E-3, -3.1295299996E-3, -2.7556000277E-3, 1.3962360099E-2, 7.8046298586E-3, -\n                9.3812197447E-3, 7.8046298586E-3, 1.3962360099E-2, -2.7556000277E-3, -3.1295299996E-3, 2.1145299543E-3, 2.5626500137E-3, 1.9235999789E-3],\n            [1.5450799838E-3, 2.1750701126E-3, 2.2253401112E-3, -4.9274001503E-4, -6.3222697936E-3, -2.7556000277E-3, 5.3632198833E-3, 7.3032598011E-3,\n                5.3632198833E-3, -2.7556000277E-3, -6.3222697936E-3, -4.9274001503E-4, 2.2253401112E-3, 2.1750701126E-3, 1.5450799838E-3],\n            [8.8387000142E-4, 1.5874400269E-3, 1.4050999889E-3, 1.2960999738E-3, -4.9274001503E-4, -3.1295299996E-3, -4.5751798898E-3, -\n                5.1014497876E-3, -4.5751798898E-3, -3.1295299996E-3, -4.9274001503E-4, 1.2960999738E-3, 1.4050999889E-3, 1.5874400269E-3, 8.8387000142E-4],\n            [-3.7829999201E-5, 7.7435001731E-4, 1.1793200392E-3, 1.4050999889E-3, 2.2253401112E-3, 2.1145299543E-3, 3.3578000148E-4, -\n                8.3368999185E-4, 3.3578000148E-4, 2.1145299543E-3, 2.2253401112E-3, 1.4050999889E-3, 1.1793200392E-3, 7.7435001731E-4, -3.7829999201E-5],\n            [-6.2596000498E-4, -3.2734998967E-4, 7.7435001731E-4, 1.5874400269E-3, 2.1750701126E-3, 2.5626500137E-3, 2.2892199922E-3, 1.9755100366E-3,\n                2.2892199922E-3, 2.5626500137E-3, 2.1750701126E-3, 1.5874400269E-3, 7.7435001731E-4, -3.2734998967E-4, -6.2596000498E-4],\n            [-4.0483998600E-4, -6.2596000498E-4, -3.7829999201E-5, 8.8387000142E-4, 1.5450799838E-3, 1.9235999789E-3, 2.0687500946E-3, 2.0898699295E-3, 2.0687500946E-3, 1.9235999789E-3, 1.5450799838E-3, 8.8387000142E-4, -3.7829999201E-5, -6.2596000498E-4, -4.0483998600E-4]]\n        ).reshape(1, 1, 15, 15)\n        filters[\"b\"] = []\n        filters[\"b\"].append(torch.tensor(\n            [-8.1125000725E-4, 4.4451598078E-3, 1.2316980399E-2, 1.3955879956E-2,  1.4179450460E-2, 1.3955879956E-2, 1.2316980399E-2, 4.4451598078E-3, -8.1125000725E-4,\n             3.9103501476E-3, 4.4565401040E-3, -5.8724298142E-3, -2.8760801069E-3, 8.5267601535E-3, -\n             2.8760801069E-3, -5.8724298142E-3, 4.4565401040E-3, 3.9103501476E-3,\n             1.3462699717E-3, -3.7740699481E-3, 8.2581602037E-3, 3.9442278445E-2, 5.3605638444E-2, 3.9442278445E-2, 8.2581602037E-3, -\n             3.7740699481E-3, 1.3462699717E-3,\n             7.4700999539E-4, -3.6522001028E-4, -2.2522680461E-2, -0.1105690673, -\n             0.1768419296, -0.1105690673, -2.2522680461E-2, -3.6522001028E-4, 7.4700999539E-4,\n             0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000,\n             -7.4700999539E-4, 3.6522001028E-4, 2.2522680461E-2, 0.1105690673, 0.1768419296, 0.1105690673, 2.2522680461E-2, 3.6522001028E-4, -7.4700999539E-4,\n             -1.3462699717E-3, 3.7740699481E-3, -8.2581602037E-3, -3.9442278445E-2, -\n             5.3605638444E-2, -3.9442278445E-2, -\n             8.2581602037E-3, 3.7740699481E-3, -1.3462699717E-3,\n             -3.9103501476E-3, -4.4565401040E-3, 5.8724298142E-3, 2.8760801069E-3, -\n             8.5267601535E-3, 2.8760801069E-3, 5.8724298142E-3, -\n             4.4565401040E-3, -3.9103501476E-3,\n             8.1125000725E-4, -4.4451598078E-3, -1.2316980399E-2, -1.3955879956E-2, -1.4179450460E-2, -1.3955879956E-2, -1.2316980399E-2, -4.4451598078E-3, 8.1125000725E-4]\n        ).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor(\n            [0.0000000000, -8.2846998703E-4, -5.7109999034E-5, 4.0110000555E-5, 4.6670897864E-3, 8.0871898681E-3, 1.4807609841E-2, 8.6204400286E-3, -3.1221499667E-3,\n             8.2846998703E-4, 0.0000000000, -9.7479997203E-4, -6.9718998857E-3, -\n             2.0865600090E-3, 2.3298799060E-3, -\n             4.4814897701E-3, 1.4917500317E-2, 8.6204400286E-3,\n             5.7109999034E-5, 9.7479997203E-4, 0.0000000000, -1.2145539746E-2, -\n             2.4427289143E-2, 5.0797060132E-2, 3.2785870135E-2, -\n             4.4814897701E-3, 1.4807609841E-2,\n             -4.0110000555E-5, 6.9718998857E-3, 1.2145539746E-2, 0.0000000000, -\n             0.1510555595, -8.2495503128E-2, 5.0797060132E-2, 2.3298799060E-3, 8.0871898681E-3,\n             -4.6670897864E-3, 2.0865600090E-3, 2.4427289143E-2, 0.1510555595, 0.0000000000, -\n             0.1510555595, -2.4427289143E-2, -2.0865600090E-3, 4.6670897864E-3,\n             -8.0871898681E-3, -2.3298799060E-3, -5.0797060132E-2, 8.2495503128E-2, 0.1510555595, 0.0000000000, -\n             1.2145539746E-2, -6.9718998857E-3, 4.0110000555E-5,\n             -1.4807609841E-2, 4.4814897701E-3, -3.2785870135E-2, -\n             5.0797060132E-2, 2.4427289143E-2, 1.2145539746E-2, 0.0000000000, -\n             9.7479997203E-4, -5.7109999034E-5,\n             -8.6204400286E-3, -1.4917500317E-2, 4.4814897701E-3, -\n             2.3298799060E-3, 2.0865600090E-3, 6.9718998857E-3, 9.7479997203E-4, 0.0000000000, -8.2846998703E-4,\n             3.1221499667E-3, -8.6204400286E-3, -1.4807609841E-2, -8.0871898681E-3, -4.6670897864E-3, -4.0110000555E-5, 5.7109999034E-5, 8.2846998703E-4, 0.0000000000]\n        ).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor(\n            [8.1125000725E-4, -3.9103501476E-3, -1.3462699717E-3, -7.4700999539E-4, 0.0000000000, 7.4700999539E-4, 1.3462699717E-3, 3.9103501476E-3, -8.1125000725E-4,\n             -4.4451598078E-3, -4.4565401040E-3, 3.7740699481E-3, 3.6522001028E-4, 0.0000000000, -\n             3.6522001028E-4, -3.7740699481E-3, 4.4565401040E-3, 4.4451598078E-3,\n             -1.2316980399E-2, 5.8724298142E-3, -8.2581602037E-3, 2.2522680461E-2, 0.0000000000, -\n             2.2522680461E-2, 8.2581602037E-3, -5.8724298142E-3, 1.2316980399E-2,\n             -1.3955879956E-2, 2.8760801069E-3, -3.9442278445E-2, 0.1105690673, 0.0000000000, -\n             0.1105690673, 3.9442278445E-2, -2.8760801069E-3, 1.3955879956E-2,\n             -1.4179450460E-2, -8.5267601535E-3, -5.3605638444E-2, 0.1768419296, 0.0000000000, -\n             0.1768419296, 5.3605638444E-2, 8.5267601535E-3, 1.4179450460E-2,\n             -1.3955879956E-2, 2.8760801069E-3, -3.9442278445E-2, 0.1105690673, 0.0000000000, -\n             0.1105690673, 3.9442278445E-2, -2.8760801069E-3, 1.3955879956E-2,\n             -1.2316980399E-2, 5.8724298142E-3, -8.2581602037E-3, 2.2522680461E-2, 0.0000000000, -\n             2.2522680461E-2, 8.2581602037E-3, -5.8724298142E-3, 1.2316980399E-2,\n             -4.4451598078E-3, -4.4565401040E-3, 3.7740699481E-3, 3.6522001028E-4, 0.0000000000, -\n             3.6522001028E-4, -3.7740699481E-3, 4.4565401040E-3, 4.4451598078E-3,\n             8.1125000725E-4, -3.9103501476E-3, -1.3462699717E-3, -7.4700999539E-4, 0.0000000000, 7.4700999539E-4, 1.3462699717E-3, 3.9103501476E-3, -8.1125000725E-4]\n        ).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor(\n            [3.1221499667E-3, -8.6204400286E-3, -1.4807609841E-2, -8.0871898681E-3, -4.6670897864E-3, -4.0110000555E-5, 5.7109999034E-5, 8.2846998703E-4, 0.0000000000,\n             -8.6204400286E-3, -1.4917500317E-2, 4.4814897701E-3, -\n             2.3298799060E-3, 2.0865600090E-3, 6.9718998857E-3, 9.7479997203E-4, -\n             0.0000000000, -8.2846998703E-4,\n             -1.4807609841E-2, 4.4814897701E-3, -3.2785870135E-2, -\n             5.0797060132E-2, 2.4427289143E-2, 1.2145539746E-2, 0.0000000000, -\n             9.7479997203E-4, -5.7109999034E-5,\n             -8.0871898681E-3, -2.3298799060E-3, -5.0797060132E-2, 8.2495503128E-2, 0.1510555595, -\n             0.0000000000, -1.2145539746E-2, -6.9718998857E-3, 4.0110000555E-5,\n             -4.6670897864E-3, 2.0865600090E-3, 2.4427289143E-2, 0.1510555595, 0.0000000000, -\n             0.1510555595, -2.4427289143E-2, -2.0865600090E-3, 4.6670897864E-3,\n             -4.0110000555E-5, 6.9718998857E-3, 1.2145539746E-2, 0.0000000000, -\n             0.1510555595, -8.2495503128E-2, 5.0797060132E-2, 2.3298799060E-3, 8.0871898681E-3,\n             5.7109999034E-5, 9.7479997203E-4, -0.0000000000, -1.2145539746E-2, -\n             2.4427289143E-2, 5.0797060132E-2, 3.2785870135E-2, -\n             4.4814897701E-3, 1.4807609841E-2,\n             8.2846998703E-4, -0.0000000000, -9.7479997203E-4, -6.9718998857E-3, -\n             2.0865600090E-3, 2.3298799060E-3, -\n             4.4814897701E-3, 1.4917500317E-2, 8.6204400286E-3,\n             0.0000000000, -8.2846998703E-4, -5.7109999034E-5, 4.0110000555E-5, 4.6670897864E-3, 8.0871898681E-3, 1.4807609841E-2, 8.6204400286E-3, -3.1221499667E-3]\n        ).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n\n    elif n_orientations == 6:\n        filters[\"l\"] = 2 * torch.tensor([\n            [0.00085404, -0.00244917, -0.00387812, -0.00944432, -\n                0.00962054, -0.00944432, -0.00387812, -0.00244917, 0.00085404],\n            [-0.00244917, -0.00523281, -0.00661117, 0.00410600, 0.01002988,\n                0.00410600, -0.00661117, -0.00523281, -0.00244917],\n            [-0.00387812, -0.00661117, 0.01396746, 0.03277038, 0.03981393,\n                0.03277038, 0.01396746, -0.00661117, -0.00387812],\n            [-0.00944432, 0.00410600, 0.03277038, 0.06426333, 0.08169618,\n                0.06426333, 0.03277038, 0.00410600, -0.00944432],\n            [-0.00962054, 0.01002988, 0.03981393, 0.08169618, 0.10096540,\n                0.08169618, 0.03981393, 0.01002988, -0.00962054],\n            [-0.00944432, 0.00410600, 0.03277038, 0.06426333, 0.08169618,\n                0.06426333, 0.03277038, 0.00410600, -0.00944432],\n            [-0.00387812, -0.00661117, 0.01396746, 0.03277038, 0.03981393,\n                0.03277038, 0.01396746, -0.00661117, -0.00387812],\n            [-0.00244917, -0.00523281, -0.00661117, 0.00410600, 0.01002988,\n                0.00410600, -0.00661117, -0.00523281, -0.00244917],\n            [0.00085404, -0.00244917, -0.00387812, -0.00944432, -0.00962054, -0.00944432, -0.00387812, -0.00244917, 0.00085404]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"l0\"] = torch.tensor([\n            [0.00341614, -0.01551246, -0.03848215, -0.01551246, 0.00341614],\n            [-0.01551246, 0.05586982, 0.15925570, 0.05586982, -0.01551246],\n            [-0.03848215, 0.15925570, 0.40304148, 0.15925570, -0.03848215],\n            [-0.01551246, 0.05586982, 0.15925570, 0.05586982, -0.01551246],\n            [0.00341614, -0.01551246, -0.03848215, -0.01551246, 0.00341614]]\n        ).reshape(1, 1, 5, 5)\n        filters[\"h0\"] = torch.tensor([\n            [-0.00033429, -0.00113093, -0.00171484, -0.00133542, -\n                0.00080639, -0.00133542, -0.00171484, -0.00113093, -0.00033429],\n            [-0.00113093, -0.00350017, -0.00243812, 0.00631653, 0.01261227,\n                0.00631653, -0.00243812, -0.00350017, -0.00113093],\n            [-0.00171484, -0.00243812, -0.00290081, -0.00673482, -\n                0.00981051, -0.00673482, -0.00290081, -0.00243812, -0.00171484],\n            [-0.00133542, 0.00631653, -0.00673482, -0.07027679, -\n                0.11435863, -0.07027679, -0.00673482, 0.00631653, -0.00133542],\n            [-0.00080639, 0.01261227, -0.00981051, -0.11435863,\n                0.81380200, -0.11435863, -0.00981051, 0.01261227, -0.00080639],\n            [-0.00133542, 0.00631653, -0.00673482, -0.07027679, -\n                0.11435863, -0.07027679, -0.00673482, 0.00631653, -0.00133542],\n            [-0.00171484, -0.00243812, -0.00290081, -0.00673482, -\n                0.00981051, -0.00673482, -0.00290081, -0.00243812, -0.00171484],\n            [-0.00113093, -0.00350017, -0.00243812, 0.00631653, 0.01261227,\n                0.00631653, -0.00243812, -0.00350017, -0.00113093],\n            [-0.00033429, -0.00113093, -0.00171484, -0.00133542, -0.00080639, -0.00133542, -0.00171484, -0.00113093, -0.00033429]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"b\"] = []\n        filters[\"b\"].append(torch.tensor([\n            0.00277643, 0.00496194, 0.01026699, 0.01455399, 0.01026699, 0.00496194, 0.00277643,\n            -0.00986904, -0.00893064, 0.01189859, 0.02755155, 0.01189859, -0.00893064, -0.00986904,\n            -0.01021852, -0.03075356, -0.08226445, -\n            0.11732297, -0.08226445, -0.03075356, -0.01021852,\n            0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000,\n            0.01021852, 0.03075356, 0.08226445, 0.11732297, 0.08226445, 0.03075356, 0.01021852,\n            0.00986904, 0.00893064, -0.01189859, -\n            0.02755155, -0.01189859, 0.00893064, 0.00986904,\n            -0.00277643, -0.00496194, -0.01026699, -0.01455399, -0.01026699, -0.00496194, -0.00277643]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor([\n            -0.00343249, -0.00640815, -0.00073141, 0.01124321, 0.00182078, 0.00285723, 0.01166982,\n            -0.00358461, -0.01977507, -0.04084211, -\n            0.00228219, 0.03930573, 0.01161195, 0.00128000,\n            0.01047717, 0.01486305, -0.04819057, -\n            0.12227230, -0.05394139, 0.00853965, -0.00459034,\n            0.00790407, 0.04435647, 0.09454202, -0.00000000, -\n            0.09454202, -0.04435647, -0.00790407,\n            0.00459034, -0.00853965, 0.05394139, 0.12227230, 0.04819057, -0.01486305, -0.01047717,\n            -0.00128000, -0.01161195, -0.03930573, 0.00228219, 0.04084211, 0.01977507, 0.00358461,\n            -0.01166982, -0.00285723, -0.00182078, -0.01124321, 0.00073141, 0.00640815, 0.00343249]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor([\n            0.00343249, 0.00358461, -0.01047717, -\n            0.00790407, -0.00459034, 0.00128000, 0.01166982,\n            0.00640815, 0.01977507, -0.01486305, -\n            0.04435647, 0.00853965, 0.01161195, 0.00285723,\n            0.00073141, 0.04084211, 0.04819057, -\n            0.09454202, -0.05394139, 0.03930573, 0.00182078,\n            -0.01124321, 0.00228219, 0.12227230, -\n            0.00000000, -0.12227230, -0.00228219, 0.01124321,\n            -0.00182078, -0.03930573, 0.05394139, 0.09454202, -\n            0.04819057, -0.04084211, -0.00073141,\n            -0.00285723, -0.01161195, -0.00853965, 0.04435647, 0.01486305, -0.01977507, -0.00640815,\n            -0.01166982, -0.00128000, 0.00459034, 0.00790407, 0.01047717, -0.00358461, -0.00343249]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor(\n            [-0.00277643, 0.00986904, 0.01021852, -0.00000000, -0.01021852, -0.00986904, 0.00277643,\n             -0.00496194, 0.00893064, 0.03075356, -\n             0.00000000, -0.03075356, -0.00893064, 0.00496194,\n             -0.01026699, -0.01189859, 0.08226445, -\n             0.00000000, -0.08226445, 0.01189859, 0.01026699,\n             -0.01455399, -0.02755155, 0.11732297, -\n             0.00000000, -0.11732297, 0.02755155, 0.01455399,\n             -0.01026699, -0.01189859, 0.08226445, -\n             0.00000000, -0.08226445, 0.01189859, 0.01026699,\n             -0.00496194, 0.00893064, 0.03075356, -\n             0.00000000, -0.03075356, -0.00893064, 0.00496194,\n             -0.00277643, 0.00986904, 0.01021852, -0.00000000, -0.01021852, -0.00986904, 0.00277643]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor([\n            -0.01166982, -0.00128000, 0.00459034, 0.00790407, 0.01047717, -0.00358461, -0.00343249,\n            -0.00285723, -0.01161195, -0.00853965, 0.04435647, 0.01486305, -0.01977507, -0.00640815,\n            -0.00182078, -0.03930573, 0.05394139, 0.09454202, -\n            0.04819057, -0.04084211, -0.00073141,\n            -0.01124321, 0.00228219, 0.12227230, -\n            0.00000000, -0.12227230, -0.00228219, 0.01124321,\n            0.00073141, 0.04084211, 0.04819057, -\n            0.09454202, -0.05394139, 0.03930573, 0.00182078,\n            0.00640815, 0.01977507, -0.01486305, -\n            0.04435647, 0.00853965, 0.01161195, 0.00285723,\n            0.00343249, 0.00358461, -0.01047717, -0.00790407, -0.00459034, 0.00128000, 0.01166982]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor([\n            -0.01166982, -0.00285723, -0.00182078, -\n            0.01124321, 0.00073141, 0.00640815, 0.00343249,\n            -0.00128000, -0.01161195, -0.03930573, 0.00228219, 0.04084211, 0.01977507, 0.00358461,\n            0.00459034, -0.00853965, 0.05394139, 0.12227230, 0.04819057, -0.01486305, -0.01047717,\n            0.00790407, 0.04435647, 0.09454202, -0.00000000, -\n            0.09454202, -0.04435647, -0.00790407,\n            0.01047717, 0.01486305, -0.04819057, -\n            0.12227230, -0.05394139, 0.00853965, -0.00459034,\n            -0.00358461, -0.01977507, -0.04084211, -\n            0.00228219, 0.03930573, 0.01161195, 0.00128000,\n            -0.00343249, -0.00640815, -0.00073141, 0.01124321, 0.00182078, 0.00285723, 0.01166982]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n\n    else:\n        raise Exception(\n            \"Steerable filters not implemented for %d orientations\" % n_orientations)\n\n    if filter_type == \"trained\":\n        if size == 5:\n            # TODO maybe also train h0 and l0 filters\n            filters = crop_steerable_pyramid_filters(filters, 5)\n            filters[\"b\"][0] = torch.tensor([\n                [-0.0356752239, -0.0223877281, -0.0009542659,\n                    0.0244821459, 0.0322226137],\n                [-0.0593218654,  0.1245803162, -\n                    0.0023863907, -0.1230178699, 0.0589442067],\n                [-0.0281576272,  0.2976626456, -\n                    0.0020888755, -0.2953369915, 0.0284542721],\n                [-0.0586092323,  0.1251581162, -\n                    0.0024624448, -0.1227868199, 0.0587830991],\n                [-0.0327464789, -0.0223652460, -\n                    0.0042342511,  0.0245472137, 0.0359398536]\n            ]).reshape(1, 1, 5, 5)\n            filters[\"b\"][1] = torch.tensor([\n                [3.9758663625e-02,  6.0679119080e-02,  3.0146904290e-02,\n                    6.1198268086e-02,  3.6218870431e-02],\n                [2.3255519569e-02, -1.2505133450e-01, -\n                    2.9738345742e-01, -1.2518258393e-01,  2.3592948914e-02],\n                [-1.3602430699e-03, -1.2058277935e-04,  2.6399988565e-04, -\n                    2.3791544663e-04,  1.8450465286e-03],\n                [-2.1563466638e-02,  1.2572696805e-01,  2.9745018482e-01,\n                    1.2458638102e-01, -2.3847281933e-02],\n                [-3.7941932678e-02, -6.1060950160e-02, -\n                    2.9489086941e-02, -6.0411967337e-02, -3.8459088653e-02]\n            ]).reshape(1, 1, 5, 5)\n\n            # Below filters were optimised on 09/02/2021\n            # 20K iterations with multiple images at more scales.\n            filters[\"b\"][0] = torch.tensor([\n                [-4.5508436859e-02, -2.1767273545e-02, -1.9399923622e-04,\n                    2.1200872958e-02,  4.5475799590e-02],\n                [-6.3554823399e-02,  1.2832683325e-01, -\n                    5.3858719184e-05, -1.2809979916e-01,  6.3842624426e-02],\n                [-3.4809380770e-02,  2.9954621196e-01,  2.9066693969e-05, -\n                    2.9957753420e-01,  3.4806568176e-02],\n                [-6.3934154809e-02,  1.2806062400e-01,  9.0917674243e-05, -\n                    1.2832444906e-01,  6.3572973013e-02],\n                [-4.5492250472e-02, -2.1125273779e-02,  4.2229349492e-04,\n                    2.1804777905e-02,  4.5236673206e-02]\n            ]).reshape(1, 1, 5, 5)\n            filters[\"b\"][1] = torch.tensor([\n                [4.8947390169e-02,  6.3575074077e-02,  3.4955859184e-02,\n                    6.4085893333e-02,  4.9838040024e-02],\n                [2.2061849013e-02, -1.2936264277e-01, -\n                    3.0093491077e-01, -1.2997294962e-01,  2.0597217605e-02],\n                [-5.1290717238e-05, -1.7305796064e-05,  2.0256420612e-05, -\n                    1.1864109547e-04,  7.3973249528e-05],\n                [-2.0749464631e-02,  1.2988376617e-01,  3.0080935359e-01,\n                    1.2921217084e-01, -2.2159902379e-02],\n                [-4.9614857882e-02, -6.4021714032e-02, -\n                    3.4676689655e-02, -6.3446544111e-02, -4.8282280564e-02]\n            ]).reshape(1, 1, 5, 5)\n\n            # Trained on 17/02/2021 to match fourier pyramid in spatial domain\n            filters[\"b\"][0] = torch.tensor([\n                [3.3370e-02,  9.3934e-02, -3.5810e-04, -9.4038e-02, -3.3115e-02],\n                [1.7716e-01,  3.9378e-01,  6.8461e-05, -3.9343e-01, -1.7685e-01],\n                [2.9213e-01,  6.1042e-01,  7.0654e-04, -6.0939e-01, -2.9177e-01],\n                [1.7684e-01,  3.9392e-01,  1.0517e-03, -3.9268e-01, -1.7668e-01],\n                [3.3000e-02,  9.4029e-02,  7.3565e-04, -9.3366e-02, -3.3008e-02]\n            ]).reshape(1, 1, 5, 5) * 0.1\n\n            filters[\"b\"][1] = torch.tensor([\n                [0.0331,  0.1763,  0.2907,  0.1753,  0.0325],\n                [0.0941,  0.3932,  0.6079,  0.3904,  0.0922],\n                [0.0008,  0.0009, -0.0010, -0.0025, -0.0015],\n                [-0.0929, -0.3919, -0.6097, -0.3944, -0.0946],\n                [-0.0328, -0.1760, -0.2915, -0.1768, -0.0333]\n            ]).reshape(1, 1, 5, 5) * 0.1\n\n        else:\n            raise Exception(\n                \"Trained filters not implemented for size %d\" % size)\n\n    if filter_type == \"cropped\":\n        filters = crop_steerable_pyramid_filters(filters, size)\n\n    return filters\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.hsv_to_rgb","title":"<code>hsv_to_rgb(image)</code>","text":"<p>Definition to convert HSV space to  RGB color space. Mostly inspired from : https://kornia.readthedocs.io/en/latest/_modules/kornia/color/hsv.html</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in HSV color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_rgb</code> (              <code>tensor</code> )          \u2013            <p>Output image in  RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def hsv_to_rgb(image):\n\n    \"\"\"\n    Definition to convert HSV space to  RGB color space. Mostly inspired from : https://kornia.readthedocs.io/en/latest/_modules/kornia/color/hsv.html\n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in HSV color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n\n    Returns\n    -------\n    image_rgb       : torch.tensor\n                      Output image in  RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    h = image[..., 0, :, :] / (2 * math.pi)\n    s = image[..., 1, :, :]\n    v = image[..., 2, :, :]\n    hi = torch.floor(h * 6) % 6\n    f = ((h * 6) % 6) - hi\n    one = torch.tensor(1.0)\n    p = v * (one - s)\n    q = v * (one - f * s)\n    t = v * (one - (one - f) * s)\n    hi = hi.long()\n    indices = torch.stack([hi, hi + 6, hi + 12], dim=-3)\n    image_rgb = torch.stack((v, q, p, p, t, v, t, v, v, q, p, p, p, p, t, v, v, q), dim=-3)\n    image_rgb = torch.gather(image_rgb, -3, indices)\n    return image_rgb\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.lab_to_srgb","title":"<code>lab_to_srgb(image)</code>","text":"<p>Definition to convert LAB space to SRGB color space. </p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in LAB color space[3 x m x n]\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_srgb</code> (              <code>tensor</code> )          \u2013            <p>Output image in SRGB color space [3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def lab_to_srgb(image):\n    \"\"\"\n    Definition to convert LAB space to SRGB color space. \n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in LAB color space[3 x m x n]\n    Returns\n    -------\n    image_srgb     : torch.tensor\n                      Output image in SRGB color space [3 x m x n].\n    \"\"\"\n\n    if image.shape[-1] == 3:\n        input_color = image.permute(2, 0, 1)  # C(H*W)\n    else:\n        input_color = image\n    # lab ---&gt; xyz\n    reference_illuminant = torch.tensor([[[0.950428545]], [[1.000000000]], [[1.088900371]]], dtype=torch.float32)\n    y = (input_color[0:1, :, :] + 16) / 116\n    a =  input_color[1:2, :, :] / 500\n    b =  input_color[2:3, :, :] / 200\n    x = y + a\n    z = y - b\n    xyz = torch.cat((x, y, z), 0)\n    delta = 6 / 29\n    factor = 3 * delta * delta\n    xyz = torch.where(xyz &gt; delta,  xyz ** 3, factor * (xyz - 4 / 29))\n    xyz_color = xyz * reference_illuminant\n    # xyz ---&gt; linear rgb\n    a11 = 3.241003275\n    a12 = -1.537398934\n    a13 = -0.498615861\n    a21 = -0.969224334\n    a22 = 1.875930071\n    a23 = 0.041554224\n    a31 = 0.055639423\n    a32 = -0.204011202\n    a33 = 1.057148933\n    A = torch.tensor([[a11, a12, a13],\n                  [a21, a22, a23],\n                  [a31, a32, a33]], dtype=torch.float32)\n\n    xyz_color = xyz_color.permute(2, 0, 1) # C(H*W)\n    linear_rgb_color = torch.matmul(A, xyz_color)\n    linear_rgb_color = linear_rgb_color.permute(1, 2, 0)\n    # linear rgb ---&gt; srgb\n    limit = 0.0031308\n    image_srgb = torch.where(linear_rgb_color &gt; limit, 1.055 * (linear_rgb_color ** (1.0 / 2.4)) - 0.055, 12.92 * linear_rgb_color)\n    return image_srgb\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.linear_rgb_to_rgb","title":"<code>linear_rgb_to_rgb(image, threshold=0.0031308)</code>","text":"<p>Definition to convert linear RGB images to RGB color space. Mostly inspired from: https://kornia.readthedocs.io/en/latest/_modules/kornia/color/rgb.html</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in linear RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> <li> <code>threshold</code>           \u2013            <pre><code>          Threshold used in calculations.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_linear</code> (              <code>tensor</code> )          \u2013            <p>Output image in RGB color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def linear_rgb_to_rgb(image, threshold = 0.0031308):\n    \"\"\"\n    Definition to convert linear RGB images to RGB color space. Mostly inspired from: https://kornia.readthedocs.io/en/latest/_modules/kornia/color/rgb.html\n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in linear RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n    threshold       : float\n                      Threshold used in calculations.\n\n    Returns\n    -------\n    image_linear    : torch.tensor\n                      Output image in RGB color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    image_linear =  torch.where(image &gt; threshold, 1.055 * torch.pow(image.clamp(min=threshold), 1 / 2.4) - 0.055, 12.92 * image)\n    return image_linear\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.linear_rgb_to_xyz","title":"<code>linear_rgb_to_xyz(image)</code>","text":"<p>Definition to convert RGB space to CIE XYZ color space. Mostly inspired from : Rochester IT Color Conversion Algorithms (https://www.cs.rit.edu/~ncs/color/)</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in linear RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_xyz</code> (              <code>tensor</code> )          \u2013            <p>Output image in XYZ (CIE 1931) color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def linear_rgb_to_xyz(image):\n    \"\"\"\n    Definition to convert RGB space to CIE XYZ color space. Mostly inspired from : Rochester IT Color Conversion Algorithms (https://www.cs.rit.edu/~ncs/color/)\n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in linear RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n\n    Returns\n    -------\n    image_xyz       : torch.tensor\n                      Output image in XYZ (CIE 1931) color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    a11 = 0.412453\n    a12 = 0.357580\n    a13 = 0.180423\n    a21 = 0.212671\n    a22 = 0.715160\n    a23 = 0.072169\n    a31 = 0.019334\n    a32 = 0.119193\n    a33 = 0.950227\n    M = torch.tensor([[a11, a12, a13], \n                      [a21, a22, a23],\n                      [a31, a32, a33]])\n    size = image.size()\n    image = image.reshape(size[0], size[1], size[2]*size[3])  # NC(HW)\n    image_xyz = torch.matmul(M, image)\n    image_xyz = image_xyz.reshape(size[0], size[1], size[2], size[3])\n    return image_xyz\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.make_3d_location_map","title":"<code>make_3d_location_map(image_pixel_size, real_image_width=0.3, real_viewing_distance=0.6)</code>","text":"<p>Makes a map of the real 3D location that each pixel in an image corresponds to, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is  perpendicular to the viewing direction.</p> <p>Parameters:</p> <ul> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                    The size of the image in pixels, as a tuple of form (height, width)\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed. Units not important, as long as they\n                    are the same as those used for real_viewing_distance\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance from the user's viewpoint to the screen.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>map</code> (              <code>tensor</code> )          \u2013            <p>The computed 3D location map, of size 3xWxH.</p> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_3d_location_map(image_pixel_size, real_image_width=0.3, real_viewing_distance=0.6):\n    \"\"\" \n    Makes a map of the real 3D location that each pixel in an image corresponds to, when displayed to\n    a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is \n    perpendicular to the viewing direction.\n\n    Parameters\n    ----------\n\n    image_pixel_size        : tuple of ints \n                                The size of the image in pixels, as a tuple of form (height, width)\n    real_image_width        : float\n                                The real width of the image as displayed. Units not important, as long as they\n                                are the same as those used for real_viewing_distance\n    real_viewing_distance   : float \n                                The real distance from the user's viewpoint to the screen.\n\n    Returns\n    -------\n\n    map                     : torch.tensor\n                                The computed 3D location map, of size 3xWxH.\n    \"\"\"\n    real_image_height = (real_image_width /\n                         image_pixel_size[-1]) * image_pixel_size[-2]\n    x_coords = torch.linspace(-0.5, 0.5, image_pixel_size[-1])*real_image_width\n    x_coords = x_coords[None, None, :].repeat(1, image_pixel_size[-2], 1)\n    y_coords = torch.linspace(-0.5, 0.5,\n                              image_pixel_size[-2])*real_image_height\n    y_coords = y_coords[None, :, None].repeat(1, 1, image_pixel_size[-1])\n    z_coords = torch.ones(\n        (1, image_pixel_size[-2], image_pixel_size[-1])) * real_viewing_distance\n\n    return torch.cat([x_coords, y_coords, z_coords], dim=0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.make_eccentricity_distance_maps","title":"<code>make_eccentricity_distance_maps(gaze_location, image_pixel_size, real_image_width=0.3, real_viewing_distance=0.6)</code>","text":"<p>Makes a map of the eccentricity of each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is  perpendicular to the viewing direction. Output in radians.</p> <p>Parameters:</p> <ul> <li> <code>gaze_location</code>           \u2013            <pre><code>                    User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                    image coordinates (ranging from 0 to 1)\n</code></pre> </li> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                    The size of the image in pixels, as a tuple of form (height, width)\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed. Units not important, as long as they\n                    are the same as those used for real_viewing_distance\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance from the user's viewpoint to the screen.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>eccentricity_map</code> (              <code>tensor</code> )          \u2013            <p>The computed eccentricity map, of size WxH.</p> </li> <li> <code>distance_map</code> (              <code>tensor</code> )          \u2013            <p>The computed distance map, of size WxH.</p> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_eccentricity_distance_maps(gaze_location, image_pixel_size, real_image_width=0.3, real_viewing_distance=0.6):\n    \"\"\" \n    Makes a map of the eccentricity of each pixel in an image for a given fixation point, when displayed to\n    a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is \n    perpendicular to the viewing direction. Output in radians.\n\n    Parameters\n    ----------\n\n    gaze_location           : tuple of floats\n                                User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                                image coordinates (ranging from 0 to 1)\n    image_pixel_size        : tuple of ints\n                                The size of the image in pixels, as a tuple of form (height, width)\n    real_image_width        : float\n                                The real width of the image as displayed. Units not important, as long as they\n                                are the same as those used for real_viewing_distance\n    real_viewing_distance   : float\n                                The real distance from the user's viewpoint to the screen.\n\n    Returns\n    -------\n\n    eccentricity_map        : torch.tensor\n                                The computed eccentricity map, of size WxH.\n    distance_map            : torch.tensor\n                                The computed distance map, of size WxH.\n    \"\"\"\n    real_image_height = (real_image_width /\n                         image_pixel_size[-1]) * image_pixel_size[-2]\n    location_map = make_3d_location_map(\n        image_pixel_size, real_image_width, real_viewing_distance)\n    distance_map = torch.sqrt(torch.sum(location_map*location_map, dim=0))\n    direction_map = location_map / distance_map\n\n    gaze_location_3d = torch.tensor([\n        (gaze_location[0]*2 - 1)*real_image_width*0.5,\n        (gaze_location[1]*2 - 1)*real_image_height*0.5,\n        real_viewing_distance])\n    gaze_dir = gaze_location_3d / \\\n        torch.sqrt(torch.sum(gaze_location_3d * gaze_location_3d))\n    gaze_dir = gaze_dir[:, None, None]\n\n    dot_prod_map = torch.sum(gaze_dir * direction_map, dim=0)\n    dot_prod_map = torch.clamp(dot_prod_map, min=-1.0, max=1.0)\n    eccentricity_map = torch.acos(dot_prod_map)\n\n    return eccentricity_map, distance_map\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.make_equi_pooling_size_map_lod","title":"<code>make_equi_pooling_size_map_lod(gaze_angles, image_pixel_size, alpha=0.3, mode='quadratic')</code>","text":"<p>This function is similar to make_equi_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from to achieve the correct pooling region areas.</p> <p>Parameters:</p> <ul> <li> <code>gaze_angles</code>           \u2013            <pre><code>                Gaze direction expressed as angles, in radians.\n</code></pre> </li> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                Dimensions of the image in pixels, as a tuple of (height, width)\n</code></pre> </li> <li> <code>alpha</code>           \u2013            <pre><code>                Parameter controlling extent of foveation\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                Foveation mode (how pooling size varies with eccentricity). Should be \"quadratic\" or \"linear\"\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>pooling_size_map</code> (              <code>tensor</code> )          \u2013            <p>The computed pooling size map, of size HxW.</p> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_equi_pooling_size_map_lod(gaze_angles, image_pixel_size, alpha=0.3, mode=\"quadratic\"):\n    \"\"\" \n    This function is similar to make_equi_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from\n    to achieve the correct pooling region areas.\n\n    Parameters\n    ----------\n\n    gaze_angles         : tuple of 2 floats\n                            Gaze direction expressed as angles, in radians.\n    image_pixel_size    : tuple of 2 ints\n                            Dimensions of the image in pixels, as a tuple of (height, width)\n    alpha               : float\n                            Parameter controlling extent of foveation\n    mode                : str\n                            Foveation mode (how pooling size varies with eccentricity). Should be \"quadratic\" or \"linear\"\n\n    Returns\n    -------\n\n    pooling_size_map        : torch.tensor\n                                The computed pooling size map, of size HxW.\n    \"\"\"\n    pooling_pixel = make_equi_pooling_size_map_pixels(gaze_angles, image_pixel_size, alpha, mode)\n    pooling_lod = torch.log2(1e-6 + pooling_pixel)\n    pooling_lod[pooling_lod &lt; 0] = 0\n    return pooling_lod\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.make_equi_pooling_size_map_pixels","title":"<code>make_equi_pooling_size_map_pixels(gaze_angles, image_pixel_size, alpha=0.3, mode='quadratic')</code>","text":"<p>This function makes a map of pooling sizes in pixels, similarly to make_pooling_size_map_pixels, but works on 360 equirectangular images. Input images are assumed to be in equirectangular form - i.e. if you consider a 3D viewing setup where y is the vertical axis,  the x location in the image corresponds to rotation around the y axis (yaw), ranging from -pi to pi. The y location in the image corresponds to pitch, ranging from -pi/2 to pi/2.</p> <p>In this setup real_image_width and real_viewing_distance have no effect.</p> <p>Note that rather than a 2D image gaze location in [0,1]^2, the gaze should be specified as gaze angles in [-pi,pi]x[-pi/2,pi/2] (yaw, then pitch).</p> <p>Parameters:</p> <ul> <li> <code>gaze_angles</code>           \u2013            <pre><code>                Gaze direction expressed as angles, in radians.\n</code></pre> </li> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                Dimensions of the image in pixels, as a tuple of (height, width)\n</code></pre> </li> <li> <code>alpha</code>           \u2013            <pre><code>                Parameter controlling extent of foveation\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                Foveation mode (how pooling size varies with eccentricity). Should be \"quadratic\" or \"linear\"\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_equi_pooling_size_map_pixels(gaze_angles, image_pixel_size, alpha=0.3, mode=\"quadratic\"):\n    \"\"\"\n    This function makes a map of pooling sizes in pixels, similarly to make_pooling_size_map_pixels, but works on 360 equirectangular images.\n    Input images are assumed to be in equirectangular form - i.e. if you consider a 3D viewing setup where y is the vertical axis, \n    the x location in the image corresponds to rotation around the y axis (yaw), ranging from -pi to pi. The y location in the image\n    corresponds to pitch, ranging from -pi/2 to pi/2.\n\n    In this setup real_image_width and real_viewing_distance have no effect.\n\n    Note that rather than a 2D image gaze location in [0,1]^2, the gaze should be specified as gaze angles in [-pi,pi]x[-pi/2,pi/2] (yaw, then pitch).\n\n    Parameters\n    ----------\n\n    gaze_angles         : tuple of 2 floats\n                            Gaze direction expressed as angles, in radians.\n    image_pixel_size    : tuple of 2 ints\n                            Dimensions of the image in pixels, as a tuple of (height, width)\n    alpha               : float\n                            Parameter controlling extent of foveation\n    mode                : str\n                            Foveation mode (how pooling size varies with eccentricity). Should be \"quadratic\" or \"linear\"\n    \"\"\"\n    view_direction = torch.tensor([math.sin(gaze_angles[0])*math.cos(gaze_angles[1]), math.sin(gaze_angles[1]), math.cos(gaze_angles[0])*math.cos(gaze_angles[1])])\n\n    yaw_angle_map = torch.linspace(-torch.pi, torch.pi, image_pixel_size[1])\n    yaw_angle_map = yaw_angle_map[None,:].repeat(image_pixel_size[0], 1)[None,...]\n    pitch_angle_map = torch.linspace(-torch.pi*0.5, torch.pi*0.5, image_pixel_size[0])\n    pitch_angle_map = pitch_angle_map[:,None].repeat(1, image_pixel_size[1])[None,...]\n\n    dir_map = torch.cat([torch.sin(yaw_angle_map)*torch.cos(pitch_angle_map), torch.sin(pitch_angle_map), torch.cos(yaw_angle_map)*torch.cos(pitch_angle_map)])\n\n    # Work out the pooling region diameter in radians\n    view_dot_dir = torch.sum(view_direction[:,None,None] * dir_map, dim=0)\n    eccentricity = torch.acos(view_dot_dir)\n    pooling_rad = alpha * eccentricity\n    if mode == \"quadratic\":\n        pooling_rad *= eccentricity\n\n    # The actual pooling region will be an ellipse in the equirectangular image - the length of the major &amp; minor axes\n    # depend on the x &amp; y resolution of the image. We find these two axis lengths (in pixels) and then the area of the ellipse\n    pixels_per_rad_x = image_pixel_size[1] / (2*torch.pi)\n    pixels_per_rad_y = image_pixel_size[0] / (torch.pi)\n    pooling_axis_x = pooling_rad * pixels_per_rad_x\n    pooling_axis_y = pooling_rad * pixels_per_rad_y\n    area = torch.pi * pooling_axis_x * pooling_axis_y * 0.25\n\n    # Now finally find the length of the side of a square of the same area.\n    size = torch.sqrt(torch.abs(area))\n    return size\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.make_pooling_size_map_lod","title":"<code>make_pooling_size_map_lod(gaze_location, image_pixel_size, alpha=0.3, real_image_width=0.3, real_viewing_distance=0.6, mode='quadratic')</code>","text":"<p>This function is similar to make_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from to achieve the correct pooling region areas.</p> <p>Parameters:</p> <ul> <li> <code>gaze_location</code>           \u2013            <pre><code>                    User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                    image coordinates (ranging from 0 to 1)\n</code></pre> </li> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                    The size of the image in pixels, as a tuple of form (height, width)\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed. Units not important, as long as they\n                    are the same as those used for real_viewing_distance\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance from the user's viewpoint to the screen.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>pooling_size_map</code> (              <code>tensor</code> )          \u2013            <p>The computed pooling size map, of size WxH.</p> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_pooling_size_map_lod(gaze_location, image_pixel_size, alpha=0.3, real_image_width=0.3, real_viewing_distance=0.6, mode=\"quadratic\"):\n    \"\"\" \n    This function is similar to make_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from\n    to achieve the correct pooling region areas.\n\n    Parameters\n    ----------\n\n    gaze_location           : tuple of floats\n                                User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                                image coordinates (ranging from 0 to 1)\n    image_pixel_size        : tuple of ints\n                                The size of the image in pixels, as a tuple of form (height, width)\n    real_image_width        : float\n                                The real width of the image as displayed. Units not important, as long as they\n                                are the same as those used for real_viewing_distance\n    real_viewing_distance   : float\n                                The real distance from the user's viewpoint to the screen.\n\n    Returns\n    -------\n\n    pooling_size_map        : torch.tensor\n                                The computed pooling size map, of size WxH.\n    \"\"\"\n    pooling_pixel = make_pooling_size_map_pixels(\n        gaze_location, image_pixel_size, alpha, real_image_width, real_viewing_distance, mode)\n    pooling_lod = torch.log2(1e-6+pooling_pixel)\n    pooling_lod[pooling_lod &lt; 0] = 0\n    return pooling_lod\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.make_pooling_size_map_pixels","title":"<code>make_pooling_size_map_pixels(gaze_location, image_pixel_size, alpha=0.3, real_image_width=0.3, real_viewing_distance=0.6, mode='quadratic')</code>","text":"<p>Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity (also in radians). </p> <p>Assumes the viewpoint is located at the centre of the image, and the screen is  perpendicular to the viewing direction. Output is the width of the pooling region in pixels.</p> <p>Parameters:</p> <ul> <li> <code>gaze_location</code>           \u2013            <pre><code>                    User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                    image coordinates (ranging from 0 to 1)\n</code></pre> </li> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                    The size of the image in pixels, as a tuple of form (height, width)\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed. Units not important, as long as they\n                    are the same as those used for real_viewing_distance\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance from the user's viewpoint to the screen.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>pooling_size_map</code> (              <code>tensor</code> )          \u2013            <p>The computed pooling size map, of size WxH.</p> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_pooling_size_map_pixels(gaze_location, image_pixel_size, alpha=0.3, real_image_width=0.3, real_viewing_distance=0.6, mode=\"quadratic\"):\n    \"\"\" \n    Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to\n    a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity\n    (also in radians). \n\n    Assumes the viewpoint is located at the centre of the image, and the screen is \n    perpendicular to the viewing direction. Output is the width of the pooling region in pixels.\n\n    Parameters\n    ----------\n\n    gaze_location           : tuple of floats\n                                User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                                image coordinates (ranging from 0 to 1)\n    image_pixel_size        : tuple of ints\n                                The size of the image in pixels, as a tuple of form (height, width)\n    real_image_width        : float\n                                The real width of the image as displayed. Units not important, as long as they\n                                are the same as those used for real_viewing_distance\n    real_viewing_distance   : float\n                                The real distance from the user's viewpoint to the screen.\n\n    Returns\n    -------\n\n    pooling_size_map        : torch.tensor\n                                The computed pooling size map, of size WxH.\n    \"\"\"\n    eccentricity, distance_to_pixel = make_eccentricity_distance_maps(\n        gaze_location, image_pixel_size, real_image_width, real_viewing_distance)\n    eccentricity_centre, _ = make_eccentricity_distance_maps(\n        [0.5, 0.5], image_pixel_size, real_image_width, real_viewing_distance)\n    pooling_rad = alpha * eccentricity\n    if mode == \"quadratic\":\n        pooling_rad *= eccentricity\n    angle_min = eccentricity_centre - pooling_rad*0.5\n    angle_max = eccentricity_centre + pooling_rad*0.5\n    major_axis = (torch.tan(angle_max) - torch.tan(angle_min)) * \\\n        real_viewing_distance\n    minor_axis = 2 * distance_to_pixel * torch.tan(pooling_rad*0.5)\n    area = math.pi * major_axis * minor_axis * 0.25\n    # Should be +ve anyway, but check to ensure we don't take sqrt of negative number\n    area = torch.abs(area)\n    pooling_real = torch.sqrt(area)\n    pooling_pixel = (pooling_real / real_image_width) * image_pixel_size[1]\n    return pooling_pixel\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.make_radial_map","title":"<code>make_radial_map(size, gaze)</code>","text":"<p>Makes a simple radial map where each pixel contains distance in pixels from the chosen gaze location.</p> <p>Parameters:</p> <ul> <li> <code>size</code>           \u2013            <pre><code>    Dimensions of the image\n</code></pre> </li> <li> <code>gaze</code>           \u2013            <pre><code>    User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n    image coordinates (ranging from 0 to 1)\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_radial_map(size, gaze):\n    \"\"\" \n    Makes a simple radial map where each pixel contains distance in pixels from the chosen gaze location.\n\n    Parameters\n    ----------\n\n    size    : tuple of ints\n                Dimensions of the image\n    gaze    : tuple of floats\n                User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                image coordinates (ranging from 0 to 1)\n    \"\"\"\n    pix_gaze = [gaze[0]*size[0], gaze[1]*size[1]]\n    rows = torch.linspace(0, size[0], size[0])\n    rows = rows[:, None].repeat(1, size[1])\n    cols = torch.linspace(0, size[1], size[1])\n    cols = cols[None, :].repeat(size[0], 1)\n    dist_sq = torch.pow(rows - pix_gaze[0], 2) + \\\n        torch.pow(cols - pix_gaze[1], 2)\n    radii = torch.sqrt(dist_sq)\n    return radii/torch.max(radii)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.pad_image_for_pyramid","title":"<code>pad_image_for_pyramid(image, n_pyramid_levels)</code>","text":"<p>Pads an image to the extent necessary to compute a steerable pyramid of the input image. This involves padding so both height and width are divisible by 2**n_pyramid_levels. Uses reflection padding.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <p>Image to pad, in NCHW format</p> </li> <li> <code>n_pyramid_levels</code>           \u2013            <p>Number of levels in the pyramid you plan to construct.</p> </li> </ul> Source code in <code>odak/learn/perception/spatial_steerable_pyramid.py</code> <pre><code>def pad_image_for_pyramid(image, n_pyramid_levels):\n    \"\"\"\n    Pads an image to the extent necessary to compute a steerable pyramid of the input image.\n    This involves padding so both height and width are divisible by 2**n_pyramid_levels.\n    Uses reflection padding.\n\n    Parameters\n    ----------\n\n    image: torch.tensor\n        Image to pad, in NCHW format\n    n_pyramid_levels: int\n        Number of levels in the pyramid you plan to construct.\n    \"\"\"\n    min_divisor = 2 ** n_pyramid_levels\n    height = image.size(2)\n    width = image.size(3)\n    required_height = math.ceil(height / min_divisor) * min_divisor\n    required_width = math.ceil(width / min_divisor) * min_divisor\n    if required_height &gt; height or required_width &gt; width:\n        # We need to pad!\n        pad = torch.nn.ReflectionPad2d(\n            (0, 0, required_height-height, required_width-width))\n        return pad(image)\n    return image\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.rgb_2_ycrcb","title":"<code>rgb_2_ycrcb(image)</code>","text":"<p>Converts an image from RGB colourspace to YCrCb colourspace.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>  Input image. Should be an RGB floating-point image with values in the range [0, 1]. Should be in NCHW format [3 x m x n] or [k x 3 x m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ycrcb</code> (              <code>tensor</code> )          \u2013            <p>Image converted to YCrCb colourspace [k x 3 m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def rgb_2_ycrcb(image):\n    \"\"\"\n    Converts an image from RGB colourspace to YCrCb colourspace.\n\n    Parameters\n    ----------\n    image   : torch.tensor\n              Input image. Should be an RGB floating-point image with values in the range [0, 1]. Should be in NCHW format [3 x m x n] or [k x 3 x m x n].\n\n    Returns\n    -------\n\n    ycrcb   : torch.tensor\n              Image converted to YCrCb colourspace [k x 3 m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n       image = image.unsqueeze(0)\n    ycrcb = torch.zeros(image.size()).to(image.device)\n    ycrcb[:, 0, :, :] = 0.299 * image[:, 0, :, :] + 0.587 * \\\n        image[:, 1, :, :] + 0.114 * image[:, 2, :, :]\n    ycrcb[:, 1, :, :] = 0.5 + 0.713 * (image[:, 0, :, :] - ycrcb[:, 0, :, :])\n    ycrcb[:, 2, :, :] = 0.5 + 0.564 * (image[:, 2, :, :] - ycrcb[:, 0, :, :])\n    return ycrcb\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.rgb_to_hsv","title":"<code>rgb_to_hsv(image, eps=1e-08)</code>","text":"<p>Definition to convert RGB space to HSV color space. Mostly inspired from : https://kornia.readthedocs.io/en/latest/_modules/kornia/color/hsv.html</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in HSV color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_hsv</code> (              <code>tensor</code> )          \u2013            <p>Output image in  RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def rgb_to_hsv(image, eps: float = 1e-8):\n\n    \"\"\"\n    Definition to convert RGB space to HSV color space. Mostly inspired from : https://kornia.readthedocs.io/en/latest/_modules/kornia/color/hsv.html\n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in HSV color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n\n    Returns\n    -------\n    image_hsv       : torch.tensor\n                      Output image in  RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    max_rgb, argmax_rgb = image.max(-3)\n    min_rgb, argmin_rgb = image.min(-3)\n    deltac = max_rgb - min_rgb\n    v = max_rgb\n    s = deltac / (max_rgb + eps)\n    deltac = torch.where(deltac == 0, torch.ones_like(deltac), deltac)\n    rc, gc, bc = torch.unbind((max_rgb.unsqueeze(-3) - image), dim=-3)\n    h1 = bc - gc\n    h2 = (rc - bc) + 2.0 * deltac\n    h3 = (gc - rc) + 4.0 * deltac\n    h = torch.stack((h1, h2, h3), dim=-3) / deltac.unsqueeze(-3)\n    h = torch.gather(h, dim=-3, index=argmax_rgb.unsqueeze(-3)).squeeze(-3)\n    h = (h / 6.0) % 1.0\n    h = 2.0 * math.pi * h \n    image_hsv = torch.stack((h, s, v), dim=-3)\n    return image_hsv\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.rgb_to_linear_rgb","title":"<code>rgb_to_linear_rgb(image, threshold=0.0031308)</code>","text":"<p>Definition to convert RGB images to linear RGB color space. Mostly inspired from: https://kornia.readthedocs.io/en/latest/_modules/kornia/color/rgb.html</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> <li> <code>threshold</code>           \u2013            <pre><code>          Threshold used in calculations.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_linear</code> (              <code>tensor</code> )          \u2013            <p>Output image in linear RGB color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def rgb_to_linear_rgb(image, threshold = 0.0031308):\n    \"\"\"\n    Definition to convert RGB images to linear RGB color space. Mostly inspired from: https://kornia.readthedocs.io/en/latest/_modules/kornia/color/rgb.html\n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n    threshold       : float\n                      Threshold used in calculations.\n\n    Returns\n    -------\n    image_linear    : torch.tensor\n                      Output image in linear RGB color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    image_linear = torch.where(image &gt; 0.04045, torch.pow(((image + 0.055) / 1.055), 2.4), image / 12.92)\n    return image_linear\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.srgb_to_lab","title":"<code>srgb_to_lab(image)</code>","text":"<p>Definition to convert SRGB space to LAB color space. </p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in SRGB color space[3 x m x n]\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_lab</code> (              <code>tensor</code> )          \u2013            <p>Output image in LAB color space [3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def srgb_to_lab(image):    \n    \"\"\"\n    Definition to convert SRGB space to LAB color space. \n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in SRGB color space[3 x m x n]\n    Returns\n    -------\n    image_lab       : torch.tensor\n                      Output image in LAB color space [3 x m x n].\n    \"\"\"\n    if image.shape[-1] == 3:\n        input_color = image.permute(2, 0, 1)  # C(H*W)\n    else:\n        input_color = image\n    # rgb ---&gt; linear rgb\n    limit = 0.04045        \n    # linear rgb ---&gt; xyz\n    linrgb_color = torch.where(input_color &gt; limit, torch.pow((input_color + 0.055) / 1.055, 2.4), input_color / 12.92)\n\n    a11 = 10135552 / 24577794\n    a12 = 8788810  / 24577794\n    a13 = 4435075  / 24577794\n    a21 = 2613072  / 12288897\n    a22 = 8788810  / 12288897\n    a23 = 887015   / 12288897\n    a31 = 1425312  / 73733382\n    a32 = 8788810  / 73733382\n    a33 = 70074185 / 73733382\n\n    A = torch.tensor([[a11, a12, a13],\n                    [a21, a22, a23],\n                    [a31, a32, a33]], dtype=torch.float32)\n\n    linrgb_color = linrgb_color.permute(2, 0, 1) # C(H*W)\n    xyz_color = torch.matmul(A, linrgb_color)\n    xyz_color = xyz_color.permute(1, 2, 0)\n    # xyz ---&gt; lab\n    inv_reference_illuminant = torch.tensor([[[1.052156925]], [[1.000000000]], [[0.918357670]]], dtype=torch.float32)\n    input_color = xyz_color * inv_reference_illuminant\n    delta = 6 / 29\n    delta_square = delta * delta\n    delta_cube = delta * delta_square\n    factor = 1 / (3 * delta_square)\n\n    input_color = torch.where(input_color &gt; delta_cube, torch.pow(input_color, 1 / 3), (factor * input_color + 4 / 29))\n\n    l = 116 * input_color[1:2, :, :] - 16\n    a = 500 * (input_color[0:1,:, :] - input_color[1:2, :, :])\n    b = 200 * (input_color[1:2, :, :] - input_color[2:3, :, :])\n\n    image_lab = torch.cat((l, a, b), 0)\n    return image_lab    \n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.xyz_to_linear_rgb","title":"<code>xyz_to_linear_rgb(image)</code>","text":"<p>Definition to convert CIE XYZ space to linear RGB color space. Mostly inspired from : Rochester IT Color Conversion Algorithms (https://www.cs.rit.edu/~ncs/color/)</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>           Input image in XYZ (CIE 1931) color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_linear_rgb</code> (              <code>tensor</code> )          \u2013            <p>Output image in linear RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def xyz_to_linear_rgb(image):\n    \"\"\"\n    Definition to convert CIE XYZ space to linear RGB color space. Mostly inspired from : Rochester IT Color Conversion Algorithms (https://www.cs.rit.edu/~ncs/color/)\n\n    Parameters\n    ----------\n    image            : torch.tensor\n                       Input image in XYZ (CIE 1931) color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n\n    Returns\n    -------\n    image_linear_rgb : torch.tensor\n                       Output image in linear RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    a11 = 3.240479\n    a12 = -1.537150\n    a13 = -0.498535\n    a21 = -0.969256 \n    a22 = 1.875992 \n    a23 = 0.041556\n    a31 = 0.055648\n    a32 = -0.204043\n    a33 = 1.057311\n    M = torch.tensor([[a11, a12, a13], \n                      [a21, a22, a23],\n                      [a31, a32, a33]])\n    size = image.size()\n    image = image.reshape(size[0], size[1], size[2]*size[3])\n    image_linear_rgb = torch.matmul(M, image)\n    image_linear_rgb = image_linear_rgb.reshape(size[0], size[1], size[2], size[3])\n    return image_linear_rgb\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.ycrcb_2_rgb","title":"<code>ycrcb_2_rgb(image)</code>","text":"<p>Converts an image from YCrCb colourspace to RGB colourspace.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>  Input image. Should be a YCrCb floating-point image with values in the range [0, 1]. Should be in NCHW format [3 x m x n] or [k x 3 x m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rgb</code> (              <code>tensor</code> )          \u2013            <p>Image converted to RGB colourspace [k x 3 m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def ycrcb_2_rgb(image):\n    \"\"\"\n    Converts an image from YCrCb colourspace to RGB colourspace.\n\n    Parameters\n    ----------\n    image   : torch.tensor\n              Input image. Should be a YCrCb floating-point image with values in the range [0, 1]. Should be in NCHW format [3 x m x n] or [k x 3 x m x n].\n\n    Returns\n    -------\n    rgb     : torch.tensor\n              Image converted to RGB colourspace [k x 3 m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n       image = image.unsqueeze(0)\n    rgb = torch.zeros(image.size(), device=image.device)\n    rgb[:, 0, :, :] = image[:, 0, :, :] + 1.403 * (image[:, 1, :, :] - 0.5)\n    rgb[:, 1, :, :] = image[:, 0, :, :] - 0.714 * \\\n        (image[:, 1, :, :] - 0.5) - 0.344 * (image[:, 2, :, :] - 0.5)\n    rgb[:, 2, :, :] = image[:, 0, :, :] + 1.773 * (image[:, 2, :, :] - 0.5)\n    return rgb\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.blur_loss.BlurLoss","title":"<code>BlurLoss</code>","text":"<p><code>BlurLoss</code> implements two different blur losses. When <code>blur_source</code> is set to <code>False</code>, it implements blur_match, trying to match the input image to the blurred target image. This tries to match the source input image to a blurred version of the target.</p> <p>When <code>blur_source</code> is set to <code>True</code>, it implements blur_lowpass, matching the blurred version of the input image to the blurred target image. This tries to match only the low frequencies of the source input image to the low frequencies of the target.</p> <p>The interface is similar to other <code>pytorch</code> loss functions, but note that the gaze location must be provided in addition to the source and target images.</p> Source code in <code>odak/learn/perception/blur_loss.py</code> <pre><code>class BlurLoss():\n    \"\"\" \n\n    `BlurLoss` implements two different blur losses. When `blur_source` is set to `False`, it implements blur_match, trying to match the input image to the blurred target image. This tries to match the source input image to a blurred version of the target.\n\n    When `blur_source` is set to `True`, it implements blur_lowpass, matching the blurred version of the input image to the blurred target image. This tries to match only the low frequencies of the source input image to the low frequencies of the target.\n\n    The interface is similar to other `pytorch` loss functions, but note that the gaze location must be provided in addition to the source and target images.\n    \"\"\"\n\n\n    def __init__(self, device=torch.device(\"cpu\"),\n                 alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode=\"quadratic\", blur_source=False, equi=False):\n        \"\"\"\n        Parameters\n        ----------\n\n        alpha                   : float\n                                    parameter controlling foveation - larger values mean bigger pooling regions.\n        real_image_width        : float \n                                    The real width of the image as displayed to the user.\n                                    Units don't matter as long as they are the same as for real_viewing_distance.\n        real_viewing_distance   : float \n                                    The real distance of the observer's eyes to the image plane.\n                                    Units don't matter as long as they are the same as for real_image_width.\n        mode                    : str \n                                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                    as you move away from the fovea. We got best results with \"quadratic\".\n        blur_source             : bool\n                                    If true, blurs the source image as well as the target before computing the loss.\n        equi                    : bool\n                                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                    [-pi,pi]x[-pi/2,pi]\n        \"\"\"\n        self.target = None\n        self.device = device\n        self.alpha = alpha\n        self.real_image_width = real_image_width\n        self.real_viewing_distance = real_viewing_distance\n        self.mode = mode\n        self.blur = None\n        self.loss_func = torch.nn.MSELoss()\n        self.blur_source = blur_source\n        self.equi = equi\n\n    def blur_image(self, image, gaze):\n        if self.blur is None:\n            self.blur = RadiallyVaryingBlur()\n        return self.blur.blur(image, self.alpha, self.real_image_width, self.real_viewing_distance, gaze, self.mode, self.equi)\n\n    def __call__(self, image, target, gaze=[0.5, 0.5]):\n        \"\"\" \n        Calculates the Blur Loss.\n\n        Parameters\n        ----------\n        image               : torch.tensor\n                                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        target              : torch.tensor\n                                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        gaze                : list\n                                Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n        Returns\n        -------\n\n        loss                : torch.tensor\n                                The computed loss.\n        \"\"\"\n        check_loss_inputs(\"BlurLoss\", image, target)\n        blurred_target = self.blur_image(target, gaze)\n        if self.blur_source:\n            blurred_image = self.blur_image(image, gaze)\n            return self.loss_func(blurred_image, blurred_target)\n        else:\n            return self.loss_func(image, blurred_target)\n\n    def to(self, device):\n        self.device = device\n        return self\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.blur_loss.BlurLoss.__call__","title":"<code>__call__(image, target, gaze=[0.5, 0.5])</code>","text":"<p>Calculates the Blur Loss.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>target</code>           \u2013            <pre><code>                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>gaze</code>           \u2013            <pre><code>                Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>tensor</code> )          \u2013            <p>The computed loss.</p> </li> </ul> Source code in <code>odak/learn/perception/blur_loss.py</code> <pre><code>def __call__(self, image, target, gaze=[0.5, 0.5]):\n    \"\"\" \n    Calculates the Blur Loss.\n\n    Parameters\n    ----------\n    image               : torch.tensor\n                            Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    target              : torch.tensor\n                            Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    gaze                : list\n                            Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n    Returns\n    -------\n\n    loss                : torch.tensor\n                            The computed loss.\n    \"\"\"\n    check_loss_inputs(\"BlurLoss\", image, target)\n    blurred_target = self.blur_image(target, gaze)\n    if self.blur_source:\n        blurred_image = self.blur_image(image, gaze)\n        return self.loss_func(blurred_image, blurred_target)\n    else:\n        return self.loss_func(image, blurred_target)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.blur_loss.BlurLoss.__init__","title":"<code>__init__(device=torch.device('cpu'), alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode='quadratic', blur_source=False, equi=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>           \u2013            <pre><code>                    parameter controlling foveation - larger values mean bigger pooling regions.\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed to the user.\n                    Units don't matter as long as they are the same as for real_viewing_distance.\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance of the observer's eyes to the image plane.\n                    Units don't matter as long as they are the same as for real_image_width.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                    as you move away from the fovea. We got best results with \"quadratic\".\n</code></pre> </li> <li> <code>blur_source</code>           \u2013            <pre><code>                    If true, blurs the source image as well as the target before computing the loss.\n</code></pre> </li> <li> <code>equi</code>           \u2013            <pre><code>                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                    [-pi,pi]x[-pi/2,pi]\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/blur_loss.py</code> <pre><code>def __init__(self, device=torch.device(\"cpu\"),\n             alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode=\"quadratic\", blur_source=False, equi=False):\n    \"\"\"\n    Parameters\n    ----------\n\n    alpha                   : float\n                                parameter controlling foveation - larger values mean bigger pooling regions.\n    real_image_width        : float \n                                The real width of the image as displayed to the user.\n                                Units don't matter as long as they are the same as for real_viewing_distance.\n    real_viewing_distance   : float \n                                The real distance of the observer's eyes to the image plane.\n                                Units don't matter as long as they are the same as for real_image_width.\n    mode                    : str \n                                Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                as you move away from the fovea. We got best results with \"quadratic\".\n    blur_source             : bool\n                                If true, blurs the source image as well as the target before computing the loss.\n    equi                    : bool\n                                If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                [-pi,pi]x[-pi/2,pi]\n    \"\"\"\n    self.target = None\n    self.device = device\n    self.alpha = alpha\n    self.real_image_width = real_image_width\n    self.real_viewing_distance = real_viewing_distance\n    self.mode = mode\n    self.blur = None\n    self.loss_func = torch.nn.MSELoss()\n    self.blur_source = blur_source\n    self.equi = equi\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs","title":"<code>display_color_hvs</code>","text":"Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>class display_color_hvs():\n\n\n    def __init__(\n                 self,\n                 resolution = [1920, 1080],\n                 distance_from_screen = 800,\n                 pixel_pitch = 0.311,\n                 read_spectrum = 'tensor',\n                 primaries_spectrum = torch.rand(3, 301),\n                 device = torch.device('cpu')\n                ):\n        '''\n        Parameters\n        ----------\n        resolution                  : list\n                                      Resolution of the display in pixels.\n        distance_from_screen        : int\n                                      Distance from the screen in mm.\n        pixel_pitch                 : float\n                                      Pixel pitch of the display in mm.\n        read_spectrum               : str\n                                      Spectrum of the display. Default is 'default' which is the spectrum of the Dell U2415 display [3 x 301].\n        device                      : torch.device\n                                      Device to run the code on. Default is None which means the code will run on CPU.\n\n        '''\n        self.device = device\n        self.read_spectrum = read_spectrum\n        self.primaries_spectrum = primaries_spectrum.to(self.device)\n        self.resolution = resolution\n        self.distance_from_screen = distance_from_screen\n        self.pixel_pitch = pixel_pitch\n        self.l_normalized, self.m_normalized, self.s_normalized = self.initialize_cones_normalized()\n        self.lms_tensor = self.construct_matrix_lms(\n                                                    self.l_normalized,\n                                                    self.m_normalized,\n                                                    self.s_normalized\n                                                   )   \n        self.primaries_tensor = self.construct_matrix_primaries(\n                                                                self.l_normalized,\n                                                                self.m_normalized,\n                                                                self.s_normalized\n                                                               )   \n        return\n\n\n    def __call__(self, input_image, ground_truth, gaze=None):\n        \"\"\"\n        Evaluating an input image against a target ground truth image for a given gaze of a viewer.\n        \"\"\"\n        lms_image_second = self.primaries_to_lms(input_image.to(self.device))\n        lms_ground_truth_second = self.primaries_to_lms(ground_truth.to(self.device))\n        lms_image_third = self.second_to_third_stage(lms_image_second)\n        lms_ground_truth_third = self.second_to_third_stage(lms_ground_truth_second)\n        loss_metamer_color = torch.mean((lms_ground_truth_third - lms_image_third) ** 2)\n        return loss_metamer_color\n\n\n    def initialize_cones_normalized(self):\n        \"\"\"\n        Internal function to initialize normalized L,M,S cones as normal distribution with given sigma, and mu values. \n\n        Returns\n        -------\n        l_cone_n                     : torch.tensor\n                                       Normalised L cone distribution.\n        m_cone_n                     : torch.tensor\n                                       Normalised M cone distribution.\n        s_cone_n                     : torch.tensor\n                                       Normalised S cone distribution.\n        \"\"\"\n        wavelength_range = torch.linspace(400, 700, steps = self.primaries_spectrum.shape[-1], device = self.device)\n        dist_l = 1 / (32.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 567.5) ** 2 / (2 * 32.5 ** 2))\n        dist_m = 1 / (27.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 545.0) ** 2 / (2 * 27.5 ** 2))\n        dist_s = 1 / (17.0 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 447.5) ** 2 / (2 * 17.0 ** 2))\n\n        l_cone_n = dist_l / dist_l.max()\n        m_cone_n = dist_m / dist_m.max()\n        s_cone_n = dist_s / dist_s.max()\n        return l_cone_n, m_cone_n, s_cone_n\n\n\n    def initialize_rgb_backlight_spectrum(self):\n        \"\"\"\n        Internal function to initialize baclight spectrum for color primaries. \n\n        Returns\n        -------\n        red_spectrum                 : torch.tensor\n                                       Normalised backlight spectrum for red color primary.\n        green_spectrum               : torch.tensor\n                                       Normalised backlight spectrum for green color primary.\n        blue_spectrum                : torch.tensor\n                                       Normalised backlight spectrum for blue color primary.\n        \"\"\"\n        wavelength_range = torch.linspace(400, 700, steps = self.primaries_spectrum.shape[-1], device = self.device)\n        red_spectrum = 1 / (14.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 650) ** 2 / (2 * 14.5 ** 2))\n        green_spectrum = 1 / (12 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 550) ** 2 / (2 * 12.0 ** 2))\n        blue_spectrum = 1 / (12 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 450) ** 2 / (2 * 12.0 ** 2))\n\n        red_spectrum = red_spectrum / red_spectrum.max()\n        green_spectrum = green_spectrum / green_spectrum.max()\n        blue_spectrum = blue_spectrum / blue_spectrum.max()\n\n        return red_spectrum, green_spectrum, blue_spectrum\n\n\n    def initialize_random_spectrum_normalized(self, dataset):\n        \"\"\"\n        Initialize normalized light spectrum via combination of 3 gaussian distribution curve fitting [L-BFGS]. \n\n        Parameters\n        ----------\n        dataset                                : torch.tensor \n                                                 spectrum value against wavelength \n        \"\"\"\n        dataset = torch.swapaxes(dataset, 0, 1)\n        x_spectrum = torch.linspace(400, 700, steps = 301) - 550\n        y_spectrum = torch.from_numpy(np_cpu.interp(x_spectrum, dataset[0].numpy(), dataset[1].numpy()))\n        max_spectrum = torch.max(y_spectrum)\n        y_spectrum /= max_spectrum\n\n        def gaussian(x, A = 1, sigma = 1, centre = 0): return A * \\\n            torch.exp(-(x - centre) ** 2 / (2 * sigma ** 2))\n\n        def function(x, weights): \n            return gaussian(x, *weights[:3]) + gaussian(x, *weights[3:6]) + gaussian(x, *weights[6:9])\n\n        weights = torch.tensor([1.0, 1.0, -0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2], requires_grad = True)\n        optimizer = torch.optim.LBFGS([weights], max_iter = 1000, lr = 0.1, line_search_fn = None)\n\n        def closure():\n            optimizer.zero_grad()\n            output = function(x_spectrum, weights)\n            loss = F.mse_loss(output, y_spectrum)\n            loss.backward()\n            return loss\n        optimizer.step(closure)\n        spectrum = function(x_spectrum, weights)\n        return spectrum.detach().to(self.device)\n\n\n    def display_spectrum_response(wavelength, function):\n        \"\"\"\n        Internal function to provide light spectrum response at particular wavelength\n\n        Parameters\n        ----------\n        wavelength                          : torch.tensor\n                                              Wavelength in nm [400...700]\n        function                            : torch.tensor\n                                              Display light spectrum distribution function\n\n        Returns\n        -------\n        ligth_response_dict                  : float\n                                               Display light spectrum response value\n        \"\"\"\n        wavelength = int(round(wavelength, 0))\n        if wavelength &gt;= 400 and wavelength &lt;= 700:\n            return function[wavelength - 400].item()\n        elif wavelength &lt; 400:\n            return function[0].item()\n        else:\n            return function[300].item()\n\n\n    def cone_response_to_spectrum(self, cone_spectrum, light_spectrum):\n        \"\"\"\n        Internal function to calculate cone response at particular light spectrum. \n\n        Parameters\n        ----------\n        cone_spectrum                         : torch.tensor\n                                                Spectrum, Wavelength [2,300] tensor \n        light_spectrum                        : torch.tensor\n                                                Spectrum, Wavelength [2,300] tensor \n\n\n        Returns\n        -------\n        response_to_spectrum                  : float\n                                                Response of cone to light spectrum [1x1] \n        \"\"\"\n        response_to_spectrum = torch.mul(cone_spectrum, light_spectrum)\n        response_to_spectrum = torch.sum(response_to_spectrum)\n        return response_to_spectrum.item()\n\n\n    def construct_matrix_lms(self, l_response, m_response, s_response):\n        '''\n        Internal function to calculate cone  response at particular light spectrum. \n\n        Parameters\n        ----------\n        l_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n        m_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n        s_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n\n\n\n        Returns\n        -------\n        lms_image_tensor                      : torch.tensor\n                                                3x3 LMSrgb tensor\n\n        '''\n        if self.read_spectrum == 'tensor':\n            logging.warning('Tensor primary spectrum is used')\n            logging.warning('The number of primaries used is {}'.format(self.primaries_spectrum.shape[0]))\n        else:\n            logging.warning(\"No Spectrum data is provided\")\n\n        self.lms_tensor = torch.zeros(self.primaries_spectrum.shape[0], 3).to(self.device)\n        for i in range(self.primaries_spectrum.shape[0]):\n            self.lms_tensor[i, 0] = self.cone_response_to_spectrum(l_response, self.primaries_spectrum[i])\n            self.lms_tensor[i, 1] = self.cone_response_to_spectrum(m_response, self.primaries_spectrum[i])\n            self.lms_tensor[i, 2] = self.cone_response_to_spectrum(s_response, self.primaries_spectrum[i]) \n        return self.lms_tensor    \n\n\n    def construct_matrix_primaries(self, l_response, m_response, s_response):\n        '''\n        Internal function to calculate cone  response at particular light spectrum. \n\n        Parameters\n        ----------\n        l_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n        m_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n        s_response                             : torch.tensor\n                                                 Cone response spectrum tensor (normalized response vs wavelength)\n\n\n\n        Returns\n        -------\n        lms_image_tensor                      : torch.tensor\n                                                3x3 LMSrgb tensor\n\n        '''\n        if self.read_spectrum == 'tensor':\n            logging.warning('Tensor primary spectrum is used')\n            logging.warning('The number of primaries used is {}'.format(self.primaries_spectrum.shape[0]))\n        else:\n            logging.warning(\"No Spectrum data is provided\")\n\n        self.primaries_tensor = torch.zeros(3, self.primaries_spectrum.shape[0]).to(self.device)\n        for i in range(self.primaries_spectrum.shape[0]):\n            self.primaries_tensor[0, i] = self.cone_response_to_spectrum(\n                                                                         l_response,\n                                                                         self.primaries_spectrum[i]\n                                                                        )\n            self.primaries_tensor[1, i] = self.cone_response_to_spectrum(\n                                                                         m_response,\n                                                                         self.primaries_spectrum[i]\n                                                                        )\n            self.primaries_tensor[2, i] = self.cone_response_to_spectrum(\n                                                                         s_response,\n                                                                         self.primaries_spectrum[i]\n                                                                        ) \n        return self.primaries_tensor    \n\n\n    def primaries_to_lms(self, primaries):\n        \"\"\"\n        Internal function to convert primaries space to LMS space \n\n        Parameters\n        ----------\n        primaries                              : torch.tensor\n                                                 Primaries data to be transformed to LMS space [BxPHxW]\n\n\n        Returns\n        -------\n        lms_color                              : torch.tensor\n                                                 LMS data transformed from Primaries space [BxPxHxW]\n        \"\"\"                \n        primaries_flatten = primaries.reshape(primaries.shape[0], primaries.shape[1], 1, -1)\n        lms = self.lms_tensor.unsqueeze(0).unsqueeze(-1)\n        lms_color = torch.sum(primaries_flatten * lms, axis = 1).reshape(primaries.shape)\n        return lms_color\n\n\n    def lms_to_primaries(self, lms_color_tensor):\n        \"\"\"\n        Internal function to convert LMS image to primaries space\n\n        Parameters\n        ----------\n        lms_color_tensor                        : torch.tensor\n                                                  LMS data to be transformed to primaries space [Bx3xHxW]\n\n\n        Returns\n        -------\n        primaries                              : torch.tensor\n                                               : Primaries data transformed from LMS space [BxPxHxW]\n        \"\"\"\n        lms_color_tensor = lms_color_tensor.permute(0, 2, 3, 1).to(self.device)\n        lms_color_flatten = torch.flatten(lms_color_tensor, start_dim=0, end_dim=1)\n        unflatten = torch.nn.Unflatten(0, (lms_color_tensor.size(0), lms_color_tensor.size(1)))\n        converted_unflatten = torch.matmul(lms_color_flatten.double(), self.lms_tensor.pinverse().double())\n        primaries = unflatten(converted_unflatten)     \n        primaries = primaries.permute(0, 3, 1, 2)   \n        return primaries\n\n\n    def second_to_third_stage(self, lms_image):\n        '''\n        This function turns second stage [L,M,S] values into third stage [(M+S)-L, (L+S)-M, L+M+S], \n        See table 1 from Schmidt et al. \"Neurobiological hypothesis of color appearance and hue perception,\" Optics Express 2014.\n\n        Parameters\n        ----------\n        lms_image                             : torch.tensor\n                                                 Image data at LMS space (second stage)\n\n        Returns\n        -------\n        third_stage                            : torch.tensor\n                                                 Image data at LMS space (third stage)\n\n        '''\n        third_stage = torch.zeros_like(lms_image)\n        third_stage[:, 0] = (lms_image[:, 1] + lms_image[:, 2]) - lms_image[:, 1]\n        third_stage[:, 1] = (lms_image[:, 0] + lms_image[:, 2]) - lms_image[:, 1]\n        third_stage[:, 2] = lms_image[:, 0] + lms_image[:, 1]  + lms_image[:, 2]\n        return third_stage\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.__call__","title":"<code>__call__(input_image, ground_truth, gaze=None)</code>","text":"<p>Evaluating an input image against a target ground truth image for a given gaze of a viewer.</p> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def __call__(self, input_image, ground_truth, gaze=None):\n    \"\"\"\n    Evaluating an input image against a target ground truth image for a given gaze of a viewer.\n    \"\"\"\n    lms_image_second = self.primaries_to_lms(input_image.to(self.device))\n    lms_ground_truth_second = self.primaries_to_lms(ground_truth.to(self.device))\n    lms_image_third = self.second_to_third_stage(lms_image_second)\n    lms_ground_truth_third = self.second_to_third_stage(lms_ground_truth_second)\n    loss_metamer_color = torch.mean((lms_ground_truth_third - lms_image_third) ** 2)\n    return loss_metamer_color\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.__init__","title":"<code>__init__(resolution=[1920, 1080], distance_from_screen=800, pixel_pitch=0.311, read_spectrum='tensor', primaries_spectrum=torch.rand(3, 301), device=torch.device('cpu'))</code>","text":"<p>Parameters:</p> <ul> <li> <code>resolution</code>           \u2013            <pre><code>                      Resolution of the display in pixels.\n</code></pre> </li> <li> <code>distance_from_screen</code>           \u2013            <pre><code>                      Distance from the screen in mm.\n</code></pre> </li> <li> <code>pixel_pitch</code>           \u2013            <pre><code>                      Pixel pitch of the display in mm.\n</code></pre> </li> <li> <code>read_spectrum</code>           \u2013            <pre><code>                      Spectrum of the display. Default is 'default' which is the spectrum of the Dell U2415 display [3 x 301].\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>                      Device to run the code on. Default is None which means the code will run on CPU.\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def __init__(\n             self,\n             resolution = [1920, 1080],\n             distance_from_screen = 800,\n             pixel_pitch = 0.311,\n             read_spectrum = 'tensor',\n             primaries_spectrum = torch.rand(3, 301),\n             device = torch.device('cpu')\n            ):\n    '''\n    Parameters\n    ----------\n    resolution                  : list\n                                  Resolution of the display in pixels.\n    distance_from_screen        : int\n                                  Distance from the screen in mm.\n    pixel_pitch                 : float\n                                  Pixel pitch of the display in mm.\n    read_spectrum               : str\n                                  Spectrum of the display. Default is 'default' which is the spectrum of the Dell U2415 display [3 x 301].\n    device                      : torch.device\n                                  Device to run the code on. Default is None which means the code will run on CPU.\n\n    '''\n    self.device = device\n    self.read_spectrum = read_spectrum\n    self.primaries_spectrum = primaries_spectrum.to(self.device)\n    self.resolution = resolution\n    self.distance_from_screen = distance_from_screen\n    self.pixel_pitch = pixel_pitch\n    self.l_normalized, self.m_normalized, self.s_normalized = self.initialize_cones_normalized()\n    self.lms_tensor = self.construct_matrix_lms(\n                                                self.l_normalized,\n                                                self.m_normalized,\n                                                self.s_normalized\n                                               )   \n    self.primaries_tensor = self.construct_matrix_primaries(\n                                                            self.l_normalized,\n                                                            self.m_normalized,\n                                                            self.s_normalized\n                                                           )   \n    return\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.cone_response_to_spectrum","title":"<code>cone_response_to_spectrum(cone_spectrum, light_spectrum)</code>","text":"<p>Internal function to calculate cone response at particular light spectrum. </p> <p>Parameters:</p> <ul> <li> <code>cone_spectrum</code>           \u2013            <pre><code>                                Spectrum, Wavelength [2,300] tensor\n</code></pre> </li> <li> <code>light_spectrum</code>           \u2013            <pre><code>                                Spectrum, Wavelength [2,300] tensor\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>response_to_spectrum</code> (              <code>float</code> )          \u2013            <p>Response of cone to light spectrum [1x1]</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def cone_response_to_spectrum(self, cone_spectrum, light_spectrum):\n    \"\"\"\n    Internal function to calculate cone response at particular light spectrum. \n\n    Parameters\n    ----------\n    cone_spectrum                         : torch.tensor\n                                            Spectrum, Wavelength [2,300] tensor \n    light_spectrum                        : torch.tensor\n                                            Spectrum, Wavelength [2,300] tensor \n\n\n    Returns\n    -------\n    response_to_spectrum                  : float\n                                            Response of cone to light spectrum [1x1] \n    \"\"\"\n    response_to_spectrum = torch.mul(cone_spectrum, light_spectrum)\n    response_to_spectrum = torch.sum(response_to_spectrum)\n    return response_to_spectrum.item()\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.construct_matrix_lms","title":"<code>construct_matrix_lms(l_response, m_response, s_response)</code>","text":"<p>Internal function to calculate cone  response at particular light spectrum. </p> <p>Parameters:</p> <ul> <li> <code>l_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> <li> <code>m_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> <li> <code>s_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>lms_image_tensor</code> (              <code>tensor</code> )          \u2013            <p>3x3 LMSrgb tensor</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def construct_matrix_lms(self, l_response, m_response, s_response):\n    '''\n    Internal function to calculate cone  response at particular light spectrum. \n\n    Parameters\n    ----------\n    l_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n    m_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n    s_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n\n\n\n    Returns\n    -------\n    lms_image_tensor                      : torch.tensor\n                                            3x3 LMSrgb tensor\n\n    '''\n    if self.read_spectrum == 'tensor':\n        logging.warning('Tensor primary spectrum is used')\n        logging.warning('The number of primaries used is {}'.format(self.primaries_spectrum.shape[0]))\n    else:\n        logging.warning(\"No Spectrum data is provided\")\n\n    self.lms_tensor = torch.zeros(self.primaries_spectrum.shape[0], 3).to(self.device)\n    for i in range(self.primaries_spectrum.shape[0]):\n        self.lms_tensor[i, 0] = self.cone_response_to_spectrum(l_response, self.primaries_spectrum[i])\n        self.lms_tensor[i, 1] = self.cone_response_to_spectrum(m_response, self.primaries_spectrum[i])\n        self.lms_tensor[i, 2] = self.cone_response_to_spectrum(s_response, self.primaries_spectrum[i]) \n    return self.lms_tensor    \n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.construct_matrix_primaries","title":"<code>construct_matrix_primaries(l_response, m_response, s_response)</code>","text":"<p>Internal function to calculate cone  response at particular light spectrum. </p> <p>Parameters:</p> <ul> <li> <code>l_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> <li> <code>m_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> <li> <code>s_response</code>           \u2013            <pre><code>                                 Cone response spectrum tensor (normalized response vs wavelength)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>lms_image_tensor</code> (              <code>tensor</code> )          \u2013            <p>3x3 LMSrgb tensor</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def construct_matrix_primaries(self, l_response, m_response, s_response):\n    '''\n    Internal function to calculate cone  response at particular light spectrum. \n\n    Parameters\n    ----------\n    l_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n    m_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n    s_response                             : torch.tensor\n                                             Cone response spectrum tensor (normalized response vs wavelength)\n\n\n\n    Returns\n    -------\n    lms_image_tensor                      : torch.tensor\n                                            3x3 LMSrgb tensor\n\n    '''\n    if self.read_spectrum == 'tensor':\n        logging.warning('Tensor primary spectrum is used')\n        logging.warning('The number of primaries used is {}'.format(self.primaries_spectrum.shape[0]))\n    else:\n        logging.warning(\"No Spectrum data is provided\")\n\n    self.primaries_tensor = torch.zeros(3, self.primaries_spectrum.shape[0]).to(self.device)\n    for i in range(self.primaries_spectrum.shape[0]):\n        self.primaries_tensor[0, i] = self.cone_response_to_spectrum(\n                                                                     l_response,\n                                                                     self.primaries_spectrum[i]\n                                                                    )\n        self.primaries_tensor[1, i] = self.cone_response_to_spectrum(\n                                                                     m_response,\n                                                                     self.primaries_spectrum[i]\n                                                                    )\n        self.primaries_tensor[2, i] = self.cone_response_to_spectrum(\n                                                                     s_response,\n                                                                     self.primaries_spectrum[i]\n                                                                    ) \n    return self.primaries_tensor    \n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.display_spectrum_response","title":"<code>display_spectrum_response(wavelength, function)</code>","text":"<p>Internal function to provide light spectrum response at particular wavelength</p> <p>Parameters:</p> <ul> <li> <code>wavelength</code>           \u2013            <pre><code>                              Wavelength in nm [400...700]\n</code></pre> </li> <li> <code>function</code>           \u2013            <pre><code>                              Display light spectrum distribution function\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ligth_response_dict</code> (              <code>float</code> )          \u2013            <p>Display light spectrum response value</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def display_spectrum_response(wavelength, function):\n    \"\"\"\n    Internal function to provide light spectrum response at particular wavelength\n\n    Parameters\n    ----------\n    wavelength                          : torch.tensor\n                                          Wavelength in nm [400...700]\n    function                            : torch.tensor\n                                          Display light spectrum distribution function\n\n    Returns\n    -------\n    ligth_response_dict                  : float\n                                           Display light spectrum response value\n    \"\"\"\n    wavelength = int(round(wavelength, 0))\n    if wavelength &gt;= 400 and wavelength &lt;= 700:\n        return function[wavelength - 400].item()\n    elif wavelength &lt; 400:\n        return function[0].item()\n    else:\n        return function[300].item()\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.initialize_cones_normalized","title":"<code>initialize_cones_normalized()</code>","text":"<p>Internal function to initialize normalized L,M,S cones as normal distribution with given sigma, and mu values. </p> <p>Returns:</p> <ul> <li> <code>l_cone_n</code> (              <code>tensor</code> )          \u2013            <p>Normalised L cone distribution.</p> </li> <li> <code>m_cone_n</code> (              <code>tensor</code> )          \u2013            <p>Normalised M cone distribution.</p> </li> <li> <code>s_cone_n</code> (              <code>tensor</code> )          \u2013            <p>Normalised S cone distribution.</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def initialize_cones_normalized(self):\n    \"\"\"\n    Internal function to initialize normalized L,M,S cones as normal distribution with given sigma, and mu values. \n\n    Returns\n    -------\n    l_cone_n                     : torch.tensor\n                                   Normalised L cone distribution.\n    m_cone_n                     : torch.tensor\n                                   Normalised M cone distribution.\n    s_cone_n                     : torch.tensor\n                                   Normalised S cone distribution.\n    \"\"\"\n    wavelength_range = torch.linspace(400, 700, steps = self.primaries_spectrum.shape[-1], device = self.device)\n    dist_l = 1 / (32.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 567.5) ** 2 / (2 * 32.5 ** 2))\n    dist_m = 1 / (27.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 545.0) ** 2 / (2 * 27.5 ** 2))\n    dist_s = 1 / (17.0 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 447.5) ** 2 / (2 * 17.0 ** 2))\n\n    l_cone_n = dist_l / dist_l.max()\n    m_cone_n = dist_m / dist_m.max()\n    s_cone_n = dist_s / dist_s.max()\n    return l_cone_n, m_cone_n, s_cone_n\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.initialize_random_spectrum_normalized","title":"<code>initialize_random_spectrum_normalized(dataset)</code>","text":"<p>Initialize normalized light spectrum via combination of 3 gaussian distribution curve fitting [L-BFGS]. </p> <p>Parameters:</p> <ul> <li> <code>dataset</code>           \u2013            <pre><code>                                 spectrum value against wavelength\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def initialize_random_spectrum_normalized(self, dataset):\n    \"\"\"\n    Initialize normalized light spectrum via combination of 3 gaussian distribution curve fitting [L-BFGS]. \n\n    Parameters\n    ----------\n    dataset                                : torch.tensor \n                                             spectrum value against wavelength \n    \"\"\"\n    dataset = torch.swapaxes(dataset, 0, 1)\n    x_spectrum = torch.linspace(400, 700, steps = 301) - 550\n    y_spectrum = torch.from_numpy(np_cpu.interp(x_spectrum, dataset[0].numpy(), dataset[1].numpy()))\n    max_spectrum = torch.max(y_spectrum)\n    y_spectrum /= max_spectrum\n\n    def gaussian(x, A = 1, sigma = 1, centre = 0): return A * \\\n        torch.exp(-(x - centre) ** 2 / (2 * sigma ** 2))\n\n    def function(x, weights): \n        return gaussian(x, *weights[:3]) + gaussian(x, *weights[3:6]) + gaussian(x, *weights[6:9])\n\n    weights = torch.tensor([1.0, 1.0, -0.2, 1.0, 1.0, 0.0, 1.0, 1.0, 0.2], requires_grad = True)\n    optimizer = torch.optim.LBFGS([weights], max_iter = 1000, lr = 0.1, line_search_fn = None)\n\n    def closure():\n        optimizer.zero_grad()\n        output = function(x_spectrum, weights)\n        loss = F.mse_loss(output, y_spectrum)\n        loss.backward()\n        return loss\n    optimizer.step(closure)\n    spectrum = function(x_spectrum, weights)\n    return spectrum.detach().to(self.device)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.initialize_rgb_backlight_spectrum","title":"<code>initialize_rgb_backlight_spectrum()</code>","text":"<p>Internal function to initialize baclight spectrum for color primaries. </p> <p>Returns:</p> <ul> <li> <code>red_spectrum</code> (              <code>tensor</code> )          \u2013            <p>Normalised backlight spectrum for red color primary.</p> </li> <li> <code>green_spectrum</code> (              <code>tensor</code> )          \u2013            <p>Normalised backlight spectrum for green color primary.</p> </li> <li> <code>blue_spectrum</code> (              <code>tensor</code> )          \u2013            <p>Normalised backlight spectrum for blue color primary.</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def initialize_rgb_backlight_spectrum(self):\n    \"\"\"\n    Internal function to initialize baclight spectrum for color primaries. \n\n    Returns\n    -------\n    red_spectrum                 : torch.tensor\n                                   Normalised backlight spectrum for red color primary.\n    green_spectrum               : torch.tensor\n                                   Normalised backlight spectrum for green color primary.\n    blue_spectrum                : torch.tensor\n                                   Normalised backlight spectrum for blue color primary.\n    \"\"\"\n    wavelength_range = torch.linspace(400, 700, steps = self.primaries_spectrum.shape[-1], device = self.device)\n    red_spectrum = 1 / (14.5 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 650) ** 2 / (2 * 14.5 ** 2))\n    green_spectrum = 1 / (12 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 550) ** 2 / (2 * 12.0 ** 2))\n    blue_spectrum = 1 / (12 * (2 * torch.pi) ** 0.5) * torch.exp(-0.5 * (wavelength_range - 450) ** 2 / (2 * 12.0 ** 2))\n\n    red_spectrum = red_spectrum / red_spectrum.max()\n    green_spectrum = green_spectrum / green_spectrum.max()\n    blue_spectrum = blue_spectrum / blue_spectrum.max()\n\n    return red_spectrum, green_spectrum, blue_spectrum\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.lms_to_primaries","title":"<code>lms_to_primaries(lms_color_tensor)</code>","text":"<p>Internal function to convert LMS image to primaries space</p> <p>Parameters:</p> <ul> <li> <code>lms_color_tensor</code>           \u2013            <pre><code>                                  LMS data to be transformed to primaries space [Bx3xHxW]\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>primaries</code> (              <code>tensor</code> )          \u2013            <p>: Primaries data transformed from LMS space [BxPxHxW]</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def lms_to_primaries(self, lms_color_tensor):\n    \"\"\"\n    Internal function to convert LMS image to primaries space\n\n    Parameters\n    ----------\n    lms_color_tensor                        : torch.tensor\n                                              LMS data to be transformed to primaries space [Bx3xHxW]\n\n\n    Returns\n    -------\n    primaries                              : torch.tensor\n                                           : Primaries data transformed from LMS space [BxPxHxW]\n    \"\"\"\n    lms_color_tensor = lms_color_tensor.permute(0, 2, 3, 1).to(self.device)\n    lms_color_flatten = torch.flatten(lms_color_tensor, start_dim=0, end_dim=1)\n    unflatten = torch.nn.Unflatten(0, (lms_color_tensor.size(0), lms_color_tensor.size(1)))\n    converted_unflatten = torch.matmul(lms_color_flatten.double(), self.lms_tensor.pinverse().double())\n    primaries = unflatten(converted_unflatten)     \n    primaries = primaries.permute(0, 3, 1, 2)   \n    return primaries\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.primaries_to_lms","title":"<code>primaries_to_lms(primaries)</code>","text":"<p>Internal function to convert primaries space to LMS space </p> <p>Parameters:</p> <ul> <li> <code>primaries</code>           \u2013            <pre><code>                                 Primaries data to be transformed to LMS space [BxPHxW]\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>lms_color</code> (              <code>tensor</code> )          \u2013            <p>LMS data transformed from Primaries space [BxPxHxW]</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def primaries_to_lms(self, primaries):\n    \"\"\"\n    Internal function to convert primaries space to LMS space \n\n    Parameters\n    ----------\n    primaries                              : torch.tensor\n                                             Primaries data to be transformed to LMS space [BxPHxW]\n\n\n    Returns\n    -------\n    lms_color                              : torch.tensor\n                                             LMS data transformed from Primaries space [BxPxHxW]\n    \"\"\"                \n    primaries_flatten = primaries.reshape(primaries.shape[0], primaries.shape[1], 1, -1)\n    lms = self.lms_tensor.unsqueeze(0).unsqueeze(-1)\n    lms_color = torch.sum(primaries_flatten * lms, axis = 1).reshape(primaries.shape)\n    return lms_color\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.display_color_hvs.second_to_third_stage","title":"<code>second_to_third_stage(lms_image)</code>","text":"<p>This function turns second stage [L,M,S] values into third stage [(M+S)-L, (L+S)-M, L+M+S],  See table 1 from Schmidt et al. \"Neurobiological hypothesis of color appearance and hue perception,\" Optics Express 2014.</p> <p>Parameters:</p> <ul> <li> <code>lms_image</code>           \u2013            <pre><code>                                 Image data at LMS space (second stage)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>third_stage</code> (              <code>tensor</code> )          \u2013            <p>Image data at LMS space (third stage)</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def second_to_third_stage(self, lms_image):\n    '''\n    This function turns second stage [L,M,S] values into third stage [(M+S)-L, (L+S)-M, L+M+S], \n    See table 1 from Schmidt et al. \"Neurobiological hypothesis of color appearance and hue perception,\" Optics Express 2014.\n\n    Parameters\n    ----------\n    lms_image                             : torch.tensor\n                                             Image data at LMS space (second stage)\n\n    Returns\n    -------\n    third_stage                            : torch.tensor\n                                             Image data at LMS space (third stage)\n\n    '''\n    third_stage = torch.zeros_like(lms_image)\n    third_stage[:, 0] = (lms_image[:, 1] + lms_image[:, 2]) - lms_image[:, 1]\n    third_stage[:, 1] = (lms_image[:, 0] + lms_image[:, 2]) - lms_image[:, 1]\n    third_stage[:, 2] = lms_image[:, 0] + lms_image[:, 1]  + lms_image[:, 2]\n    return third_stage\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.color_map","title":"<code>color_map(input_image, target_image, model='Lab Stats')</code>","text":"<p>Internal function to map the color of an image to another image. Reference: Color transfer between images, Reinhard et al., 2001.</p> <p>Parameters:</p> <ul> <li> <code>input_image</code>           \u2013            <pre><code>              Input image in RGB color space [3 x m x n].\n</code></pre> </li> <li> <code>target_image</code>           \u2013            </li> </ul> <p>Returns:</p> <ul> <li> <code>mapped_image</code> (              <code>Tensor</code> )          \u2013            <p>Input image with the color the distribution of the target image [3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def color_map(input_image, target_image, model = 'Lab Stats'):\n    \"\"\"\n    Internal function to map the color of an image to another image.\n    Reference: Color transfer between images, Reinhard et al., 2001.\n\n    Parameters\n    ----------\n    input_image         : torch.Tensor\n                          Input image in RGB color space [3 x m x n].\n    target_image        : torch.Tensor\n\n    Returns\n    -------\n    mapped_image           : torch.Tensor\n                             Input image with the color the distribution of the target image [3 x m x n].\n    \"\"\"\n    if model == 'Lab Stats':\n        lab_input = srgb_to_lab(input_image)\n        lab_target = srgb_to_lab(target_image)\n        input_mean_L = torch.mean(lab_input[0, :, :])\n        input_mean_a = torch.mean(lab_input[1, :, :])\n        input_mean_b = torch.mean(lab_input[2, :, :])\n        input_std_L = torch.std(lab_input[0, :, :])\n        input_std_a = torch.std(lab_input[1, :, :])\n        input_std_b = torch.std(lab_input[2, :, :])\n        target_mean_L = torch.mean(lab_target[0, :, :])\n        target_mean_a = torch.mean(lab_target[1, :, :])\n        target_mean_b = torch.mean(lab_target[2, :, :])\n        target_std_L = torch.std(lab_target[0, :, :])\n        target_std_a = torch.std(lab_target[1, :, :])\n        target_std_b = torch.std(lab_target[2, :, :])\n        lab_input[0, :, :] = (lab_input[0, :, :] - input_mean_L) * (target_std_L / input_std_L) + target_mean_L\n        lab_input[1, :, :] = (lab_input[1, :, :] - input_mean_a) * (target_std_a / input_std_a) + target_mean_a\n        lab_input[2, :, :] = (lab_input[2, :, :] - input_mean_b) * (target_std_b / input_std_b) + target_mean_b\n        mapped_image = lab_to_srgb(lab_input.permute(1, 2, 0))\n        return mapped_image\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.hsv_to_rgb","title":"<code>hsv_to_rgb(image)</code>","text":"<p>Definition to convert HSV space to  RGB color space. Mostly inspired from : https://kornia.readthedocs.io/en/latest/_modules/kornia/color/hsv.html</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in HSV color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_rgb</code> (              <code>tensor</code> )          \u2013            <p>Output image in  RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def hsv_to_rgb(image):\n\n    \"\"\"\n    Definition to convert HSV space to  RGB color space. Mostly inspired from : https://kornia.readthedocs.io/en/latest/_modules/kornia/color/hsv.html\n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in HSV color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n\n    Returns\n    -------\n    image_rgb       : torch.tensor\n                      Output image in  RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    h = image[..., 0, :, :] / (2 * math.pi)\n    s = image[..., 1, :, :]\n    v = image[..., 2, :, :]\n    hi = torch.floor(h * 6) % 6\n    f = ((h * 6) % 6) - hi\n    one = torch.tensor(1.0)\n    p = v * (one - s)\n    q = v * (one - f * s)\n    t = v * (one - (one - f) * s)\n    hi = hi.long()\n    indices = torch.stack([hi, hi + 6, hi + 12], dim=-3)\n    image_rgb = torch.stack((v, q, p, p, t, v, t, v, v, q, p, p, p, p, t, v, v, q), dim=-3)\n    image_rgb = torch.gather(image_rgb, -3, indices)\n    return image_rgb\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.lab_to_srgb","title":"<code>lab_to_srgb(image)</code>","text":"<p>Definition to convert LAB space to SRGB color space. </p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in LAB color space[3 x m x n]\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_srgb</code> (              <code>tensor</code> )          \u2013            <p>Output image in SRGB color space [3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def lab_to_srgb(image):\n    \"\"\"\n    Definition to convert LAB space to SRGB color space. \n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in LAB color space[3 x m x n]\n    Returns\n    -------\n    image_srgb     : torch.tensor\n                      Output image in SRGB color space [3 x m x n].\n    \"\"\"\n\n    if image.shape[-1] == 3:\n        input_color = image.permute(2, 0, 1)  # C(H*W)\n    else:\n        input_color = image\n    # lab ---&gt; xyz\n    reference_illuminant = torch.tensor([[[0.950428545]], [[1.000000000]], [[1.088900371]]], dtype=torch.float32)\n    y = (input_color[0:1, :, :] + 16) / 116\n    a =  input_color[1:2, :, :] / 500\n    b =  input_color[2:3, :, :] / 200\n    x = y + a\n    z = y - b\n    xyz = torch.cat((x, y, z), 0)\n    delta = 6 / 29\n    factor = 3 * delta * delta\n    xyz = torch.where(xyz &gt; delta,  xyz ** 3, factor * (xyz - 4 / 29))\n    xyz_color = xyz * reference_illuminant\n    # xyz ---&gt; linear rgb\n    a11 = 3.241003275\n    a12 = -1.537398934\n    a13 = -0.498615861\n    a21 = -0.969224334\n    a22 = 1.875930071\n    a23 = 0.041554224\n    a31 = 0.055639423\n    a32 = -0.204011202\n    a33 = 1.057148933\n    A = torch.tensor([[a11, a12, a13],\n                  [a21, a22, a23],\n                  [a31, a32, a33]], dtype=torch.float32)\n\n    xyz_color = xyz_color.permute(2, 0, 1) # C(H*W)\n    linear_rgb_color = torch.matmul(A, xyz_color)\n    linear_rgb_color = linear_rgb_color.permute(1, 2, 0)\n    # linear rgb ---&gt; srgb\n    limit = 0.0031308\n    image_srgb = torch.where(linear_rgb_color &gt; limit, 1.055 * (linear_rgb_color ** (1.0 / 2.4)) - 0.055, 12.92 * linear_rgb_color)\n    return image_srgb\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.linear_rgb_to_rgb","title":"<code>linear_rgb_to_rgb(image, threshold=0.0031308)</code>","text":"<p>Definition to convert linear RGB images to RGB color space. Mostly inspired from: https://kornia.readthedocs.io/en/latest/_modules/kornia/color/rgb.html</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in linear RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> <li> <code>threshold</code>           \u2013            <pre><code>          Threshold used in calculations.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_linear</code> (              <code>tensor</code> )          \u2013            <p>Output image in RGB color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def linear_rgb_to_rgb(image, threshold = 0.0031308):\n    \"\"\"\n    Definition to convert linear RGB images to RGB color space. Mostly inspired from: https://kornia.readthedocs.io/en/latest/_modules/kornia/color/rgb.html\n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in linear RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n    threshold       : float\n                      Threshold used in calculations.\n\n    Returns\n    -------\n    image_linear    : torch.tensor\n                      Output image in RGB color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    image_linear =  torch.where(image &gt; threshold, 1.055 * torch.pow(image.clamp(min=threshold), 1 / 2.4) - 0.055, 12.92 * image)\n    return image_linear\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.linear_rgb_to_xyz","title":"<code>linear_rgb_to_xyz(image)</code>","text":"<p>Definition to convert RGB space to CIE XYZ color space. Mostly inspired from : Rochester IT Color Conversion Algorithms (https://www.cs.rit.edu/~ncs/color/)</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in linear RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_xyz</code> (              <code>tensor</code> )          \u2013            <p>Output image in XYZ (CIE 1931) color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def linear_rgb_to_xyz(image):\n    \"\"\"\n    Definition to convert RGB space to CIE XYZ color space. Mostly inspired from : Rochester IT Color Conversion Algorithms (https://www.cs.rit.edu/~ncs/color/)\n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in linear RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n\n    Returns\n    -------\n    image_xyz       : torch.tensor\n                      Output image in XYZ (CIE 1931) color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    a11 = 0.412453\n    a12 = 0.357580\n    a13 = 0.180423\n    a21 = 0.212671\n    a22 = 0.715160\n    a23 = 0.072169\n    a31 = 0.019334\n    a32 = 0.119193\n    a33 = 0.950227\n    M = torch.tensor([[a11, a12, a13], \n                      [a21, a22, a23],\n                      [a31, a32, a33]])\n    size = image.size()\n    image = image.reshape(size[0], size[1], size[2]*size[3])  # NC(HW)\n    image_xyz = torch.matmul(M, image)\n    image_xyz = image_xyz.reshape(size[0], size[1], size[2], size[3])\n    return image_xyz\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.rgb_2_ycrcb","title":"<code>rgb_2_ycrcb(image)</code>","text":"<p>Converts an image from RGB colourspace to YCrCb colourspace.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>  Input image. Should be an RGB floating-point image with values in the range [0, 1]. Should be in NCHW format [3 x m x n] or [k x 3 x m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ycrcb</code> (              <code>tensor</code> )          \u2013            <p>Image converted to YCrCb colourspace [k x 3 m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def rgb_2_ycrcb(image):\n    \"\"\"\n    Converts an image from RGB colourspace to YCrCb colourspace.\n\n    Parameters\n    ----------\n    image   : torch.tensor\n              Input image. Should be an RGB floating-point image with values in the range [0, 1]. Should be in NCHW format [3 x m x n] or [k x 3 x m x n].\n\n    Returns\n    -------\n\n    ycrcb   : torch.tensor\n              Image converted to YCrCb colourspace [k x 3 m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n       image = image.unsqueeze(0)\n    ycrcb = torch.zeros(image.size()).to(image.device)\n    ycrcb[:, 0, :, :] = 0.299 * image[:, 0, :, :] + 0.587 * \\\n        image[:, 1, :, :] + 0.114 * image[:, 2, :, :]\n    ycrcb[:, 1, :, :] = 0.5 + 0.713 * (image[:, 0, :, :] - ycrcb[:, 0, :, :])\n    ycrcb[:, 2, :, :] = 0.5 + 0.564 * (image[:, 2, :, :] - ycrcb[:, 0, :, :])\n    return ycrcb\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.rgb_to_hsv","title":"<code>rgb_to_hsv(image, eps=1e-08)</code>","text":"<p>Definition to convert RGB space to HSV color space. Mostly inspired from : https://kornia.readthedocs.io/en/latest/_modules/kornia/color/hsv.html</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in HSV color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_hsv</code> (              <code>tensor</code> )          \u2013            <p>Output image in  RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def rgb_to_hsv(image, eps: float = 1e-8):\n\n    \"\"\"\n    Definition to convert RGB space to HSV color space. Mostly inspired from : https://kornia.readthedocs.io/en/latest/_modules/kornia/color/hsv.html\n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in HSV color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n\n    Returns\n    -------\n    image_hsv       : torch.tensor\n                      Output image in  RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    max_rgb, argmax_rgb = image.max(-3)\n    min_rgb, argmin_rgb = image.min(-3)\n    deltac = max_rgb - min_rgb\n    v = max_rgb\n    s = deltac / (max_rgb + eps)\n    deltac = torch.where(deltac == 0, torch.ones_like(deltac), deltac)\n    rc, gc, bc = torch.unbind((max_rgb.unsqueeze(-3) - image), dim=-3)\n    h1 = bc - gc\n    h2 = (rc - bc) + 2.0 * deltac\n    h3 = (gc - rc) + 4.0 * deltac\n    h = torch.stack((h1, h2, h3), dim=-3) / deltac.unsqueeze(-3)\n    h = torch.gather(h, dim=-3, index=argmax_rgb.unsqueeze(-3)).squeeze(-3)\n    h = (h / 6.0) % 1.0\n    h = 2.0 * math.pi * h \n    image_hsv = torch.stack((h, s, v), dim=-3)\n    return image_hsv\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.rgb_to_linear_rgb","title":"<code>rgb_to_linear_rgb(image, threshold=0.0031308)</code>","text":"<p>Definition to convert RGB images to linear RGB color space. Mostly inspired from: https://kornia.readthedocs.io/en/latest/_modules/kornia/color/rgb.html</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> <li> <code>threshold</code>           \u2013            <pre><code>          Threshold used in calculations.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_linear</code> (              <code>tensor</code> )          \u2013            <p>Output image in linear RGB color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def rgb_to_linear_rgb(image, threshold = 0.0031308):\n    \"\"\"\n    Definition to convert RGB images to linear RGB color space. Mostly inspired from: https://kornia.readthedocs.io/en/latest/_modules/kornia/color/rgb.html\n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in RGB color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n    threshold       : float\n                      Threshold used in calculations.\n\n    Returns\n    -------\n    image_linear    : torch.tensor\n                      Output image in linear RGB color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    image_linear = torch.where(image &gt; 0.04045, torch.pow(((image + 0.055) / 1.055), 2.4), image / 12.92)\n    return image_linear\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.srgb_to_lab","title":"<code>srgb_to_lab(image)</code>","text":"<p>Definition to convert SRGB space to LAB color space. </p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>          Input image in SRGB color space[3 x m x n]\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_lab</code> (              <code>tensor</code> )          \u2013            <p>Output image in LAB color space [3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def srgb_to_lab(image):    \n    \"\"\"\n    Definition to convert SRGB space to LAB color space. \n\n    Parameters\n    ----------\n    image           : torch.tensor\n                      Input image in SRGB color space[3 x m x n]\n    Returns\n    -------\n    image_lab       : torch.tensor\n                      Output image in LAB color space [3 x m x n].\n    \"\"\"\n    if image.shape[-1] == 3:\n        input_color = image.permute(2, 0, 1)  # C(H*W)\n    else:\n        input_color = image\n    # rgb ---&gt; linear rgb\n    limit = 0.04045        \n    # linear rgb ---&gt; xyz\n    linrgb_color = torch.where(input_color &gt; limit, torch.pow((input_color + 0.055) / 1.055, 2.4), input_color / 12.92)\n\n    a11 = 10135552 / 24577794\n    a12 = 8788810  / 24577794\n    a13 = 4435075  / 24577794\n    a21 = 2613072  / 12288897\n    a22 = 8788810  / 12288897\n    a23 = 887015   / 12288897\n    a31 = 1425312  / 73733382\n    a32 = 8788810  / 73733382\n    a33 = 70074185 / 73733382\n\n    A = torch.tensor([[a11, a12, a13],\n                    [a21, a22, a23],\n                    [a31, a32, a33]], dtype=torch.float32)\n\n    linrgb_color = linrgb_color.permute(2, 0, 1) # C(H*W)\n    xyz_color = torch.matmul(A, linrgb_color)\n    xyz_color = xyz_color.permute(1, 2, 0)\n    # xyz ---&gt; lab\n    inv_reference_illuminant = torch.tensor([[[1.052156925]], [[1.000000000]], [[0.918357670]]], dtype=torch.float32)\n    input_color = xyz_color * inv_reference_illuminant\n    delta = 6 / 29\n    delta_square = delta * delta\n    delta_cube = delta * delta_square\n    factor = 1 / (3 * delta_square)\n\n    input_color = torch.where(input_color &gt; delta_cube, torch.pow(input_color, 1 / 3), (factor * input_color + 4 / 29))\n\n    l = 116 * input_color[1:2, :, :] - 16\n    a = 500 * (input_color[0:1,:, :] - input_color[1:2, :, :])\n    b = 200 * (input_color[1:2, :, :] - input_color[2:3, :, :])\n\n    image_lab = torch.cat((l, a, b), 0)\n    return image_lab    \n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.xyz_to_linear_rgb","title":"<code>xyz_to_linear_rgb(image)</code>","text":"<p>Definition to convert CIE XYZ space to linear RGB color space. Mostly inspired from : Rochester IT Color Conversion Algorithms (https://www.cs.rit.edu/~ncs/color/)</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>           Input image in XYZ (CIE 1931) color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image_linear_rgb</code> (              <code>tensor</code> )          \u2013            <p>Output image in linear RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def xyz_to_linear_rgb(image):\n    \"\"\"\n    Definition to convert CIE XYZ space to linear RGB color space. Mostly inspired from : Rochester IT Color Conversion Algorithms (https://www.cs.rit.edu/~ncs/color/)\n\n    Parameters\n    ----------\n    image            : torch.tensor\n                       Input image in XYZ (CIE 1931) color space [k x 3 x m x n] or [3 x m x n]. Image(s) must be normalized between zero and one.\n\n    Returns\n    -------\n    image_linear_rgb : torch.tensor\n                       Output image in linear RGB  color space [k x 3 x m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    a11 = 3.240479\n    a12 = -1.537150\n    a13 = -0.498535\n    a21 = -0.969256 \n    a22 = 1.875992 \n    a23 = 0.041556\n    a31 = 0.055648\n    a32 = -0.204043\n    a33 = 1.057311\n    M = torch.tensor([[a11, a12, a13], \n                      [a21, a22, a23],\n                      [a31, a32, a33]])\n    size = image.size()\n    image = image.reshape(size[0], size[1], size[2]*size[3])\n    image_linear_rgb = torch.matmul(M, image)\n    image_linear_rgb = image_linear_rgb.reshape(size[0], size[1], size[2], size[3])\n    return image_linear_rgb\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.color_conversion.ycrcb_2_rgb","title":"<code>ycrcb_2_rgb(image)</code>","text":"<p>Converts an image from YCrCb colourspace to RGB colourspace.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>  Input image. Should be a YCrCb floating-point image with values in the range [0, 1]. Should be in NCHW format [3 x m x n] or [k x 3 x m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rgb</code> (              <code>tensor</code> )          \u2013            <p>Image converted to RGB colourspace [k x 3 m x n] or [1 x 3 x m x n].</p> </li> </ul> Source code in <code>odak/learn/perception/color_conversion.py</code> <pre><code>def ycrcb_2_rgb(image):\n    \"\"\"\n    Converts an image from YCrCb colourspace to RGB colourspace.\n\n    Parameters\n    ----------\n    image   : torch.tensor\n              Input image. Should be a YCrCb floating-point image with values in the range [0, 1]. Should be in NCHW format [3 x m x n] or [k x 3 x m x n].\n\n    Returns\n    -------\n    rgb     : torch.tensor\n              Image converted to RGB colourspace [k x 3 m x n] or [1 x 3 x m x n].\n    \"\"\"\n    if len(image.shape) == 3:\n       image = image.unsqueeze(0)\n    rgb = torch.zeros(image.size(), device=image.device)\n    rgb[:, 0, :, :] = image[:, 0, :, :] + 1.403 * (image[:, 1, :, :] - 0.5)\n    rgb[:, 1, :, :] = image[:, 0, :, :] - 0.714 * \\\n        (image[:, 1, :, :] - 0.5) - 0.344 * (image[:, 2, :, :] - 0.5)\n    rgb[:, 2, :, :] = image[:, 0, :, :] + 1.773 * (image[:, 2, :, :] - 0.5)\n    return rgb\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.foveation.make_3d_location_map","title":"<code>make_3d_location_map(image_pixel_size, real_image_width=0.3, real_viewing_distance=0.6)</code>","text":"<p>Makes a map of the real 3D location that each pixel in an image corresponds to, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is  perpendicular to the viewing direction.</p> <p>Parameters:</p> <ul> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                    The size of the image in pixels, as a tuple of form (height, width)\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed. Units not important, as long as they\n                    are the same as those used for real_viewing_distance\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance from the user's viewpoint to the screen.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>map</code> (              <code>tensor</code> )          \u2013            <p>The computed 3D location map, of size 3xWxH.</p> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_3d_location_map(image_pixel_size, real_image_width=0.3, real_viewing_distance=0.6):\n    \"\"\" \n    Makes a map of the real 3D location that each pixel in an image corresponds to, when displayed to\n    a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is \n    perpendicular to the viewing direction.\n\n    Parameters\n    ----------\n\n    image_pixel_size        : tuple of ints \n                                The size of the image in pixels, as a tuple of form (height, width)\n    real_image_width        : float\n                                The real width of the image as displayed. Units not important, as long as they\n                                are the same as those used for real_viewing_distance\n    real_viewing_distance   : float \n                                The real distance from the user's viewpoint to the screen.\n\n    Returns\n    -------\n\n    map                     : torch.tensor\n                                The computed 3D location map, of size 3xWxH.\n    \"\"\"\n    real_image_height = (real_image_width /\n                         image_pixel_size[-1]) * image_pixel_size[-2]\n    x_coords = torch.linspace(-0.5, 0.5, image_pixel_size[-1])*real_image_width\n    x_coords = x_coords[None, None, :].repeat(1, image_pixel_size[-2], 1)\n    y_coords = torch.linspace(-0.5, 0.5,\n                              image_pixel_size[-2])*real_image_height\n    y_coords = y_coords[None, :, None].repeat(1, 1, image_pixel_size[-1])\n    z_coords = torch.ones(\n        (1, image_pixel_size[-2], image_pixel_size[-1])) * real_viewing_distance\n\n    return torch.cat([x_coords, y_coords, z_coords], dim=0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.foveation.make_eccentricity_distance_maps","title":"<code>make_eccentricity_distance_maps(gaze_location, image_pixel_size, real_image_width=0.3, real_viewing_distance=0.6)</code>","text":"<p>Makes a map of the eccentricity of each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is  perpendicular to the viewing direction. Output in radians.</p> <p>Parameters:</p> <ul> <li> <code>gaze_location</code>           \u2013            <pre><code>                    User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                    image coordinates (ranging from 0 to 1)\n</code></pre> </li> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                    The size of the image in pixels, as a tuple of form (height, width)\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed. Units not important, as long as they\n                    are the same as those used for real_viewing_distance\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance from the user's viewpoint to the screen.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>eccentricity_map</code> (              <code>tensor</code> )          \u2013            <p>The computed eccentricity map, of size WxH.</p> </li> <li> <code>distance_map</code> (              <code>tensor</code> )          \u2013            <p>The computed distance map, of size WxH.</p> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_eccentricity_distance_maps(gaze_location, image_pixel_size, real_image_width=0.3, real_viewing_distance=0.6):\n    \"\"\" \n    Makes a map of the eccentricity of each pixel in an image for a given fixation point, when displayed to\n    a user on a flat screen. Assumes the viewpoint is located at the centre of the image, and the screen is \n    perpendicular to the viewing direction. Output in radians.\n\n    Parameters\n    ----------\n\n    gaze_location           : tuple of floats\n                                User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                                image coordinates (ranging from 0 to 1)\n    image_pixel_size        : tuple of ints\n                                The size of the image in pixels, as a tuple of form (height, width)\n    real_image_width        : float\n                                The real width of the image as displayed. Units not important, as long as they\n                                are the same as those used for real_viewing_distance\n    real_viewing_distance   : float\n                                The real distance from the user's viewpoint to the screen.\n\n    Returns\n    -------\n\n    eccentricity_map        : torch.tensor\n                                The computed eccentricity map, of size WxH.\n    distance_map            : torch.tensor\n                                The computed distance map, of size WxH.\n    \"\"\"\n    real_image_height = (real_image_width /\n                         image_pixel_size[-1]) * image_pixel_size[-2]\n    location_map = make_3d_location_map(\n        image_pixel_size, real_image_width, real_viewing_distance)\n    distance_map = torch.sqrt(torch.sum(location_map*location_map, dim=0))\n    direction_map = location_map / distance_map\n\n    gaze_location_3d = torch.tensor([\n        (gaze_location[0]*2 - 1)*real_image_width*0.5,\n        (gaze_location[1]*2 - 1)*real_image_height*0.5,\n        real_viewing_distance])\n    gaze_dir = gaze_location_3d / \\\n        torch.sqrt(torch.sum(gaze_location_3d * gaze_location_3d))\n    gaze_dir = gaze_dir[:, None, None]\n\n    dot_prod_map = torch.sum(gaze_dir * direction_map, dim=0)\n    dot_prod_map = torch.clamp(dot_prod_map, min=-1.0, max=1.0)\n    eccentricity_map = torch.acos(dot_prod_map)\n\n    return eccentricity_map, distance_map\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.foveation.make_equi_pooling_size_map_lod","title":"<code>make_equi_pooling_size_map_lod(gaze_angles, image_pixel_size, alpha=0.3, mode='quadratic')</code>","text":"<p>This function is similar to make_equi_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from to achieve the correct pooling region areas.</p> <p>Parameters:</p> <ul> <li> <code>gaze_angles</code>           \u2013            <pre><code>                Gaze direction expressed as angles, in radians.\n</code></pre> </li> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                Dimensions of the image in pixels, as a tuple of (height, width)\n</code></pre> </li> <li> <code>alpha</code>           \u2013            <pre><code>                Parameter controlling extent of foveation\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                Foveation mode (how pooling size varies with eccentricity). Should be \"quadratic\" or \"linear\"\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>pooling_size_map</code> (              <code>tensor</code> )          \u2013            <p>The computed pooling size map, of size HxW.</p> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_equi_pooling_size_map_lod(gaze_angles, image_pixel_size, alpha=0.3, mode=\"quadratic\"):\n    \"\"\" \n    This function is similar to make_equi_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from\n    to achieve the correct pooling region areas.\n\n    Parameters\n    ----------\n\n    gaze_angles         : tuple of 2 floats\n                            Gaze direction expressed as angles, in radians.\n    image_pixel_size    : tuple of 2 ints\n                            Dimensions of the image in pixels, as a tuple of (height, width)\n    alpha               : float\n                            Parameter controlling extent of foveation\n    mode                : str\n                            Foveation mode (how pooling size varies with eccentricity). Should be \"quadratic\" or \"linear\"\n\n    Returns\n    -------\n\n    pooling_size_map        : torch.tensor\n                                The computed pooling size map, of size HxW.\n    \"\"\"\n    pooling_pixel = make_equi_pooling_size_map_pixels(gaze_angles, image_pixel_size, alpha, mode)\n    pooling_lod = torch.log2(1e-6 + pooling_pixel)\n    pooling_lod[pooling_lod &lt; 0] = 0\n    return pooling_lod\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.foveation.make_equi_pooling_size_map_pixels","title":"<code>make_equi_pooling_size_map_pixels(gaze_angles, image_pixel_size, alpha=0.3, mode='quadratic')</code>","text":"<p>This function makes a map of pooling sizes in pixels, similarly to make_pooling_size_map_pixels, but works on 360 equirectangular images. Input images are assumed to be in equirectangular form - i.e. if you consider a 3D viewing setup where y is the vertical axis,  the x location in the image corresponds to rotation around the y axis (yaw), ranging from -pi to pi. The y location in the image corresponds to pitch, ranging from -pi/2 to pi/2.</p> <p>In this setup real_image_width and real_viewing_distance have no effect.</p> <p>Note that rather than a 2D image gaze location in [0,1]^2, the gaze should be specified as gaze angles in [-pi,pi]x[-pi/2,pi/2] (yaw, then pitch).</p> <p>Parameters:</p> <ul> <li> <code>gaze_angles</code>           \u2013            <pre><code>                Gaze direction expressed as angles, in radians.\n</code></pre> </li> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                Dimensions of the image in pixels, as a tuple of (height, width)\n</code></pre> </li> <li> <code>alpha</code>           \u2013            <pre><code>                Parameter controlling extent of foveation\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                Foveation mode (how pooling size varies with eccentricity). Should be \"quadratic\" or \"linear\"\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_equi_pooling_size_map_pixels(gaze_angles, image_pixel_size, alpha=0.3, mode=\"quadratic\"):\n    \"\"\"\n    This function makes a map of pooling sizes in pixels, similarly to make_pooling_size_map_pixels, but works on 360 equirectangular images.\n    Input images are assumed to be in equirectangular form - i.e. if you consider a 3D viewing setup where y is the vertical axis, \n    the x location in the image corresponds to rotation around the y axis (yaw), ranging from -pi to pi. The y location in the image\n    corresponds to pitch, ranging from -pi/2 to pi/2.\n\n    In this setup real_image_width and real_viewing_distance have no effect.\n\n    Note that rather than a 2D image gaze location in [0,1]^2, the gaze should be specified as gaze angles in [-pi,pi]x[-pi/2,pi/2] (yaw, then pitch).\n\n    Parameters\n    ----------\n\n    gaze_angles         : tuple of 2 floats\n                            Gaze direction expressed as angles, in radians.\n    image_pixel_size    : tuple of 2 ints\n                            Dimensions of the image in pixels, as a tuple of (height, width)\n    alpha               : float\n                            Parameter controlling extent of foveation\n    mode                : str\n                            Foveation mode (how pooling size varies with eccentricity). Should be \"quadratic\" or \"linear\"\n    \"\"\"\n    view_direction = torch.tensor([math.sin(gaze_angles[0])*math.cos(gaze_angles[1]), math.sin(gaze_angles[1]), math.cos(gaze_angles[0])*math.cos(gaze_angles[1])])\n\n    yaw_angle_map = torch.linspace(-torch.pi, torch.pi, image_pixel_size[1])\n    yaw_angle_map = yaw_angle_map[None,:].repeat(image_pixel_size[0], 1)[None,...]\n    pitch_angle_map = torch.linspace(-torch.pi*0.5, torch.pi*0.5, image_pixel_size[0])\n    pitch_angle_map = pitch_angle_map[:,None].repeat(1, image_pixel_size[1])[None,...]\n\n    dir_map = torch.cat([torch.sin(yaw_angle_map)*torch.cos(pitch_angle_map), torch.sin(pitch_angle_map), torch.cos(yaw_angle_map)*torch.cos(pitch_angle_map)])\n\n    # Work out the pooling region diameter in radians\n    view_dot_dir = torch.sum(view_direction[:,None,None] * dir_map, dim=0)\n    eccentricity = torch.acos(view_dot_dir)\n    pooling_rad = alpha * eccentricity\n    if mode == \"quadratic\":\n        pooling_rad *= eccentricity\n\n    # The actual pooling region will be an ellipse in the equirectangular image - the length of the major &amp; minor axes\n    # depend on the x &amp; y resolution of the image. We find these two axis lengths (in pixels) and then the area of the ellipse\n    pixels_per_rad_x = image_pixel_size[1] / (2*torch.pi)\n    pixels_per_rad_y = image_pixel_size[0] / (torch.pi)\n    pooling_axis_x = pooling_rad * pixels_per_rad_x\n    pooling_axis_y = pooling_rad * pixels_per_rad_y\n    area = torch.pi * pooling_axis_x * pooling_axis_y * 0.25\n\n    # Now finally find the length of the side of a square of the same area.\n    size = torch.sqrt(torch.abs(area))\n    return size\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.foveation.make_pooling_size_map_lod","title":"<code>make_pooling_size_map_lod(gaze_location, image_pixel_size, alpha=0.3, real_image_width=0.3, real_viewing_distance=0.6, mode='quadratic')</code>","text":"<p>This function is similar to make_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from to achieve the correct pooling region areas.</p> <p>Parameters:</p> <ul> <li> <code>gaze_location</code>           \u2013            <pre><code>                    User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                    image coordinates (ranging from 0 to 1)\n</code></pre> </li> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                    The size of the image in pixels, as a tuple of form (height, width)\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed. Units not important, as long as they\n                    are the same as those used for real_viewing_distance\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance from the user's viewpoint to the screen.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>pooling_size_map</code> (              <code>tensor</code> )          \u2013            <p>The computed pooling size map, of size WxH.</p> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_pooling_size_map_lod(gaze_location, image_pixel_size, alpha=0.3, real_image_width=0.3, real_viewing_distance=0.6, mode=\"quadratic\"):\n    \"\"\" \n    This function is similar to make_pooling_size_map_pixels, but instead returns a map of LOD levels to sample from\n    to achieve the correct pooling region areas.\n\n    Parameters\n    ----------\n\n    gaze_location           : tuple of floats\n                                User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                                image coordinates (ranging from 0 to 1)\n    image_pixel_size        : tuple of ints\n                                The size of the image in pixels, as a tuple of form (height, width)\n    real_image_width        : float\n                                The real width of the image as displayed. Units not important, as long as they\n                                are the same as those used for real_viewing_distance\n    real_viewing_distance   : float\n                                The real distance from the user's viewpoint to the screen.\n\n    Returns\n    -------\n\n    pooling_size_map        : torch.tensor\n                                The computed pooling size map, of size WxH.\n    \"\"\"\n    pooling_pixel = make_pooling_size_map_pixels(\n        gaze_location, image_pixel_size, alpha, real_image_width, real_viewing_distance, mode)\n    pooling_lod = torch.log2(1e-6+pooling_pixel)\n    pooling_lod[pooling_lod &lt; 0] = 0\n    return pooling_lod\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.foveation.make_pooling_size_map_pixels","title":"<code>make_pooling_size_map_pixels(gaze_location, image_pixel_size, alpha=0.3, real_image_width=0.3, real_viewing_distance=0.6, mode='quadratic')</code>","text":"<p>Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity (also in radians). </p> <p>Assumes the viewpoint is located at the centre of the image, and the screen is  perpendicular to the viewing direction. Output is the width of the pooling region in pixels.</p> <p>Parameters:</p> <ul> <li> <code>gaze_location</code>           \u2013            <pre><code>                    User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                    image coordinates (ranging from 0 to 1)\n</code></pre> </li> <li> <code>image_pixel_size</code>           \u2013            <pre><code>                    The size of the image in pixels, as a tuple of form (height, width)\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed. Units not important, as long as they\n                    are the same as those used for real_viewing_distance\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance from the user's viewpoint to the screen.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>pooling_size_map</code> (              <code>tensor</code> )          \u2013            <p>The computed pooling size map, of size WxH.</p> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_pooling_size_map_pixels(gaze_location, image_pixel_size, alpha=0.3, real_image_width=0.3, real_viewing_distance=0.6, mode=\"quadratic\"):\n    \"\"\" \n    Makes a map of the pooling size associated with each pixel in an image for a given fixation point, when displayed to\n    a user on a flat screen. Follows the idea that pooling size (in radians) should be directly proportional to eccentricity\n    (also in radians). \n\n    Assumes the viewpoint is located at the centre of the image, and the screen is \n    perpendicular to the viewing direction. Output is the width of the pooling region in pixels.\n\n    Parameters\n    ----------\n\n    gaze_location           : tuple of floats\n                                User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                                image coordinates (ranging from 0 to 1)\n    image_pixel_size        : tuple of ints\n                                The size of the image in pixels, as a tuple of form (height, width)\n    real_image_width        : float\n                                The real width of the image as displayed. Units not important, as long as they\n                                are the same as those used for real_viewing_distance\n    real_viewing_distance   : float\n                                The real distance from the user's viewpoint to the screen.\n\n    Returns\n    -------\n\n    pooling_size_map        : torch.tensor\n                                The computed pooling size map, of size WxH.\n    \"\"\"\n    eccentricity, distance_to_pixel = make_eccentricity_distance_maps(\n        gaze_location, image_pixel_size, real_image_width, real_viewing_distance)\n    eccentricity_centre, _ = make_eccentricity_distance_maps(\n        [0.5, 0.5], image_pixel_size, real_image_width, real_viewing_distance)\n    pooling_rad = alpha * eccentricity\n    if mode == \"quadratic\":\n        pooling_rad *= eccentricity\n    angle_min = eccentricity_centre - pooling_rad*0.5\n    angle_max = eccentricity_centre + pooling_rad*0.5\n    major_axis = (torch.tan(angle_max) - torch.tan(angle_min)) * \\\n        real_viewing_distance\n    minor_axis = 2 * distance_to_pixel * torch.tan(pooling_rad*0.5)\n    area = math.pi * major_axis * minor_axis * 0.25\n    # Should be +ve anyway, but check to ensure we don't take sqrt of negative number\n    area = torch.abs(area)\n    pooling_real = torch.sqrt(area)\n    pooling_pixel = (pooling_real / real_image_width) * image_pixel_size[1]\n    return pooling_pixel\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.foveation.make_radial_map","title":"<code>make_radial_map(size, gaze)</code>","text":"<p>Makes a simple radial map where each pixel contains distance in pixels from the chosen gaze location.</p> <p>Parameters:</p> <ul> <li> <code>size</code>           \u2013            <pre><code>    Dimensions of the image\n</code></pre> </li> <li> <code>gaze</code>           \u2013            <pre><code>    User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n    image coordinates (ranging from 0 to 1)\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/foveation.py</code> <pre><code>def make_radial_map(size, gaze):\n    \"\"\" \n    Makes a simple radial map where each pixel contains distance in pixels from the chosen gaze location.\n\n    Parameters\n    ----------\n\n    size    : tuple of ints\n                Dimensions of the image\n    gaze    : tuple of floats\n                User's gaze (fixation point) in the image. Should be given as a tuple with normalized\n                image coordinates (ranging from 0 to 1)\n    \"\"\"\n    pix_gaze = [gaze[0]*size[0], gaze[1]*size[1]]\n    rows = torch.linspace(0, size[0], size[0])\n    rows = rows[:, None].repeat(1, size[1])\n    cols = torch.linspace(0, size[1], size[1])\n    cols = cols[None, :].repeat(size[0], 1)\n    dist_sq = torch.pow(rows - pix_gaze[0], 2) + \\\n        torch.pow(cols - pix_gaze[1], 2)\n    radii = torch.sqrt(dist_sq)\n    return radii/torch.max(radii)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.image_quality_losses.MSSSIM","title":"<code>MSSSIM</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class to calculate multi-scale structural similarity index of an image with respect to a ground truth image.</p> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>class MSSSIM(nn.Module):\n    '''\n    A class to calculate multi-scale structural similarity index of an image with respect to a ground truth image.\n    '''\n\n    def __init__(self):\n        super(MSSSIM, self).__init__()\n\n    def forward(self, predictions, targets):\n        \"\"\"\n        Parameters\n        ----------\n        predictions : torch.tensor\n                      The predicted images.\n        targets     : torch.tensor\n                      The ground truth images.\n\n        Returns\n        -------\n        result      : torch.tensor \n                      The computed MS-SSIM value if successful, otherwise 0.0.\n        \"\"\"\n        try:\n            from torchmetrics.functional.image import multiscale_structural_similarity_index_measure\n            if len(predictions.shape) == 3:\n                predictions = predictions.unsqueeze(0)\n                targets = targets.unsqueeze(0)\n            l_MSSSIM = multiscale_structural_similarity_index_measure(predictions, targets, data_range = 1.0)\n            return l_MSSSIM  \n        except Exception as e:\n            logging.warning('MS-SSIM failed to compute.')\n            logging.warning(e)\n            return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.image_quality_losses.MSSSIM.forward","title":"<code>forward(predictions, targets)</code>","text":"<p>Parameters:</p> <ul> <li> <code>predictions</code>               (<code>tensor</code>)           \u2013            <pre><code>      The predicted images.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>      The ground truth images.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>The computed MS-SSIM value if successful, otherwise 0.0.</p> </li> </ul> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>def forward(self, predictions, targets):\n    \"\"\"\n    Parameters\n    ----------\n    predictions : torch.tensor\n                  The predicted images.\n    targets     : torch.tensor\n                  The ground truth images.\n\n    Returns\n    -------\n    result      : torch.tensor \n                  The computed MS-SSIM value if successful, otherwise 0.0.\n    \"\"\"\n    try:\n        from torchmetrics.functional.image import multiscale_structural_similarity_index_measure\n        if len(predictions.shape) == 3:\n            predictions = predictions.unsqueeze(0)\n            targets = targets.unsqueeze(0)\n        l_MSSSIM = multiscale_structural_similarity_index_measure(predictions, targets, data_range = 1.0)\n        return l_MSSSIM  \n    except Exception as e:\n        logging.warning('MS-SSIM failed to compute.')\n        logging.warning(e)\n        return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.image_quality_losses.PSNR","title":"<code>PSNR</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class to calculate peak-signal-to-noise ratio of an image with respect to a ground truth image.</p> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>class PSNR(nn.Module):\n    '''\n    A class to calculate peak-signal-to-noise ratio of an image with respect to a ground truth image.\n    '''\n\n    def __init__(self):\n        super(PSNR, self).__init__()\n\n    def forward(self, predictions, targets, peak_value = 1.0):\n        \"\"\"\n        A function to calculate peak-signal-to-noise ratio of an image with respect to a ground truth image.\n\n        Parameters\n        ----------\n        predictions   : torch.tensor\n                        Image to be tested.\n        targets       : torch.tensor\n                        Ground truth image.\n        peak_value    : float\n                        Peak value that given tensors could have.\n\n        Returns\n        -------\n        result        : torch.tensor\n                        Peak-signal-to-noise ratio.\n        \"\"\"\n        mse = torch.mean((targets - predictions) ** 2)\n        result = 20 * torch.log10(peak_value / torch.sqrt(mse))\n        return result\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.image_quality_losses.PSNR.forward","title":"<code>forward(predictions, targets, peak_value=1.0)</code>","text":"<p>A function to calculate peak-signal-to-noise ratio of an image with respect to a ground truth image.</p> <p>Parameters:</p> <ul> <li> <code>predictions</code>           \u2013            <pre><code>        Image to be tested.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>        Ground truth image.\n</code></pre> </li> <li> <code>peak_value</code>           \u2013            <pre><code>        Peak value that given tensors could have.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Peak-signal-to-noise ratio.</p> </li> </ul> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>def forward(self, predictions, targets, peak_value = 1.0):\n    \"\"\"\n    A function to calculate peak-signal-to-noise ratio of an image with respect to a ground truth image.\n\n    Parameters\n    ----------\n    predictions   : torch.tensor\n                    Image to be tested.\n    targets       : torch.tensor\n                    Ground truth image.\n    peak_value    : float\n                    Peak value that given tensors could have.\n\n    Returns\n    -------\n    result        : torch.tensor\n                    Peak-signal-to-noise ratio.\n    \"\"\"\n    mse = torch.mean((targets - predictions) ** 2)\n    result = 20 * torch.log10(peak_value / torch.sqrt(mse))\n    return result\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.image_quality_losses.SSIM","title":"<code>SSIM</code>","text":"<p>               Bases: <code>Module</code></p> <p>A class to calculate structural similarity index of an image with respect to a ground truth image.</p> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>class SSIM(nn.Module):\n    '''\n    A class to calculate structural similarity index of an image with respect to a ground truth image.\n    '''\n\n    def __init__(self):\n        super(SSIM, self).__init__()\n\n    def forward(self, predictions, targets):\n        \"\"\"\n        Parameters\n        ----------\n        predictions : torch.tensor\n                      The predicted images.\n        targets     : torch.tensor\n                      The ground truth images.\n\n        Returns\n        -------\n        result      : torch.tensor \n                      The computed SSIM value if successful, otherwise 0.0.\n        \"\"\"\n        try:\n            from torchmetrics.functional.image import structural_similarity_index_measure\n            if len(predictions.shape) == 3:\n                predictions = predictions.unsqueeze(0)\n                targets = targets.unsqueeze(0)\n            l_SSIM = structural_similarity_index_measure(predictions, targets)\n            return l_SSIM\n        except Exception as e:\n            logging.warning('SSIM failed to compute.')\n            logging.warning(e)\n            return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.image_quality_losses.SSIM.forward","title":"<code>forward(predictions, targets)</code>","text":"<p>Parameters:</p> <ul> <li> <code>predictions</code>               (<code>tensor</code>)           \u2013            <pre><code>      The predicted images.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>      The ground truth images.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>The computed SSIM value if successful, otherwise 0.0.</p> </li> </ul> Source code in <code>odak/learn/perception/image_quality_losses.py</code> <pre><code>def forward(self, predictions, targets):\n    \"\"\"\n    Parameters\n    ----------\n    predictions : torch.tensor\n                  The predicted images.\n    targets     : torch.tensor\n                  The ground truth images.\n\n    Returns\n    -------\n    result      : torch.tensor \n                  The computed SSIM value if successful, otherwise 0.0.\n    \"\"\"\n    try:\n        from torchmetrics.functional.image import structural_similarity_index_measure\n        if len(predictions.shape) == 3:\n            predictions = predictions.unsqueeze(0)\n            targets = targets.unsqueeze(0)\n        l_SSIM = structural_similarity_index_measure(predictions, targets)\n        return l_SSIM\n    except Exception as e:\n        logging.warning('SSIM failed to compute.')\n        logging.warning(e)\n        return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.learned_perceptual_losses.CVVDP","title":"<code>CVVDP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>class CVVDP(nn.Module):\n    def __init__(self, device = torch.device('cpu')):\n        \"\"\"\n        Initializes the CVVDP model with a specified device.\n\n        Parameters\n        ----------\n        device   : torch.device\n                    The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n        \"\"\"\n        super(CVVDP, self).__init__()\n        try:\n            import pycvvdp\n            self.cvvdp = pycvvdp.cvvdp(display_name = 'standard_4k', device = device)\n        except Exception as e:\n            logging.warning('ColorVideoVDP is missing, consider installing by running \"pip install -U git+https://github.com/gfxdisp/ColorVideoVDP\"')\n            logging.warning(e)\n\n\n    def forward(self, predictions, targets, dim_order = 'BCHW'):\n        \"\"\"\n        Parameters\n        ----------\n        predictions   : torch.tensor\n                        The predicted images.\n        targets    h  : torch.tensor\n                        The ground truth images.\n        dim_order     : str\n                        The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n\n        Returns\n        -------\n        result        : torch.tensor\n                        The computed loss if successful, otherwise 0.0.\n        \"\"\"\n        try:\n            if len(predictions.shape) == 3:\n                predictions = predictions.unsqueeze(0)\n                targets = targets.unsqueeze(0)\n            l_ColorVideoVDP = self.cvvdp.predict(predictions, targets, dim_order = dim_order)[0]\n            return l_ColorVideoVDP\n        except Exception as e:\n            logging.warning('ColorVideoVDP failed to compute.')\n            logging.warning(e)\n            return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.learned_perceptual_losses.CVVDP.__init__","title":"<code>__init__(device=torch.device('cpu'))</code>","text":"<p>Initializes the CVVDP model with a specified device.</p> <p>Parameters:</p> <ul> <li> <code>device</code>           \u2013            <pre><code>    The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def __init__(self, device = torch.device('cpu')):\n    \"\"\"\n    Initializes the CVVDP model with a specified device.\n\n    Parameters\n    ----------\n    device   : torch.device\n                The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n    \"\"\"\n    super(CVVDP, self).__init__()\n    try:\n        import pycvvdp\n        self.cvvdp = pycvvdp.cvvdp(display_name = 'standard_4k', device = device)\n    except Exception as e:\n        logging.warning('ColorVideoVDP is missing, consider installing by running \"pip install -U git+https://github.com/gfxdisp/ColorVideoVDP\"')\n        logging.warning(e)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.learned_perceptual_losses.CVVDP.forward","title":"<code>forward(predictions, targets, dim_order='BCHW')</code>","text":"<p>Parameters:</p> <ul> <li> <code>predictions</code>           \u2013            <pre><code>        The predicted images.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>        The ground truth images.\n</code></pre> </li> <li> <code>dim_order</code>           \u2013            <pre><code>        The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>The computed loss if successful, otherwise 0.0.</p> </li> </ul> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def forward(self, predictions, targets, dim_order = 'BCHW'):\n    \"\"\"\n    Parameters\n    ----------\n    predictions   : torch.tensor\n                    The predicted images.\n    targets    h  : torch.tensor\n                    The ground truth images.\n    dim_order     : str\n                    The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n\n    Returns\n    -------\n    result        : torch.tensor\n                    The computed loss if successful, otherwise 0.0.\n    \"\"\"\n    try:\n        if len(predictions.shape) == 3:\n            predictions = predictions.unsqueeze(0)\n            targets = targets.unsqueeze(0)\n        l_ColorVideoVDP = self.cvvdp.predict(predictions, targets, dim_order = dim_order)[0]\n        return l_ColorVideoVDP\n    except Exception as e:\n        logging.warning('ColorVideoVDP failed to compute.')\n        logging.warning(e)\n        return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.learned_perceptual_losses.FVVDP","title":"<code>FVVDP</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>class FVVDP(nn.Module):\n    def __init__(self, device = torch.device('cpu')):\n        \"\"\"\n        Initializes the FVVDP model with a specified device.\n\n        Parameters\n        ----------\n        device   : torch.device\n                    The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n        \"\"\"\n        super(FVVDP, self).__init__()\n        try:\n            import pyfvvdp\n            self.fvvdp = pyfvvdp.fvvdp(display_name = 'standard_4k', heatmap = 'none', device = device)\n        except Exception as e:\n            logging.warning('FovVideoVDP is missing, consider installing by running \"pip install pyfvvdp\"')\n            logging.warning(e)\n\n\n    def forward(self, predictions, targets, dim_order = 'BCHW'):\n        \"\"\"\n        Parameters\n        ----------\n        predictions   : torch.tensor\n                        The predicted images.\n        targets       : torch.tensor\n                        The ground truth images.\n        dim_order     : str\n                        The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n\n        Returns\n        -------\n        result        : torch.tensor\n                          The computed loss if successful, otherwise 0.0.\n        \"\"\"\n        try:\n            if len(predictions.shape) == 3:\n                predictions = predictions.unsqueeze(0)\n                targets = targets.unsqueeze(0)\n            l_FovVideoVDP = self.fvvdp.predict(predictions, targets, dim_order = dim_order)[0]\n            return l_FovVideoVDP\n        except Exception as e:\n            logging.warning('FovVideoVDP failed to compute.')\n            logging.warning(e)\n            return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.learned_perceptual_losses.FVVDP.__init__","title":"<code>__init__(device=torch.device('cpu'))</code>","text":"<p>Initializes the FVVDP model with a specified device.</p> <p>Parameters:</p> <ul> <li> <code>device</code>           \u2013            <pre><code>    The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def __init__(self, device = torch.device('cpu')):\n    \"\"\"\n    Initializes the FVVDP model with a specified device.\n\n    Parameters\n    ----------\n    device   : torch.device\n                The device (CPU/GPU) on which the computations will be performed. Defaults to CPU.\n    \"\"\"\n    super(FVVDP, self).__init__()\n    try:\n        import pyfvvdp\n        self.fvvdp = pyfvvdp.fvvdp(display_name = 'standard_4k', heatmap = 'none', device = device)\n    except Exception as e:\n        logging.warning('FovVideoVDP is missing, consider installing by running \"pip install pyfvvdp\"')\n        logging.warning(e)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.learned_perceptual_losses.FVVDP.forward","title":"<code>forward(predictions, targets, dim_order='BCHW')</code>","text":"<p>Parameters:</p> <ul> <li> <code>predictions</code>           \u2013            <pre><code>        The predicted images.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>        The ground truth images.\n</code></pre> </li> <li> <code>dim_order</code>           \u2013            <pre><code>        The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>The computed loss if successful, otherwise 0.0.</p> </li> </ul> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def forward(self, predictions, targets, dim_order = 'BCHW'):\n    \"\"\"\n    Parameters\n    ----------\n    predictions   : torch.tensor\n                    The predicted images.\n    targets       : torch.tensor\n                    The ground truth images.\n    dim_order     : str\n                    The dimension order of the input images. Defaults to 'BCHW' (channels, height, width).\n\n    Returns\n    -------\n    result        : torch.tensor\n                      The computed loss if successful, otherwise 0.0.\n    \"\"\"\n    try:\n        if len(predictions.shape) == 3:\n            predictions = predictions.unsqueeze(0)\n            targets = targets.unsqueeze(0)\n        l_FovVideoVDP = self.fvvdp.predict(predictions, targets, dim_order = dim_order)[0]\n        return l_FovVideoVDP\n    except Exception as e:\n        logging.warning('FovVideoVDP failed to compute.')\n        logging.warning(e)\n        return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.learned_perceptual_losses.LPIPS","title":"<code>LPIPS</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>class LPIPS(nn.Module):\n\n    def __init__(self):\n        \"\"\"\n        Initializes the LPIPS (Learned Perceptual Image Patch Similarity) model.\n\n        \"\"\"\n        super(LPIPS, self).__init__()\n        try:\n            import torchmetrics\n            self.lpips = torchmetrics.image.lpip.LearnedPerceptualImagePatchSimilarity(net_type = 'squeeze')\n        except Exception as e:\n            logging.warning('torchmetrics is missing, consider installing by running \"pip install torchmetrics\"')\n            logging.warning(e)\n\n\n    def forward(self, predictions, targets):\n        \"\"\"\n        Parameters\n        ----------\n        predictions   : torch.tensor\n                        The predicted images.\n        targets       : torch.tensor\n                        The ground truth images.\n\n        Returns\n        -------\n        result        : torch.tensor\n                        The computed loss if successful, otherwise 0.0.\n        \"\"\"\n        try:\n            if len(predictions.shape) == 3:\n                predictions = predictions.unsqueeze(0)\n                targets = targets.unsqueeze(0)\n            lpips_image = predictions\n            lpips_target = targets\n            if len(lpips_image.shape) == 3:\n                lpips_image = lpips_image.unsqueeze(0)\n                lpips_target = lpips_target.unsqueeze(0)\n            if lpips_image.shape[1] == 1:\n                lpips_image = lpips_image.repeat(1, 3, 1, 1)\n                lpips_target = lpips_target.repeat(1, 3, 1, 1)\n            lpips_image = (lpips_image * 2 - 1).clamp(-1, 1)\n            lpips_target = (lpips_target * 2 - 1).clamp(-1, 1)\n            l_LPIPS = self.lpips(lpips_image, lpips_target)\n            return l_LPIPS\n        except Exception as e:\n            logging.warning('LPIPS failed to compute.')\n            logging.warning(e)\n            return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.learned_perceptual_losses.LPIPS.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the LPIPS (Learned Perceptual Image Patch Similarity) model.</p> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the LPIPS (Learned Perceptual Image Patch Similarity) model.\n\n    \"\"\"\n    super(LPIPS, self).__init__()\n    try:\n        import torchmetrics\n        self.lpips = torchmetrics.image.lpip.LearnedPerceptualImagePatchSimilarity(net_type = 'squeeze')\n    except Exception as e:\n        logging.warning('torchmetrics is missing, consider installing by running \"pip install torchmetrics\"')\n        logging.warning(e)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.learned_perceptual_losses.LPIPS.forward","title":"<code>forward(predictions, targets)</code>","text":"<p>Parameters:</p> <ul> <li> <code>predictions</code>           \u2013            <pre><code>        The predicted images.\n</code></pre> </li> <li> <code>targets</code>           \u2013            <pre><code>        The ground truth images.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>The computed loss if successful, otherwise 0.0.</p> </li> </ul> Source code in <code>odak/learn/perception/learned_perceptual_losses.py</code> <pre><code>def forward(self, predictions, targets):\n    \"\"\"\n    Parameters\n    ----------\n    predictions   : torch.tensor\n                    The predicted images.\n    targets       : torch.tensor\n                    The ground truth images.\n\n    Returns\n    -------\n    result        : torch.tensor\n                    The computed loss if successful, otherwise 0.0.\n    \"\"\"\n    try:\n        if len(predictions.shape) == 3:\n            predictions = predictions.unsqueeze(0)\n            targets = targets.unsqueeze(0)\n        lpips_image = predictions\n        lpips_target = targets\n        if len(lpips_image.shape) == 3:\n            lpips_image = lpips_image.unsqueeze(0)\n            lpips_target = lpips_target.unsqueeze(0)\n        if lpips_image.shape[1] == 1:\n            lpips_image = lpips_image.repeat(1, 3, 1, 1)\n            lpips_target = lpips_target.repeat(1, 3, 1, 1)\n        lpips_image = (lpips_image * 2 - 1).clamp(-1, 1)\n        lpips_target = (lpips_target * 2 - 1).clamp(-1, 1)\n        l_LPIPS = self.lpips(lpips_image, lpips_target)\n        return l_LPIPS\n    except Exception as e:\n        logging.warning('LPIPS failed to compute.')\n        logging.warning(e)\n        return torch.tensor(0.0)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.metameric_loss.MetamericLoss","title":"<code>MetamericLoss</code>","text":"<p>The <code>MetamericLoss</code> class provides a perceptual loss function.</p> <p>Rather than exactly match the source image to the target, it tries to ensure the source is a metamer to the target image.</p> <p>Its interface is similar to other <code>pytorch</code> loss functions, but note that the gaze location must be provided in addition to the source and target images.</p> Source code in <code>odak/learn/perception/metameric_loss.py</code> <pre><code>class MetamericLoss():\n    \"\"\"\n    The `MetamericLoss` class provides a perceptual loss function.\n\n    Rather than exactly match the source image to the target, it tries to ensure the source is a *metamer* to the target image.\n\n    Its interface is similar to other `pytorch` loss functions, but note that the gaze location must be provided in addition to the source and target images.\n    \"\"\"\n\n\n    def __init__(self, device=torch.device('cpu'), alpha=0.2, real_image_width=0.2,\n                 real_viewing_distance=0.7, n_pyramid_levels=5, mode=\"quadratic\",\n                 n_orientations=2, use_l2_foveal_loss=True, fovea_weight=20.0, use_radial_weight=False,\n                 use_fullres_l0=False, equi=False):\n        \"\"\"\n        Parameters\n        ----------\n\n        alpha                   : float\n                                    parameter controlling foveation - larger values mean bigger pooling regions.\n        real_image_width        : float \n                                    The real width of the image as displayed to the user.\n                                    Units don't matter as long as they are the same as for real_viewing_distance.\n        real_viewing_distance   : float \n                                    The real distance of the observer's eyes to the image plane.\n                                    Units don't matter as long as they are the same as for real_image_width.\n        n_pyramid_levels        : int \n                                    Number of levels of the steerable pyramid. Note that the image is padded\n                                    so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                                    too high will slow down the calculation a lot.\n        mode                    : str \n                                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                    as you move away from the fovea. We got best results with \"quadratic\".\n        n_orientations          : int \n                                    Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                                    Increasing this will increase runtime.\n        use_l2_foveal_loss      : bool \n                                    If true, for all the pixels that have pooling size 1 pixel in the \n                                    largest scale will use direct L2 against target rather than pooling over pyramid levels.\n                                    In practice this gives better results when the loss is used for holography.\n        fovea_weight            : float \n                                    A weight to apply to the foveal region if use_l2_foveal_loss is set to True.\n        use_radial_weight       : bool \n                                    If True, will apply a radial weighting when calculating the difference between\n                                    the source and target stats maps. This weights stats closer to the fovea more than those\n                                    further away.\n        use_fullres_l0          : bool \n                                    If true, stats for the lowpass residual are replaced with blurred versions\n                                    of the full-resolution source and target images.\n        equi                    : bool\n                                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                    [-pi,pi]x[-pi/2,pi]\n        \"\"\"\n        self.device = device\n        self.pyramid_maker = None\n        self.alpha = alpha\n        self.real_image_width = real_image_width\n        self.real_viewing_distance = real_viewing_distance\n        self.blurs = None\n        self.n_pyramid_levels = n_pyramid_levels\n        self.n_orientations = n_orientations\n        self.mode = mode\n        self.use_l2_foveal_loss = use_l2_foveal_loss\n        self.fovea_weight = fovea_weight\n        self.use_radial_weight = use_radial_weight\n        self.use_fullres_l0 = use_fullres_l0\n        self.equi = equi\n        if self.use_fullres_l0 and self.use_l2_foveal_loss:\n            raise Exception(\n                \"Can't use use_fullres_l0 and use_l2_foveal_loss options together in MetamericLoss!\")\n\n    def calc_statsmaps(self, image, gaze=None, alpha=0.01, real_image_width=0.3,\n                       real_viewing_distance=0.6, mode=\"quadratic\", equi=False):\n\n        if self.pyramid_maker is None or \\\n                self.pyramid_maker.device != self.device or \\\n                len(self.pyramid_maker.band_filters) != self.n_orientations or\\\n                self.pyramid_maker.filt_h0.size(0) != image.size(1):\n            self.pyramid_maker = SpatialSteerablePyramid(\n                use_bilinear_downup=False, n_channels=image.size(1),\n                device=self.device, n_orientations=self.n_orientations, filter_type=\"cropped\", filter_size=5)\n\n        if self.blurs is None or len(self.blurs) != self.n_pyramid_levels:\n            self.blurs = [RadiallyVaryingBlur()\n                          for i in range(self.n_pyramid_levels)]\n\n        def find_stats(image_pyr_level, blur):\n            image_means = blur.blur(\n                image_pyr_level, alpha, real_image_width, real_viewing_distance, centre=gaze, mode=mode, equi=self.equi)\n            image_meansq = blur.blur(image_pyr_level*image_pyr_level, alpha,\n                                     real_image_width, real_viewing_distance, centre=gaze, mode=mode, equi=self.equi)\n\n            image_vars = image_meansq - (image_means*image_means)\n            image_vars[image_vars &lt; 1e-7] = 1e-7\n            image_std = torch.sqrt(image_vars)\n            if torch.any(torch.isnan(image_means)):\n                print(image_means)\n                raise Exception(\"NaN in image means!\")\n            if torch.any(torch.isnan(image_std)):\n                print(image_std)\n                raise Exception(\"NaN in image stdevs!\")\n            if self.use_fullres_l0:\n                mask = blur.lod_map &gt; 1e-6\n                mask = mask[None, None, ...]\n                if image_means.size(1) &gt; 1:\n                    mask = mask.repeat(image_means.size(0), image_means.size(1), 1, 1)\n                matte = torch.zeros_like(image_means)\n                matte[mask] = 1.0\n                return image_means * matte, image_std * matte\n            return image_means, image_std\n        output_stats = []\n        image_pyramid = self.pyramid_maker.construct_pyramid(\n            image, self.n_pyramid_levels)\n        means, variances = find_stats(image_pyramid[0]['h'], self.blurs[0])\n        if self.use_l2_foveal_loss:\n            base_mask = 1.0 - (self.blurs[0].lod_map / torch.max(self.blurs[0].lod_map))\n            base_mask[self.blurs[0].lod_map &lt; 1e-6] = 1.0\n            self.fovea_mask = base_mask[None, None, ...].expand(image.size()).clone()\n            self.fovea_mask = torch.pow(self.fovea_mask, 10.0)\n            #self.fovea_mask     = torch.nn.functional.interpolate(self.fovea_mask, scale_factor=0.125, mode=\"area\")\n            #self.fovea_mask     = torch.nn.functional.interpolate(self.fovea_mask, size=(image.size(-2), image.size(-1)), mode=\"bilinear\")\n            periphery_mask = 1.0 - self.fovea_mask\n            self.periphery_mask = periphery_mask.clone()\n            output_stats.append(means * periphery_mask)\n            output_stats.append(variances * periphery_mask)\n        else:\n            output_stats.append(means)\n            output_stats.append(variances)\n\n        for l in range(0, len(image_pyramid)-1):\n            for o in range(len(image_pyramid[l]['b'])):\n                means, variances = find_stats(\n                    image_pyramid[l]['b'][o], self.blurs[l])\n                if self.use_l2_foveal_loss:\n                    output_stats.append(means * periphery_mask)\n                    output_stats.append(variances * periphery_mask)\n                else:\n                    output_stats.append(means)\n                    output_stats.append(variances)\n            if self.use_l2_foveal_loss:\n                periphery_mask = torch.nn.functional.interpolate(\n                    periphery_mask, scale_factor=0.5, mode=\"area\", recompute_scale_factor=False)\n\n        if self.use_l2_foveal_loss:\n            output_stats.append(image_pyramid[-1][\"l\"] * periphery_mask)\n        elif self.use_fullres_l0:\n            output_stats.append(self.blurs[0].blur(\n                image, alpha, real_image_width, real_viewing_distance, gaze, mode))\n        else:\n            output_stats.append(image_pyramid[-1][\"l\"])\n        return output_stats\n\n    def metameric_loss_stats(self, statsmap_a, statsmap_b, gaze):\n        loss = 0.0\n        for a, b in zip(statsmap_a, statsmap_b):\n            if self.use_radial_weight:\n                radii = make_radial_map(\n                    [a.size(-2), a.size(-1)], gaze).to(a.device)\n                weights = 1.1 - (radii * radii * radii * radii)\n                weights = weights[None, None, ...].repeat(a.size(0), a.size(1), 1, 1)\n                loss += torch.nn.MSELoss()(weights*a, weights*b)\n            else:\n                loss += torch.nn.MSELoss()(a, b)\n        loss /= len(statsmap_a)\n        return loss\n\n    def visualise_loss_map(self, image_stats):\n        batch_size = image_stats[0].size(0)\n        loss_map = torch.zeros((batch_size,) + image_stats[0].size()[-2:])\n        for i in range(len(image_stats)):\n            stats = image_stats[i]\n            target_stats = self.target_stats[i]\n            stat_mse_map = torch.sqrt(torch.pow(stats - target_stats, 2))\n            stat_mse_map = torch.nn.functional.interpolate(stat_mse_map, size=loss_map.size()[1:],\n                mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n            loss_map += stat_mse_map[:, 0, ...]\n        self.loss_map = loss_map\n\n    def __call__(self, image, target, gaze=[0.5, 0.5], image_colorspace=\"RGB\", visualise_loss=False):\n        \"\"\" \n        Calculates the Metameric Loss.\n\n        Parameters\n        ----------\n        image               : torch.tensor\n                                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        target              : torch.tensor\n                                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        image_colorspace    : str\n                                The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                                accepted values: RGB, YCrCb.\n        gaze                : list\n                                Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n        visualise_loss      : bool\n                                Shows a heatmap indicating which parts of the image contributed most to the loss. \n\n        Returns\n        -------\n\n        loss                : torch.tensor\n                                The computed loss.\n        \"\"\"\n        check_loss_inputs(\"MetamericLoss\", image, target)\n        # Pad image and target if necessary\n        image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n        target = pad_image_for_pyramid(target, self.n_pyramid_levels)\n        # If input is RGB, convert to YCrCb.\n        if image.size(1) == 3 and image_colorspace == \"RGB\":\n            image = rgb_2_ycrcb(image)\n            target = rgb_2_ycrcb(target)\n\n        self.target_stats = self.calc_statsmaps(\n            target,\n            gaze=gaze,\n            alpha=self.alpha,\n            real_image_width=self.real_image_width,\n            real_viewing_distance=self.real_viewing_distance,\n            mode=self.mode\n        )\n\n        image_stats = self.calc_statsmaps(\n            image,\n            gaze=gaze,\n            alpha=self.alpha,\n            real_image_width=self.real_image_width,\n            real_viewing_distance=self.real_viewing_distance,\n            mode=self.mode\n        )\n\n        if visualise_loss:\n            self.visualise_loss_map(image_stats)\n\n        if self.use_l2_foveal_loss:\n            peripheral_loss = self.metameric_loss_stats(\n                image_stats, self.target_stats, gaze)\n            foveal_loss = torch.nn.MSELoss()(self.fovea_mask*image, self.fovea_mask*target)\n            # New weighting - evenly weight fovea and periphery.\n            loss = peripheral_loss + self.fovea_weight * foveal_loss\n        else:\n            loss = self.metameric_loss_stats(\n                image_stats, self.target_stats, gaze)\n        return loss\n\n    def to(self, device):\n        self.device = device\n        return self\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.metameric_loss.MetamericLoss.__call__","title":"<code>__call__(image, target, gaze=[0.5, 0.5], image_colorspace='RGB', visualise_loss=False)</code>","text":"<p>Calculates the Metameric Loss.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>target</code>           \u2013            <pre><code>                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>image_colorspace</code>           \u2013            <pre><code>                The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                accepted values: RGB, YCrCb.\n</code></pre> </li> <li> <code>gaze</code>           \u2013            <pre><code>                Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n</code></pre> </li> <li> <code>visualise_loss</code>           \u2013            <pre><code>                Shows a heatmap indicating which parts of the image contributed most to the loss.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>tensor</code> )          \u2013            <p>The computed loss.</p> </li> </ul> Source code in <code>odak/learn/perception/metameric_loss.py</code> <pre><code>def __call__(self, image, target, gaze=[0.5, 0.5], image_colorspace=\"RGB\", visualise_loss=False):\n    \"\"\" \n    Calculates the Metameric Loss.\n\n    Parameters\n    ----------\n    image               : torch.tensor\n                            Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    target              : torch.tensor\n                            Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    image_colorspace    : str\n                            The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                            accepted values: RGB, YCrCb.\n    gaze                : list\n                            Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n    visualise_loss      : bool\n                            Shows a heatmap indicating which parts of the image contributed most to the loss. \n\n    Returns\n    -------\n\n    loss                : torch.tensor\n                            The computed loss.\n    \"\"\"\n    check_loss_inputs(\"MetamericLoss\", image, target)\n    # Pad image and target if necessary\n    image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n    target = pad_image_for_pyramid(target, self.n_pyramid_levels)\n    # If input is RGB, convert to YCrCb.\n    if image.size(1) == 3 and image_colorspace == \"RGB\":\n        image = rgb_2_ycrcb(image)\n        target = rgb_2_ycrcb(target)\n\n    self.target_stats = self.calc_statsmaps(\n        target,\n        gaze=gaze,\n        alpha=self.alpha,\n        real_image_width=self.real_image_width,\n        real_viewing_distance=self.real_viewing_distance,\n        mode=self.mode\n    )\n\n    image_stats = self.calc_statsmaps(\n        image,\n        gaze=gaze,\n        alpha=self.alpha,\n        real_image_width=self.real_image_width,\n        real_viewing_distance=self.real_viewing_distance,\n        mode=self.mode\n    )\n\n    if visualise_loss:\n        self.visualise_loss_map(image_stats)\n\n    if self.use_l2_foveal_loss:\n        peripheral_loss = self.metameric_loss_stats(\n            image_stats, self.target_stats, gaze)\n        foveal_loss = torch.nn.MSELoss()(self.fovea_mask*image, self.fovea_mask*target)\n        # New weighting - evenly weight fovea and periphery.\n        loss = peripheral_loss + self.fovea_weight * foveal_loss\n    else:\n        loss = self.metameric_loss_stats(\n            image_stats, self.target_stats, gaze)\n    return loss\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.metameric_loss.MetamericLoss.__init__","title":"<code>__init__(device=torch.device('cpu'), alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, n_pyramid_levels=5, mode='quadratic', n_orientations=2, use_l2_foveal_loss=True, fovea_weight=20.0, use_radial_weight=False, use_fullres_l0=False, equi=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>           \u2013            <pre><code>                    parameter controlling foveation - larger values mean bigger pooling regions.\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed to the user.\n                    Units don't matter as long as they are the same as for real_viewing_distance.\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance of the observer's eyes to the image plane.\n                    Units don't matter as long as they are the same as for real_image_width.\n</code></pre> </li> <li> <code>n_pyramid_levels</code>           \u2013            <pre><code>                    Number of levels of the steerable pyramid. Note that the image is padded\n                    so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                    too high will slow down the calculation a lot.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                    as you move away from the fovea. We got best results with \"quadratic\".\n</code></pre> </li> <li> <code>n_orientations</code>           \u2013            <pre><code>                    Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                    Increasing this will increase runtime.\n</code></pre> </li> <li> <code>use_l2_foveal_loss</code>           \u2013            <pre><code>                    If true, for all the pixels that have pooling size 1 pixel in the \n                    largest scale will use direct L2 against target rather than pooling over pyramid levels.\n                    In practice this gives better results when the loss is used for holography.\n</code></pre> </li> <li> <code>fovea_weight</code>           \u2013            <pre><code>                    A weight to apply to the foveal region if use_l2_foveal_loss is set to True.\n</code></pre> </li> <li> <code>use_radial_weight</code>           \u2013            <pre><code>                    If True, will apply a radial weighting when calculating the difference between\n                    the source and target stats maps. This weights stats closer to the fovea more than those\n                    further away.\n</code></pre> </li> <li> <code>use_fullres_l0</code>           \u2013            <pre><code>                    If true, stats for the lowpass residual are replaced with blurred versions\n                    of the full-resolution source and target images.\n</code></pre> </li> <li> <code>equi</code>           \u2013            <pre><code>                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                    [-pi,pi]x[-pi/2,pi]\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/metameric_loss.py</code> <pre><code>def __init__(self, device=torch.device('cpu'), alpha=0.2, real_image_width=0.2,\n             real_viewing_distance=0.7, n_pyramid_levels=5, mode=\"quadratic\",\n             n_orientations=2, use_l2_foveal_loss=True, fovea_weight=20.0, use_radial_weight=False,\n             use_fullres_l0=False, equi=False):\n    \"\"\"\n    Parameters\n    ----------\n\n    alpha                   : float\n                                parameter controlling foveation - larger values mean bigger pooling regions.\n    real_image_width        : float \n                                The real width of the image as displayed to the user.\n                                Units don't matter as long as they are the same as for real_viewing_distance.\n    real_viewing_distance   : float \n                                The real distance of the observer's eyes to the image plane.\n                                Units don't matter as long as they are the same as for real_image_width.\n    n_pyramid_levels        : int \n                                Number of levels of the steerable pyramid. Note that the image is padded\n                                so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                                too high will slow down the calculation a lot.\n    mode                    : str \n                                Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                as you move away from the fovea. We got best results with \"quadratic\".\n    n_orientations          : int \n                                Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                                Increasing this will increase runtime.\n    use_l2_foveal_loss      : bool \n                                If true, for all the pixels that have pooling size 1 pixel in the \n                                largest scale will use direct L2 against target rather than pooling over pyramid levels.\n                                In practice this gives better results when the loss is used for holography.\n    fovea_weight            : float \n                                A weight to apply to the foveal region if use_l2_foveal_loss is set to True.\n    use_radial_weight       : bool \n                                If True, will apply a radial weighting when calculating the difference between\n                                the source and target stats maps. This weights stats closer to the fovea more than those\n                                further away.\n    use_fullres_l0          : bool \n                                If true, stats for the lowpass residual are replaced with blurred versions\n                                of the full-resolution source and target images.\n    equi                    : bool\n                                If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                [-pi,pi]x[-pi/2,pi]\n    \"\"\"\n    self.device = device\n    self.pyramid_maker = None\n    self.alpha = alpha\n    self.real_image_width = real_image_width\n    self.real_viewing_distance = real_viewing_distance\n    self.blurs = None\n    self.n_pyramid_levels = n_pyramid_levels\n    self.n_orientations = n_orientations\n    self.mode = mode\n    self.use_l2_foveal_loss = use_l2_foveal_loss\n    self.fovea_weight = fovea_weight\n    self.use_radial_weight = use_radial_weight\n    self.use_fullres_l0 = use_fullres_l0\n    self.equi = equi\n    if self.use_fullres_l0 and self.use_l2_foveal_loss:\n        raise Exception(\n            \"Can't use use_fullres_l0 and use_l2_foveal_loss options together in MetamericLoss!\")\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.metameric_loss_uniform.MetamericLossUniform","title":"<code>MetamericLossUniform</code>","text":"<p>Measures metameric loss between a given image and a metamer of the given target image. This variant of the metameric loss is not foveated - it applies uniform pooling sizes to the whole input image.</p> Source code in <code>odak/learn/perception/metameric_loss_uniform.py</code> <pre><code>class MetamericLossUniform():\n    \"\"\"\n    Measures metameric loss between a given image and a metamer of the given target image.\n    This variant of the metameric loss is not foveated - it applies uniform pooling sizes to the whole input image.\n    \"\"\"\n\n    def __init__(self, device=torch.device('cpu'), pooling_size=32, n_pyramid_levels=5, n_orientations=2):\n        \"\"\"\n\n        Parameters\n        ----------\n        pooling_size            : int\n                                  Pooling size, in pixels. For example 32 will pool over 32x32 blocks of the image.\n        n_pyramid_levels        : int \n                                  Number of levels of the steerable pyramid. Note that the image is padded\n                                  so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                                  too high will slow down the calculation a lot.\n        n_orientations          : int \n                                  Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                                  Increasing this will increase runtime.\n\n        \"\"\"\n        self.target = None\n        self.device = device\n        self.pyramid_maker = None\n        self.pooling_size = pooling_size\n        self.n_pyramid_levels = n_pyramid_levels\n        self.n_orientations = n_orientations\n\n    def calc_statsmaps(self, image, pooling_size):\n\n        if self.pyramid_maker is None or \\\n                self.pyramid_maker.device != self.device or \\\n                len(self.pyramid_maker.band_filters) != self.n_orientations or\\\n                self.pyramid_maker.filt_h0.size(0) != image.size(1):\n            self.pyramid_maker = SpatialSteerablePyramid(\n                use_bilinear_downup=False, n_channels=image.size(1),\n                device=self.device, n_orientations=self.n_orientations, filter_type=\"cropped\", filter_size=5)\n\n\n        def find_stats(image_pyr_level, pooling_size):\n            image_means = uniform_blur(image_pyr_level, pooling_size)\n            image_meansq = uniform_blur(image_pyr_level*image_pyr_level, pooling_size)\n            image_vars = image_meansq - (image_means*image_means)\n            image_vars[image_vars &lt; 1e-7] = 1e-7\n            image_std = torch.sqrt(image_vars)\n            if torch.any(torch.isnan(image_means)):\n                print(image_means)\n                raise Exception(\"NaN in image means!\")\n            if torch.any(torch.isnan(image_std)):\n                print(image_std)\n                raise Exception(\"NaN in image stdevs!\")\n            return image_means, image_std\n\n        output_stats = []\n        image_pyramid = self.pyramid_maker.construct_pyramid(\n            image, self.n_pyramid_levels)\n        curr_pooling_size = pooling_size\n        means, variances = find_stats(image_pyramid[0]['h'], curr_pooling_size)\n        output_stats.append(means)\n        output_stats.append(variances)\n\n        for l in range(0, len(image_pyramid)-1):\n            for o in range(len(image_pyramid[l]['b'])):\n                means, variances = find_stats(\n                    image_pyramid[l]['b'][o], curr_pooling_size)\n                output_stats.append(means)\n                output_stats.append(variances)\n            curr_pooling_size /= 2\n\n        output_stats.append(image_pyramid[-1][\"l\"])\n        return output_stats\n\n    def metameric_loss_stats(self, statsmap_a, statsmap_b):\n        loss = 0.0\n        for a, b in zip(statsmap_a, statsmap_b):\n            loss += torch.nn.MSELoss()(a, b)\n        loss /= len(statsmap_a)\n        return loss\n\n    def visualise_loss_map(self, image_stats):\n        loss_map = torch.zeros(image_stats[0].size()[-2:])\n        for i in range(len(image_stats)):\n            stats = image_stats[i]\n            target_stats = self.target_stats[i]\n            stat_mse_map = torch.sqrt(torch.pow(stats - target_stats, 2))\n            stat_mse_map = torch.nn.functional.interpolate(stat_mse_map, size=loss_map.size(\n            ), mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n            loss_map += stat_mse_map[0, 0, ...]\n        self.loss_map = loss_map\n\n    def __call__(self, image, target, image_colorspace=\"RGB\", visualise_loss=False):\n        \"\"\" \n        Calculates the Metameric Loss.\n\n        Parameters\n        ----------\n        image               : torch.tensor\n                                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        target              : torch.tensor\n                                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        image_colorspace    : str\n                                The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                                accepted values: RGB, YCrCb.\n        visualise_loss      : bool\n                                Shows a heatmap indicating which parts of the image contributed most to the loss. \n\n        Returns\n        -------\n\n        loss                : torch.tensor\n                                The computed loss.\n        \"\"\"\n        check_loss_inputs(\"MetamericLossUniform\", image, target)\n        # Pad image and target if necessary\n        image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n        target = pad_image_for_pyramid(target, self.n_pyramid_levels)\n        # If input is RGB, convert to YCrCb.\n        if image.size(1) == 3 and image_colorspace == \"RGB\":\n            image = rgb_2_ycrcb(image)\n            target = rgb_2_ycrcb(target)\n        if self.target is None:\n            self.target = torch.zeros(target.shape).to(target.device)\n        if type(target) == type(self.target):\n            if not torch.all(torch.eq(target, self.target)):\n                self.target = target.detach().clone()\n                self.target_stats = self.calc_statsmaps(self.target, self.pooling_size)\n                self.target = target.detach().clone()\n            image_stats = self.calc_statsmaps(image, self.pooling_size)\n\n            if visualise_loss:\n                self.visualise_loss_map(image_stats)\n            loss = self.metameric_loss_stats(\n                image_stats, self.target_stats)\n            return loss\n        else:\n            raise Exception(\"Target of incorrect type\")\n\n    def gen_metamer(self, image):\n        \"\"\" \n        Generates a metamer for an image, following the method in [this paper](https://dl.acm.org/doi/abs/10.1145/3450626.3459943)\n        This function can be used on its own to generate a metamer for a desired image.\n\n        Parameters\n        ----------\n        image   : torch.tensor\n                  Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n\n        Returns\n        -------\n        metamer : torch.tensor\n                  The generated metamer image\n        \"\"\"\n        image = rgb_2_ycrcb(image)\n        image_size = image.size()\n        image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n\n        target_stats = self.calc_statsmaps(\n            image, self.pooling_size)\n        target_means = target_stats[::2]\n        target_stdevs = target_stats[1::2]\n        torch.manual_seed(0)\n        noise_image = torch.rand_like(image)\n        noise_pyramid = self.pyramid_maker.construct_pyramid(\n            noise_image, self.n_pyramid_levels)\n        input_pyramid = self.pyramid_maker.construct_pyramid(\n            image, self.n_pyramid_levels)\n\n        def match_level(input_level, target_mean, target_std):\n            level = input_level.clone()\n            level -= torch.mean(level)\n            input_std = torch.sqrt(torch.mean(level * level))\n            eps = 1e-6\n            # Safeguard against divide by zero\n            input_std[input_std &lt; eps] = eps\n            level /= input_std\n            level *= target_std\n            level += target_mean\n            return level\n\n        nbands = len(noise_pyramid[0][\"b\"])\n        noise_pyramid[0][\"h\"] = match_level(\n            noise_pyramid[0][\"h\"], target_means[0], target_stdevs[0])\n        for l in range(len(noise_pyramid)-1):\n            for b in range(nbands):\n                noise_pyramid[l][\"b\"][b] = match_level(\n                    noise_pyramid[l][\"b\"][b], target_means[1 + l * nbands + b], target_stdevs[1 + l * nbands + b])\n        noise_pyramid[-1][\"l\"] = input_pyramid[-1][\"l\"]\n\n        metamer = self.pyramid_maker.reconstruct_from_pyramid(\n            noise_pyramid)\n        metamer = ycrcb_2_rgb(metamer)\n        # Crop to remove any padding\n        metamer = metamer[:image_size[0], :image_size[1], :image_size[2], :image_size[3]]\n        return metamer\n\n    def to(self, device):\n        self.device = device\n        return self\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.metameric_loss_uniform.MetamericLossUniform.__call__","title":"<code>__call__(image, target, image_colorspace='RGB', visualise_loss=False)</code>","text":"<p>Calculates the Metameric Loss.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>                Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>target</code>           \u2013            <pre><code>                Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>image_colorspace</code>           \u2013            <pre><code>                The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                accepted values: RGB, YCrCb.\n</code></pre> </li> <li> <code>visualise_loss</code>           \u2013            <pre><code>                Shows a heatmap indicating which parts of the image contributed most to the loss.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>tensor</code> )          \u2013            <p>The computed loss.</p> </li> </ul> Source code in <code>odak/learn/perception/metameric_loss_uniform.py</code> <pre><code>def __call__(self, image, target, image_colorspace=\"RGB\", visualise_loss=False):\n    \"\"\" \n    Calculates the Metameric Loss.\n\n    Parameters\n    ----------\n    image               : torch.tensor\n                            Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    target              : torch.tensor\n                            Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    image_colorspace    : str\n                            The current colorspace of your image and target. Ignored if input does not have 3 channels.\n                            accepted values: RGB, YCrCb.\n    visualise_loss      : bool\n                            Shows a heatmap indicating which parts of the image contributed most to the loss. \n\n    Returns\n    -------\n\n    loss                : torch.tensor\n                            The computed loss.\n    \"\"\"\n    check_loss_inputs(\"MetamericLossUniform\", image, target)\n    # Pad image and target if necessary\n    image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n    target = pad_image_for_pyramid(target, self.n_pyramid_levels)\n    # If input is RGB, convert to YCrCb.\n    if image.size(1) == 3 and image_colorspace == \"RGB\":\n        image = rgb_2_ycrcb(image)\n        target = rgb_2_ycrcb(target)\n    if self.target is None:\n        self.target = torch.zeros(target.shape).to(target.device)\n    if type(target) == type(self.target):\n        if not torch.all(torch.eq(target, self.target)):\n            self.target = target.detach().clone()\n            self.target_stats = self.calc_statsmaps(self.target, self.pooling_size)\n            self.target = target.detach().clone()\n        image_stats = self.calc_statsmaps(image, self.pooling_size)\n\n        if visualise_loss:\n            self.visualise_loss_map(image_stats)\n        loss = self.metameric_loss_stats(\n            image_stats, self.target_stats)\n        return loss\n    else:\n        raise Exception(\"Target of incorrect type\")\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.metameric_loss_uniform.MetamericLossUniform.__init__","title":"<code>__init__(device=torch.device('cpu'), pooling_size=32, n_pyramid_levels=5, n_orientations=2)</code>","text":"<p>Parameters:</p> <ul> <li> <code>pooling_size</code>           \u2013            <pre><code>                  Pooling size, in pixels. For example 32 will pool over 32x32 blocks of the image.\n</code></pre> </li> <li> <code>n_pyramid_levels</code>           \u2013            <pre><code>                  Number of levels of the steerable pyramid. Note that the image is padded\n                  so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                  too high will slow down the calculation a lot.\n</code></pre> </li> <li> <code>n_orientations</code>           \u2013            <pre><code>                  Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                  Increasing this will increase runtime.\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/metameric_loss_uniform.py</code> <pre><code>def __init__(self, device=torch.device('cpu'), pooling_size=32, n_pyramid_levels=5, n_orientations=2):\n    \"\"\"\n\n    Parameters\n    ----------\n    pooling_size            : int\n                              Pooling size, in pixels. For example 32 will pool over 32x32 blocks of the image.\n    n_pyramid_levels        : int \n                              Number of levels of the steerable pyramid. Note that the image is padded\n                              so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                              too high will slow down the calculation a lot.\n    n_orientations          : int \n                              Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                              Increasing this will increase runtime.\n\n    \"\"\"\n    self.target = None\n    self.device = device\n    self.pyramid_maker = None\n    self.pooling_size = pooling_size\n    self.n_pyramid_levels = n_pyramid_levels\n    self.n_orientations = n_orientations\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.metameric_loss_uniform.MetamericLossUniform.gen_metamer","title":"<code>gen_metamer(image)</code>","text":"<p>Generates a metamer for an image, following the method in this paper This function can be used on its own to generate a metamer for a desired image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>  Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>metamer</code> (              <code>tensor</code> )          \u2013            <p>The generated metamer image</p> </li> </ul> Source code in <code>odak/learn/perception/metameric_loss_uniform.py</code> <pre><code>def gen_metamer(self, image):\n    \"\"\" \n    Generates a metamer for an image, following the method in [this paper](https://dl.acm.org/doi/abs/10.1145/3450626.3459943)\n    This function can be used on its own to generate a metamer for a desired image.\n\n    Parameters\n    ----------\n    image   : torch.tensor\n              Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n\n    Returns\n    -------\n    metamer : torch.tensor\n              The generated metamer image\n    \"\"\"\n    image = rgb_2_ycrcb(image)\n    image_size = image.size()\n    image = pad_image_for_pyramid(image, self.n_pyramid_levels)\n\n    target_stats = self.calc_statsmaps(\n        image, self.pooling_size)\n    target_means = target_stats[::2]\n    target_stdevs = target_stats[1::2]\n    torch.manual_seed(0)\n    noise_image = torch.rand_like(image)\n    noise_pyramid = self.pyramid_maker.construct_pyramid(\n        noise_image, self.n_pyramid_levels)\n    input_pyramid = self.pyramid_maker.construct_pyramid(\n        image, self.n_pyramid_levels)\n\n    def match_level(input_level, target_mean, target_std):\n        level = input_level.clone()\n        level -= torch.mean(level)\n        input_std = torch.sqrt(torch.mean(level * level))\n        eps = 1e-6\n        # Safeguard against divide by zero\n        input_std[input_std &lt; eps] = eps\n        level /= input_std\n        level *= target_std\n        level += target_mean\n        return level\n\n    nbands = len(noise_pyramid[0][\"b\"])\n    noise_pyramid[0][\"h\"] = match_level(\n        noise_pyramid[0][\"h\"], target_means[0], target_stdevs[0])\n    for l in range(len(noise_pyramid)-1):\n        for b in range(nbands):\n            noise_pyramid[l][\"b\"][b] = match_level(\n                noise_pyramid[l][\"b\"][b], target_means[1 + l * nbands + b], target_stdevs[1 + l * nbands + b])\n    noise_pyramid[-1][\"l\"] = input_pyramid[-1][\"l\"]\n\n    metamer = self.pyramid_maker.reconstruct_from_pyramid(\n        noise_pyramid)\n    metamer = ycrcb_2_rgb(metamer)\n    # Crop to remove any padding\n    metamer = metamer[:image_size[0], :image_size[1], :image_size[2], :image_size[3]]\n    return metamer\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.metamer_mse_loss.MetamerMSELoss","title":"<code>MetamerMSELoss</code>","text":"<p>The <code>MetamerMSELoss</code> class provides a perceptual loss function. This generates a metamer for the target image, and then optimises the source image to be the same as this target image metamer.</p> <p>Please note this is different to <code>MetamericLoss</code> which optimises the source image to be any metamer of the target image.</p> <p>Its interface is similar to other <code>pytorch</code> loss functions, but note that the gaze location must be provided in addition to the source and target images.</p> Source code in <code>odak/learn/perception/metamer_mse_loss.py</code> <pre><code>class MetamerMSELoss():\n    \"\"\" \n    The `MetamerMSELoss` class provides a perceptual loss function. This generates a metamer for the target image, and then optimises the source image to be the same as this target image metamer.\n\n    Please note this is different to `MetamericLoss` which optimises the source image to be any metamer of the target image.\n\n    Its interface is similar to other `pytorch` loss functions, but note that the gaze location must be provided in addition to the source and target images.\n    \"\"\"\n\n\n    def __init__(self, device=torch.device(\"cpu\"),\n                 alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode=\"quadratic\",\n                 n_pyramid_levels=5, n_orientations=2, equi=False):\n        \"\"\"\n        Parameters\n        ----------\n        alpha                   : float\n                                    parameter controlling foveation - larger values mean bigger pooling regions.\n        real_image_width        : float \n                                    The real width of the image as displayed to the user.\n                                    Units don't matter as long as they are the same as for real_viewing_distance.\n        real_viewing_distance   : float \n                                    The real distance of the observer's eyes to the image plane.\n                                    Units don't matter as long as they are the same as for real_image_width.\n        n_pyramid_levels        : int \n                                    Number of levels of the steerable pyramid. Note that the image is padded\n                                    so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                                    too high will slow down the calculation a lot.\n        mode                    : str \n                                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                    as you move away from the fovea. We got best results with \"quadratic\".\n        n_orientations          : int \n                                    Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                                    Increasing this will increase runtime.\n        equi                    : bool\n                                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                    [-pi,pi]x[-pi/2,pi]\n        \"\"\"\n        self.target = None\n        self.target_metamer = None\n        self.metameric_loss = MetamericLoss(device=device, alpha=alpha, real_image_width=real_image_width,\n                                            real_viewing_distance=real_viewing_distance,\n                                            n_pyramid_levels=n_pyramid_levels, n_orientations=n_orientations, use_l2_foveal_loss=False, equi=equi)\n        self.loss_func = torch.nn.MSELoss()\n        self.noise = None\n\n\n    def gen_metamer(self, image, gaze):\n        \"\"\" \n        Generates a metamer for an image, following the method in [this paper](https://dl.acm.org/doi/abs/10.1145/3450626.3459943)\n        This function can be used on its own to generate a metamer for a desired image.\n\n        Parameters\n        ----------\n        image   : torch.tensor\n                Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n        gaze    : list\n                Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n        Returns\n        -------\n\n        metamer : torch.tensor\n                The generated metamer image\n        \"\"\"\n        image = rgb_2_ycrcb(image)\n        image_size = image.size()\n        image = pad_image_for_pyramid(image, self.metameric_loss.n_pyramid_levels)\n\n        target_stats = self.metameric_loss.calc_statsmaps(\n            image, gaze=gaze, alpha=self.metameric_loss.alpha)\n        target_means = target_stats[::2]\n        target_stdevs = target_stats[1::2]\n        if self.noise is None or self.noise.size() != image.size():\n            torch.manual_seed(0)\n            noise_image = torch.rand_like(image)\n        noise_pyramid = self.metameric_loss.pyramid_maker.construct_pyramid(\n            noise_image, self.metameric_loss.n_pyramid_levels)\n        input_pyramid = self.metameric_loss.pyramid_maker.construct_pyramid(\n            image, self.metameric_loss.n_pyramid_levels)\n\n\n        def match_level(input_level, target_mean, target_std):\n            level = input_level.clone()\n            level -= torch.mean(level)\n            input_std = torch.sqrt(torch.mean(level * level))\n            eps = 1e-6\n            # Safeguard against divide by zero\n            input_std[input_std &lt; eps] = eps\n            level /= input_std\n            level *= target_std\n            level += target_mean\n            return level\n\n        nbands = len(noise_pyramid[0][\"b\"])\n        noise_pyramid[0][\"h\"] = match_level(\n            noise_pyramid[0][\"h\"], target_means[0], target_stdevs[0])\n        for l in range(len(noise_pyramid)-1):\n            for b in range(nbands):\n                noise_pyramid[l][\"b\"][b] = match_level(\n                    noise_pyramid[l][\"b\"][b], target_means[1 + l * nbands + b], target_stdevs[1 + l * nbands + b])\n        noise_pyramid[-1][\"l\"] = input_pyramid[-1][\"l\"]\n\n        metamer = self.metameric_loss.pyramid_maker.reconstruct_from_pyramid(\n            noise_pyramid)\n        metamer = ycrcb_2_rgb(metamer)\n        # Crop to remove any padding\n        metamer = metamer[:image_size[0], :image_size[1], :image_size[2], :image_size[3]]\n        return metamer\n\n\n    def __call__(self, image, target, gaze = [0.5, 0.5]):\n        \"\"\" \n        Calculates the Metamer MSE Loss.\n\n        Parameters\n        ----------\n        image   : torch.tensor\n                  Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        target  : torch.tensor\n                  Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n        gaze    : list\n                   Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n        Returns\n        -------\n\n        loss    : torch.tensor\n                  The computed loss.\n        \"\"\"\n        check_loss_inputs(\"MetamerMSELoss\", image, target)\n        # Pad image and target if necessary\n        image = pad_image_for_pyramid(image, self.metameric_loss.n_pyramid_levels)\n        target = pad_image_for_pyramid(target, self.metameric_loss.n_pyramid_levels)\n\n        if target is not self.target or self.target is None:\n            self.target_metamer = self.gen_metamer(target, gaze)\n            self.target = target\n\n        return self.loss_func(image, self.target_metamer)\n\n\n    def to(self, device):\n        self.metameric_loss = self.metameric_loss.to(device)\n        return self\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.metamer_mse_loss.MetamerMSELoss.__call__","title":"<code>__call__(image, target, gaze=[0.5, 0.5])</code>","text":"<p>Calculates the Metamer MSE Loss.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>  Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>target</code>           \u2013            <pre><code>  Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>gaze</code>           \u2013            <pre><code>   Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>tensor</code> )          \u2013            <p>The computed loss.</p> </li> </ul> Source code in <code>odak/learn/perception/metamer_mse_loss.py</code> <pre><code>def __call__(self, image, target, gaze = [0.5, 0.5]):\n    \"\"\" \n    Calculates the Metamer MSE Loss.\n\n    Parameters\n    ----------\n    image   : torch.tensor\n              Image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    target  : torch.tensor\n              Ground truth target image to compute loss for. Should be an RGB image in NCHW format (4 dimensions)\n    gaze    : list\n               Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n    Returns\n    -------\n\n    loss    : torch.tensor\n              The computed loss.\n    \"\"\"\n    check_loss_inputs(\"MetamerMSELoss\", image, target)\n    # Pad image and target if necessary\n    image = pad_image_for_pyramid(image, self.metameric_loss.n_pyramid_levels)\n    target = pad_image_for_pyramid(target, self.metameric_loss.n_pyramid_levels)\n\n    if target is not self.target or self.target is None:\n        self.target_metamer = self.gen_metamer(target, gaze)\n        self.target = target\n\n    return self.loss_func(image, self.target_metamer)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.metamer_mse_loss.MetamerMSELoss.__init__","title":"<code>__init__(device=torch.device('cpu'), alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode='quadratic', n_pyramid_levels=5, n_orientations=2, equi=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>alpha</code>           \u2013            <pre><code>                    parameter controlling foveation - larger values mean bigger pooling regions.\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed to the user.\n                    Units don't matter as long as they are the same as for real_viewing_distance.\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance of the observer's eyes to the image plane.\n                    Units don't matter as long as they are the same as for real_image_width.\n</code></pre> </li> <li> <code>n_pyramid_levels</code>           \u2013            <pre><code>                    Number of levels of the steerable pyramid. Note that the image is padded\n                    so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                    too high will slow down the calculation a lot.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                    as you move away from the fovea. We got best results with \"quadratic\".\n</code></pre> </li> <li> <code>n_orientations</code>           \u2013            <pre><code>                    Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                    Increasing this will increase runtime.\n</code></pre> </li> <li> <code>equi</code>           \u2013            <pre><code>                    If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                    The gaze argument is instead interpreted as gaze angles, and should be in the range\n                    [-pi,pi]x[-pi/2,pi]\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/metamer_mse_loss.py</code> <pre><code>def __init__(self, device=torch.device(\"cpu\"),\n             alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, mode=\"quadratic\",\n             n_pyramid_levels=5, n_orientations=2, equi=False):\n    \"\"\"\n    Parameters\n    ----------\n    alpha                   : float\n                                parameter controlling foveation - larger values mean bigger pooling regions.\n    real_image_width        : float \n                                The real width of the image as displayed to the user.\n                                Units don't matter as long as they are the same as for real_viewing_distance.\n    real_viewing_distance   : float \n                                The real distance of the observer's eyes to the image plane.\n                                Units don't matter as long as they are the same as for real_image_width.\n    n_pyramid_levels        : int \n                                Number of levels of the steerable pyramid. Note that the image is padded\n                                so that both height and width are multiples of 2^(n_pyramid_levels), so setting this value\n                                too high will slow down the calculation a lot.\n    mode                    : str \n                                Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                as you move away from the fovea. We got best results with \"quadratic\".\n    n_orientations          : int \n                                Number of orientations in the steerable pyramid. Can be 1, 2, 4 or 6.\n                                Increasing this will increase runtime.\n    equi                    : bool\n                                If true, run the loss in equirectangular mode. The input is assumed to be an equirectangular\n                                format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                The gaze argument is instead interpreted as gaze angles, and should be in the range\n                                [-pi,pi]x[-pi/2,pi]\n    \"\"\"\n    self.target = None\n    self.target_metamer = None\n    self.metameric_loss = MetamericLoss(device=device, alpha=alpha, real_image_width=real_image_width,\n                                        real_viewing_distance=real_viewing_distance,\n                                        n_pyramid_levels=n_pyramid_levels, n_orientations=n_orientations, use_l2_foveal_loss=False, equi=equi)\n    self.loss_func = torch.nn.MSELoss()\n    self.noise = None\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.metamer_mse_loss.MetamerMSELoss.gen_metamer","title":"<code>gen_metamer(image, gaze)</code>","text":"<p>Generates a metamer for an image, following the method in this paper This function can be used on its own to generate a metamer for a desired image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n</code></pre> </li> <li> <code>gaze</code>           \u2013            <pre><code>Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>metamer</code> (              <code>tensor</code> )          \u2013            <p>The generated metamer image</p> </li> </ul> Source code in <code>odak/learn/perception/metamer_mse_loss.py</code> <pre><code>def gen_metamer(self, image, gaze):\n    \"\"\" \n    Generates a metamer for an image, following the method in [this paper](https://dl.acm.org/doi/abs/10.1145/3450626.3459943)\n    This function can be used on its own to generate a metamer for a desired image.\n\n    Parameters\n    ----------\n    image   : torch.tensor\n            Image to compute metamer for. Should be an RGB image in NCHW format (4 dimensions)\n    gaze    : list\n            Gaze location in the image, in normalized image coordinates (range [0, 1]) relative to the top left of the image.\n\n    Returns\n    -------\n\n    metamer : torch.tensor\n            The generated metamer image\n    \"\"\"\n    image = rgb_2_ycrcb(image)\n    image_size = image.size()\n    image = pad_image_for_pyramid(image, self.metameric_loss.n_pyramid_levels)\n\n    target_stats = self.metameric_loss.calc_statsmaps(\n        image, gaze=gaze, alpha=self.metameric_loss.alpha)\n    target_means = target_stats[::2]\n    target_stdevs = target_stats[1::2]\n    if self.noise is None or self.noise.size() != image.size():\n        torch.manual_seed(0)\n        noise_image = torch.rand_like(image)\n    noise_pyramid = self.metameric_loss.pyramid_maker.construct_pyramid(\n        noise_image, self.metameric_loss.n_pyramid_levels)\n    input_pyramid = self.metameric_loss.pyramid_maker.construct_pyramid(\n        image, self.metameric_loss.n_pyramid_levels)\n\n\n    def match_level(input_level, target_mean, target_std):\n        level = input_level.clone()\n        level -= torch.mean(level)\n        input_std = torch.sqrt(torch.mean(level * level))\n        eps = 1e-6\n        # Safeguard against divide by zero\n        input_std[input_std &lt; eps] = eps\n        level /= input_std\n        level *= target_std\n        level += target_mean\n        return level\n\n    nbands = len(noise_pyramid[0][\"b\"])\n    noise_pyramid[0][\"h\"] = match_level(\n        noise_pyramid[0][\"h\"], target_means[0], target_stdevs[0])\n    for l in range(len(noise_pyramid)-1):\n        for b in range(nbands):\n            noise_pyramid[l][\"b\"][b] = match_level(\n                noise_pyramid[l][\"b\"][b], target_means[1 + l * nbands + b], target_stdevs[1 + l * nbands + b])\n    noise_pyramid[-1][\"l\"] = input_pyramid[-1][\"l\"]\n\n    metamer = self.metameric_loss.pyramid_maker.reconstruct_from_pyramid(\n        noise_pyramid)\n    metamer = ycrcb_2_rgb(metamer)\n    # Crop to remove any padding\n    metamer = metamer[:image_size[0], :image_size[1], :image_size[2], :image_size[3]]\n    return metamer\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.radially_varying_blur.RadiallyVaryingBlur","title":"<code>RadiallyVaryingBlur</code>","text":"<p>The <code>RadiallyVaryingBlur</code> class provides a way to apply a radially varying blur to an image. Given a gaze location and information about the image and foveation, it applies a blur that will achieve the proper pooling size. The pooling size is chosen to appear the same at a range of display sizes and viewing distances, for a given <code>alpha</code> parameter value. For more information on how the pooling sizes are computed, please see link coming soon.</p> <p>The blur is accelerated by generating and sampling from MIP maps of the input image.</p> <p>This class caches the foveation information. This means that if it is run repeatedly with the same foveation parameters, gaze location and image size (e.g. in an optimisation loop) it won't recalculate the pooling maps.</p> <p>If you are repeatedly applying blur to images of different sizes (e.g. a pyramid) for best performance use one instance of this class per image size.</p> Source code in <code>odak/learn/perception/radially_varying_blur.py</code> <pre><code>class RadiallyVaryingBlur():\n    \"\"\" \n\n    The `RadiallyVaryingBlur` class provides a way to apply a radially varying blur to an image. Given a gaze location and information about the image and foveation, it applies a blur that will achieve the proper pooling size. The pooling size is chosen to appear the same at a range of display sizes and viewing distances, for a given `alpha` parameter value. For more information on how the pooling sizes are computed, please see [link coming soon]().\n\n    The blur is accelerated by generating and sampling from MIP maps of the input image.\n\n    This class caches the foveation information. This means that if it is run repeatedly with the same foveation parameters, gaze location and image size (e.g. in an optimisation loop) it won't recalculate the pooling maps.\n\n    If you are repeatedly applying blur to images of different sizes (e.g. a pyramid) for best performance use one instance of this class per image size.\n\n    \"\"\"\n\n    def __init__(self):\n        self.lod_map = None\n        self.equi = None\n\n    def blur(self, image, alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, centre=None, mode=\"quadratic\", equi=False):\n        \"\"\"\n        Apply the radially varying blur to an image.\n\n        Parameters\n        ----------\n\n        image                   : torch.tensor\n                                    The image to blur, in NCHW format.\n        alpha                   : float\n                                    parameter controlling foveation - larger values mean bigger pooling regions.\n        real_image_width        : float \n                                    The real width of the image as displayed to the user.\n                                    Units don't matter as long as they are the same as for real_viewing_distance.\n                                    Ignored in equirectangular mode (equi==True)\n        real_viewing_distance   : float \n                                    The real distance of the observer's eyes to the image plane.\n                                    Units don't matter as long as they are the same as for real_image_width.\n                                    Ignored in equirectangular mode (equi==True)\n        centre                  : tuple of floats\n                                    The centre of the radially varying blur (the gaze location).\n                                    Should be a tuple of floats containing normalised image coordinates in range [0,1]\n                                    In equirectangular mode this should be yaw &amp; pitch angles in [-pi,pi]x[-pi/2,pi/2]\n        mode                    : str \n                                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                    as you move away from the fovea. We got best results with \"quadratic\".\n        equi                    : bool\n                                    If true, run the blur function in equirectangular mode. The input is assumed to be an equirectangular\n                                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                    The centre argument is instead interpreted as gaze angles, and should be in the range\n                                    [-pi,pi]x[-pi/2,pi]\n\n        Returns\n        -------\n\n        output                  : torch.tensor\n                                    The blurred image\n        \"\"\"\n        size = (image.size(-2), image.size(-1))\n\n        # LOD map caching\n        if self.lod_map is None or\\\n                self.size != size or\\\n                self.n_channels != image.size(1) or\\\n                self.alpha != alpha or\\\n                self.real_image_width != real_image_width or\\\n                self.real_viewing_distance != real_viewing_distance or\\\n                self.centre != centre or\\\n                self.mode != mode or\\\n                self.equi != equi:\n            if not equi:\n                self.lod_map = make_pooling_size_map_lod(\n                    centre, (image.size(-2), image.size(-1)), alpha, real_image_width, real_viewing_distance, mode)\n            else:\n                self.lod_map = make_equi_pooling_size_map_lod(\n                    centre, (image.size(-2), image.size(-1)), alpha, mode)\n            self.size = size\n            self.n_channels = image.size(1)\n            self.alpha = alpha\n            self.real_image_width = real_image_width\n            self.real_viewing_distance = real_viewing_distance\n            self.centre = centre\n            self.lod_map = self.lod_map.to(image.device)\n            self.lod_fraction = torch.fmod(self.lod_map, 1.0)\n            self.lod_fraction = self.lod_fraction[None, None, ...].repeat(\n                1, image.size(1), 1, 1)\n            self.mode = mode\n            self.equi = equi\n\n        if self.lod_map.device != image.device:\n            self.lod_map = self.lod_map.to(image.device)\n        if self.lod_fraction.device != image.device:\n            self.lod_fraction = self.lod_fraction.to(image.device)\n\n        mipmap = [image]\n        while mipmap[-1].size(-1) &gt; 1 and mipmap[-1].size(-2) &gt; 1:\n            mipmap.append(torch.nn.functional.interpolate(\n                mipmap[-1], scale_factor=0.5, mode=\"area\", recompute_scale_factor=False))\n        if mipmap[-1].size(-1) == 2:\n            final_mip = torch.mean(mipmap[-1], axis=-1)[..., None]\n            mipmap.append(final_mip)\n        if mipmap[-1].size(-2) == 2:\n            final_mip = torch.mean(mipmap[-2], axis=-2)[..., None, :]\n            mipmap.append(final_mip)\n\n        for l in range(len(mipmap)):\n            if l == len(mipmap)-1:\n                mipmap[l] = mipmap[l] * \\\n                    torch.ones(image.size(), device=image.device)\n            else:\n                for l2 in range(l-1, -1, -1):\n                    mipmap[l] = torch.nn.functional.interpolate(mipmap[l], size=(\n                        image.size(-2), image.size(-1)), mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n\n        output = torch.zeros(image.size(), device=image.device)\n        for l in range(len(mipmap)):\n            if l == 0:\n                mask = self.lod_map &lt; (l+1)\n            elif l == len(mipmap)-1:\n                mask = self.lod_map &gt;= l\n            else:\n                mask = torch.logical_and(\n                    self.lod_map &gt;= l, self.lod_map &lt; (l+1))\n\n            if l == len(mipmap)-1:\n                blended_levels = mipmap[l]\n            else:\n                blended_levels = (1 - self.lod_fraction) * \\\n                    mipmap[l] + self.lod_fraction*mipmap[l+1]\n            mask = mask[None, None, ...]\n            mask = mask.repeat(image.size(0), image.size(1), 1, 1)\n            output[mask] = blended_levels[mask]\n\n        return output\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.radially_varying_blur.RadiallyVaryingBlur.blur","title":"<code>blur(image, alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, centre=None, mode='quadratic', equi=False)</code>","text":"<p>Apply the radially varying blur to an image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>                    The image to blur, in NCHW format.\n</code></pre> </li> <li> <code>alpha</code>           \u2013            <pre><code>                    parameter controlling foveation - larger values mean bigger pooling regions.\n</code></pre> </li> <li> <code>real_image_width</code>           \u2013            <pre><code>                    The real width of the image as displayed to the user.\n                    Units don't matter as long as they are the same as for real_viewing_distance.\n                    Ignored in equirectangular mode (equi==True)\n</code></pre> </li> <li> <code>real_viewing_distance</code>           \u2013            <pre><code>                    The real distance of the observer's eyes to the image plane.\n                    Units don't matter as long as they are the same as for real_image_width.\n                    Ignored in equirectangular mode (equi==True)\n</code></pre> </li> <li> <code>centre</code>           \u2013            <pre><code>                    The centre of the radially varying blur (the gaze location).\n                    Should be a tuple of floats containing normalised image coordinates in range [0,1]\n                    In equirectangular mode this should be yaw &amp; pitch angles in [-pi,pi]x[-pi/2,pi/2]\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>                    Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                    as you move away from the fovea. We got best results with \"quadratic\".\n</code></pre> </li> <li> <code>equi</code>           \u2013            <pre><code>                    If true, run the blur function in equirectangular mode. The input is assumed to be an equirectangular\n                    format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                    The centre argument is instead interpreted as gaze angles, and should be in the range\n                    [-pi,pi]x[-pi/2,pi]\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>The blurred image</p> </li> </ul> Source code in <code>odak/learn/perception/radially_varying_blur.py</code> <pre><code>def blur(self, image, alpha=0.2, real_image_width=0.2, real_viewing_distance=0.7, centre=None, mode=\"quadratic\", equi=False):\n    \"\"\"\n    Apply the radially varying blur to an image.\n\n    Parameters\n    ----------\n\n    image                   : torch.tensor\n                                The image to blur, in NCHW format.\n    alpha                   : float\n                                parameter controlling foveation - larger values mean bigger pooling regions.\n    real_image_width        : float \n                                The real width of the image as displayed to the user.\n                                Units don't matter as long as they are the same as for real_viewing_distance.\n                                Ignored in equirectangular mode (equi==True)\n    real_viewing_distance   : float \n                                The real distance of the observer's eyes to the image plane.\n                                Units don't matter as long as they are the same as for real_image_width.\n                                Ignored in equirectangular mode (equi==True)\n    centre                  : tuple of floats\n                                The centre of the radially varying blur (the gaze location).\n                                Should be a tuple of floats containing normalised image coordinates in range [0,1]\n                                In equirectangular mode this should be yaw &amp; pitch angles in [-pi,pi]x[-pi/2,pi/2]\n    mode                    : str \n                                Foveation mode, either \"quadratic\" or \"linear\". Controls how pooling regions grow\n                                as you move away from the fovea. We got best results with \"quadratic\".\n    equi                    : bool\n                                If true, run the blur function in equirectangular mode. The input is assumed to be an equirectangular\n                                format 360 image. The settings real_image_width and real_viewing distance are ignored.\n                                The centre argument is instead interpreted as gaze angles, and should be in the range\n                                [-pi,pi]x[-pi/2,pi]\n\n    Returns\n    -------\n\n    output                  : torch.tensor\n                                The blurred image\n    \"\"\"\n    size = (image.size(-2), image.size(-1))\n\n    # LOD map caching\n    if self.lod_map is None or\\\n            self.size != size or\\\n            self.n_channels != image.size(1) or\\\n            self.alpha != alpha or\\\n            self.real_image_width != real_image_width or\\\n            self.real_viewing_distance != real_viewing_distance or\\\n            self.centre != centre or\\\n            self.mode != mode or\\\n            self.equi != equi:\n        if not equi:\n            self.lod_map = make_pooling_size_map_lod(\n                centre, (image.size(-2), image.size(-1)), alpha, real_image_width, real_viewing_distance, mode)\n        else:\n            self.lod_map = make_equi_pooling_size_map_lod(\n                centre, (image.size(-2), image.size(-1)), alpha, mode)\n        self.size = size\n        self.n_channels = image.size(1)\n        self.alpha = alpha\n        self.real_image_width = real_image_width\n        self.real_viewing_distance = real_viewing_distance\n        self.centre = centre\n        self.lod_map = self.lod_map.to(image.device)\n        self.lod_fraction = torch.fmod(self.lod_map, 1.0)\n        self.lod_fraction = self.lod_fraction[None, None, ...].repeat(\n            1, image.size(1), 1, 1)\n        self.mode = mode\n        self.equi = equi\n\n    if self.lod_map.device != image.device:\n        self.lod_map = self.lod_map.to(image.device)\n    if self.lod_fraction.device != image.device:\n        self.lod_fraction = self.lod_fraction.to(image.device)\n\n    mipmap = [image]\n    while mipmap[-1].size(-1) &gt; 1 and mipmap[-1].size(-2) &gt; 1:\n        mipmap.append(torch.nn.functional.interpolate(\n            mipmap[-1], scale_factor=0.5, mode=\"area\", recompute_scale_factor=False))\n    if mipmap[-1].size(-1) == 2:\n        final_mip = torch.mean(mipmap[-1], axis=-1)[..., None]\n        mipmap.append(final_mip)\n    if mipmap[-1].size(-2) == 2:\n        final_mip = torch.mean(mipmap[-2], axis=-2)[..., None, :]\n        mipmap.append(final_mip)\n\n    for l in range(len(mipmap)):\n        if l == len(mipmap)-1:\n            mipmap[l] = mipmap[l] * \\\n                torch.ones(image.size(), device=image.device)\n        else:\n            for l2 in range(l-1, -1, -1):\n                mipmap[l] = torch.nn.functional.interpolate(mipmap[l], size=(\n                    image.size(-2), image.size(-1)), mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n\n    output = torch.zeros(image.size(), device=image.device)\n    for l in range(len(mipmap)):\n        if l == 0:\n            mask = self.lod_map &lt; (l+1)\n        elif l == len(mipmap)-1:\n            mask = self.lod_map &gt;= l\n        else:\n            mask = torch.logical_and(\n                self.lod_map &gt;= l, self.lod_map &lt; (l+1))\n\n        if l == len(mipmap)-1:\n            blended_levels = mipmap[l]\n        else:\n            blended_levels = (1 - self.lod_fraction) * \\\n                mipmap[l] + self.lod_fraction*mipmap[l+1]\n        mask = mask[None, None, ...]\n        mask = mask.repeat(image.size(0), image.size(1), 1, 1)\n        output[mask] = blended_levels[mask]\n\n    return output\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.spatial_steerable_pyramid.SpatialSteerablePyramid","title":"<code>SpatialSteerablePyramid</code>","text":"<p>This implements a real-valued steerable pyramid where the filtering is carried out spatially (using convolution) as opposed to multiplication in the Fourier domain. This has a number of optimisations over previous implementations that increase efficiency, but introduce some reconstruction error.</p> Source code in <code>odak/learn/perception/spatial_steerable_pyramid.py</code> <pre><code>class SpatialSteerablePyramid():\n    \"\"\"\n    This implements a real-valued steerable pyramid where the filtering is carried out spatially (using convolution)\n    as opposed to multiplication in the Fourier domain.\n    This has a number of optimisations over previous implementations that increase efficiency, but introduce some\n    reconstruction error.\n    \"\"\"\n\n\n    def __init__(self, use_bilinear_downup=True, n_channels=1,\n                 filter_size=9, n_orientations=6, filter_type=\"full\",\n                 device=torch.device('cpu')):\n        \"\"\"\n        Parameters\n        ----------\n\n        use_bilinear_downup     : bool\n                                    This uses bilinear filtering when upsampling/downsampling, rather than the original approach\n                                    of applying a large lowpass kernel and sampling even rows/columns\n        n_channels              : int\n                                    Number of channels in the input images (e.g. 3 for RGB input)\n        filter_size             : int\n                                    Desired size of filters (e.g. 3 will use 3x3 filters).\n        n_orientations          : int\n                                    Number of oriented bands in each level of the pyramid.\n        filter_type             : str\n                                    This can be used to select smaller filters than the original ones if desired.\n                                    full: Original filter sizes\n                                    cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate.\n                                    trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels.\n        device                  : torch.device\n                                    torch device the input images will be supplied from.\n        \"\"\"\n        self.use_bilinear_downup = use_bilinear_downup\n        self.device = device\n\n        filters = get_steerable_pyramid_filters(\n            filter_size, n_orientations, filter_type)\n\n        def make_pad(filter):\n            filter_size = filter.size(-1)\n            pad_amt = (filter_size-1) // 2\n            return torch.nn.ReflectionPad2d((pad_amt, pad_amt, pad_amt, pad_amt))\n\n        if not self.use_bilinear_downup:\n            self.filt_l = filters[\"l\"].to(device)\n            self.pad_l = make_pad(self.filt_l)\n        self.filt_l0 = filters[\"l0\"].to(device)\n        self.pad_l0 = make_pad(self.filt_l0)\n        self.filt_h0 = filters[\"h0\"].to(device)\n        self.pad_h0 = make_pad(self.filt_h0)\n        for b in range(len(filters[\"b\"])):\n            filters[\"b\"][b] = filters[\"b\"][b].to(device)\n        self.band_filters = filters[\"b\"]\n        self.pad_b = make_pad(self.band_filters[0])\n\n        if n_channels != 1:\n            def add_channels_to_filter(filter):\n                padded = torch.zeros(n_channels, n_channels, filter.size()[\n                                     2], filter.size()[3]).to(device)\n                for channel in range(n_channels):\n                    padded[channel, channel, :, :] = filter\n                return padded\n            self.filt_h0 = add_channels_to_filter(self.filt_h0)\n            for b in range(len(self.band_filters)):\n                self.band_filters[b] = add_channels_to_filter(\n                    self.band_filters[b])\n            self.filt_l0 = add_channels_to_filter(self.filt_l0)\n            if not self.use_bilinear_downup:\n                self.filt_l = add_channels_to_filter(self.filt_l)\n\n    def construct_pyramid(self, image, n_levels, multiple_highpass=False):\n        \"\"\"\n        Constructs and returns a steerable pyramid for the provided image.\n\n        Parameters\n        ----------\n\n        image               : torch.tensor\n                                The input image, in NCHW format. The number of channels C should match num_channels\n                                when the pyramid maker was created.\n        n_levels            : int\n                                Number of levels in the constructed steerable pyramid.\n        multiple_highpass   : bool\n                                If true, computes a highpass for each level of the pyramid.\n                                These extra levels are redundant (not used for reconstruction).\n\n        Returns\n        -------\n\n        pyramid             : list of dicts of torch.tensor\n                                The computed steerable pyramid.\n                                Each level is an entry in a list. The pyramid is ordered from largest levels to smallest levels.\n                                Each level is stored as a dict, with the following keys:\n                                \"h\" Highpass residual\n                                \"l\" Lowpass residual\n                                \"b\" Oriented bands (a list of torch.tensor)\n        \"\"\"\n        pyramid = []\n\n        # Make level 0, containing highpass, lowpass and the bands\n        level0 = {}\n        level0['h'] = torch.nn.functional.conv2d(\n            self.pad_h0(image), self.filt_h0)\n        lowpass = torch.nn.functional.conv2d(self.pad_l0(image), self.filt_l0)\n        level0['l'] = lowpass.clone()\n        bands = []\n        for filt_b in self.band_filters:\n            bands.append(torch.nn.functional.conv2d(\n                self.pad_b(lowpass), filt_b))\n        level0['b'] = bands\n        pyramid.append(level0)\n\n        # Make intermediate levels\n        for l in range(n_levels-2):\n            level = {}\n            if self.use_bilinear_downup:\n                lowpass = torch.nn.functional.interpolate(\n                    lowpass, scale_factor=0.5, mode=\"area\", recompute_scale_factor=False)\n            else:\n                lowpass = torch.nn.functional.conv2d(\n                    self.pad_l(lowpass), self.filt_l)\n                lowpass = lowpass[:, :, ::2, ::2]\n            level['l'] = lowpass.clone()\n            bands = []\n            for filt_b in self.band_filters:\n                bands.append(torch.nn.functional.conv2d(\n                    self.pad_b(lowpass), filt_b))\n            level['b'] = bands\n            if multiple_highpass:\n                level['h'] = torch.nn.functional.conv2d(\n                    self.pad_h0(lowpass), self.filt_h0)\n            pyramid.append(level)\n\n        # Make final level (lowpass residual)\n        level = {}\n        if self.use_bilinear_downup:\n            lowpass = torch.nn.functional.interpolate(\n                lowpass, scale_factor=0.5, mode=\"area\", recompute_scale_factor=False)\n        else:\n            lowpass = torch.nn.functional.conv2d(\n                self.pad_l(lowpass), self.filt_l)\n            lowpass = lowpass[:, :, ::2, ::2]\n        level['l'] = lowpass\n        pyramid.append(level)\n\n        return pyramid\n\n    def reconstruct_from_pyramid(self, pyramid):\n        \"\"\"\n        Reconstructs an input image from a steerable pyramid.\n\n        Parameters\n        ----------\n\n        pyramid : list of dicts of torch.tensor\n                    The steerable pyramid.\n                    Should be in the same format as output by construct_steerable_pyramid().\n                    The number of channels should match num_channels when the pyramid maker was created.\n\n        Returns\n        -------\n\n        image   : torch.tensor\n                    The reconstructed image, in NCHW format.         \n        \"\"\"\n        def upsample(image, size):\n            if self.use_bilinear_downup:\n                return torch.nn.functional.interpolate(image, size=size, mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n            else:\n                zeros = torch.zeros((image.size()[0], image.size()[1], image.size()[\n                                    2]*2, image.size()[3]*2)).to(self.device)\n                zeros[:, :, ::2, ::2] = image\n                zeros = torch.nn.functional.conv2d(\n                    self.pad_l(zeros), self.filt_l)\n                return zeros\n\n        image = pyramid[-1]['l']\n        for level in reversed(pyramid[:-1]):\n            image = upsample(image, level['b'][0].size()[2:])\n            for b in range(len(level['b'])):\n                b_filtered = torch.nn.functional.conv2d(\n                    self.pad_b(level['b'][b]), -self.band_filters[b])\n                image += b_filtered\n\n        image = torch.nn.functional.conv2d(self.pad_l0(image), self.filt_l0)\n        image += torch.nn.functional.conv2d(\n            self.pad_h0(pyramid[0]['h']), self.filt_h0)\n\n        return image\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.spatial_steerable_pyramid.SpatialSteerablePyramid.__init__","title":"<code>__init__(use_bilinear_downup=True, n_channels=1, filter_size=9, n_orientations=6, filter_type='full', device=torch.device('cpu'))</code>","text":"<p>Parameters:</p> <ul> <li> <code>use_bilinear_downup</code>           \u2013            <pre><code>                    This uses bilinear filtering when upsampling/downsampling, rather than the original approach\n                    of applying a large lowpass kernel and sampling even rows/columns\n</code></pre> </li> <li> <code>n_channels</code>           \u2013            <pre><code>                    Number of channels in the input images (e.g. 3 for RGB input)\n</code></pre> </li> <li> <code>filter_size</code>           \u2013            <pre><code>                    Desired size of filters (e.g. 3 will use 3x3 filters).\n</code></pre> </li> <li> <code>n_orientations</code>           \u2013            <pre><code>                    Number of oriented bands in each level of the pyramid.\n</code></pre> </li> <li> <code>filter_type</code>           \u2013            <pre><code>                    This can be used to select smaller filters than the original ones if desired.\n                    full: Original filter sizes\n                    cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate.\n                    trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>                    torch device the input images will be supplied from.\n</code></pre> </li> </ul> Source code in <code>odak/learn/perception/spatial_steerable_pyramid.py</code> <pre><code>def __init__(self, use_bilinear_downup=True, n_channels=1,\n             filter_size=9, n_orientations=6, filter_type=\"full\",\n             device=torch.device('cpu')):\n    \"\"\"\n    Parameters\n    ----------\n\n    use_bilinear_downup     : bool\n                                This uses bilinear filtering when upsampling/downsampling, rather than the original approach\n                                of applying a large lowpass kernel and sampling even rows/columns\n    n_channels              : int\n                                Number of channels in the input images (e.g. 3 for RGB input)\n    filter_size             : int\n                                Desired size of filters (e.g. 3 will use 3x3 filters).\n    n_orientations          : int\n                                Number of oriented bands in each level of the pyramid.\n    filter_type             : str\n                                This can be used to select smaller filters than the original ones if desired.\n                                full: Original filter sizes\n                                cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate.\n                                trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels.\n    device                  : torch.device\n                                torch device the input images will be supplied from.\n    \"\"\"\n    self.use_bilinear_downup = use_bilinear_downup\n    self.device = device\n\n    filters = get_steerable_pyramid_filters(\n        filter_size, n_orientations, filter_type)\n\n    def make_pad(filter):\n        filter_size = filter.size(-1)\n        pad_amt = (filter_size-1) // 2\n        return torch.nn.ReflectionPad2d((pad_amt, pad_amt, pad_amt, pad_amt))\n\n    if not self.use_bilinear_downup:\n        self.filt_l = filters[\"l\"].to(device)\n        self.pad_l = make_pad(self.filt_l)\n    self.filt_l0 = filters[\"l0\"].to(device)\n    self.pad_l0 = make_pad(self.filt_l0)\n    self.filt_h0 = filters[\"h0\"].to(device)\n    self.pad_h0 = make_pad(self.filt_h0)\n    for b in range(len(filters[\"b\"])):\n        filters[\"b\"][b] = filters[\"b\"][b].to(device)\n    self.band_filters = filters[\"b\"]\n    self.pad_b = make_pad(self.band_filters[0])\n\n    if n_channels != 1:\n        def add_channels_to_filter(filter):\n            padded = torch.zeros(n_channels, n_channels, filter.size()[\n                                 2], filter.size()[3]).to(device)\n            for channel in range(n_channels):\n                padded[channel, channel, :, :] = filter\n            return padded\n        self.filt_h0 = add_channels_to_filter(self.filt_h0)\n        for b in range(len(self.band_filters)):\n            self.band_filters[b] = add_channels_to_filter(\n                self.band_filters[b])\n        self.filt_l0 = add_channels_to_filter(self.filt_l0)\n        if not self.use_bilinear_downup:\n            self.filt_l = add_channels_to_filter(self.filt_l)\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.spatial_steerable_pyramid.SpatialSteerablePyramid.construct_pyramid","title":"<code>construct_pyramid(image, n_levels, multiple_highpass=False)</code>","text":"<p>Constructs and returns a steerable pyramid for the provided image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>                The input image, in NCHW format. The number of channels C should match num_channels\n                when the pyramid maker was created.\n</code></pre> </li> <li> <code>n_levels</code>           \u2013            <pre><code>                Number of levels in the constructed steerable pyramid.\n</code></pre> </li> <li> <code>multiple_highpass</code>           \u2013            <pre><code>                If true, computes a highpass for each level of the pyramid.\n                These extra levels are redundant (not used for reconstruction).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>pyramid</code> (              <code>list of dicts of torch.tensor</code> )          \u2013            <p>The computed steerable pyramid. Each level is an entry in a list. The pyramid is ordered from largest levels to smallest levels. Each level is stored as a dict, with the following keys: \"h\" Highpass residual \"l\" Lowpass residual \"b\" Oriented bands (a list of torch.tensor)</p> </li> </ul> Source code in <code>odak/learn/perception/spatial_steerable_pyramid.py</code> <pre><code>def construct_pyramid(self, image, n_levels, multiple_highpass=False):\n    \"\"\"\n    Constructs and returns a steerable pyramid for the provided image.\n\n    Parameters\n    ----------\n\n    image               : torch.tensor\n                            The input image, in NCHW format. The number of channels C should match num_channels\n                            when the pyramid maker was created.\n    n_levels            : int\n                            Number of levels in the constructed steerable pyramid.\n    multiple_highpass   : bool\n                            If true, computes a highpass for each level of the pyramid.\n                            These extra levels are redundant (not used for reconstruction).\n\n    Returns\n    -------\n\n    pyramid             : list of dicts of torch.tensor\n                            The computed steerable pyramid.\n                            Each level is an entry in a list. The pyramid is ordered from largest levels to smallest levels.\n                            Each level is stored as a dict, with the following keys:\n                            \"h\" Highpass residual\n                            \"l\" Lowpass residual\n                            \"b\" Oriented bands (a list of torch.tensor)\n    \"\"\"\n    pyramid = []\n\n    # Make level 0, containing highpass, lowpass and the bands\n    level0 = {}\n    level0['h'] = torch.nn.functional.conv2d(\n        self.pad_h0(image), self.filt_h0)\n    lowpass = torch.nn.functional.conv2d(self.pad_l0(image), self.filt_l0)\n    level0['l'] = lowpass.clone()\n    bands = []\n    for filt_b in self.band_filters:\n        bands.append(torch.nn.functional.conv2d(\n            self.pad_b(lowpass), filt_b))\n    level0['b'] = bands\n    pyramid.append(level0)\n\n    # Make intermediate levels\n    for l in range(n_levels-2):\n        level = {}\n        if self.use_bilinear_downup:\n            lowpass = torch.nn.functional.interpolate(\n                lowpass, scale_factor=0.5, mode=\"area\", recompute_scale_factor=False)\n        else:\n            lowpass = torch.nn.functional.conv2d(\n                self.pad_l(lowpass), self.filt_l)\n            lowpass = lowpass[:, :, ::2, ::2]\n        level['l'] = lowpass.clone()\n        bands = []\n        for filt_b in self.band_filters:\n            bands.append(torch.nn.functional.conv2d(\n                self.pad_b(lowpass), filt_b))\n        level['b'] = bands\n        if multiple_highpass:\n            level['h'] = torch.nn.functional.conv2d(\n                self.pad_h0(lowpass), self.filt_h0)\n        pyramid.append(level)\n\n    # Make final level (lowpass residual)\n    level = {}\n    if self.use_bilinear_downup:\n        lowpass = torch.nn.functional.interpolate(\n            lowpass, scale_factor=0.5, mode=\"area\", recompute_scale_factor=False)\n    else:\n        lowpass = torch.nn.functional.conv2d(\n            self.pad_l(lowpass), self.filt_l)\n        lowpass = lowpass[:, :, ::2, ::2]\n    level['l'] = lowpass\n    pyramid.append(level)\n\n    return pyramid\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.spatial_steerable_pyramid.SpatialSteerablePyramid.reconstruct_from_pyramid","title":"<code>reconstruct_from_pyramid(pyramid)</code>","text":"<p>Reconstructs an input image from a steerable pyramid.</p> <p>Parameters:</p> <ul> <li> <code>pyramid</code>               (<code>list of dicts of torch.tensor</code>)           \u2013            <pre><code>    The steerable pyramid.\n    Should be in the same format as output by construct_steerable_pyramid().\n    The number of channels should match num_channels when the pyramid maker was created.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image</code> (              <code>tensor</code> )          \u2013            <p>The reconstructed image, in NCHW format.</p> </li> </ul> Source code in <code>odak/learn/perception/spatial_steerable_pyramid.py</code> <pre><code>def reconstruct_from_pyramid(self, pyramid):\n    \"\"\"\n    Reconstructs an input image from a steerable pyramid.\n\n    Parameters\n    ----------\n\n    pyramid : list of dicts of torch.tensor\n                The steerable pyramid.\n                Should be in the same format as output by construct_steerable_pyramid().\n                The number of channels should match num_channels when the pyramid maker was created.\n\n    Returns\n    -------\n\n    image   : torch.tensor\n                The reconstructed image, in NCHW format.         \n    \"\"\"\n    def upsample(image, size):\n        if self.use_bilinear_downup:\n            return torch.nn.functional.interpolate(image, size=size, mode=\"bilinear\", align_corners=False, recompute_scale_factor=False)\n        else:\n            zeros = torch.zeros((image.size()[0], image.size()[1], image.size()[\n                                2]*2, image.size()[3]*2)).to(self.device)\n            zeros[:, :, ::2, ::2] = image\n            zeros = torch.nn.functional.conv2d(\n                self.pad_l(zeros), self.filt_l)\n            return zeros\n\n    image = pyramid[-1]['l']\n    for level in reversed(pyramid[:-1]):\n        image = upsample(image, level['b'][0].size()[2:])\n        for b in range(len(level['b'])):\n            b_filtered = torch.nn.functional.conv2d(\n                self.pad_b(level['b'][b]), -self.band_filters[b])\n            image += b_filtered\n\n    image = torch.nn.functional.conv2d(self.pad_l0(image), self.filt_l0)\n    image += torch.nn.functional.conv2d(\n        self.pad_h0(pyramid[0]['h']), self.filt_h0)\n\n    return image\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.spatial_steerable_pyramid.pad_image_for_pyramid","title":"<code>pad_image_for_pyramid(image, n_pyramid_levels)</code>","text":"<p>Pads an image to the extent necessary to compute a steerable pyramid of the input image. This involves padding so both height and width are divisible by 2**n_pyramid_levels. Uses reflection padding.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <p>Image to pad, in NCHW format</p> </li> <li> <code>n_pyramid_levels</code>           \u2013            <p>Number of levels in the pyramid you plan to construct.</p> </li> </ul> Source code in <code>odak/learn/perception/spatial_steerable_pyramid.py</code> <pre><code>def pad_image_for_pyramid(image, n_pyramid_levels):\n    \"\"\"\n    Pads an image to the extent necessary to compute a steerable pyramid of the input image.\n    This involves padding so both height and width are divisible by 2**n_pyramid_levels.\n    Uses reflection padding.\n\n    Parameters\n    ----------\n\n    image: torch.tensor\n        Image to pad, in NCHW format\n    n_pyramid_levels: int\n        Number of levels in the pyramid you plan to construct.\n    \"\"\"\n    min_divisor = 2 ** n_pyramid_levels\n    height = image.size(2)\n    width = image.size(3)\n    required_height = math.ceil(height / min_divisor) * min_divisor\n    required_width = math.ceil(width / min_divisor) * min_divisor\n    if required_height &gt; height or required_width &gt; width:\n        # We need to pad!\n        pad = torch.nn.ReflectionPad2d(\n            (0, 0, required_height-height, required_width-width))\n        return pad(image)\n    return image\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.steerable_pyramid_filters.crop_steerable_pyramid_filters","title":"<code>crop_steerable_pyramid_filters(filters, size)</code>","text":"<p>Given original 9x9 NYU filters, this crops them to the desired size. The size must be an odd number &gt;= 3 Note this only crops the h0, l0 and band filters (not the l downsampling filter)</p> <p>Parameters:</p> <ul> <li> <code>filters</code>           \u2013            <pre><code>        Filters to crop (should in format used by get_steerable_pyramid_filters.)\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>        Size to crop to. For example, an input of 3 will crop the filters to a size of 3x3.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>filters</code> (              <code>dict of torch.tensor</code> )          \u2013            <p>The cropped filters.</p> </li> </ul> Source code in <code>odak/learn/perception/steerable_pyramid_filters.py</code> <pre><code>def crop_steerable_pyramid_filters(filters, size):\n    \"\"\"\n    Given original 9x9 NYU filters, this crops them to the desired size.\n    The size must be an odd number &gt;= 3\n    Note this only crops the h0, l0 and band filters (not the l downsampling filter)\n\n    Parameters\n    ----------\n    filters     : dict of torch.tensor\n                    Filters to crop (should in format used by get_steerable_pyramid_filters.)\n    size        : int\n                    Size to crop to. For example, an input of 3 will crop the filters to a size of 3x3.\n\n    Returns\n    -------\n    filters     : dict of torch.tensor\n                    The cropped filters.\n    \"\"\"\n    assert(size &gt;= 3)\n    assert(size % 2 == 1)\n    r = (size-1) // 2\n\n    def crop_filter(filter, r, normalise=True):\n        r2 = (filter.size(-1)-1)//2\n        filter = filter[:, :, r2-r:r2+r+1, r2-r:r2+r+1]\n        if normalise:\n            filter -= torch.sum(filter)\n        return filter\n\n    filters[\"h0\"] = crop_filter(filters[\"h0\"], r, normalise=False)\n    sum_l = torch.sum(filters[\"l\"])\n    filters[\"l\"] = crop_filter(filters[\"l\"], 6, normalise=False)\n    filters[\"l\"] *= sum_l / torch.sum(filters[\"l\"])\n    sum_l0 = torch.sum(filters[\"l0\"])\n    filters[\"l0\"] = crop_filter(filters[\"l0\"], 2, normalise=False)\n    filters[\"l0\"] *= sum_l0 / torch.sum(filters[\"l0\"])\n    for b in range(len(filters[\"b\"])):\n        filters[\"b\"][b] = crop_filter(filters[\"b\"][b], r, normalise=True)\n    return filters\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.steerable_pyramid_filters.get_steerable_pyramid_filters","title":"<code>get_steerable_pyramid_filters(size, n_orientations, filter_type)</code>","text":"<p>This returns filters for a real-valued steerable pyramid.</p> <p>Parameters:</p> <ul> <li> <code>size</code>           \u2013            <pre><code>            Width of the filters (e.g. 3 will return 3x3 filters)\n</code></pre> </li> <li> <code>n_orientations</code>           \u2013            <pre><code>            Number of oriented band filters\n</code></pre> </li> <li> <code>filter_type</code>           \u2013            <pre><code>            This can be used to select between the original NYU filters and cropped or trained alternatives.\n            full: Original NYU filters from https://github.com/LabForComputationalVision/pyrtools/blob/master/pyrtools/pyramids/filters.py\n            cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate.\n            trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>filters</code> (              <code>dict of torch.tensor</code> )          \u2013            <p>The steerable pyramid filters. Returned as a dict with the following keys: \"l\" The lowpass downsampling filter \"l0\" The lowpass residual filter \"h0\" The highpass residual filter \"b\" The band filters (a list of torch.tensor filters, one for each orientation).</p> </li> </ul> Source code in <code>odak/learn/perception/steerable_pyramid_filters.py</code> <pre><code>def get_steerable_pyramid_filters(size, n_orientations, filter_type):\n    \"\"\"\n    This returns filters for a real-valued steerable pyramid.\n\n    Parameters\n    ----------\n\n    size            : int\n                        Width of the filters (e.g. 3 will return 3x3 filters)\n    n_orientations  : int\n                        Number of oriented band filters\n    filter_type     :  str\n                        This can be used to select between the original NYU filters and cropped or trained alternatives.\n                        full: Original NYU filters from https://github.com/LabForComputationalVision/pyrtools/blob/master/pyrtools/pyramids/filters.py\n                        cropped: Some filters are cut back in size by extracting the centre and scaling as appropriate.\n                        trained: Same as reduced, but the oriented kernels are replaced by learned 5x5 kernels.\n\n    Returns\n    -------\n    filters         : dict of torch.tensor\n                        The steerable pyramid filters. Returned as a dict with the following keys:\n                        \"l\" The lowpass downsampling filter\n                        \"l0\" The lowpass residual filter\n                        \"h0\" The highpass residual filter\n                        \"b\" The band filters (a list of torch.tensor filters, one for each orientation).\n    \"\"\"\n\n    if filter_type != \"full\" and filter_type != \"cropped\" and filter_type != \"trained\":\n        raise Exception(\n            \"Unknown filter type %s! Only filter types are full, cropped or trained.\" % filter_type)\n\n    filters = {}\n    if n_orientations == 1:\n        filters[\"l\"] = torch.tensor([\n            [-2.257000e-04, -8.064400e-04, -5.686000e-05, 8.741400e-04, -1.862800e-04, -1.031640e-03, -\n                1.871920e-03, -1.031640e-03, -1.862800e-04, 8.741400e-04, -5.686000e-05, -8.064400e-04, -2.257000e-04],\n            [-8.064400e-04, 1.417620e-03, -1.903800e-04, -2.449060e-03, -4.596420e-03, -7.006740e-03, -\n                6.948900e-03, -7.006740e-03, -4.596420e-03, -2.449060e-03, -1.903800e-04, 1.417620e-03, -8.064400e-04],\n            [-5.686000e-05, -1.903800e-04, -3.059760e-03, -6.401000e-03, -6.720800e-03, -5.236180e-03, -\n                3.781600e-03, -5.236180e-03, -6.720800e-03, -6.401000e-03, -3.059760e-03, -1.903800e-04, -5.686000e-05],\n            [8.741400e-04, -2.449060e-03, -6.401000e-03, -5.260020e-03, 3.938620e-03, 1.722078e-02, 2.449600e-02,\n                1.722078e-02, 3.938620e-03, -5.260020e-03, -6.401000e-03, -2.449060e-03, 8.741400e-04],\n            [-1.862800e-04, -4.596420e-03, -6.720800e-03, 3.938620e-03, 3.220744e-02, 6.306262e-02, 7.624674e-02,\n                6.306262e-02, 3.220744e-02, 3.938620e-03, -6.720800e-03, -4.596420e-03, -1.862800e-04],\n            [-1.031640e-03, -7.006740e-03, -5.236180e-03, 1.722078e-02, 6.306262e-02, 1.116388e-01, 1.348999e-01,\n                1.116388e-01, 6.306262e-02, 1.722078e-02, -5.236180e-03, -7.006740e-03, -1.031640e-03],\n            [-1.871920e-03, -6.948900e-03, -3.781600e-03, 2.449600e-02, 7.624674e-02, 1.348999e-01, 1.576508e-01,\n                1.348999e-01, 7.624674e-02, 2.449600e-02, -3.781600e-03, -6.948900e-03, -1.871920e-03],\n            [-1.031640e-03, -7.006740e-03, -5.236180e-03, 1.722078e-02, 6.306262e-02, 1.116388e-01, 1.348999e-01,\n                1.116388e-01, 6.306262e-02, 1.722078e-02, -5.236180e-03, -7.006740e-03, -1.031640e-03],\n            [-1.862800e-04, -4.596420e-03, -6.720800e-03, 3.938620e-03, 3.220744e-02, 6.306262e-02, 7.624674e-02,\n                6.306262e-02, 3.220744e-02, 3.938620e-03, -6.720800e-03, -4.596420e-03, -1.862800e-04],\n            [8.741400e-04, -2.449060e-03, -6.401000e-03, -5.260020e-03, 3.938620e-03, 1.722078e-02, 2.449600e-02,\n                1.722078e-02, 3.938620e-03, -5.260020e-03, -6.401000e-03, -2.449060e-03, 8.741400e-04],\n            [-5.686000e-05, -1.903800e-04, -3.059760e-03, -6.401000e-03, -6.720800e-03, -5.236180e-03, -\n                3.781600e-03, -5.236180e-03, -6.720800e-03, -6.401000e-03, -3.059760e-03, -1.903800e-04, -5.686000e-05],\n            [-8.064400e-04, 1.417620e-03, -1.903800e-04, -2.449060e-03, -4.596420e-03, -7.006740e-03, -\n                6.948900e-03, -7.006740e-03, -4.596420e-03, -2.449060e-03, -1.903800e-04, 1.417620e-03, -8.064400e-04],\n            [-2.257000e-04, -8.064400e-04, -5.686000e-05, 8.741400e-04, -1.862800e-04, -1.031640e-03, -1.871920e-03, -1.031640e-03, -1.862800e-04, 8.741400e-04, -5.686000e-05, -8.064400e-04, -2.257000e-04]]\n        ).reshape(1, 1, 13, 13)\n        filters[\"l0\"] = torch.tensor([\n            [-4.514000e-04, -1.137100e-04, -3.725800e-04, -\n                3.743860e-03, -3.725800e-04, -1.137100e-04, -4.514000e-04],\n            [-1.137100e-04, -6.119520e-03, -1.344160e-02, -\n                7.563200e-03, -1.344160e-02, -6.119520e-03, -1.137100e-04],\n            [-3.725800e-04, -1.344160e-02, 6.441488e-02, 1.524935e-01,\n                6.441488e-02, -1.344160e-02, -3.725800e-04],\n            [-3.743860e-03, -7.563200e-03, 1.524935e-01, 3.153017e-01,\n                1.524935e-01, -7.563200e-03, -3.743860e-03],\n            [-3.725800e-04, -1.344160e-02, 6.441488e-02, 1.524935e-01,\n                6.441488e-02, -1.344160e-02, -3.725800e-04],\n            [-1.137100e-04, -6.119520e-03, -1.344160e-02, -\n                7.563200e-03, -1.344160e-02, -6.119520e-03, -1.137100e-04],\n            [-4.514000e-04, -1.137100e-04, -3.725800e-04, -3.743860e-03, -3.725800e-04, -1.137100e-04, -4.514000e-04]]\n        ).reshape(1, 1, 7, 7)\n        filters[\"h0\"] = torch.tensor([\n            [5.997200e-04, -6.068000e-05, -3.324900e-04, -3.325600e-04, -\n                2.406600e-04, -3.325600e-04, -3.324900e-04, -6.068000e-05, 5.997200e-04],\n            [-6.068000e-05, 1.263100e-04, 4.927100e-04, 1.459700e-04, -\n                3.732100e-04, 1.459700e-04, 4.927100e-04, 1.263100e-04, -6.068000e-05],\n            [-3.324900e-04, 4.927100e-04, -1.616650e-03, -1.437358e-02, -\n                2.420138e-02, -1.437358e-02, -1.616650e-03, 4.927100e-04, -3.324900e-04],\n            [-3.325600e-04, 1.459700e-04, -1.437358e-02, -6.300923e-02, -\n                9.623594e-02, -6.300923e-02, -1.437358e-02, 1.459700e-04, -3.325600e-04],\n            [-2.406600e-04, -3.732100e-04, -2.420138e-02, -9.623594e-02,\n                8.554893e-01, -9.623594e-02, -2.420138e-02, -3.732100e-04, -2.406600e-04],\n            [-3.325600e-04, 1.459700e-04, -1.437358e-02, -6.300923e-02, -\n                9.623594e-02, -6.300923e-02, -1.437358e-02, 1.459700e-04, -3.325600e-04],\n            [-3.324900e-04, 4.927100e-04, -1.616650e-03, -1.437358e-02, -\n                2.420138e-02, -1.437358e-02, -1.616650e-03, 4.927100e-04, -3.324900e-04],\n            [-6.068000e-05, 1.263100e-04, 4.927100e-04, 1.459700e-04, -\n                3.732100e-04, 1.459700e-04, 4.927100e-04, 1.263100e-04, -6.068000e-05],\n            [5.997200e-04, -6.068000e-05, -3.324900e-04, -3.325600e-04, -2.406600e-04, -3.325600e-04, -3.324900e-04, -6.068000e-05, 5.997200e-04]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"b\"] = []\n        filters[\"b\"].append(torch.tensor([\n            -9.066000e-05, -1.738640e-03, -4.942500e-03, -7.889390e-03, -\n            1.009473e-02, -7.889390e-03, -4.942500e-03, -1.738640e-03, -9.066000e-05,\n            -1.738640e-03, -4.625150e-03, -7.272540e-03, -7.623410e-03, -\n            9.091950e-03, -7.623410e-03, -7.272540e-03, -4.625150e-03, -1.738640e-03,\n            -4.942500e-03, -7.272540e-03, -2.129540e-02, -2.435662e-02, -\n            3.487008e-02, -2.435662e-02, -2.129540e-02, -7.272540e-03, -4.942500e-03,\n            -7.889390e-03, -7.623410e-03, -2.435662e-02, -1.730466e-02, -\n            3.158605e-02, -1.730466e-02, -2.435662e-02, -7.623410e-03, -7.889390e-03,\n            -1.009473e-02, -9.091950e-03, -3.487008e-02, -3.158605e-02, 9.464195e-01, -\n            3.158605e-02, -3.487008e-02, -9.091950e-03, -1.009473e-02,\n            -7.889390e-03, -7.623410e-03, -2.435662e-02, -1.730466e-02, -\n            3.158605e-02, -1.730466e-02, -2.435662e-02, -7.623410e-03, -7.889390e-03,\n            -4.942500e-03, -7.272540e-03, -2.129540e-02, -2.435662e-02, -\n            3.487008e-02, -2.435662e-02, -2.129540e-02, -7.272540e-03, -4.942500e-03,\n            -1.738640e-03, -4.625150e-03, -7.272540e-03, -7.623410e-03, -\n            9.091950e-03, -7.623410e-03, -7.272540e-03, -4.625150e-03, -1.738640e-03,\n            -9.066000e-05, -1.738640e-03, -4.942500e-03, -7.889390e-03, -1.009473e-02, -7.889390e-03, -4.942500e-03, -1.738640e-03, -9.066000e-05]\n        ).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n\n    elif n_orientations == 2:\n        filters[\"l\"] = torch.tensor(\n            [[-4.350000e-05, 1.207800e-04, -6.771400e-04, -1.243400e-04, -8.006400e-04, -1.597040e-03, -2.516800e-04, -4.202000e-04, 1.262000e-03, -4.202000e-04, -2.516800e-04, -1.597040e-03, -8.006400e-04, -1.243400e-04, -6.771400e-04, 1.207800e-04, -4.350000e-05],\n             [1.207800e-04, 4.460600e-04, -5.814600e-04, 5.621600e-04, -1.368800e-04, 2.325540e-03, 2.889860e-03, 4.287280e-03, 5.589400e-03,\n                 4.287280e-03, 2.889860e-03, 2.325540e-03, -1.368800e-04, 5.621600e-04, -5.814600e-04, 4.460600e-04, 1.207800e-04],\n             [-6.771400e-04, -5.814600e-04, 1.460780e-03, 2.160540e-03, 3.761360e-03, 3.080980e-03, 4.112200e-03, 2.221220e-03, 5.538200e-04,\n                 2.221220e-03, 4.112200e-03, 3.080980e-03, 3.761360e-03, 2.160540e-03, 1.460780e-03, -5.814600e-04, -6.771400e-04],\n             [-1.243400e-04, 5.621600e-04, 2.160540e-03, 3.175780e-03, 3.184680e-03, -1.777480e-03, -7.431700e-03, -9.056920e-03, -\n                 9.637220e-03, -9.056920e-03, -7.431700e-03, -1.777480e-03, 3.184680e-03, 3.175780e-03, 2.160540e-03, 5.621600e-04, -1.243400e-04],\n             [-8.006400e-04, -1.368800e-04, 3.761360e-03, 3.184680e-03, -3.530640e-03, -1.260420e-02, -1.884744e-02, -1.750818e-02, -\n                 1.648568e-02, -1.750818e-02, -1.884744e-02, -1.260420e-02, -3.530640e-03, 3.184680e-03, 3.761360e-03, -1.368800e-04, -8.006400e-04],\n             [-1.597040e-03, 2.325540e-03, 3.080980e-03, -1.777480e-03, -1.260420e-02, -2.022938e-02, -1.109170e-02, 3.955660e-03, 1.438512e-02,\n                 3.955660e-03, -1.109170e-02, -2.022938e-02, -1.260420e-02, -1.777480e-03, 3.080980e-03, 2.325540e-03, -1.597040e-03],\n             [-2.516800e-04, 2.889860e-03, 4.112200e-03, -7.431700e-03, -1.884744e-02, -1.109170e-02, 2.190660e-02, 6.806584e-02, 9.058014e-02,\n                 6.806584e-02, 2.190660e-02, -1.109170e-02, -1.884744e-02, -7.431700e-03, 4.112200e-03, 2.889860e-03, -2.516800e-04],\n             [-4.202000e-04, 4.287280e-03, 2.221220e-03, -9.056920e-03, -1.750818e-02, 3.955660e-03, 6.806584e-02, 1.445500e-01, 1.773651e-01,\n                 1.445500e-01, 6.806584e-02, 3.955660e-03, -1.750818e-02, -9.056920e-03, 2.221220e-03, 4.287280e-03, -4.202000e-04],\n             [1.262000e-03, 5.589400e-03, 5.538200e-04, -9.637220e-03, -1.648568e-02, 1.438512e-02, 9.058014e-02, 1.773651e-01, 2.120374e-01,\n                 1.773651e-01, 9.058014e-02, 1.438512e-02, -1.648568e-02, -9.637220e-03, 5.538200e-04, 5.589400e-03, 1.262000e-03],\n             [-4.202000e-04, 4.287280e-03, 2.221220e-03, -9.056920e-03, -1.750818e-02, 3.955660e-03, 6.806584e-02, 1.445500e-01, 1.773651e-01,\n                 1.445500e-01, 6.806584e-02, 3.955660e-03, -1.750818e-02, -9.056920e-03, 2.221220e-03, 4.287280e-03, -4.202000e-04],\n             [-2.516800e-04, 2.889860e-03, 4.112200e-03, -7.431700e-03, -1.884744e-02, -1.109170e-02, 2.190660e-02, 6.806584e-02, 9.058014e-02,\n                 6.806584e-02, 2.190660e-02, -1.109170e-02, -1.884744e-02, -7.431700e-03, 4.112200e-03, 2.889860e-03, -2.516800e-04],\n             [-1.597040e-03, 2.325540e-03, 3.080980e-03, -1.777480e-03, -1.260420e-02, -2.022938e-02, -1.109170e-02, 3.955660e-03, 1.438512e-02,\n                 3.955660e-03, -1.109170e-02, -2.022938e-02, -1.260420e-02, -1.777480e-03, 3.080980e-03, 2.325540e-03, -1.597040e-03],\n             [-8.006400e-04, -1.368800e-04, 3.761360e-03, 3.184680e-03, -3.530640e-03, -1.260420e-02, -1.884744e-02, -1.750818e-02, -\n                 1.648568e-02, -1.750818e-02, -1.884744e-02, -1.260420e-02, -3.530640e-03, 3.184680e-03, 3.761360e-03, -1.368800e-04, -8.006400e-04],\n             [-1.243400e-04, 5.621600e-04, 2.160540e-03, 3.175780e-03, 3.184680e-03, -1.777480e-03, -7.431700e-03, -9.056920e-03, -\n                 9.637220e-03, -9.056920e-03, -7.431700e-03, -1.777480e-03, 3.184680e-03, 3.175780e-03, 2.160540e-03, 5.621600e-04, -1.243400e-04],\n             [-6.771400e-04, -5.814600e-04, 1.460780e-03, 2.160540e-03, 3.761360e-03, 3.080980e-03, 4.112200e-03, 2.221220e-03, 5.538200e-04,\n                 2.221220e-03, 4.112200e-03, 3.080980e-03, 3.761360e-03, 2.160540e-03, 1.460780e-03, -5.814600e-04, -6.771400e-04],\n             [1.207800e-04, 4.460600e-04, -5.814600e-04, 5.621600e-04, -1.368800e-04, 2.325540e-03, 2.889860e-03, 4.287280e-03, 5.589400e-03,\n                 4.287280e-03, 2.889860e-03, 2.325540e-03, -1.368800e-04, 5.621600e-04, -5.814600e-04, 4.460600e-04, 1.207800e-04],\n             [-4.350000e-05, 1.207800e-04, -6.771400e-04, -1.243400e-04, -8.006400e-04, -1.597040e-03, -2.516800e-04, -4.202000e-04, 1.262000e-03, -4.202000e-04, -2.516800e-04, -1.597040e-03, -8.006400e-04, -1.243400e-04, -6.771400e-04, 1.207800e-04, -4.350000e-05]]\n        ).reshape(1, 1, 17, 17)\n        filters[\"l0\"] = torch.tensor(\n            [[-8.701000e-05, -1.354280e-03, -1.601260e-03, -5.033700e-04, 2.524010e-03, -5.033700e-04, -1.601260e-03, -1.354280e-03, -8.701000e-05],\n             [-1.354280e-03, 2.921580e-03, 7.522720e-03, 8.224420e-03, 1.107620e-03,\n                 8.224420e-03, 7.522720e-03, 2.921580e-03, -1.354280e-03],\n             [-1.601260e-03, 7.522720e-03, -7.061290e-03, -3.769487e-02, -\n                 3.297137e-02, -3.769487e-02, -7.061290e-03, 7.522720e-03, -1.601260e-03],\n             [-5.033700e-04, 8.224420e-03, -3.769487e-02, 4.381320e-02, 1.811603e-01,\n                 4.381320e-02, -3.769487e-02, 8.224420e-03, -5.033700e-04],\n             [2.524010e-03, 1.107620e-03, -3.297137e-02, 1.811603e-01, 4.376250e-01,\n                 1.811603e-01, -3.297137e-02, 1.107620e-03, 2.524010e-03],\n             [-5.033700e-04, 8.224420e-03, -3.769487e-02, 4.381320e-02, 1.811603e-01,\n                 4.381320e-02, -3.769487e-02, 8.224420e-03, -5.033700e-04],\n             [-1.601260e-03, 7.522720e-03, -7.061290e-03, -3.769487e-02, -\n                 3.297137e-02, -3.769487e-02, -7.061290e-03, 7.522720e-03, -1.601260e-03],\n             [-1.354280e-03, 2.921580e-03, 7.522720e-03, 8.224420e-03, 1.107620e-03,\n                 8.224420e-03, 7.522720e-03, 2.921580e-03, -1.354280e-03],\n             [-8.701000e-05, -1.354280e-03, -1.601260e-03, -5.033700e-04, 2.524010e-03, -5.033700e-04, -1.601260e-03, -1.354280e-03, -8.701000e-05]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"h0\"] = torch.tensor(\n            [[-9.570000e-04, -2.424100e-04, -1.424720e-03, -8.742600e-04, -1.166810e-03, -8.742600e-04, -1.424720e-03, -2.424100e-04, -9.570000e-04],\n             [-2.424100e-04, -4.317530e-03, 8.998600e-04, 9.156420e-03, 1.098012e-02,\n                 9.156420e-03, 8.998600e-04, -4.317530e-03, -2.424100e-04],\n             [-1.424720e-03, 8.998600e-04, 1.706347e-02, 1.094866e-02, -\n                 5.897780e-03, 1.094866e-02, 1.706347e-02, 8.998600e-04, -1.424720e-03],\n             [-8.742600e-04, 9.156420e-03, 1.094866e-02, -7.841370e-02, -\n                 1.562827e-01, -7.841370e-02, 1.094866e-02, 9.156420e-03, -8.742600e-04],\n             [-1.166810e-03, 1.098012e-02, -5.897780e-03, -1.562827e-01,\n                 7.282593e-01, -1.562827e-01, -5.897780e-03, 1.098012e-02, -1.166810e-03],\n             [-8.742600e-04, 9.156420e-03, 1.094866e-02, -7.841370e-02, -\n                 1.562827e-01, -7.841370e-02, 1.094866e-02, 9.156420e-03, -8.742600e-04],\n             [-1.424720e-03, 8.998600e-04, 1.706347e-02, 1.094866e-02, -\n                 5.897780e-03, 1.094866e-02, 1.706347e-02, 8.998600e-04, -1.424720e-03],\n             [-2.424100e-04, -4.317530e-03, 8.998600e-04, 9.156420e-03, 1.098012e-02,\n                 9.156420e-03, 8.998600e-04, -4.317530e-03, -2.424100e-04],\n             [-9.570000e-04, -2.424100e-04, -1.424720e-03, -8.742600e-04, -1.166810e-03, -8.742600e-04, -1.424720e-03, -2.424100e-04, -9.570000e-04]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"b\"] = []\n        filters[\"b\"].append(torch.tensor(\n            [6.125880e-03, -8.052600e-03, -2.103714e-02, -1.536890e-02, -1.851466e-02, -1.536890e-02, -2.103714e-02, -8.052600e-03, 6.125880e-03,\n             -1.287416e-02, -9.611520e-03, 1.023569e-02, 6.009450e-03, 1.872620e-03, 6.009450e-03, 1.023569e-02, -\n             9.611520e-03, -1.287416e-02,\n             -5.641530e-03, 4.168400e-03, -2.382180e-02, -5.375324e-02, -\n             2.076086e-02, -5.375324e-02, -2.382180e-02, 4.168400e-03, -5.641530e-03,\n             -8.957260e-03, -1.751170e-03, -1.836909e-02, 1.265655e-01, 2.996168e-01, 1.265655e-01, -\n             1.836909e-02, -1.751170e-03, -8.957260e-03,\n             0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.000000e+00,\n             8.957260e-03, 1.751170e-03, 1.836909e-02, -1.265655e-01, -\n             2.996168e-01, -1.265655e-01, 1.836909e-02, 1.751170e-03, 8.957260e-03,\n             5.641530e-03, -4.168400e-03, 2.382180e-02, 5.375324e-02, 2.076086e-02, 5.375324e-02, 2.382180e-02, -\n             4.168400e-03, 5.641530e-03,\n             1.287416e-02, 9.611520e-03, -1.023569e-02, -6.009450e-03, -\n             1.872620e-03, -6.009450e-03, -1.023569e-02, 9.611520e-03, 1.287416e-02,\n             -6.125880e-03, 8.052600e-03, 2.103714e-02, 1.536890e-02, 1.851466e-02, 1.536890e-02, 2.103714e-02, 8.052600e-03, -6.125880e-03]).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor(\n            [-6.125880e-03, 1.287416e-02, 5.641530e-03, 8.957260e-03, 0.000000e+00, -8.957260e-03, -5.641530e-03, -1.287416e-02, 6.125880e-03,\n             8.052600e-03, 9.611520e-03, -4.168400e-03, 1.751170e-03, 0.000000e+00, -\n             1.751170e-03, 4.168400e-03, -9.611520e-03, -8.052600e-03,\n             2.103714e-02, -1.023569e-02, 2.382180e-02, 1.836909e-02, 0.000000e+00, -\n             1.836909e-02, -2.382180e-02, 1.023569e-02, -2.103714e-02,\n             1.536890e-02, -6.009450e-03, 5.375324e-02, -\n             1.265655e-01, 0.000000e+00, 1.265655e-01, -\n             5.375324e-02, 6.009450e-03, -1.536890e-02,\n             1.851466e-02, -1.872620e-03, 2.076086e-02, -\n             2.996168e-01, 0.000000e+00, 2.996168e-01, -\n             2.076086e-02, 1.872620e-03, -1.851466e-02,\n             1.536890e-02, -6.009450e-03, 5.375324e-02, -\n             1.265655e-01, 0.000000e+00, 1.265655e-01, -\n             5.375324e-02, 6.009450e-03, -1.536890e-02,\n             2.103714e-02, -1.023569e-02, 2.382180e-02, 1.836909e-02, 0.000000e+00, -\n             1.836909e-02, -2.382180e-02, 1.023569e-02, -2.103714e-02,\n             8.052600e-03, 9.611520e-03, -4.168400e-03, 1.751170e-03, 0.000000e+00, -\n             1.751170e-03, 4.168400e-03, -9.611520e-03, -8.052600e-03,\n             -6.125880e-03, 1.287416e-02, 5.641530e-03, 8.957260e-03, 0.000000e+00, -8.957260e-03, -5.641530e-03, -1.287416e-02, 6.125880e-03]).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n\n    elif n_orientations == 4:\n        filters[\"l\"] = torch.tensor([\n            [-4.3500000174E-5, 1.2078000145E-4, -6.7714002216E-4, -1.2434000382E-4, -8.0063997302E-4, -1.5970399836E-3, -2.5168000138E-4, -4.2019999819E-4,\n                1.2619999470E-3, -4.2019999819E-4, -2.5168000138E-4, -1.5970399836E-3, -8.0063997302E-4, -1.2434000382E-4, -6.7714002216E-4, 1.2078000145E-4, -4.3500000174E-5],\n            [1.2078000145E-4, 4.4606000301E-4, -5.8146001538E-4, 5.6215998484E-4, -1.3688000035E-4, 2.3255399428E-3, 2.8898599558E-3, 4.2872801423E-3, 5.5893999524E-3,\n                4.2872801423E-3, 2.8898599558E-3, 2.3255399428E-3, -1.3688000035E-4, 5.6215998484E-4, -5.8146001538E-4, 4.4606000301E-4, 1.2078000145E-4],\n            [-6.7714002216E-4, -5.8146001538E-4, 1.4607800404E-3, 2.1605400834E-3, 3.7613599561E-3, 3.0809799209E-3, 4.1121998802E-3, 2.2212199401E-3, 5.5381999118E-4,\n                2.2212199401E-3, 4.1121998802E-3, 3.0809799209E-3, 3.7613599561E-3, 2.1605400834E-3, 1.4607800404E-3, -5.8146001538E-4, -6.7714002216E-4],\n            [-1.2434000382E-4, 5.6215998484E-4, 2.1605400834E-3, 3.1757799443E-3, 3.1846798956E-3, -1.7774800071E-3, -7.4316998944E-3, -9.0569201857E-3, -\n                9.6372198313E-3, -9.0569201857E-3, -7.4316998944E-3, -1.7774800071E-3, 3.1846798956E-3, 3.1757799443E-3, 2.1605400834E-3, 5.6215998484E-4, -1.2434000382E-4],\n            [-8.0063997302E-4, -1.3688000035E-4, 3.7613599561E-3, 3.1846798956E-3, -3.5306399222E-3, -1.2604200281E-2, -1.8847439438E-2, -1.7508180812E-2, -\n                1.6485679895E-2, -1.7508180812E-2, -1.8847439438E-2, -1.2604200281E-2, -3.5306399222E-3, 3.1846798956E-3, 3.7613599561E-3, -1.3688000035E-4, -8.0063997302E-4],\n            [-1.5970399836E-3, 2.3255399428E-3, 3.0809799209E-3, -1.7774800071E-3, -1.2604200281E-2, -2.0229380578E-2, -1.1091699824E-2, 3.9556599222E-3, 1.4385120012E-2,\n                3.9556599222E-3, -1.1091699824E-2, -2.0229380578E-2, -1.2604200281E-2, -1.7774800071E-3, 3.0809799209E-3, 2.3255399428E-3, -1.5970399836E-3],\n            [-2.5168000138E-4, 2.8898599558E-3, 4.1121998802E-3, -7.4316998944E-3, -1.8847439438E-2, -1.1091699824E-2, 2.1906599402E-2, 6.8065837026E-2, 9.0580143034E-2,\n                6.8065837026E-2, 2.1906599402E-2, -1.1091699824E-2, -1.8847439438E-2, -7.4316998944E-3, 4.1121998802E-3, 2.8898599558E-3, -2.5168000138E-4],\n            [-4.2019999819E-4, 4.2872801423E-3, 2.2212199401E-3, -9.0569201857E-3, -1.7508180812E-2, 3.9556599222E-3, 6.8065837026E-2, 0.1445499808, 0.1773651242,\n                0.1445499808, 6.8065837026E-2, 3.9556599222E-3, -1.7508180812E-2, -9.0569201857E-3, 2.2212199401E-3, 4.2872801423E-3, -4.2019999819E-4],\n            [1.2619999470E-3, 5.5893999524E-3, 5.5381999118E-4, -9.6372198313E-3, -1.6485679895E-2, 1.4385120012E-2, 9.0580143034E-2, 0.1773651242, 0.2120374441,\n                0.1773651242, 9.0580143034E-2, 1.4385120012E-2, -1.6485679895E-2, -9.6372198313E-3, 5.5381999118E-4, 5.5893999524E-3, 1.2619999470E-3],\n            [-4.2019999819E-4, 4.2872801423E-3, 2.2212199401E-3, -9.0569201857E-3, -1.7508180812E-2, 3.9556599222E-3, 6.8065837026E-2, 0.1445499808, 0.1773651242,\n                0.1445499808, 6.8065837026E-2, 3.9556599222E-3, -1.7508180812E-2, -9.0569201857E-3, 2.2212199401E-3, 4.2872801423E-3, -4.2019999819E-4],\n            [-2.5168000138E-4, 2.8898599558E-3, 4.1121998802E-3, -7.4316998944E-3, -1.8847439438E-2, -1.1091699824E-2, 2.1906599402E-2, 6.8065837026E-2, 9.0580143034E-2,\n                6.8065837026E-2, 2.1906599402E-2, -1.1091699824E-2, -1.8847439438E-2, -7.4316998944E-3, 4.1121998802E-3, 2.8898599558E-3, -2.5168000138E-4],\n            [-1.5970399836E-3, 2.3255399428E-3, 3.0809799209E-3, -1.7774800071E-3, -1.2604200281E-2, -2.0229380578E-2, -1.1091699824E-2, 3.9556599222E-3, 1.4385120012E-2,\n                3.9556599222E-3, -1.1091699824E-2, -2.0229380578E-2, -1.2604200281E-2, -1.7774800071E-3, 3.0809799209E-3, 2.3255399428E-3, -1.5970399836E-3],\n            [-8.0063997302E-4, -1.3688000035E-4, 3.7613599561E-3, 3.1846798956E-3, -3.5306399222E-3, -1.2604200281E-2, -1.8847439438E-2, -1.7508180812E-2, -\n                1.6485679895E-2, -1.7508180812E-2, -1.8847439438E-2, -1.2604200281E-2, -3.5306399222E-3, 3.1846798956E-3, 3.7613599561E-3, -1.3688000035E-4, -8.0063997302E-4],\n            [-1.2434000382E-4, 5.6215998484E-4, 2.1605400834E-3, 3.1757799443E-3, 3.1846798956E-3, -1.7774800071E-3, -7.4316998944E-3, -9.0569201857E-3, -\n                9.6372198313E-3, -9.0569201857E-3, -7.4316998944E-3, -1.7774800071E-3, 3.1846798956E-3, 3.1757799443E-3, 2.1605400834E-3, 5.6215998484E-4, -1.2434000382E-4],\n            [-6.7714002216E-4, -5.8146001538E-4, 1.4607800404E-3, 2.1605400834E-3, 3.7613599561E-3, 3.0809799209E-3, 4.1121998802E-3, 2.2212199401E-3, 5.5381999118E-4,\n                2.2212199401E-3, 4.1121998802E-3, 3.0809799209E-3, 3.7613599561E-3, 2.1605400834E-3, 1.4607800404E-3, -5.8146001538E-4, -6.7714002216E-4],\n            [1.2078000145E-4, 4.4606000301E-4, -5.8146001538E-4, 5.6215998484E-4, -1.3688000035E-4, 2.3255399428E-3, 2.8898599558E-3, 4.2872801423E-3, 5.5893999524E-3,\n                4.2872801423E-3, 2.8898599558E-3, 2.3255399428E-3, -1.3688000035E-4, 5.6215998484E-4, -5.8146001538E-4, 4.4606000301E-4, 1.2078000145E-4],\n            [-4.3500000174E-5, 1.2078000145E-4, -6.7714002216E-4, -1.2434000382E-4, -8.0063997302E-4, -1.5970399836E-3, -2.5168000138E-4, -4.2019999819E-4, 1.2619999470E-3, -4.2019999819E-4, -2.5168000138E-4, -1.5970399836E-3, -8.0063997302E-4, -1.2434000382E-4, -6.7714002216E-4, 1.2078000145E-4, -4.3500000174E-5]]\n        ).reshape(1, 1, 17, 17)\n        filters[\"l0\"] = torch.tensor([\n            [-8.7009997515E-5, -1.3542800443E-3, -1.6012600390E-3, -5.0337001448E-4,\n                2.5240099058E-3, -5.0337001448E-4, -1.6012600390E-3, -1.3542800443E-3, -8.7009997515E-5],\n            [-1.3542800443E-3, 2.9215801042E-3, 7.5227199122E-3, 8.2244202495E-3, 1.1076199589E-3,\n                8.2244202495E-3, 7.5227199122E-3, 2.9215801042E-3, -1.3542800443E-3],\n            [-1.6012600390E-3, 7.5227199122E-3, -7.0612900890E-3, -3.7694871426E-2, -\n                3.2971370965E-2, -3.7694871426E-2, -7.0612900890E-3, 7.5227199122E-3, -1.6012600390E-3],\n            [-5.0337001448E-4, 8.2244202495E-3, -3.7694871426E-2, 4.3813198805E-2, 0.1811603010,\n                4.3813198805E-2, -3.7694871426E-2, 8.2244202495E-3, -5.0337001448E-4],\n            [2.5240099058E-3, 1.1076199589E-3, -3.2971370965E-2, 0.1811603010, 0.4376249909,\n                0.1811603010, -3.2971370965E-2, 1.1076199589E-3, 2.5240099058E-3],\n            [-5.0337001448E-4, 8.2244202495E-3, -3.7694871426E-2, 4.3813198805E-2, 0.1811603010,\n                4.3813198805E-2, -3.7694871426E-2, 8.2244202495E-3, -5.0337001448E-4],\n            [-1.6012600390E-3, 7.5227199122E-3, -7.0612900890E-3, -3.7694871426E-2, -\n                3.2971370965E-2, -3.7694871426E-2, -7.0612900890E-3, 7.5227199122E-3, -1.6012600390E-3],\n            [-1.3542800443E-3, 2.9215801042E-3, 7.5227199122E-3, 8.2244202495E-3, 1.1076199589E-3,\n                8.2244202495E-3, 7.5227199122E-3, 2.9215801042E-3, -1.3542800443E-3],\n            [-8.7009997515E-5, -1.3542800443E-3, -1.6012600390E-3, -5.0337001448E-4, 2.5240099058E-3, -5.0337001448E-4, -1.6012600390E-3, -1.3542800443E-3, -8.7009997515E-5]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"h0\"] = torch.tensor([\n            [-4.0483998600E-4, -6.2596000498E-4, -3.7829999201E-5, 8.8387000142E-4, 1.5450799838E-3, 1.9235999789E-3, 2.0687500946E-3, 2.0898699295E-3,\n                2.0687500946E-3, 1.9235999789E-3, 1.5450799838E-3, 8.8387000142E-4, -3.7829999201E-5, -6.2596000498E-4, -4.0483998600E-4],\n            [-6.2596000498E-4, -3.2734998967E-4, 7.7435001731E-4, 1.5874400269E-3, 2.1750701126E-3, 2.5626500137E-3, 2.2892199922E-3, 1.9755100366E-3,\n                2.2892199922E-3, 2.5626500137E-3, 2.1750701126E-3, 1.5874400269E-3, 7.7435001731E-4, -3.2734998967E-4, -6.2596000498E-4],\n            [-3.7829999201E-5, 7.7435001731E-4, 1.1793200392E-3, 1.4050999889E-3, 2.2253401112E-3, 2.1145299543E-3, 3.3578000148E-4, -\n                8.3368999185E-4, 3.3578000148E-4, 2.1145299543E-3, 2.2253401112E-3, 1.4050999889E-3, 1.1793200392E-3, 7.7435001731E-4, -3.7829999201E-5],\n            [8.8387000142E-4, 1.5874400269E-3, 1.4050999889E-3, 1.2960999738E-3, -4.9274001503E-4, -3.1295299996E-3, -4.5751798898E-3, -\n                5.1014497876E-3, -4.5751798898E-3, -3.1295299996E-3, -4.9274001503E-4, 1.2960999738E-3, 1.4050999889E-3, 1.5874400269E-3, 8.8387000142E-4],\n            [1.5450799838E-3, 2.1750701126E-3, 2.2253401112E-3, -4.9274001503E-4, -6.3222697936E-3, -2.7556000277E-3, 5.3632198833E-3, 7.3032598011E-3,\n                5.3632198833E-3, -2.7556000277E-3, -6.3222697936E-3, -4.9274001503E-4, 2.2253401112E-3, 2.1750701126E-3, 1.5450799838E-3],\n            [1.9235999789E-3, 2.5626500137E-3, 2.1145299543E-3, -3.1295299996E-3, -2.7556000277E-3, 1.3962360099E-2, 7.8046298586E-3, -\n                9.3812197447E-3, 7.8046298586E-3, 1.3962360099E-2, -2.7556000277E-3, -3.1295299996E-3, 2.1145299543E-3, 2.5626500137E-3, 1.9235999789E-3],\n            [2.0687500946E-3, 2.2892199922E-3, 3.3578000148E-4, -4.5751798898E-3, 5.3632198833E-3, 7.8046298586E-3, -7.9501636326E-2, -\n                0.1554141641, -7.9501636326E-2, 7.8046298586E-3, 5.3632198833E-3, -4.5751798898E-3, 3.3578000148E-4, 2.2892199922E-3, 2.0687500946E-3],\n            [2.0898699295E-3, 1.9755100366E-3, -8.3368999185E-4, -5.1014497876E-3, 7.3032598011E-3, -9.3812197447E-3, -0.1554141641,\n                0.7303866148, -0.1554141641, -9.3812197447E-3, 7.3032598011E-3, -5.1014497876E-3, -8.3368999185E-4, 1.9755100366E-3, 2.0898699295E-3],\n            [2.0687500946E-3, 2.2892199922E-3, 3.3578000148E-4, -4.5751798898E-3, 5.3632198833E-3, 7.8046298586E-3, -7.9501636326E-2, -\n                0.1554141641, -7.9501636326E-2, 7.8046298586E-3, 5.3632198833E-3, -4.5751798898E-3, 3.3578000148E-4, 2.2892199922E-3, 2.0687500946E-3],\n            [1.9235999789E-3, 2.5626500137E-3, 2.1145299543E-3, -3.1295299996E-3, -2.7556000277E-3, 1.3962360099E-2, 7.8046298586E-3, -\n                9.3812197447E-3, 7.8046298586E-3, 1.3962360099E-2, -2.7556000277E-3, -3.1295299996E-3, 2.1145299543E-3, 2.5626500137E-3, 1.9235999789E-3],\n            [1.5450799838E-3, 2.1750701126E-3, 2.2253401112E-3, -4.9274001503E-4, -6.3222697936E-3, -2.7556000277E-3, 5.3632198833E-3, 7.3032598011E-3,\n                5.3632198833E-3, -2.7556000277E-3, -6.3222697936E-3, -4.9274001503E-4, 2.2253401112E-3, 2.1750701126E-3, 1.5450799838E-3],\n            [8.8387000142E-4, 1.5874400269E-3, 1.4050999889E-3, 1.2960999738E-3, -4.9274001503E-4, -3.1295299996E-3, -4.5751798898E-3, -\n                5.1014497876E-3, -4.5751798898E-3, -3.1295299996E-3, -4.9274001503E-4, 1.2960999738E-3, 1.4050999889E-3, 1.5874400269E-3, 8.8387000142E-4],\n            [-3.7829999201E-5, 7.7435001731E-4, 1.1793200392E-3, 1.4050999889E-3, 2.2253401112E-3, 2.1145299543E-3, 3.3578000148E-4, -\n                8.3368999185E-4, 3.3578000148E-4, 2.1145299543E-3, 2.2253401112E-3, 1.4050999889E-3, 1.1793200392E-3, 7.7435001731E-4, -3.7829999201E-5],\n            [-6.2596000498E-4, -3.2734998967E-4, 7.7435001731E-4, 1.5874400269E-3, 2.1750701126E-3, 2.5626500137E-3, 2.2892199922E-3, 1.9755100366E-3,\n                2.2892199922E-3, 2.5626500137E-3, 2.1750701126E-3, 1.5874400269E-3, 7.7435001731E-4, -3.2734998967E-4, -6.2596000498E-4],\n            [-4.0483998600E-4, -6.2596000498E-4, -3.7829999201E-5, 8.8387000142E-4, 1.5450799838E-3, 1.9235999789E-3, 2.0687500946E-3, 2.0898699295E-3, 2.0687500946E-3, 1.9235999789E-3, 1.5450799838E-3, 8.8387000142E-4, -3.7829999201E-5, -6.2596000498E-4, -4.0483998600E-4]]\n        ).reshape(1, 1, 15, 15)\n        filters[\"b\"] = []\n        filters[\"b\"].append(torch.tensor(\n            [-8.1125000725E-4, 4.4451598078E-3, 1.2316980399E-2, 1.3955879956E-2,  1.4179450460E-2, 1.3955879956E-2, 1.2316980399E-2, 4.4451598078E-3, -8.1125000725E-4,\n             3.9103501476E-3, 4.4565401040E-3, -5.8724298142E-3, -2.8760801069E-3, 8.5267601535E-3, -\n             2.8760801069E-3, -5.8724298142E-3, 4.4565401040E-3, 3.9103501476E-3,\n             1.3462699717E-3, -3.7740699481E-3, 8.2581602037E-3, 3.9442278445E-2, 5.3605638444E-2, 3.9442278445E-2, 8.2581602037E-3, -\n             3.7740699481E-3, 1.3462699717E-3,\n             7.4700999539E-4, -3.6522001028E-4, -2.2522680461E-2, -0.1105690673, -\n             0.1768419296, -0.1105690673, -2.2522680461E-2, -3.6522001028E-4, 7.4700999539E-4,\n             0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000, 0.0000000000,\n             -7.4700999539E-4, 3.6522001028E-4, 2.2522680461E-2, 0.1105690673, 0.1768419296, 0.1105690673, 2.2522680461E-2, 3.6522001028E-4, -7.4700999539E-4,\n             -1.3462699717E-3, 3.7740699481E-3, -8.2581602037E-3, -3.9442278445E-2, -\n             5.3605638444E-2, -3.9442278445E-2, -\n             8.2581602037E-3, 3.7740699481E-3, -1.3462699717E-3,\n             -3.9103501476E-3, -4.4565401040E-3, 5.8724298142E-3, 2.8760801069E-3, -\n             8.5267601535E-3, 2.8760801069E-3, 5.8724298142E-3, -\n             4.4565401040E-3, -3.9103501476E-3,\n             8.1125000725E-4, -4.4451598078E-3, -1.2316980399E-2, -1.3955879956E-2, -1.4179450460E-2, -1.3955879956E-2, -1.2316980399E-2, -4.4451598078E-3, 8.1125000725E-4]\n        ).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor(\n            [0.0000000000, -8.2846998703E-4, -5.7109999034E-5, 4.0110000555E-5, 4.6670897864E-3, 8.0871898681E-3, 1.4807609841E-2, 8.6204400286E-3, -3.1221499667E-3,\n             8.2846998703E-4, 0.0000000000, -9.7479997203E-4, -6.9718998857E-3, -\n             2.0865600090E-3, 2.3298799060E-3, -\n             4.4814897701E-3, 1.4917500317E-2, 8.6204400286E-3,\n             5.7109999034E-5, 9.7479997203E-4, 0.0000000000, -1.2145539746E-2, -\n             2.4427289143E-2, 5.0797060132E-2, 3.2785870135E-2, -\n             4.4814897701E-3, 1.4807609841E-2,\n             -4.0110000555E-5, 6.9718998857E-3, 1.2145539746E-2, 0.0000000000, -\n             0.1510555595, -8.2495503128E-2, 5.0797060132E-2, 2.3298799060E-3, 8.0871898681E-3,\n             -4.6670897864E-3, 2.0865600090E-3, 2.4427289143E-2, 0.1510555595, 0.0000000000, -\n             0.1510555595, -2.4427289143E-2, -2.0865600090E-3, 4.6670897864E-3,\n             -8.0871898681E-3, -2.3298799060E-3, -5.0797060132E-2, 8.2495503128E-2, 0.1510555595, 0.0000000000, -\n             1.2145539746E-2, -6.9718998857E-3, 4.0110000555E-5,\n             -1.4807609841E-2, 4.4814897701E-3, -3.2785870135E-2, -\n             5.0797060132E-2, 2.4427289143E-2, 1.2145539746E-2, 0.0000000000, -\n             9.7479997203E-4, -5.7109999034E-5,\n             -8.6204400286E-3, -1.4917500317E-2, 4.4814897701E-3, -\n             2.3298799060E-3, 2.0865600090E-3, 6.9718998857E-3, 9.7479997203E-4, 0.0000000000, -8.2846998703E-4,\n             3.1221499667E-3, -8.6204400286E-3, -1.4807609841E-2, -8.0871898681E-3, -4.6670897864E-3, -4.0110000555E-5, 5.7109999034E-5, 8.2846998703E-4, 0.0000000000]\n        ).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor(\n            [8.1125000725E-4, -3.9103501476E-3, -1.3462699717E-3, -7.4700999539E-4, 0.0000000000, 7.4700999539E-4, 1.3462699717E-3, 3.9103501476E-3, -8.1125000725E-4,\n             -4.4451598078E-3, -4.4565401040E-3, 3.7740699481E-3, 3.6522001028E-4, 0.0000000000, -\n             3.6522001028E-4, -3.7740699481E-3, 4.4565401040E-3, 4.4451598078E-3,\n             -1.2316980399E-2, 5.8724298142E-3, -8.2581602037E-3, 2.2522680461E-2, 0.0000000000, -\n             2.2522680461E-2, 8.2581602037E-3, -5.8724298142E-3, 1.2316980399E-2,\n             -1.3955879956E-2, 2.8760801069E-3, -3.9442278445E-2, 0.1105690673, 0.0000000000, -\n             0.1105690673, 3.9442278445E-2, -2.8760801069E-3, 1.3955879956E-2,\n             -1.4179450460E-2, -8.5267601535E-3, -5.3605638444E-2, 0.1768419296, 0.0000000000, -\n             0.1768419296, 5.3605638444E-2, 8.5267601535E-3, 1.4179450460E-2,\n             -1.3955879956E-2, 2.8760801069E-3, -3.9442278445E-2, 0.1105690673, 0.0000000000, -\n             0.1105690673, 3.9442278445E-2, -2.8760801069E-3, 1.3955879956E-2,\n             -1.2316980399E-2, 5.8724298142E-3, -8.2581602037E-3, 2.2522680461E-2, 0.0000000000, -\n             2.2522680461E-2, 8.2581602037E-3, -5.8724298142E-3, 1.2316980399E-2,\n             -4.4451598078E-3, -4.4565401040E-3, 3.7740699481E-3, 3.6522001028E-4, 0.0000000000, -\n             3.6522001028E-4, -3.7740699481E-3, 4.4565401040E-3, 4.4451598078E-3,\n             8.1125000725E-4, -3.9103501476E-3, -1.3462699717E-3, -7.4700999539E-4, 0.0000000000, 7.4700999539E-4, 1.3462699717E-3, 3.9103501476E-3, -8.1125000725E-4]\n        ).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor(\n            [3.1221499667E-3, -8.6204400286E-3, -1.4807609841E-2, -8.0871898681E-3, -4.6670897864E-3, -4.0110000555E-5, 5.7109999034E-5, 8.2846998703E-4, 0.0000000000,\n             -8.6204400286E-3, -1.4917500317E-2, 4.4814897701E-3, -\n             2.3298799060E-3, 2.0865600090E-3, 6.9718998857E-3, 9.7479997203E-4, -\n             0.0000000000, -8.2846998703E-4,\n             -1.4807609841E-2, 4.4814897701E-3, -3.2785870135E-2, -\n             5.0797060132E-2, 2.4427289143E-2, 1.2145539746E-2, 0.0000000000, -\n             9.7479997203E-4, -5.7109999034E-5,\n             -8.0871898681E-3, -2.3298799060E-3, -5.0797060132E-2, 8.2495503128E-2, 0.1510555595, -\n             0.0000000000, -1.2145539746E-2, -6.9718998857E-3, 4.0110000555E-5,\n             -4.6670897864E-3, 2.0865600090E-3, 2.4427289143E-2, 0.1510555595, 0.0000000000, -\n             0.1510555595, -2.4427289143E-2, -2.0865600090E-3, 4.6670897864E-3,\n             -4.0110000555E-5, 6.9718998857E-3, 1.2145539746E-2, 0.0000000000, -\n             0.1510555595, -8.2495503128E-2, 5.0797060132E-2, 2.3298799060E-3, 8.0871898681E-3,\n             5.7109999034E-5, 9.7479997203E-4, -0.0000000000, -1.2145539746E-2, -\n             2.4427289143E-2, 5.0797060132E-2, 3.2785870135E-2, -\n             4.4814897701E-3, 1.4807609841E-2,\n             8.2846998703E-4, -0.0000000000, -9.7479997203E-4, -6.9718998857E-3, -\n             2.0865600090E-3, 2.3298799060E-3, -\n             4.4814897701E-3, 1.4917500317E-2, 8.6204400286E-3,\n             0.0000000000, -8.2846998703E-4, -5.7109999034E-5, 4.0110000555E-5, 4.6670897864E-3, 8.0871898681E-3, 1.4807609841E-2, 8.6204400286E-3, -3.1221499667E-3]\n        ).reshape(1, 1, 9, 9).permute(0, 1, 3, 2))\n\n    elif n_orientations == 6:\n        filters[\"l\"] = 2 * torch.tensor([\n            [0.00085404, -0.00244917, -0.00387812, -0.00944432, -\n                0.00962054, -0.00944432, -0.00387812, -0.00244917, 0.00085404],\n            [-0.00244917, -0.00523281, -0.00661117, 0.00410600, 0.01002988,\n                0.00410600, -0.00661117, -0.00523281, -0.00244917],\n            [-0.00387812, -0.00661117, 0.01396746, 0.03277038, 0.03981393,\n                0.03277038, 0.01396746, -0.00661117, -0.00387812],\n            [-0.00944432, 0.00410600, 0.03277038, 0.06426333, 0.08169618,\n                0.06426333, 0.03277038, 0.00410600, -0.00944432],\n            [-0.00962054, 0.01002988, 0.03981393, 0.08169618, 0.10096540,\n                0.08169618, 0.03981393, 0.01002988, -0.00962054],\n            [-0.00944432, 0.00410600, 0.03277038, 0.06426333, 0.08169618,\n                0.06426333, 0.03277038, 0.00410600, -0.00944432],\n            [-0.00387812, -0.00661117, 0.01396746, 0.03277038, 0.03981393,\n                0.03277038, 0.01396746, -0.00661117, -0.00387812],\n            [-0.00244917, -0.00523281, -0.00661117, 0.00410600, 0.01002988,\n                0.00410600, -0.00661117, -0.00523281, -0.00244917],\n            [0.00085404, -0.00244917, -0.00387812, -0.00944432, -0.00962054, -0.00944432, -0.00387812, -0.00244917, 0.00085404]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"l0\"] = torch.tensor([\n            [0.00341614, -0.01551246, -0.03848215, -0.01551246, 0.00341614],\n            [-0.01551246, 0.05586982, 0.15925570, 0.05586982, -0.01551246],\n            [-0.03848215, 0.15925570, 0.40304148, 0.15925570, -0.03848215],\n            [-0.01551246, 0.05586982, 0.15925570, 0.05586982, -0.01551246],\n            [0.00341614, -0.01551246, -0.03848215, -0.01551246, 0.00341614]]\n        ).reshape(1, 1, 5, 5)\n        filters[\"h0\"] = torch.tensor([\n            [-0.00033429, -0.00113093, -0.00171484, -0.00133542, -\n                0.00080639, -0.00133542, -0.00171484, -0.00113093, -0.00033429],\n            [-0.00113093, -0.00350017, -0.00243812, 0.00631653, 0.01261227,\n                0.00631653, -0.00243812, -0.00350017, -0.00113093],\n            [-0.00171484, -0.00243812, -0.00290081, -0.00673482, -\n                0.00981051, -0.00673482, -0.00290081, -0.00243812, -0.00171484],\n            [-0.00133542, 0.00631653, -0.00673482, -0.07027679, -\n                0.11435863, -0.07027679, -0.00673482, 0.00631653, -0.00133542],\n            [-0.00080639, 0.01261227, -0.00981051, -0.11435863,\n                0.81380200, -0.11435863, -0.00981051, 0.01261227, -0.00080639],\n            [-0.00133542, 0.00631653, -0.00673482, -0.07027679, -\n                0.11435863, -0.07027679, -0.00673482, 0.00631653, -0.00133542],\n            [-0.00171484, -0.00243812, -0.00290081, -0.00673482, -\n                0.00981051, -0.00673482, -0.00290081, -0.00243812, -0.00171484],\n            [-0.00113093, -0.00350017, -0.00243812, 0.00631653, 0.01261227,\n                0.00631653, -0.00243812, -0.00350017, -0.00113093],\n            [-0.00033429, -0.00113093, -0.00171484, -0.00133542, -0.00080639, -0.00133542, -0.00171484, -0.00113093, -0.00033429]]\n        ).reshape(1, 1, 9, 9)\n        filters[\"b\"] = []\n        filters[\"b\"].append(torch.tensor([\n            0.00277643, 0.00496194, 0.01026699, 0.01455399, 0.01026699, 0.00496194, 0.00277643,\n            -0.00986904, -0.00893064, 0.01189859, 0.02755155, 0.01189859, -0.00893064, -0.00986904,\n            -0.01021852, -0.03075356, -0.08226445, -\n            0.11732297, -0.08226445, -0.03075356, -0.01021852,\n            0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0.00000000,\n            0.01021852, 0.03075356, 0.08226445, 0.11732297, 0.08226445, 0.03075356, 0.01021852,\n            0.00986904, 0.00893064, -0.01189859, -\n            0.02755155, -0.01189859, 0.00893064, 0.00986904,\n            -0.00277643, -0.00496194, -0.01026699, -0.01455399, -0.01026699, -0.00496194, -0.00277643]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor([\n            -0.00343249, -0.00640815, -0.00073141, 0.01124321, 0.00182078, 0.00285723, 0.01166982,\n            -0.00358461, -0.01977507, -0.04084211, -\n            0.00228219, 0.03930573, 0.01161195, 0.00128000,\n            0.01047717, 0.01486305, -0.04819057, -\n            0.12227230, -0.05394139, 0.00853965, -0.00459034,\n            0.00790407, 0.04435647, 0.09454202, -0.00000000, -\n            0.09454202, -0.04435647, -0.00790407,\n            0.00459034, -0.00853965, 0.05394139, 0.12227230, 0.04819057, -0.01486305, -0.01047717,\n            -0.00128000, -0.01161195, -0.03930573, 0.00228219, 0.04084211, 0.01977507, 0.00358461,\n            -0.01166982, -0.00285723, -0.00182078, -0.01124321, 0.00073141, 0.00640815, 0.00343249]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor([\n            0.00343249, 0.00358461, -0.01047717, -\n            0.00790407, -0.00459034, 0.00128000, 0.01166982,\n            0.00640815, 0.01977507, -0.01486305, -\n            0.04435647, 0.00853965, 0.01161195, 0.00285723,\n            0.00073141, 0.04084211, 0.04819057, -\n            0.09454202, -0.05394139, 0.03930573, 0.00182078,\n            -0.01124321, 0.00228219, 0.12227230, -\n            0.00000000, -0.12227230, -0.00228219, 0.01124321,\n            -0.00182078, -0.03930573, 0.05394139, 0.09454202, -\n            0.04819057, -0.04084211, -0.00073141,\n            -0.00285723, -0.01161195, -0.00853965, 0.04435647, 0.01486305, -0.01977507, -0.00640815,\n            -0.01166982, -0.00128000, 0.00459034, 0.00790407, 0.01047717, -0.00358461, -0.00343249]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor(\n            [-0.00277643, 0.00986904, 0.01021852, -0.00000000, -0.01021852, -0.00986904, 0.00277643,\n             -0.00496194, 0.00893064, 0.03075356, -\n             0.00000000, -0.03075356, -0.00893064, 0.00496194,\n             -0.01026699, -0.01189859, 0.08226445, -\n             0.00000000, -0.08226445, 0.01189859, 0.01026699,\n             -0.01455399, -0.02755155, 0.11732297, -\n             0.00000000, -0.11732297, 0.02755155, 0.01455399,\n             -0.01026699, -0.01189859, 0.08226445, -\n             0.00000000, -0.08226445, 0.01189859, 0.01026699,\n             -0.00496194, 0.00893064, 0.03075356, -\n             0.00000000, -0.03075356, -0.00893064, 0.00496194,\n             -0.00277643, 0.00986904, 0.01021852, -0.00000000, -0.01021852, -0.00986904, 0.00277643]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor([\n            -0.01166982, -0.00128000, 0.00459034, 0.00790407, 0.01047717, -0.00358461, -0.00343249,\n            -0.00285723, -0.01161195, -0.00853965, 0.04435647, 0.01486305, -0.01977507, -0.00640815,\n            -0.00182078, -0.03930573, 0.05394139, 0.09454202, -\n            0.04819057, -0.04084211, -0.00073141,\n            -0.01124321, 0.00228219, 0.12227230, -\n            0.00000000, -0.12227230, -0.00228219, 0.01124321,\n            0.00073141, 0.04084211, 0.04819057, -\n            0.09454202, -0.05394139, 0.03930573, 0.00182078,\n            0.00640815, 0.01977507, -0.01486305, -\n            0.04435647, 0.00853965, 0.01161195, 0.00285723,\n            0.00343249, 0.00358461, -0.01047717, -0.00790407, -0.00459034, 0.00128000, 0.01166982]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n        filters[\"b\"].append(torch.tensor([\n            -0.01166982, -0.00285723, -0.00182078, -\n            0.01124321, 0.00073141, 0.00640815, 0.00343249,\n            -0.00128000, -0.01161195, -0.03930573, 0.00228219, 0.04084211, 0.01977507, 0.00358461,\n            0.00459034, -0.00853965, 0.05394139, 0.12227230, 0.04819057, -0.01486305, -0.01047717,\n            0.00790407, 0.04435647, 0.09454202, -0.00000000, -\n            0.09454202, -0.04435647, -0.00790407,\n            0.01047717, 0.01486305, -0.04819057, -\n            0.12227230, -0.05394139, 0.00853965, -0.00459034,\n            -0.00358461, -0.01977507, -0.04084211, -\n            0.00228219, 0.03930573, 0.01161195, 0.00128000,\n            -0.00343249, -0.00640815, -0.00073141, 0.01124321, 0.00182078, 0.00285723, 0.01166982]\n        ).reshape(1, 1, 7, 7).permute(0, 1, 3, 2))\n\n    else:\n        raise Exception(\n            \"Steerable filters not implemented for %d orientations\" % n_orientations)\n\n    if filter_type == \"trained\":\n        if size == 5:\n            # TODO maybe also train h0 and l0 filters\n            filters = crop_steerable_pyramid_filters(filters, 5)\n            filters[\"b\"][0] = torch.tensor([\n                [-0.0356752239, -0.0223877281, -0.0009542659,\n                    0.0244821459, 0.0322226137],\n                [-0.0593218654,  0.1245803162, -\n                    0.0023863907, -0.1230178699, 0.0589442067],\n                [-0.0281576272,  0.2976626456, -\n                    0.0020888755, -0.2953369915, 0.0284542721],\n                [-0.0586092323,  0.1251581162, -\n                    0.0024624448, -0.1227868199, 0.0587830991],\n                [-0.0327464789, -0.0223652460, -\n                    0.0042342511,  0.0245472137, 0.0359398536]\n            ]).reshape(1, 1, 5, 5)\n            filters[\"b\"][1] = torch.tensor([\n                [3.9758663625e-02,  6.0679119080e-02,  3.0146904290e-02,\n                    6.1198268086e-02,  3.6218870431e-02],\n                [2.3255519569e-02, -1.2505133450e-01, -\n                    2.9738345742e-01, -1.2518258393e-01,  2.3592948914e-02],\n                [-1.3602430699e-03, -1.2058277935e-04,  2.6399988565e-04, -\n                    2.3791544663e-04,  1.8450465286e-03],\n                [-2.1563466638e-02,  1.2572696805e-01,  2.9745018482e-01,\n                    1.2458638102e-01, -2.3847281933e-02],\n                [-3.7941932678e-02, -6.1060950160e-02, -\n                    2.9489086941e-02, -6.0411967337e-02, -3.8459088653e-02]\n            ]).reshape(1, 1, 5, 5)\n\n            # Below filters were optimised on 09/02/2021\n            # 20K iterations with multiple images at more scales.\n            filters[\"b\"][0] = torch.tensor([\n                [-4.5508436859e-02, -2.1767273545e-02, -1.9399923622e-04,\n                    2.1200872958e-02,  4.5475799590e-02],\n                [-6.3554823399e-02,  1.2832683325e-01, -\n                    5.3858719184e-05, -1.2809979916e-01,  6.3842624426e-02],\n                [-3.4809380770e-02,  2.9954621196e-01,  2.9066693969e-05, -\n                    2.9957753420e-01,  3.4806568176e-02],\n                [-6.3934154809e-02,  1.2806062400e-01,  9.0917674243e-05, -\n                    1.2832444906e-01,  6.3572973013e-02],\n                [-4.5492250472e-02, -2.1125273779e-02,  4.2229349492e-04,\n                    2.1804777905e-02,  4.5236673206e-02]\n            ]).reshape(1, 1, 5, 5)\n            filters[\"b\"][1] = torch.tensor([\n                [4.8947390169e-02,  6.3575074077e-02,  3.4955859184e-02,\n                    6.4085893333e-02,  4.9838040024e-02],\n                [2.2061849013e-02, -1.2936264277e-01, -\n                    3.0093491077e-01, -1.2997294962e-01,  2.0597217605e-02],\n                [-5.1290717238e-05, -1.7305796064e-05,  2.0256420612e-05, -\n                    1.1864109547e-04,  7.3973249528e-05],\n                [-2.0749464631e-02,  1.2988376617e-01,  3.0080935359e-01,\n                    1.2921217084e-01, -2.2159902379e-02],\n                [-4.9614857882e-02, -6.4021714032e-02, -\n                    3.4676689655e-02, -6.3446544111e-02, -4.8282280564e-02]\n            ]).reshape(1, 1, 5, 5)\n\n            # Trained on 17/02/2021 to match fourier pyramid in spatial domain\n            filters[\"b\"][0] = torch.tensor([\n                [3.3370e-02,  9.3934e-02, -3.5810e-04, -9.4038e-02, -3.3115e-02],\n                [1.7716e-01,  3.9378e-01,  6.8461e-05, -3.9343e-01, -1.7685e-01],\n                [2.9213e-01,  6.1042e-01,  7.0654e-04, -6.0939e-01, -2.9177e-01],\n                [1.7684e-01,  3.9392e-01,  1.0517e-03, -3.9268e-01, -1.7668e-01],\n                [3.3000e-02,  9.4029e-02,  7.3565e-04, -9.3366e-02, -3.3008e-02]\n            ]).reshape(1, 1, 5, 5) * 0.1\n\n            filters[\"b\"][1] = torch.tensor([\n                [0.0331,  0.1763,  0.2907,  0.1753,  0.0325],\n                [0.0941,  0.3932,  0.6079,  0.3904,  0.0922],\n                [0.0008,  0.0009, -0.0010, -0.0025, -0.0015],\n                [-0.0929, -0.3919, -0.6097, -0.3944, -0.0946],\n                [-0.0328, -0.1760, -0.2915, -0.1768, -0.0333]\n            ]).reshape(1, 1, 5, 5) * 0.1\n\n        else:\n            raise Exception(\n                \"Trained filters not implemented for size %d\" % size)\n\n    if filter_type == \"cropped\":\n        filters = crop_steerable_pyramid_filters(filters, size)\n\n    return filters\n</code></pre>"},{"location":"odak/learn_perception/#odak.learn.perception.util.slice_rgbd_targets","title":"<code>slice_rgbd_targets(target, depth, depth_plane_positions)</code>","text":"<p>Slices the target RGBD image and depth map into multiple layers based on depth plane positions.</p> <p>Parameters:</p> <ul> <li> <code>target</code>           \u2013            <pre><code>                 The RGBD target tensor with shape (C, H, W).\n</code></pre> </li> <li> <code>depth</code>           \u2013            <pre><code>                 The depth map corresponding to the target image with shape (H, W).\n</code></pre> </li> <li> <code>depth_plane_positions</code>           \u2013            <pre><code>                 The positions of the depth planes used for slicing.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>targets</code> (              <code>Tensor</code> )          \u2013            <p>A tensor of shape (N, C, H, W) where N is the number of depth planes. Contains the sliced targets for each depth plane.</p> </li> <li> <code>masks</code> (              <code>Tensor</code> )          \u2013            <p>A tensor of shape (N, C, H, W) containing binary masks for each depth plane.</p> </li> </ul> Source code in <code>odak/learn/perception/util.py</code> <pre><code>def slice_rgbd_targets(target, depth, depth_plane_positions):\n    \"\"\"\n    Slices the target RGBD image and depth map into multiple layers based on depth plane positions.\n\n    Parameters\n    ----------\n    target                 : torch.Tensor\n                             The RGBD target tensor with shape (C, H, W).\n    depth                  : torch.Tensor\n                             The depth map corresponding to the target image with shape (H, W).\n    depth_plane_positions  : list or torch.Tensor\n                             The positions of the depth planes used for slicing.\n\n    Returns\n    -------\n    targets              : torch.Tensor\n                           A tensor of shape (N, C, H, W) where N is the number of depth planes. Contains the sliced targets for each depth plane.\n    masks                : torch.Tensor\n                           A tensor of shape (N, C, H, W) containing binary masks for each depth plane.\n    \"\"\"\n    device = target.device\n    number_of_planes = len(depth_plane_positions) - 1\n    targets = torch.zeros(\n                        number_of_planes,\n                        target.shape[0],\n                        target.shape[1],\n                        target.shape[2],\n                        requires_grad = False,\n                        device = device\n                        )\n    masks = torch.zeros_like(targets, dtype = torch.int).to(device)\n    mask_zeros = torch.zeros_like(depth, dtype = torch.int)\n    mask_ones = torch.ones_like(depth, dtype = torch.int)\n    for i in range(1, number_of_planes+1):\n        for ch in range(target.shape[0]):\n            pos = depth_plane_positions[i] \n            prev_pos = depth_plane_positions[i-1] \n            if i &lt;= (number_of_planes - 1):\n                condition = torch.logical_and(prev_pos &lt;= depth, depth &lt; pos)\n            else:\n                condition = torch.logical_and(prev_pos &lt;= depth, depth &lt;= pos)\n            mask = torch.where(condition, mask_ones, mask_zeros)\n            new_target = target[ch] * mask\n            targets[i-1, ch] = new_target.squeeze(0)\n            masks[i-1, ch] = mask.detach().clone() \n    return targets, masks\n</code></pre>"},{"location":"odak/learn_raytracing/","title":"odak.learn.raytracing","text":"<p><code>odak.learn.raytracing</code></p> <p>Provides necessary definitions for geometric optics. See \"General Ray tracing procedure\" from G.H. Spencerand M.V.R.K Murty for more theoratical explanation.</p>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.planar_mesh","title":"<code>planar_mesh</code>","text":"Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>class planar_mesh():\n\n\n    def __init__(\n                 self,\n                 size = [1., 1.],\n                 number_of_meshes = [10, 10],\n                 angles = torch.tensor([0., 0., 0.]),\n                 offset = torch.tensor([0., 0., 0.]),\n                 device = torch.device('cpu'),\n                 heights = None\n                ):\n        \"\"\"\n        Definition to generate a plane with meshes.\n\n\n        Parameters\n        -----------\n        number_of_meshes  : torch.tensor\n                            Number of squares over plane.\n                            There are two triangles at each square.\n        size              : torch.tensor\n                            Size of the plane.\n        angles            : torch.tensor\n                            Rotation angles in degrees.\n        offset            : torch.tensor\n                            Offset along XYZ axes.\n                            Expected dimension is [1 x 3] or offset for each triangle [m x 3].\n                            m here refers to `2 * number_of_meshes[0]` times  `number_of_meshes[1]`.\n        device            : torch.device\n                            Computational resource to be used (e.g., cpu, cuda).\n        heights           : torch.tensor\n                            Load surface heights from a tensor.\n        \"\"\"\n        self.device = device\n        self.angles = angles.to(self.device)\n        self.offset = offset.to(self.device)\n        self.size = size.to(self.device)\n        self.number_of_meshes = number_of_meshes.to(self.device)\n        self.init_heights(heights)\n\n\n    def init_heights(self, heights = None):\n        \"\"\"\n        Internal function to initialize a height map.\n        Note that self.heights is a differentiable variable, and can be optimized or learned.\n        See unit test `test/test_learn_ray_detector.py` or `test/test_learn_ray_mesh.py` as examples.\n        \"\"\"\n        if not isinstance(heights, type(None)):\n            self.heights = heights.to(self.device)\n            self.heights.requires_grad = True\n        else:\n            self.heights = torch.zeros(\n                                       (self.number_of_meshes[0], self.number_of_meshes[1], 1),\n                                       requires_grad = True,\n                                       device = self.device,\n                                      )\n        x = torch.linspace(-self.size[0] / 2., self.size[0] / 2., self.number_of_meshes[0], device = self.device) \n        y = torch.linspace(-self.size[1] / 2., self.size[1] / 2., self.number_of_meshes[1], device = self.device)\n        X, Y = torch.meshgrid(x, y, indexing = 'ij')\n        self.X = X.unsqueeze(-1)\n        self.Y = Y.unsqueeze(-1)\n\n\n    def save_heights(self, filename = 'heights.pt'):\n        \"\"\"\n        Function to save heights to a file.\n\n        Parameters\n        ----------\n        filename          : str\n                            Filename.\n        \"\"\"\n        save_torch_tensor(filename, self.heights.detach().clone())\n\n\n    def save_heights_as_PLY(self, filename = 'mesh.ply'):\n        \"\"\"\n        Function to save mesh to a PLY file.\n\n        Parameters\n        ----------\n        filename          : str\n                            Filename.\n        \"\"\"\n        triangles = self.get_triangles()\n        write_PLY(triangles, filename)\n\n\n    def get_squares(self):\n        \"\"\"\n        Internal function to initiate squares over a plane.\n\n        Returns\n        -------\n        squares     : torch.tensor\n                      Squares over a plane.\n                      Expected size is [m x n x 3].\n        \"\"\"\n        squares = torch.cat((\n                             self.X,\n                             self.Y,\n                             self.heights\n                            ), dim = -1)\n        return squares\n\n\n    def get_triangles(self):\n        \"\"\"\n        Internal function to get triangles.\n        \"\"\" \n        squares = self.get_squares()\n        triangles = torch.zeros(2, self.number_of_meshes[0], self.number_of_meshes[1], 3, 3, device = self.device)\n        for i in range(0, self.number_of_meshes[0] - 1):\n            for j in range(0, self.number_of_meshes[1] - 1):\n                first_triangle = torch.cat((\n                                            squares[i + 1, j].unsqueeze(0),\n                                            squares[i + 1, j + 1].unsqueeze(0),\n                                            squares[i, j + 1].unsqueeze(0),\n                                           ), dim = 0)\n                second_triangle = torch.cat((\n                                             squares[i + 1, j].unsqueeze(0),\n                                             squares[i, j + 1].unsqueeze(0),\n                                             squares[i, j].unsqueeze(0),\n                                            ), dim = 0)\n                triangles[0, i, j], _, _, _ = rotate_points(first_triangle, angles = self.angles)\n                triangles[1, i, j], _, _, _ = rotate_points(second_triangle, angles = self.angles)\n        triangles = triangles.view(-1, 3, 3) + self.offset\n        return triangles \n\n\n    def mirror(self, rays):\n        \"\"\"\n        Function to bounce light rays off the meshes.\n\n        Parameters\n        ----------\n        rays              : torch.tensor\n                            Rays to be bounced.\n                            Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n\n        Returns\n        -------\n        reflected_rays    : torch.tensor\n                            Reflected rays.\n                            Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n        reflected_normals : torch.tensor\n                            Reflected normals.\n                            Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n\n        \"\"\"\n        if len(rays.shape) == 2:\n            rays = rays.unsqueeze(0)\n        triangles = self.get_triangles()\n        reflected_rays = torch.empty((0, 2, 3), requires_grad = True, device = self.device)\n        reflected_normals = torch.empty((0, 2, 3), requires_grad = True, device = self.device)\n        for triangle in triangles:\n            _, _, intersecting_rays, intersecting_normals, check = intersect_w_triangle(\n                                                                                        rays,\n                                                                                        triangle\n                                                                                       ) \n            triangle_reflected_rays = reflect(intersecting_rays, intersecting_normals)\n            if triangle_reflected_rays.shape[0] &gt; 0:\n                reflected_rays = torch.cat((\n                                            reflected_rays,\n                                            triangle_reflected_rays\n                                          ))\n                reflected_normals = torch.cat((\n                                               reflected_normals,\n                                               intersecting_normals\n                                              ))\n        return reflected_rays, reflected_normals\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.planar_mesh.__init__","title":"<code>__init__(size=[1.0, 1.0], number_of_meshes=[10, 10], angles=torch.tensor([0.0, 0.0, 0.0]), offset=torch.tensor([0.0, 0.0, 0.0]), device=torch.device('cpu'), heights=None)</code>","text":"<p>Definition to generate a plane with meshes.</p> <p>Parameters:</p> <ul> <li> <code>number_of_meshes</code>           \u2013            <pre><code>            Number of squares over plane.\n            There are two triangles at each square.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>            Size of the plane.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>            Rotation angles in degrees.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>            Offset along XYZ axes.\n            Expected dimension is [1 x 3] or offset for each triangle [m x 3].\n            m here refers to `2 * number_of_meshes[0]` times  `number_of_meshes[1]`.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>            Computational resource to be used (e.g., cpu, cuda).\n</code></pre> </li> <li> <code>heights</code>           \u2013            <pre><code>            Load surface heights from a tensor.\n</code></pre> </li> </ul> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def __init__(\n             self,\n             size = [1., 1.],\n             number_of_meshes = [10, 10],\n             angles = torch.tensor([0., 0., 0.]),\n             offset = torch.tensor([0., 0., 0.]),\n             device = torch.device('cpu'),\n             heights = None\n            ):\n    \"\"\"\n    Definition to generate a plane with meshes.\n\n\n    Parameters\n    -----------\n    number_of_meshes  : torch.tensor\n                        Number of squares over plane.\n                        There are two triangles at each square.\n    size              : torch.tensor\n                        Size of the plane.\n    angles            : torch.tensor\n                        Rotation angles in degrees.\n    offset            : torch.tensor\n                        Offset along XYZ axes.\n                        Expected dimension is [1 x 3] or offset for each triangle [m x 3].\n                        m here refers to `2 * number_of_meshes[0]` times  `number_of_meshes[1]`.\n    device            : torch.device\n                        Computational resource to be used (e.g., cpu, cuda).\n    heights           : torch.tensor\n                        Load surface heights from a tensor.\n    \"\"\"\n    self.device = device\n    self.angles = angles.to(self.device)\n    self.offset = offset.to(self.device)\n    self.size = size.to(self.device)\n    self.number_of_meshes = number_of_meshes.to(self.device)\n    self.init_heights(heights)\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.planar_mesh.get_squares","title":"<code>get_squares()</code>","text":"<p>Internal function to initiate squares over a plane.</p> <p>Returns:</p> <ul> <li> <code>squares</code> (              <code>tensor</code> )          \u2013            <p>Squares over a plane. Expected size is [m x n x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def get_squares(self):\n    \"\"\"\n    Internal function to initiate squares over a plane.\n\n    Returns\n    -------\n    squares     : torch.tensor\n                  Squares over a plane.\n                  Expected size is [m x n x 3].\n    \"\"\"\n    squares = torch.cat((\n                         self.X,\n                         self.Y,\n                         self.heights\n                        ), dim = -1)\n    return squares\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.planar_mesh.get_triangles","title":"<code>get_triangles()</code>","text":"<p>Internal function to get triangles.</p> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def get_triangles(self):\n    \"\"\"\n    Internal function to get triangles.\n    \"\"\" \n    squares = self.get_squares()\n    triangles = torch.zeros(2, self.number_of_meshes[0], self.number_of_meshes[1], 3, 3, device = self.device)\n    for i in range(0, self.number_of_meshes[0] - 1):\n        for j in range(0, self.number_of_meshes[1] - 1):\n            first_triangle = torch.cat((\n                                        squares[i + 1, j].unsqueeze(0),\n                                        squares[i + 1, j + 1].unsqueeze(0),\n                                        squares[i, j + 1].unsqueeze(0),\n                                       ), dim = 0)\n            second_triangle = torch.cat((\n                                         squares[i + 1, j].unsqueeze(0),\n                                         squares[i, j + 1].unsqueeze(0),\n                                         squares[i, j].unsqueeze(0),\n                                        ), dim = 0)\n            triangles[0, i, j], _, _, _ = rotate_points(first_triangle, angles = self.angles)\n            triangles[1, i, j], _, _, _ = rotate_points(second_triangle, angles = self.angles)\n    triangles = triangles.view(-1, 3, 3) + self.offset\n    return triangles \n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.planar_mesh.init_heights","title":"<code>init_heights(heights=None)</code>","text":"<p>Internal function to initialize a height map. Note that self.heights is a differentiable variable, and can be optimized or learned. See unit test <code>test/test_learn_ray_detector.py</code> or <code>test/test_learn_ray_mesh.py</code> as examples.</p> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def init_heights(self, heights = None):\n    \"\"\"\n    Internal function to initialize a height map.\n    Note that self.heights is a differentiable variable, and can be optimized or learned.\n    See unit test `test/test_learn_ray_detector.py` or `test/test_learn_ray_mesh.py` as examples.\n    \"\"\"\n    if not isinstance(heights, type(None)):\n        self.heights = heights.to(self.device)\n        self.heights.requires_grad = True\n    else:\n        self.heights = torch.zeros(\n                                   (self.number_of_meshes[0], self.number_of_meshes[1], 1),\n                                   requires_grad = True,\n                                   device = self.device,\n                                  )\n    x = torch.linspace(-self.size[0] / 2., self.size[0] / 2., self.number_of_meshes[0], device = self.device) \n    y = torch.linspace(-self.size[1] / 2., self.size[1] / 2., self.number_of_meshes[1], device = self.device)\n    X, Y = torch.meshgrid(x, y, indexing = 'ij')\n    self.X = X.unsqueeze(-1)\n    self.Y = Y.unsqueeze(-1)\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.planar_mesh.mirror","title":"<code>mirror(rays)</code>","text":"<p>Function to bounce light rays off the meshes.</p> <p>Parameters:</p> <ul> <li> <code>rays</code>           \u2013            <pre><code>            Rays to be bounced.\n            Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>reflected_rays</code> (              <code>tensor</code> )          \u2013            <p>Reflected rays. Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].</p> </li> <li> <code>reflected_normals</code> (              <code>tensor</code> )          \u2013            <p>Reflected normals. Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def mirror(self, rays):\n    \"\"\"\n    Function to bounce light rays off the meshes.\n\n    Parameters\n    ----------\n    rays              : torch.tensor\n                        Rays to be bounced.\n                        Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n\n    Returns\n    -------\n    reflected_rays    : torch.tensor\n                        Reflected rays.\n                        Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n    reflected_normals : torch.tensor\n                        Reflected normals.\n                        Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n\n    \"\"\"\n    if len(rays.shape) == 2:\n        rays = rays.unsqueeze(0)\n    triangles = self.get_triangles()\n    reflected_rays = torch.empty((0, 2, 3), requires_grad = True, device = self.device)\n    reflected_normals = torch.empty((0, 2, 3), requires_grad = True, device = self.device)\n    for triangle in triangles:\n        _, _, intersecting_rays, intersecting_normals, check = intersect_w_triangle(\n                                                                                    rays,\n                                                                                    triangle\n                                                                                   ) \n        triangle_reflected_rays = reflect(intersecting_rays, intersecting_normals)\n        if triangle_reflected_rays.shape[0] &gt; 0:\n            reflected_rays = torch.cat((\n                                        reflected_rays,\n                                        triangle_reflected_rays\n                                      ))\n            reflected_normals = torch.cat((\n                                           reflected_normals,\n                                           intersecting_normals\n                                          ))\n    return reflected_rays, reflected_normals\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.planar_mesh.save_heights","title":"<code>save_heights(filename='heights.pt')</code>","text":"<p>Function to save heights to a file.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>            Filename.\n</code></pre> </li> </ul> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def save_heights(self, filename = 'heights.pt'):\n    \"\"\"\n    Function to save heights to a file.\n\n    Parameters\n    ----------\n    filename          : str\n                        Filename.\n    \"\"\"\n    save_torch_tensor(filename, self.heights.detach().clone())\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.planar_mesh.save_heights_as_PLY","title":"<code>save_heights_as_PLY(filename='mesh.ply')</code>","text":"<p>Function to save mesh to a PLY file.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>            Filename.\n</code></pre> </li> </ul> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def save_heights_as_PLY(self, filename = 'mesh.ply'):\n    \"\"\"\n    Function to save mesh to a PLY file.\n\n    Parameters\n    ----------\n    filename          : str\n                        Filename.\n    \"\"\"\n    triangles = self.get_triangles()\n    write_PLY(triangles, filename)\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.center_of_triangle","title":"<code>center_of_triangle(triangle)</code>","text":"<p>Definition to calculate center of a triangle.</p> <p>Parameters:</p> <ul> <li> <code>triangle</code>           \u2013            <pre><code>        An array that contains three points defining a triangle (Mx3). \n        It can also parallel process many triangles (NxMx3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>centers</code> (              <code>tensor</code> )          \u2013            <p>Triangle centers.</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def center_of_triangle(triangle):\n    \"\"\"\n    Definition to calculate center of a triangle.\n\n    Parameters\n    ----------\n    triangle      : torch.tensor\n                    An array that contains three points defining a triangle (Mx3). \n                    It can also parallel process many triangles (NxMx3).\n\n    Returns\n    -------\n    centers       : torch.tensor\n                    Triangle centers.\n    \"\"\"\n    if len(triangle.shape) == 2:\n        triangle = triangle.view((1, 3, 3))\n    center = torch.mean(triangle, axis=1)\n    return center\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.create_ray","title":"<code>create_ray(xyz, abg, direction=False)</code>","text":"<p>Definition to create a ray.</p> <p>Parameters:</p> <ul> <li> <code>xyz</code>           \u2013            <pre><code>       List that contains X,Y and Z start locations of a ray.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> <li> <code>abg</code>           \u2013            <pre><code>       List that contains angles in degrees with respect to the X,Y and Z axes.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> <li> <code>direction</code>           \u2013            <pre><code>       If set to True, cosines of `abg` is not calculated.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray. Size will be either [1 x 3] or [m x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray(xyz, abg, direction = False):\n    \"\"\"\n    Definition to create a ray.\n\n    Parameters\n    ----------\n    xyz          : torch.tensor\n                   List that contains X,Y and Z start locations of a ray.\n                   Size could be [1 x 3], [3], [m x 3].\n    abg          : torch.tensor\n                   List that contains angles in degrees with respect to the X,Y and Z axes.\n                   Size could be [1 x 3], [3], [m x 3].\n    direction    : bool\n                   If set to True, cosines of `abg` is not calculated.\n\n    Returns\n    ----------\n    ray          : torch.tensor\n                   Array that contains starting points and cosines of a created ray.\n                   Size will be either [1 x 3] or [m x 3].\n    \"\"\"\n    points = xyz\n    angles = abg\n    if len(xyz) == 1:\n        points = xyz.unsqueeze(0)\n    if len(abg) == 1:\n        angles = abg.unsqueeze(0)\n    ray = torch.zeros(points.shape[0], 2, 3, device = points.device)\n    ray[:, 0] = points\n    if direction:\n        ray[:, 1] = abg\n    else:\n        ray[:, 1] = torch.cos(torch.deg2rad(abg))\n    return ray\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.create_ray_from_all_pairs","title":"<code>create_ray_from_all_pairs(x0y0z0, x1y1z1)</code>","text":"<p>Creates rays from all possible pairs of points in x0y0z0 and x1y1z1.</p> <p>Parameters:</p> <ul> <li> <code>x0y0z0</code>           \u2013            <pre><code>       Tensor that contains X, Y, and Z start locations of rays.\n       Size should be [m x 3].\n</code></pre> </li> <li> <code>x1y1z1</code>           \u2013            <pre><code>       Tensor that contains X, Y, and Z end locations of rays.\n       Size should be [n x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rays</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray(s). Size of [n*m x 2 x 3]</p> </li> </ul> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray_from_all_pairs(x0y0z0, x1y1z1):\n    \"\"\"\n    Creates rays from all possible pairs of points in x0y0z0 and x1y1z1.\n\n    Parameters\n    ----------\n    x0y0z0       : torch.tensor\n                   Tensor that contains X, Y, and Z start locations of rays.\n                   Size should be [m x 3].\n    x1y1z1       : torch.tensor\n                   Tensor that contains X, Y, and Z end locations of rays.\n                   Size should be [n x 3].\n\n    Returns\n    ----------\n    rays         : torch.tensor\n                   Array that contains starting points and cosines of a created ray(s). Size of [n*m x 2 x 3]\n    \"\"\"\n\n    if len(x0y0z0.shape) == 1:\n        x0y0z0 = x0y0z0.unsqueeze(0)\n    if len(x1y1z1.shape) == 1:\n        x1y1z1 = x1y1z1.unsqueeze(0)\n\n    m, n = x0y0z0.shape[0], x1y1z1.shape[0]\n    start_points = x0y0z0.unsqueeze(1).expand(-1, n, -1).reshape(-1, 3)\n    end_points = x1y1z1.unsqueeze(0).expand(m, -1, -1).reshape(-1, 3)\n\n    directions = end_points - start_points\n    norms = torch.norm(directions, p=2, dim=1, keepdim=True)\n    norms[norms == 0] = float('nan')\n\n    normalized_directions = directions / norms\n\n    rays = torch.zeros(m * n, 2, 3, device=x0y0z0.device)\n    rays[:, 0, :] = start_points\n    rays[:, 1, :] = normalized_directions\n\n    return rays\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.create_ray_from_grid_w_luminous_angle","title":"<code>create_ray_from_grid_w_luminous_angle(center, size, no, tilt, num_ray_per_light, angle_limit)</code>","text":"<p>Generate a 2D array of lights, each emitting rays within a specified solid angle and tilt.</p> Parameters: <p>center              : torch.tensor                       The center point of the light array, shape [3]. size                : list[int]                       The size of the light array [height, width] no                  : list[int]                       The number of the light arary [number of lights in height , number of lights inwidth] tilt                : torch.tensor                       The tilt angles in degrees along x, y, z axes for the rays, shape [3]. angle_limit         : float                       The maximum angle in degrees from the initial direction vector within which to emit rays. num_rays_per_light  : int                       The number of rays each light should emit.</p> Returns: <p>rays : torch.tensor        Array that contains starting points and cosines of a created ray(s). Size of [n x 2 x 3]</p> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray_from_grid_w_luminous_angle(center, size, no, tilt, num_ray_per_light, angle_limit):\n    \"\"\"\n    Generate a 2D array of lights, each emitting rays within a specified solid angle and tilt.\n\n    Parameters:\n    ----------\n    center              : torch.tensor\n                          The center point of the light array, shape [3].\n    size                : list[int]\n                          The size of the light array [height, width]\n    no                  : list[int]\n                          The number of the light arary [number of lights in height , number of lights inwidth]\n    tilt                : torch.tensor\n                          The tilt angles in degrees along x, y, z axes for the rays, shape [3].\n    angle_limit         : float\n                          The maximum angle in degrees from the initial direction vector within which to emit rays.\n    num_rays_per_light  : int\n                          The number of rays each light should emit.\n\n    Returns:\n    ----------\n    rays : torch.tensor\n           Array that contains starting points and cosines of a created ray(s). Size of [n x 2 x 3]\n    \"\"\"\n\n    samples = torch.zeros((no[0], no[1], 3))\n\n    x = torch.linspace(-size[0] / 2., size[0] / 2., no[0])\n    y = torch.linspace(-size[1] / 2., size[1] / 2., no[1])\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n\n    samples[:, :, 0] = X.detach().clone()\n    samples[:, :, 1] = Y.detach().clone()\n    samples = samples.reshape((no[0]*no[1], 3))\n\n    samples, *_ = rotate_points(samples, angles=tilt)\n\n    samples = samples + center\n    angle_limit = torch.as_tensor(angle_limit)\n    cos_alpha = torch.cos(angle_limit * torch.pi / 180)\n    tilt = tilt * torch.pi / 180\n\n    theta = torch.acos(1 - 2 * torch.rand(num_ray_per_light*samples.size(0)) * (1-cos_alpha))\n    phi = 2 * torch.pi * torch.rand(num_ray_per_light*samples.size(0))  \n\n    directions = torch.stack([\n        torch.sin(theta) * torch.cos(phi),  \n        torch.sin(theta) * torch.sin(phi),  \n        torch.cos(theta)                    \n    ], dim=1)\n\n    c, s = torch.cos(tilt), torch.sin(tilt)\n\n    Rx = torch.tensor([\n        [1, 0, 0],\n        [0, c[0], -s[0]],\n        [0, s[0], c[0]]\n    ])\n\n    Ry = torch.tensor([\n        [c[1], 0, s[1]],\n        [0, 1, 0],\n        [-s[1], 0, c[1]]\n    ])\n\n    Rz = torch.tensor([\n        [c[2], -s[2], 0],\n        [s[2], c[2], 0],\n        [0, 0, 1]\n    ])\n\n    origins = samples.repeat(num_ray_per_light, 1)\n\n    directions = torch.matmul(directions, (Rz@Ry@Rx).T)\n\n\n    rays = torch.zeros(num_ray_per_light*samples.size(0), 2, 3)\n    rays[:, 0, :] = origins\n    rays[:, 1, :] = directions\n\n    return rays\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.create_ray_from_point_w_luminous_angle","title":"<code>create_ray_from_point_w_luminous_angle(origin, num_ray, tilt, angle_limit)</code>","text":"<p>Generate rays from a point, tilted by specific angles along x, y, z axes, within a specified solid angle.</p> Parameters: <p>origin      : torch.tensor               The origin point of the rays, shape [3]. num_rays    : int               The total number of rays to generate. tilt        : torch.tensor               The tilt angles in degrees along x, y, z axes, shape [3]. angle_limit : float               The maximum angle in degrees from the initial direction vector within which to emit rays.</p> Returns: <p>rays : torch.tensor        Array that contains starting points and cosines of a created ray(s). Size of [n x 2 x 3]</p> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray_from_point_w_luminous_angle(origin, num_ray, tilt, angle_limit):\n    \"\"\"\n    Generate rays from a point, tilted by specific angles along x, y, z axes, within a specified solid angle.\n\n    Parameters:\n    ----------\n    origin      : torch.tensor\n                  The origin point of the rays, shape [3].\n    num_rays    : int\n                  The total number of rays to generate.\n    tilt        : torch.tensor\n                  The tilt angles in degrees along x, y, z axes, shape [3].\n    angle_limit : float\n                  The maximum angle in degrees from the initial direction vector within which to emit rays.\n\n    Returns:\n    ----------\n    rays : torch.tensor\n           Array that contains starting points and cosines of a created ray(s). Size of [n x 2 x 3]\n    \"\"\"\n    angle_limit = torch.as_tensor(angle_limit) \n    cos_alpha = torch.cos(angle_limit * torch.pi / 180)\n    tilt = tilt * torch.pi / 180\n\n    theta = torch.acos(1 - 2 * torch.rand(num_ray) * (1-cos_alpha))\n    phi = 2 * torch.pi * torch.rand(num_ray)  \n\n\n    directions = torch.stack([\n        torch.sin(theta) * torch.cos(phi),  \n        torch.sin(theta) * torch.sin(phi),  \n        torch.cos(theta)                    \n    ], dim=1)\n\n    c, s = torch.cos(tilt), torch.sin(tilt)\n\n    Rx = torch.tensor([\n        [1, 0, 0],\n        [0, c[0], -s[0]],\n        [0, s[0], c[0]]\n    ])\n\n    Ry = torch.tensor([\n        [c[1], 0, s[1]],\n        [0, 1, 0],\n        [-s[1], 0, c[1]]\n    ])\n\n    Rz = torch.tensor([\n        [c[2], -s[2], 0],\n        [s[2], c[2], 0],\n        [0, 0, 1]\n    ])\n\n    origins = origin.repeat(num_ray, 1)\n    directions = torch.matmul(directions, (Rz@Ry@Rx).T)\n\n\n    rays = torch.zeros(num_ray, 2, 3)\n    rays[:, 0, :] = origins\n    rays[:, 1, :] = directions\n\n    return rays\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.create_ray_from_two_points","title":"<code>create_ray_from_two_points(x0y0z0, x1y1z1)</code>","text":"<p>Definition to create a ray from two given points. Note that both inputs must match in shape.</p> <p>Parameters:</p> <ul> <li> <code>x0y0z0</code>           \u2013            <pre><code>       List that contains X,Y and Z start locations of a ray.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> <li> <code>x1y1z1</code>           \u2013            <pre><code>       List that contains X,Y and Z ending locations of a ray or batch of rays.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray(s).</p> </li> </ul> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray_from_two_points(x0y0z0, x1y1z1):\n    \"\"\"\n    Definition to create a ray from two given points. Note that both inputs must match in shape.\n\n    Parameters\n    ----------\n    x0y0z0       : torch.tensor\n                   List that contains X,Y and Z start locations of a ray.\n                   Size could be [1 x 3], [3], [m x 3].\n    x1y1z1       : torch.tensor\n                   List that contains X,Y and Z ending locations of a ray or batch of rays.\n                   Size could be [1 x 3], [3], [m x 3].\n\n    Returns\n    ----------\n    ray          : torch.tensor\n                   Array that contains starting points and cosines of a created ray(s).\n    \"\"\"\n    if len(x0y0z0.shape) == 1:\n        x0y0z0 = x0y0z0.unsqueeze(0)\n    if len(x1y1z1.shape) == 1:\n        x1y1z1 = x1y1z1.unsqueeze(0)\n    xdiff = x1y1z1[:, 0] - x0y0z0[:, 0]\n    ydiff = x1y1z1[:, 1] - x0y0z0[:, 1]\n    zdiff = x1y1z1[:, 2] - x0y0z0[:, 2]\n    s = (xdiff ** 2 + ydiff ** 2 + zdiff ** 2) ** 0.5\n    s[s == 0] = float('nan')\n    cosines = torch.zeros_like(x0y0z0 * x1y1z1)\n    cosines[:, 0] = xdiff / s\n    cosines[:, 1] = ydiff / s\n    cosines[:, 2] = zdiff / s\n    ray = torch.zeros(xdiff.shape[0], 2, 3, device = x0y0z0.device)\n    ray[:, 0] = x0y0z0\n    ray[:, 1] = cosines\n    return ray\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.define_circle","title":"<code>define_circle(center, radius, angles)</code>","text":"<p>Definition to describe a circle in a single variable packed form.</p> <p>Parameters:</p> <ul> <li> <code>center</code>           \u2013            <pre><code>  Center of a circle to be defined in 3D space.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>  Radius of a circle to be defined.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>  Angular tilt of a circle represented by rotations about x, y, and z axes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>circle</code> (              <code>list</code> )          \u2013            <p>Single variable packed form.</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def define_circle(center, radius, angles):\n    \"\"\"\n    Definition to describe a circle in a single variable packed form.\n\n    Parameters\n    ----------\n    center  : torch.Tensor\n              Center of a circle to be defined in 3D space.\n    radius  : float\n              Radius of a circle to be defined.\n    angles  : torch.Tensor\n              Angular tilt of a circle represented by rotations about x, y, and z axes.\n\n    Returns\n    ----------\n    circle  : list\n              Single variable packed form.\n    \"\"\"\n    points = define_plane(center, angles=angles)\n    circle = [\n        points,\n        center,\n        torch.tensor([radius])\n    ]\n    return circle\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.define_plane","title":"<code>define_plane(point, angles=torch.tensor([0.0, 0.0, 0.0]))</code>","text":"<p>Definition to generate a rotation matrix along X axis.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>       A point that is at the center of a plane.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>plane</code> (              <code>tensor</code> )          \u2013            <p>Points defining plane.</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def define_plane(point, angles = torch.tensor([0., 0., 0.])):\n    \"\"\" \n    Definition to generate a rotation matrix along X axis.\n\n    Parameters\n    ----------\n    point        : torch.tensor\n                   A point that is at the center of a plane.\n    angles       : torch.tensor\n                   Rotation angles in degrees.\n\n    Returns\n    ----------\n    plane        : torch.tensor\n                   Points defining plane.\n    \"\"\"\n    plane = torch.tensor([\n                          [10., 10., 0.],\n                          [0., 10., 0.],\n                          [0.,  0., 0.]\n                         ], device = point.device)\n    for i in range(0, plane.shape[0]):\n        plane[i], _, _, _ = rotate_points(plane[i], angles = angles.to(point.device))\n        plane[i] = plane[i] + point\n    return plane\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.define_plane_mesh","title":"<code>define_plane_mesh(number_of_meshes=[10, 10], size=[1.0, 1.0], angles=torch.tensor([0.0, 0.0, 0.0]), offset=torch.tensor([[0.0, 0.0, 0.0]]))</code>","text":"<p>Definition to generate a plane with meshes.</p> <p>Parameters:</p> <ul> <li> <code>number_of_meshes</code>           \u2013            <pre><code>            Number of squares over plane.\n            There are two triangles at each square.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>            Size of the plane.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>            Rotation angles in degrees.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>            Offset along XYZ axes.\n            Expected dimension is [1 x 3] or offset for each triangle [m x 3].\n            m here refers to `2 * number_of_meshes[0]` times  `number_of_meshes[1]`.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>triangles</code> (              <code>tensor</code> )          \u2013            <p>Triangles [m x 3 x 3], where m is <code>2 * number_of_meshes[0]</code> times  <code>number_of_meshes[1]</code>.</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def define_plane_mesh(\n                      number_of_meshes = [10, 10], \n                      size = [1., 1.], \n                      angles = torch.tensor([0., 0., 0.]), \n                      offset = torch.tensor([[0., 0., 0.]])\n                     ):\n    \"\"\"\n    Definition to generate a plane with meshes.\n\n\n    Parameters\n    -----------\n    number_of_meshes  : torch.tensor\n                        Number of squares over plane.\n                        There are two triangles at each square.\n    size              : list\n                        Size of the plane.\n    angles            : torch.tensor\n                        Rotation angles in degrees.\n    offset            : torch.tensor\n                        Offset along XYZ axes.\n                        Expected dimension is [1 x 3] or offset for each triangle [m x 3].\n                        m here refers to `2 * number_of_meshes[0]` times  `number_of_meshes[1]`. \n\n    Returns\n    -------\n    triangles         : torch.tensor\n                        Triangles [m x 3 x 3], where m is `2 * number_of_meshes[0]` times  `number_of_meshes[1]`.\n    \"\"\"\n    triangles = torch.zeros(2, number_of_meshes[0], number_of_meshes[1], 3, 3)\n    step = [size[0] / number_of_meshes[0], size[1] / number_of_meshes[1]]\n    for i in range(0, number_of_meshes[0] - 1):\n        for j in range(0, number_of_meshes[1] - 1):\n            first_triangle = torch.tensor([\n                                           [       -size[0] / 2. + step[0] * i,       -size[1] / 2. + step[0] * j, 0.],\n                                           [ -size[0] / 2. + step[0] * (i + 1),       -size[1] / 2. + step[0] * j, 0.],\n                                           [       -size[0] / 2. + step[0] * i, -size[1] / 2. + step[0] * (j + 1), 0.]\n                                          ])\n            second_triangle = torch.tensor([\n                                            [ -size[0] / 2. + step[0] * (i + 1), -size[1] / 2. + step[0] * (j + 1), 0.],\n                                            [ -size[0] / 2. + step[0] * (i + 1),       -size[1] / 2. + step[0] * j, 0.],\n                                            [       -size[0] / 2. + step[0] * i, -size[1] / 2. + step[0] * (j + 1), 0.]\n                                           ])\n            triangles[0, i, j], _, _, _ = rotate_points(first_triangle, angles = angles)\n            triangles[1, i, j], _, _, _ = rotate_points(second_triangle, angles = angles)\n    triangles = triangles.view(-1, 3, 3) + offset\n    return triangles\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.define_sphere","title":"<code>define_sphere(center=torch.tensor([[0.0, 0.0, 0.0]]), radius=torch.tensor([1.0]))</code>","text":"<p>Definition to define a sphere.</p> <p>Parameters:</p> <ul> <li> <code>center</code>           \u2013            <pre><code>      Center of the sphere(s) along XYZ axes.\n      Expected size is [3], [1, 3] or [m, 3].\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of that sphere(s).\n      Expected size is [1], [1, 1], [m] or [m, 1].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>parameters</code> (              <code>tensor</code> )          \u2013            <p>Parameters of defined sphere(s). Expected size is [1, 3] or [m x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def define_sphere(center = torch.tensor([[0., 0., 0.]]), radius = torch.tensor([1.])):\n    \"\"\"\n    Definition to define a sphere.\n\n    Parameters\n    ----------\n    center      : torch.tensor\n                  Center of the sphere(s) along XYZ axes.\n                  Expected size is [3], [1, 3] or [m, 3].\n    radius      : torch.tensor\n                  Radius of that sphere(s).\n                  Expected size is [1], [1, 1], [m] or [m, 1].\n\n    Returns\n    -------\n    parameters  : torch.tensor\n                  Parameters of defined sphere(s).\n                  Expected size is [1, 3] or [m x 3].\n    \"\"\"\n    if len(radius.shape) == 1:\n        radius = radius.unsqueeze(0)\n    if len(center.shape) == 1:\n        center = center.unsqueeze(1)\n    parameters = torch.cat((center, radius), dim = 1)\n    return parameters\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.distance_between_two_points","title":"<code>distance_between_two_points(point1, point2)</code>","text":"<p>Definition to calculate distance between two given points.</p> <p>Parameters:</p> <ul> <li> <code>point1</code>           \u2013            <pre><code>      First point in X,Y,Z.\n</code></pre> </li> <li> <code>point2</code>           \u2013            <pre><code>      Second point in X,Y,Z.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>Tensor</code> )          \u2013            <p>Distance in between given two points.</p> </li> </ul> Source code in <code>odak/learn/tools/vector.py</code> <pre><code>def distance_between_two_points(point1, point2):\n    \"\"\"\n    Definition to calculate distance between two given points.\n\n    Parameters\n    ----------\n    point1      : torch.Tensor\n                  First point in X,Y,Z.\n    point2      : torch.Tensor\n                  Second point in X,Y,Z.\n\n    Returns\n    ----------\n    distance    : torch.Tensor\n                  Distance in between given two points.\n    \"\"\"\n    point1 = torch.tensor(point1) if not isinstance(point1, torch.Tensor) else point1\n    point2 = torch.tensor(point2) if not isinstance(point2, torch.Tensor) else point2\n\n    if len(point1.shape) == 1 and len(point2.shape) == 1:\n        distance = torch.sqrt(torch.sum((point1 - point2) ** 2))\n    elif len(point1.shape) == 2 or len(point2.shape) == 2:\n        distance = torch.sqrt(torch.sum((point1 - point2) ** 2, dim=-1))\n\n    return distance\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.get_sphere_normal_torch","title":"<code>get_sphere_normal_torch(point, sphere)</code>","text":"<p>Definition to get a normal of a point on a given sphere.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>        Point on sphere in X,Y,Z.\n</code></pre> </li> <li> <code>sphere</code>           \u2013            <pre><code>        Center defined in X,Y,Z and radius.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal_vector</code> (              <code>tensor</code> )          \u2013            <p>Normal vector.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def get_sphere_normal_torch(point, sphere):\n    \"\"\"\n    Definition to get a normal of a point on a given sphere.\n\n    Parameters\n    ----------\n    point         : torch.tensor\n                    Point on sphere in X,Y,Z.\n    sphere        : torch.tensor\n                    Center defined in X,Y,Z and radius.\n\n    Returns\n    ----------\n    normal_vector : torch.tensor\n                    Normal vector.\n    \"\"\"\n    if len(point.shape) == 1:\n        point = point.reshape((1, 3))\n    normal_vector = create_ray_from_two_points(point, sphere[0:3])\n    return normal_vector\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.get_triangle_normal","title":"<code>get_triangle_normal(triangle, triangle_center=None)</code>","text":"<p>Definition to calculate surface normal of a triangle.</p> <p>Parameters:</p> <ul> <li> <code>triangle</code>           \u2013            <pre><code>          Set of points in X,Y and Z to define a planar surface (3,3). It can also be list of triangles (mx3x3).\n</code></pre> </li> <li> <code>triangle_center</code>               (<code>tensor</code>, default:                   <code>None</code> )           \u2013            <pre><code>          Center point of the given triangle. See odak.learn.raytracing.center_of_triangle for more. In many scenarios you can accelerate things by precomputing triangle centers.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def get_triangle_normal(triangle, triangle_center=None):\n    \"\"\"\n    Definition to calculate surface normal of a triangle.\n\n    Parameters\n    ----------\n    triangle        : torch.tensor\n                      Set of points in X,Y and Z to define a planar surface (3,3). It can also be list of triangles (mx3x3).\n    triangle_center : torch.tensor\n                      Center point of the given triangle. See odak.learn.raytracing.center_of_triangle for more. In many scenarios you can accelerate things by precomputing triangle centers.\n\n    Returns\n    ----------\n    normal          : torch.tensor\n                      Surface normal at the point of intersection.\n    \"\"\"\n    if len(triangle.shape) == 2:\n        triangle = triangle.view((1, 3, 3))\n    normal = torch.zeros((triangle.shape[0], 2, 3)).to(triangle.device)\n    direction = torch.linalg.cross(\n                                   triangle[:, 0] - triangle[:, 1], \n                                   triangle[:, 2] - triangle[:, 1]\n                                  )\n    if type(triangle_center) == type(None):\n        normal[:, 0] = center_of_triangle(triangle)\n    else:\n        normal[:, 0] = triangle_center\n    normal[:, 1] = direction / torch.sum(direction, axis=1)[0]\n    if normal.shape[0] == 1:\n        normal = normal.view((2, 3))\n    return normal\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.grid_sample","title":"<code>grid_sample(no=[10, 10], size=[100.0, 100.0], center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate samples over a surface.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Physical size of the surface.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>tensor</code> )          \u2013            <p>Samples generated.</p> </li> <li> <code>rotx</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at X axis.</p> </li> <li> <code>roty</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at Y axis.</p> </li> <li> <code>rotz</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at Z axis.</p> </li> </ul> Source code in <code>odak/learn/tools/sample.py</code> <pre><code>def grid_sample(\n                no = [10, 10],\n                size = [100., 100.], \n                center = [0., 0., 0.], \n                angles = [0., 0., 0.]):\n    \"\"\"\n    Definition to generate samples over a surface.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    size        : list\n                  Physical size of the surface.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    -------\n    samples     : torch.tensor\n                  Samples generated.\n    rotx        : torch.tensor\n                  Rotation matrix at X axis.\n    roty        : torch.tensor\n                  Rotation matrix at Y axis.\n    rotz        : torch.tensor\n                  Rotation matrix at Z axis.\n    \"\"\"\n    center = torch.tensor(center)\n    angles = torch.tensor(angles)\n    size = torch.tensor(size)\n    samples = torch.zeros((no[0], no[1], 3))\n    x = torch.linspace(-size[0] / 2., size[0] / 2., no[0])\n    y = torch.linspace(-size[1] / 2., size[1] / 2., no[1])\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    samples[:, :, 0] = X.detach().clone()\n    samples[:, :, 1] = Y.detach().clone()\n    samples = samples.reshape((samples.shape[0] * samples.shape[1], samples.shape[2]))\n    samples, rotx, roty, rotz = rotate_points(samples, angles = angles, offset = center)\n    return samples, rotx, roty, rotz\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.intersect_w_circle","title":"<code>intersect_w_circle(ray, circle)</code>","text":"<p>Definition to find intersection point of a ray with a circle.  Returns distance as zero if there isn't an intersection.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>circle</code>           \u2013            <pre><code>       A list that contains (0) Set of points in X,Y and Z to define plane of a circle, (1) circle center, and (2) circle radius.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>Tensor</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>Tensor</code> )          \u2013            <p>Distance in between a starting point of a ray and the intersection point with a given triangle.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_circle(ray, circle):\n    \"\"\"\n    Definition to find intersection point of a ray with a circle. \n    Returns distance as zero if there isn't an intersection.\n\n    Parameters\n    ----------\n    ray          : torch.Tensor\n                   A vector/ray.\n    circle       : list\n                   A list that contains (0) Set of points in X,Y and Z to define plane of a circle, (1) circle center, and (2) circle radius.\n\n    Returns\n    ----------\n    normal       : torch.Tensor\n                   Surface normal at the point of intersection.\n    distance     : torch.Tensor\n                   Distance in between a starting point of a ray and the intersection point with a given triangle.\n    \"\"\"\n    normal, distance = intersect_w_surface(ray, circle[0])\n\n    if len(normal.shape) == 2:\n        normal = normal.unsqueeze(0)\n\n    distance_to_center = distance_between_two_points(normal[:, 0], circle[1])\n    mask = distance_to_center &gt; circle[2]\n    distance[mask] = 0\n\n    if len(ray.shape) == 2:\n        normal = normal.squeeze(0)\n\n    return normal, distance\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.intersect_w_sphere","title":"<code>intersect_w_sphere(ray, sphere, learning_rate=0.2, number_of_steps=5000, error_threshold=0.01)</code>","text":"<p>Definition to find the intersection between ray(s) and sphere(s).</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>              Input ray(s).\n              Expected size is [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> <li> <code>sphere</code>           \u2013            <pre><code>              Input sphere.\n              Expected size is [1 x 4].\n</code></pre> </li> <li> <code>learning_rate</code>           \u2013            <pre><code>              Learning rate used in the optimizer for finding the propagation distances of the rays.\n</code></pre> </li> <li> <code>number_of_steps</code>           \u2013            <pre><code>              Number of steps used in the optimizer.\n</code></pre> </li> <li> <code>error_threshold</code>           \u2013            <pre><code>              The error threshold that will help deciding intersection or no intersection.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>intersecting_ray</code> (              <code>tensor</code> )          \u2013            <p>Ray(s) that intersecting with the given sphere. Expected size is [n x 2 x 3], where n could be any real number.</p> </li> <li> <code>intersecting_normal</code> (              <code>tensor</code> )          \u2013            <p>Normal(s) for the ray(s) intersecting with the given sphere Expected size is [n x 2 x 3], where n could be any real number.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_sphere(ray, sphere, learning_rate = 2e-1, number_of_steps = 5000, error_threshold = 1e-2):\n    \"\"\"\n    Definition to find the intersection between ray(s) and sphere(s).\n\n    Parameters\n    ----------\n    ray                 : torch.tensor\n                          Input ray(s).\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3].\n    sphere              : torch.tensor\n                          Input sphere.\n                          Expected size is [1 x 4].\n    learning_rate       : float\n                          Learning rate used in the optimizer for finding the propagation distances of the rays.\n    number_of_steps     : int\n                          Number of steps used in the optimizer.\n    error_threshold     : float\n                          The error threshold that will help deciding intersection or no intersection.\n\n    Returns\n    -------\n    intersecting_ray    : torch.tensor\n                          Ray(s) that intersecting with the given sphere.\n                          Expected size is [n x 2 x 3], where n could be any real number.\n    intersecting_normal : torch.tensor\n                          Normal(s) for the ray(s) intersecting with the given sphere\n                          Expected size is [n x 2 x 3], where n could be any real number.\n\n    \"\"\"\n    if len(ray.shape) == 2:\n        ray = ray.unsqueeze(0)\n    if len(sphere.shape) == 1:\n        sphere = sphere.unsqueeze(0)\n    distance = torch.zeros(ray.shape[0], device = ray.device, requires_grad = True)\n    loss_l2 = torch.nn.MSELoss(reduction = 'sum')\n    optimizer = torch.optim.AdamW([distance], lr = learning_rate)    \n    t = tqdm(range(number_of_steps), leave = False, dynamic_ncols = True)\n    for step in t:\n        optimizer.zero_grad()\n        propagated_ray = propagate_ray(ray, distance)\n        test = torch.abs((propagated_ray[:, 0, 0] - sphere[:, 0]) ** 2 + (propagated_ray[:, 0, 1] - sphere[:, 1]) ** 2 + (propagated_ray[:, 0, 2] - sphere[:, 2]) ** 2 - sphere[:, 3] ** 2)\n        loss = loss_l2(\n                       test,\n                       torch.zeros_like(test)\n                      )\n        loss.backward(retain_graph = True)\n        optimizer.step()\n        t.set_description('Sphere intersection loss: {}'.format(loss.item()))\n    check = test &lt; error_threshold\n    intersecting_ray = propagate_ray(ray[check == True], distance[check == True])\n    intersecting_normal = create_ray_from_two_points(\n                                                     sphere[:, 0:3],\n                                                     intersecting_ray[:, 0]\n                                                    )\n    return intersecting_ray, intersecting_normal, distance, check\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.intersect_w_surface","title":"<code>intersect_w_surface(ray, points)</code>","text":"<p>Definition to find intersection point inbetween a surface and a ray. For more see: http://geomalgorithms.com/a06-_intersect-2.html</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>points</code>           \u2013            <pre><code>       Set of points in X,Y and Z to define a planar surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between starting point of a ray with it's intersection with a planar surface.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_surface(ray, points):\n    \"\"\"\n    Definition to find intersection point inbetween a surface and a ray. For more see: http://geomalgorithms.com/a06-_intersect-2.html\n\n    Parameters\n    ----------\n    ray          : torch.tensor\n                   A vector/ray.\n    points       : torch.tensor\n                   Set of points in X,Y and Z to define a planar surface.\n\n    Returns\n    ----------\n    normal       : torch.tensor\n                   Surface normal at the point of intersection.\n    distance     : float\n                   Distance in between starting point of a ray with it's intersection with a planar surface.\n    \"\"\"\n    normal = get_triangle_normal(points)\n    if len(ray.shape) == 2:\n        ray = ray.unsqueeze(0)\n    if len(points.shape) == 2:\n        points = points.unsqueeze(0)\n    if len(normal.shape) == 2:\n        normal = normal.unsqueeze(0)\n    f = normal[:, 0] - ray[:, 0]\n    distance = (torch.mm(normal[:, 1], f.T) / torch.mm(normal[:, 1], ray[:, 1].T)).T\n    new_normal = torch.zeros_like(ray)\n    new_normal[:, 0] = ray[:, 0] + distance * ray[:, 1]\n    new_normal[:, 1] = normal[:, 1]\n    new_normal = torch.nan_to_num(\n                                  new_normal,\n                                  nan = float('nan'),\n                                  posinf = float('nan'),\n                                  neginf = float('nan')\n                                 )\n    distance = torch.nan_to_num(\n                                distance,\n                                nan = float('nan'),\n                                posinf = float('nan'),\n                                neginf = float('nan')\n                               )\n    return new_normal, distance\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.intersect_w_surface_batch","title":"<code>intersect_w_surface_batch(ray, triangle)</code>","text":"<p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray (2 x 3). It can also be a list of rays (n x 2 x 3).\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>       Set of points in X,Y and Z to define a planar surface. It can also be a list of triangles (m x 3 x 3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection (m x n x 2 x 3).</p> </li> <li> <code>distance</code> (              <code>tensor</code> )          \u2013            <p>Distance in between starting point of a ray with it's intersection with a planar surface (m x n).</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_surface_batch(ray, triangle):\n    \"\"\"\n    Parameters\n    ----------\n    ray          : torch.tensor\n                   A vector/ray (2 x 3). It can also be a list of rays (n x 2 x 3).\n    triangle     : torch.tensor\n                   Set of points in X,Y and Z to define a planar surface. It can also be a list of triangles (m x 3 x 3).\n\n    Returns\n    ----------\n    normal       : torch.tensor\n                   Surface normal at the point of intersection (m x n x 2 x 3).\n    distance     : torch.tensor\n                   Distance in between starting point of a ray with it's intersection with a planar surface (m x n).\n    \"\"\"\n    normal = get_triangle_normal(triangle)\n    if len(ray.shape) == 2:\n        ray = ray.unsqueeze(0)\n    if len(triangle.shape) == 2:\n        triangle = triangle.unsqueeze(0)\n    if len(normal.shape) == 2:\n        normal = normal.unsqueeze(0)\n\n    f = normal[:, None, 0] - ray[None, :, 0]\n    distance = (torch.bmm(normal[:, None, 1], f.permute(0, 2, 1)).squeeze(1) / torch.mm(normal[:, 1], ray[:, 1].T)).T\n\n    new_normal = torch.zeros((triangle.shape[0], )+ray.shape)\n    new_normal[:, :, 0] = ray[None, :, 0] + (distance[:, :, None] * ray[:, None, 1]).permute(1, 0, 2)\n    new_normal[:, :, 1] = normal[:, None, 1]\n    new_normal = torch.nan_to_num(\n                                  new_normal,\n                                  nan = float('nan'),\n                                  posinf = float('nan'),\n                                  neginf = float('nan')\n                                 )\n    distance = torch.nan_to_num(\n                                distance,\n                                nan = float('nan'),\n                                posinf = float('nan'),\n                                neginf = float('nan')\n                               )\n    return new_normal, distance.T\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.intersect_w_triangle","title":"<code>intersect_w_triangle(ray, triangle)</code>","text":"<p>Definition to find intersection point of a ray with a triangle. </p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>              A ray [1 x 2 x 3] or a batch of ray [m x 2 x 3].\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>              Set of points in X,Y and Z to define a single triangle [1 x 3 x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection with the surface of triangle. This could also involve surface normals that are not on the triangle. Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between a starting point of a ray and the intersection point with a given triangle. Expected size is [1 x 1] or [m x 1] depending on the input.</p> </li> <li> <code>intersecting_ray</code> (              <code>tensor</code> )          \u2013            <p>Rays that intersect with the triangle plane and on the triangle. Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.</p> </li> <li> <code>intersecting_normal</code> (              <code>tensor</code> )          \u2013            <p>Normals that intersect with the triangle plane and on the triangle. Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.</p> </li> <li> <code>check</code> (              <code>tensor</code> )          \u2013            <p>A list that provides a bool as True or False for each ray used as input. A test to see is a ray could be on the given triangle. Expected size is [1] or [m].</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_triangle(ray, triangle):\n    \"\"\"\n    Definition to find intersection point of a ray with a triangle. \n\n    Parameters\n    ----------\n    ray                 : torch.tensor\n                          A ray [1 x 2 x 3] or a batch of ray [m x 2 x 3].\n    triangle            : torch.tensor\n                          Set of points in X,Y and Z to define a single triangle [1 x 3 x 3].\n\n    Returns\n    ----------\n    normal              : torch.tensor\n                          Surface normal at the point of intersection with the surface of triangle.\n                          This could also involve surface normals that are not on the triangle.\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.\n    distance            : float\n                          Distance in between a starting point of a ray and the intersection point with a given triangle.\n                          Expected size is [1 x 1] or [m x 1] depending on the input.\n    intersecting_ray    : torch.tensor\n                          Rays that intersect with the triangle plane and on the triangle.\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.\n    intersecting_normal : torch.tensor\n                          Normals that intersect with the triangle plane and on the triangle.\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.\n    check               : torch.tensor\n                          A list that provides a bool as True or False for each ray used as input.\n                          A test to see is a ray could be on the given triangle.\n                          Expected size is [1] or [m].\n    \"\"\"\n    if len(triangle.shape) == 2:\n       triangle = triangle.unsqueeze(0)\n    if len(ray.shape) == 2:\n       ray = ray.unsqueeze(0)\n    normal, distance = intersect_w_surface(ray, triangle)\n    check = is_it_on_triangle(normal[:, 0], triangle)\n    intersecting_ray = ray.unsqueeze(0)\n    intersecting_ray = intersecting_ray.repeat(triangle.shape[0], 1, 1, 1)\n    intersecting_ray = intersecting_ray[check == True]\n    intersecting_normal = normal.unsqueeze(0)\n    intersecting_normal = intersecting_normal.repeat(triangle.shape[0], 1, 1, 1)\n    intersecting_normal = intersecting_normal[check ==  True]\n    return normal, distance, intersecting_ray, intersecting_normal, check\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.intersect_w_triangle_batch","title":"<code>intersect_w_triangle_batch(ray, triangle)</code>","text":"<p>Definition to find intersection points of rays with triangles. Returns False for each variable if the rays doesn't intersect with given triangles.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       vectors/rays (n x 2 x 3).\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>       Set of points in X,Y and Z to define triangles (m x 3 x 3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection (m x n x 2 x 3).</p> </li> <li> <code>distance</code> (              <code>List</code> )          \u2013            <p>Distance in between starting point of a ray with it's intersection with a planar surface (m x n).</p> </li> <li> <code>intersect_ray</code> (              <code>List</code> )          \u2013            <p>List of intersecting rays (k x 2 x 3) where k &lt;= n.</p> </li> <li> <code>intersect_normal</code> (              <code>List</code> )          \u2013            <p>List of intersecting normals (k x 2 x 3) where k &lt;= n*m.</p> </li> <li> <code>check</code> (              <code>tensor</code> )          \u2013            <p>Boolean tensor (m x n) indicating whether each ray intersects with a triangle or not.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_triangle_batch(ray, triangle):\n    \"\"\"\n    Definition to find intersection points of rays with triangles. Returns False for each variable if the rays doesn't intersect with given triangles.\n\n    Parameters\n    ----------\n    ray          : torch.tensor\n                   vectors/rays (n x 2 x 3).\n    triangle     : torch.tensor\n                   Set of points in X,Y and Z to define triangles (m x 3 x 3).\n\n    Returns\n    ----------\n    normal          : torch.tensor\n                      Surface normal at the point of intersection (m x n x 2 x 3).\n    distance        : List\n                      Distance in between starting point of a ray with it's intersection with a planar surface (m x n).\n    intersect_ray   : List\n                      List of intersecting rays (k x 2 x 3) where k &lt;= n.\n    intersect_normal: List\n                      List of intersecting normals (k x 2 x 3) where k &lt;= n*m.\n    check           : torch.tensor\n                      Boolean tensor (m x n) indicating whether each ray intersects with a triangle or not.\n    \"\"\"\n    if len(triangle.shape) == 2:\n       triangle = triangle.unsqueeze(0)\n    if len(ray.shape) == 2:\n       ray = ray.unsqueeze(0)\n\n    normal, distance = intersect_w_surface_batch(ray, triangle)\n\n    check = is_it_on_triangle_batch(normal[:, :, 0], triangle)\n\n    flat_check = check.flatten()\n    flat_normal = normal.view(-1, normal.size(-2), normal.size(-1))\n    flat_ray = ray.repeat(normal.size(0), 1, 1)\n    flat_distance = distance.flatten()\n\n    filtered_normal = torch.masked_select(flat_normal, flat_check.unsqueeze(-1).unsqueeze(-1).repeat(1, 2, 3))\n    filtered_ray = torch.masked_select(flat_ray, flat_check.unsqueeze(-1).unsqueeze(-1).repeat(1, 2, 3))\n    filtered_distnace = torch.masked_select(flat_distance, flat_check)\n\n    check_count = check.sum(dim=1).tolist()\n    split_size_ray_and_normal = [count * 2 * 3 for count in check_count]\n    split_size_distance = [count for count in check_count]\n\n    normal_grouped = torch.split(filtered_normal, split_size_ray_and_normal)\n    ray_grouped = torch.split(filtered_ray, split_size_ray_and_normal)\n    distance_grouped = torch.split(filtered_distnace, split_size_distance)\n\n    intersecting_normal = [g.view(-1, 2, 3) for g in normal_grouped if g.numel() &gt; 0]\n    intersecting_ray = [g.view(-1, 2, 3) for g in ray_grouped if g.numel() &gt; 0]\n    new_distance = [g for g in distance_grouped if g.numel() &gt; 0]\n\n    return normal, new_distance, intersecting_ray, intersecting_normal, check\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.is_it_on_triangle","title":"<code>is_it_on_triangle(point_to_check, triangle)</code>","text":"<p>Definition to check if a given point is inside a triangle.  If the given point is inside a defined triangle, this definition returns True. For more details, visit: https://blackpawn.com/texts/pointinpoly/.</p> <p>Parameters:</p> <ul> <li> <code>point_to_check</code>           \u2013            <pre><code>          Point(s) to check.\n          Expected size is [3], [1 x 3] or [m x 3].\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>          Triangle described with three points.\n          Expected size is [3 x 3], [1 x 3 x 3] or [m x 3 x3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Is it on a triangle? Returns NaN if condition not satisfied. Expected size is [1] or [m] depending on the input.</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def is_it_on_triangle(point_to_check, triangle):\n    \"\"\"\n    Definition to check if a given point is inside a triangle. \n    If the given point is inside a defined triangle, this definition returns True.\n    For more details, visit: [https://blackpawn.com/texts/pointinpoly/](https://blackpawn.com/texts/pointinpoly/).\n\n    Parameters\n    ----------\n    point_to_check  : torch.tensor\n                      Point(s) to check.\n                      Expected size is [3], [1 x 3] or [m x 3].\n    triangle        : torch.tensor\n                      Triangle described with three points.\n                      Expected size is [3 x 3], [1 x 3 x 3] or [m x 3 x3].\n\n    Returns\n    -------\n    result          : torch.tensor\n                      Is it on a triangle? Returns NaN if condition not satisfied.\n                      Expected size is [1] or [m] depending on the input.\n    \"\"\"\n    if len(point_to_check.shape) == 1:\n        point_to_check = point_to_check.unsqueeze(0)\n    if len(triangle.shape) == 2:\n        triangle = triangle.unsqueeze(0)\n    v0 = triangle[:, 2] - triangle[:, 0]\n    v1 = triangle[:, 1] - triangle[:, 0]\n    v2 = point_to_check - triangle[:, 0]\n    if len(v0.shape) == 1:\n        v0 = v0.unsqueeze(0)\n    if len(v1.shape) == 1:\n        v1 = v1.unsqueeze(0)\n    if len(v2.shape) == 1:\n        v2 = v2.unsqueeze(0)\n    dot00 = torch.mm(v0, v0.T)\n    dot01 = torch.mm(v0, v1.T)\n    dot02 = torch.mm(v0, v2.T) \n    dot11 = torch.mm(v1, v1.T)\n    dot12 = torch.mm(v1, v2.T)\n    invDenom = 1. / (dot00 * dot11 - dot01 * dot01)\n    u = (dot11 * dot02 - dot01 * dot12) * invDenom\n    v = (dot00 * dot12 - dot01 * dot02) * invDenom\n    result = (u &gt;= 0.) &amp; (v &gt;= 0.) &amp; ((u + v) &lt; 1)\n    return result\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.is_it_on_triangle_batch","title":"<code>is_it_on_triangle_batch(point_to_check, triangle)</code>","text":"<p>Definition to check if given points are inside triangles. If the given points are inside defined triangles, this definition returns True.</p> <p>Parameters:</p> <ul> <li> <code>point_to_check</code>           \u2013            <pre><code>          Points to check (m x n x 3).\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>          Triangles (m x 3 x 3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>torch.tensor (m x n)</code> )          \u2013            </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def is_it_on_triangle_batch(point_to_check, triangle):\n    \"\"\"\n    Definition to check if given points are inside triangles. If the given points are inside defined triangles, this definition returns True.\n\n    Parameters\n    ----------\n    point_to_check  : torch.tensor\n                      Points to check (m x n x 3).\n    triangle        : torch.tensor \n                      Triangles (m x 3 x 3).\n\n    Returns\n    ----------\n    result          : torch.tensor (m x n)\n\n    \"\"\"\n    if len(point_to_check.shape) == 1:\n        point_to_check = point_to_check.unsqueeze(0)\n    if len(triangle.shape) == 2:\n        triangle = triangle.unsqueeze(0)\n    v0 = triangle[:, 2] - triangle[:, 0]\n    v1 = triangle[:, 1] - triangle[:, 0]\n    v2 = point_to_check - triangle[:, None, 0]\n    if len(v0.shape) == 1:\n        v0 = v0.unsqueeze(0)\n    if len(v1.shape) == 1:\n        v1 = v1.unsqueeze(0)\n    if len(v2.shape) == 1:\n        v2 = v2.unsqueeze(0)\n\n    dot00 = torch.bmm(v0.unsqueeze(1), v0.unsqueeze(1).permute(0, 2, 1)).squeeze(1)\n    dot01 = torch.bmm(v0.unsqueeze(1), v1.unsqueeze(1).permute(0, 2, 1)).squeeze(1)\n    dot02 = torch.bmm(v0.unsqueeze(1), v2.permute(0, 2, 1)).squeeze(1)\n    dot11 = torch.bmm(v1.unsqueeze(1), v1.unsqueeze(1).permute(0, 2, 1)).squeeze(1)\n    dot12 = torch.bmm(v1.unsqueeze(1), v2.permute(0, 2, 1)).squeeze(1)\n    invDenom = 1. / (dot00 * dot11 - dot01 * dot01)\n    u = (dot11 * dot02 - dot01 * dot12) * invDenom\n    v = (dot00 * dot12 - dot01 * dot02) * invDenom\n    result = (u &gt;= 0.) &amp; (v &gt;= 0.) &amp; ((u + v) &lt; 1)\n\n    return result\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.propagate_ray","title":"<code>propagate_ray(ray, distance)</code>","text":"<p>Definition to propagate a ray at a certain given distance.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>     A ray with a size of [2 x 3], [1 x 2 x 3] or a batch of rays with [m x 2 x 3].\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>     Distance with a size of [1], [1, m] or distances with a size of [m], [1, m].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_ray</code> (              <code>tensor</code> )          \u2013            <p>Propagated ray with a size of [1 x 2 x 3] or batch of rays with [m x 2 x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def propagate_ray(ray, distance):\n    \"\"\"\n    Definition to propagate a ray at a certain given distance.\n\n    Parameters\n    ----------\n    ray        : torch.tensor\n                 A ray with a size of [2 x 3], [1 x 2 x 3] or a batch of rays with [m x 2 x 3].\n    distance   : torch.tensor\n                 Distance with a size of [1], [1, m] or distances with a size of [m], [1, m].\n\n    Returns\n    ----------\n    new_ray    : torch.tensor\n                 Propagated ray with a size of [1 x 2 x 3] or batch of rays with [m x 2 x 3].\n    \"\"\"\n    if len(ray.shape) == 2:\n        ray = ray.unsqueeze(0)\n    if len(distance.shape) == 2:\n        distance = distance.squeeze(-1)\n    new_ray = torch.zeros_like(ray)\n    new_ray[:, 0, 0] = distance * ray[:, 1, 0] + ray[:, 0, 0]\n    new_ray[:, 0, 1] = distance * ray[:, 1, 1] + ray[:, 0, 1]\n    new_ray[:, 0, 2] = distance * ray[:, 1, 2] + ray[:, 0, 2]\n    return new_ray\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.reflect","title":"<code>reflect(input_ray, normal)</code>","text":"<p>Definition to reflect an incoming ray from a surface defined by a surface normal.  Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.</p> <p>Parameters:</p> <ul> <li> <code>input_ray</code>           \u2013            <pre><code>       A ray or rays.\n       Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> <li> <code>normal</code>           \u2013            <pre><code>       A surface normal(s).\n       Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output_ray</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a reflected ray. Expected size is [1 x 2 x 3] or [m x 2 x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def reflect(input_ray, normal):\n    \"\"\" \n    Definition to reflect an incoming ray from a surface defined by a surface normal. \n    Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.\n\n\n    Parameters\n    ----------\n    input_ray    : torch.tensor\n                   A ray or rays.\n                   Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n    normal       : torch.tensor\n                   A surface normal(s).\n                   Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n\n    Returns\n    ----------\n    output_ray   : torch.tensor\n                   Array that contains starting points and cosines of a reflected ray.\n                   Expected size is [1 x 2 x 3] or [m x 2 x 3].\n    \"\"\"\n    if len(input_ray.shape) == 2:\n        input_ray = input_ray.unsqueeze(0)\n    if len(normal.shape) == 2:\n        normal = normal.unsqueeze(0)\n    mu = 1\n    div = normal[:, 1, 0]**2 + normal[:, 1, 1]**2 + normal[:, 1, 2]**2 + 1e-8\n    a = mu * (input_ray[:, 1, 0] * normal[:, 1, 0] + input_ray[:, 1, 1] * normal[:, 1, 1] + input_ray[:, 1, 2] * normal[:, 1, 2]) / div\n    a = a.unsqueeze(1)\n    n = int(torch.amax(torch.tensor([normal.shape[0], input_ray.shape[0]])))\n    output_ray = torch.zeros((n, 2, 3)).to(input_ray.device)\n    output_ray[:, 0] = normal[:, 0]\n    output_ray[:, 1] = input_ray[:, 1] - 2 * a * normal[:, 1]\n    return output_ray\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.refract","title":"<code>refract(vector, normvector, n1, n2, error=0.01)</code>","text":"<p>Definition to refract an incoming ray. Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.</p> <p>Parameters:</p> <ul> <li> <code>vector</code>           \u2013            <pre><code>         Incoming ray.\n         Expected size is [2, 3], [1, 2, 3] or [m, 2, 3].\n</code></pre> </li> <li> <code>normvector</code>           \u2013            <pre><code>         Normal vector.\n         Expected size is [2, 3], [1, 2, 3] or [m, 2, 3]].\n</code></pre> </li> <li> <code>n1</code>           \u2013            <pre><code>         Refractive index of the incoming medium.\n</code></pre> </li> <li> <code>n2</code>           \u2013            <pre><code>         Refractive index of the outgoing medium.\n</code></pre> </li> <li> <code>error</code>           \u2013            <pre><code>         Desired error.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Refracted ray. Expected size is [1, 2, 3]</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def refract(vector, normvector, n1, n2, error = 0.01):\n    \"\"\"\n    Definition to refract an incoming ray.\n    Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.\n\n\n    Parameters\n    ----------\n    vector         : torch.tensor\n                     Incoming ray.\n                     Expected size is [2, 3], [1, 2, 3] or [m, 2, 3].\n    normvector     : torch.tensor\n                     Normal vector.\n                     Expected size is [2, 3], [1, 2, 3] or [m, 2, 3]].\n    n1             : float\n                     Refractive index of the incoming medium.\n    n2             : float\n                     Refractive index of the outgoing medium.\n    error          : float \n                     Desired error.\n\n    Returns\n    -------\n    output         : torch.tensor\n                     Refracted ray.\n                     Expected size is [1, 2, 3]\n    \"\"\"\n    if len(vector.shape) == 2:\n        vector = vector.unsqueeze(0)\n    if len(normvector.shape) == 2:\n        normvector = normvector.unsqueeze(0)\n    mu    = n1 / n2\n    div   = normvector[:, 1, 0] ** 2  + normvector[:, 1, 1] ** 2 + normvector[:, 1, 2] ** 2\n    a     = mu * (vector[:, 1, 0] * normvector[:, 1, 0] + vector[:, 1, 1] * normvector[:, 1, 1] + vector[:, 1, 2] * normvector[:, 1, 2]) / div\n    b     = (mu ** 2 - 1) / div\n    to    = - b * 0.5 / a\n    num   = 0\n    eps   = torch.ones(vector.shape[0], device = vector.device) * error * 2\n    while len(eps[eps &gt; error]) &gt; 0:\n       num   += 1\n       oldto  = to\n       v      = to ** 2 + 2 * a * to + b\n       deltav = 2 * (to + a)\n       to     = to - v / deltav\n       eps    = abs(oldto - to)\n    output = torch.zeros_like(vector)\n    output[:, 0, 0] = normvector[:, 0, 0]\n    output[:, 0, 1] = normvector[:, 0, 1]\n    output[:, 0, 2] = normvector[:, 0, 2]\n    output[:, 1, 0] = mu * vector[:, 1, 0] + to * normvector[:, 1, 0]\n    output[:, 1, 1] = mu * vector[:, 1, 1] + to * normvector[:, 1, 1]\n    output[:, 1, 2] = mu * vector[:, 1, 2] + to * normvector[:, 1, 2]\n    return output\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.rotate_points","title":"<code>rotate_points(point, angles=torch.tensor([[0, 0, 0]]), mode='XYZ', origin=torch.tensor([[0, 0, 0]]), offset=torch.tensor([[0, 0, 0]]))</code>","text":"<p>Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>       A point with size of [3] or [1, 3] or [m, 3].\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>       Rotation mode determines ordering of the rotations at each axis.\n       There are XYZ,YXZ,ZXY and ZYX modes.\n</code></pre> </li> <li> <code>origin</code>           \u2013            <pre><code>       Reference point for a rotation.\n       Expected size is [3] or [1, 3].\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>       Shift with the given offset.\n       Expected size is [3] or [1, 3] or [m, 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the rotation [1 x 3] or [m x 3].</p> </li> <li> <code>rotx</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along X axis [3 x 3].</p> </li> <li> <code>roty</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along Y axis [3 x 3].</p> </li> <li> <code>rotz</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along Z axis [3 x 3].</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def rotate_points(\n                 point,\n                 angles = torch.tensor([[0, 0, 0]]), \n                 mode='XYZ', \n                 origin = torch.tensor([[0, 0, 0]]), \n                 offset = torch.tensor([[0, 0, 0]])\n                ):\n    \"\"\"\n    Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.\n\n    Parameters\n    ----------\n    point        : torch.tensor\n                   A point with size of [3] or [1, 3] or [m, 3].\n    angles       : torch.tensor\n                   Rotation angles in degrees. \n    mode         : str\n                   Rotation mode determines ordering of the rotations at each axis.\n                   There are XYZ,YXZ,ZXY and ZYX modes.\n    origin       : torch.tensor\n                   Reference point for a rotation.\n                   Expected size is [3] or [1, 3].\n    offset       : torch.tensor\n                   Shift with the given offset.\n                   Expected size is [3] or [1, 3] or [m, 3].\n\n    Returns\n    ----------\n    result       : torch.tensor\n                   Result of the rotation [1 x 3] or [m x 3].\n    rotx         : torch.tensor\n                   Rotation matrix along X axis [3 x 3].\n    roty         : torch.tensor\n                   Rotation matrix along Y axis [3 x 3].\n    rotz         : torch.tensor\n                   Rotation matrix along Z axis [3 x 3].\n    \"\"\"\n    origin = origin.to(point.device)\n    offset = offset.to(point.device)\n    if len(point.shape) == 1:\n        point = point.unsqueeze(0)\n    if len(angles.shape) == 1:\n        angles = angles.unsqueeze(0)\n    rotx = rotmatx(angles[:, 0])\n    roty = rotmaty(angles[:, 1])\n    rotz = rotmatz(angles[:, 2])\n    new_points = (point - origin).T\n    if angles.shape[0] &gt; 1:\n        new_points = new_points.unsqueeze(0)\n        if len(origin.shape) == 2:\n            origin = origin.unsqueeze(1)\n        if len(offset.shape) == 2:\n            offset = offset.unsqueeze(1)\n    if mode == 'XYZ':\n        result = (rotz @ (roty @ (rotx @ new_points))).mT\n    elif mode == 'XZY':\n        result = torch.mm(roty, torch.mm(rotz, torch.mm(rotx, new_points))).T\n    elif mode == 'YXZ':\n        result = torch.mm(rotz, torch.mm(rotx, torch.mm(roty, new_points))).T\n    elif mode == 'ZXY':\n        result = torch.mm(roty, torch.mm(rotx, torch.mm(rotz, new_points))).T\n    elif mode == 'ZYX':\n        result = torch.mm(rotx, torch.mm(roty, torch.mm(rotz, new_points))).T\n    result += origin\n    result += offset\n    return result, rotx, roty, rotz\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.same_side","title":"<code>same_side(p1, p2, a, b)</code>","text":"<p>Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.</p> <p>Parameters:</p> <ul> <li> <code>p1</code>           \u2013            <pre><code>      Point(s) to check.\n</code></pre> </li> <li> <code>p2</code>           \u2013            <pre><code>      This is the point check against.\n</code></pre> </li> <li> <code>a</code>           \u2013            <pre><code>      First point that forms the line.\n</code></pre> </li> <li> <code>b</code>           \u2013            <pre><code>      Second point that forms the line.\n</code></pre> </li> </ul> Source code in <code>odak/learn/tools/vector.py</code> <pre><code>def same_side(p1, p2, a, b):\n    \"\"\"\n    Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.\n\n    Parameters\n    ----------\n    p1          : list\n                  Point(s) to check.\n    p2          : list\n                  This is the point check against.\n    a           : list\n                  First point that forms the line.\n    b           : list\n                  Second point that forms the line.\n    \"\"\"\n    ba = torch.subtract(b, a)\n    p1a = torch.subtract(p1, a)\n    p2a = torch.subtract(p2, a)\n    cp1 = torch.cross(ba, p1a)\n    cp2 = torch.cross(ba, p2a)\n    test = torch.dot(cp1, cp2)\n    if len(p1.shape) &gt; 1:\n        return test &gt;= 0\n    if test &gt;= 0:\n        return True\n    return False\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.save_torch_tensor","title":"<code>save_torch_tensor(fn, tensor)</code>","text":"<p>Definition to save a torch tensor.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>tensor</code>           \u2013            <pre><code>       Torch tensor to be saved.\n</code></pre> </li> </ul> Source code in <code>odak/learn/tools/file.py</code> <pre><code>def save_torch_tensor(fn, tensor):\n    \"\"\"\n    Definition to save a torch tensor.\n\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    tensor       : torch.tensor\n                   Torch tensor to be saved.\n    \"\"\" \n    torch.save(tensor, expanduser(fn))\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.write_PLY","title":"<code>write_PLY(triangles, savefn='output.ply')</code>","text":"<p>Definition to generate a PLY file from given points.</p> <p>Parameters:</p> <ul> <li> <code>triangles</code>           \u2013            <pre><code>      List of triangles with the size of Mx3x3.\n</code></pre> </li> <li> <code>savefn</code>           \u2013            <pre><code>      Filename for a PLY file.\n</code></pre> </li> </ul> Source code in <code>odak/tools/asset.py</code> <pre><code>def write_PLY(triangles, savefn = 'output.ply'):\n    \"\"\"\n    Definition to generate a PLY file from given points.\n\n    Parameters\n    ----------\n    triangles   : ndarray\n                  List of triangles with the size of Mx3x3.\n    savefn      : string\n                  Filename for a PLY file.\n    \"\"\"\n    tris = []\n    pnts = []\n    color = [255, 255, 255]\n    for tri_id in range(triangles.shape[0]):\n        tris.append(\n            (\n                [3*tri_id, 3*tri_id+1, 3*tri_id+2],\n                color[0],\n                color[1],\n                color[2]\n            )\n        )\n        for i in range(0, 3):\n            pnts.append(\n                (\n                    float(triangles[tri_id][i][0]),\n                    float(triangles[tri_id][i][1]),\n                    float(triangles[tri_id][i][2])\n                )\n            )\n    tris = np.asarray(tris, dtype=[\n                          ('vertex_indices', 'i4', (3,)), ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')])\n    pnts = np.asarray(pnts, dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])\n    # Save mesh.\n    el1 = PlyElement.describe(pnts, 'vertex', comments=['Vertex data'])\n    el2 = PlyElement.describe(tris, 'face', comments=['Face data'])\n    PlyData([el1, el2], text=\"True\").write(savefn)\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.boundary.get_sphere_normal_torch","title":"<code>get_sphere_normal_torch(point, sphere)</code>","text":"<p>Definition to get a normal of a point on a given sphere.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>        Point on sphere in X,Y,Z.\n</code></pre> </li> <li> <code>sphere</code>           \u2013            <pre><code>        Center defined in X,Y,Z and radius.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal_vector</code> (              <code>tensor</code> )          \u2013            <p>Normal vector.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def get_sphere_normal_torch(point, sphere):\n    \"\"\"\n    Definition to get a normal of a point on a given sphere.\n\n    Parameters\n    ----------\n    point         : torch.tensor\n                    Point on sphere in X,Y,Z.\n    sphere        : torch.tensor\n                    Center defined in X,Y,Z and radius.\n\n    Returns\n    ----------\n    normal_vector : torch.tensor\n                    Normal vector.\n    \"\"\"\n    if len(point.shape) == 1:\n        point = point.reshape((1, 3))\n    normal_vector = create_ray_from_two_points(point, sphere[0:3])\n    return normal_vector\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.boundary.get_triangle_normal","title":"<code>get_triangle_normal(triangle, triangle_center=None)</code>","text":"<p>Definition to calculate surface normal of a triangle.</p> <p>Parameters:</p> <ul> <li> <code>triangle</code>           \u2013            <pre><code>          Set of points in X,Y and Z to define a planar surface (3,3). It can also be list of triangles (mx3x3).\n</code></pre> </li> <li> <code>triangle_center</code>               (<code>tensor</code>, default:                   <code>None</code> )           \u2013            <pre><code>          Center point of the given triangle. See odak.learn.raytracing.center_of_triangle for more. In many scenarios you can accelerate things by precomputing triangle centers.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def get_triangle_normal(triangle, triangle_center=None):\n    \"\"\"\n    Definition to calculate surface normal of a triangle.\n\n    Parameters\n    ----------\n    triangle        : torch.tensor\n                      Set of points in X,Y and Z to define a planar surface (3,3). It can also be list of triangles (mx3x3).\n    triangle_center : torch.tensor\n                      Center point of the given triangle. See odak.learn.raytracing.center_of_triangle for more. In many scenarios you can accelerate things by precomputing triangle centers.\n\n    Returns\n    ----------\n    normal          : torch.tensor\n                      Surface normal at the point of intersection.\n    \"\"\"\n    if len(triangle.shape) == 2:\n        triangle = triangle.view((1, 3, 3))\n    normal = torch.zeros((triangle.shape[0], 2, 3)).to(triangle.device)\n    direction = torch.linalg.cross(\n                                   triangle[:, 0] - triangle[:, 1], \n                                   triangle[:, 2] - triangle[:, 1]\n                                  )\n    if type(triangle_center) == type(None):\n        normal[:, 0] = center_of_triangle(triangle)\n    else:\n        normal[:, 0] = triangle_center\n    normal[:, 1] = direction / torch.sum(direction, axis=1)[0]\n    if normal.shape[0] == 1:\n        normal = normal.view((2, 3))\n    return normal\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.boundary.intersect_w_circle","title":"<code>intersect_w_circle(ray, circle)</code>","text":"<p>Definition to find intersection point of a ray with a circle.  Returns distance as zero if there isn't an intersection.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>circle</code>           \u2013            <pre><code>       A list that contains (0) Set of points in X,Y and Z to define plane of a circle, (1) circle center, and (2) circle radius.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>Tensor</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>Tensor</code> )          \u2013            <p>Distance in between a starting point of a ray and the intersection point with a given triangle.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_circle(ray, circle):\n    \"\"\"\n    Definition to find intersection point of a ray with a circle. \n    Returns distance as zero if there isn't an intersection.\n\n    Parameters\n    ----------\n    ray          : torch.Tensor\n                   A vector/ray.\n    circle       : list\n                   A list that contains (0) Set of points in X,Y and Z to define plane of a circle, (1) circle center, and (2) circle radius.\n\n    Returns\n    ----------\n    normal       : torch.Tensor\n                   Surface normal at the point of intersection.\n    distance     : torch.Tensor\n                   Distance in between a starting point of a ray and the intersection point with a given triangle.\n    \"\"\"\n    normal, distance = intersect_w_surface(ray, circle[0])\n\n    if len(normal.shape) == 2:\n        normal = normal.unsqueeze(0)\n\n    distance_to_center = distance_between_two_points(normal[:, 0], circle[1])\n    mask = distance_to_center &gt; circle[2]\n    distance[mask] = 0\n\n    if len(ray.shape) == 2:\n        normal = normal.squeeze(0)\n\n    return normal, distance\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.boundary.intersect_w_sphere","title":"<code>intersect_w_sphere(ray, sphere, learning_rate=0.2, number_of_steps=5000, error_threshold=0.01)</code>","text":"<p>Definition to find the intersection between ray(s) and sphere(s).</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>              Input ray(s).\n              Expected size is [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> <li> <code>sphere</code>           \u2013            <pre><code>              Input sphere.\n              Expected size is [1 x 4].\n</code></pre> </li> <li> <code>learning_rate</code>           \u2013            <pre><code>              Learning rate used in the optimizer for finding the propagation distances of the rays.\n</code></pre> </li> <li> <code>number_of_steps</code>           \u2013            <pre><code>              Number of steps used in the optimizer.\n</code></pre> </li> <li> <code>error_threshold</code>           \u2013            <pre><code>              The error threshold that will help deciding intersection or no intersection.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>intersecting_ray</code> (              <code>tensor</code> )          \u2013            <p>Ray(s) that intersecting with the given sphere. Expected size is [n x 2 x 3], where n could be any real number.</p> </li> <li> <code>intersecting_normal</code> (              <code>tensor</code> )          \u2013            <p>Normal(s) for the ray(s) intersecting with the given sphere Expected size is [n x 2 x 3], where n could be any real number.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_sphere(ray, sphere, learning_rate = 2e-1, number_of_steps = 5000, error_threshold = 1e-2):\n    \"\"\"\n    Definition to find the intersection between ray(s) and sphere(s).\n\n    Parameters\n    ----------\n    ray                 : torch.tensor\n                          Input ray(s).\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3].\n    sphere              : torch.tensor\n                          Input sphere.\n                          Expected size is [1 x 4].\n    learning_rate       : float\n                          Learning rate used in the optimizer for finding the propagation distances of the rays.\n    number_of_steps     : int\n                          Number of steps used in the optimizer.\n    error_threshold     : float\n                          The error threshold that will help deciding intersection or no intersection.\n\n    Returns\n    -------\n    intersecting_ray    : torch.tensor\n                          Ray(s) that intersecting with the given sphere.\n                          Expected size is [n x 2 x 3], where n could be any real number.\n    intersecting_normal : torch.tensor\n                          Normal(s) for the ray(s) intersecting with the given sphere\n                          Expected size is [n x 2 x 3], where n could be any real number.\n\n    \"\"\"\n    if len(ray.shape) == 2:\n        ray = ray.unsqueeze(0)\n    if len(sphere.shape) == 1:\n        sphere = sphere.unsqueeze(0)\n    distance = torch.zeros(ray.shape[0], device = ray.device, requires_grad = True)\n    loss_l2 = torch.nn.MSELoss(reduction = 'sum')\n    optimizer = torch.optim.AdamW([distance], lr = learning_rate)    \n    t = tqdm(range(number_of_steps), leave = False, dynamic_ncols = True)\n    for step in t:\n        optimizer.zero_grad()\n        propagated_ray = propagate_ray(ray, distance)\n        test = torch.abs((propagated_ray[:, 0, 0] - sphere[:, 0]) ** 2 + (propagated_ray[:, 0, 1] - sphere[:, 1]) ** 2 + (propagated_ray[:, 0, 2] - sphere[:, 2]) ** 2 - sphere[:, 3] ** 2)\n        loss = loss_l2(\n                       test,\n                       torch.zeros_like(test)\n                      )\n        loss.backward(retain_graph = True)\n        optimizer.step()\n        t.set_description('Sphere intersection loss: {}'.format(loss.item()))\n    check = test &lt; error_threshold\n    intersecting_ray = propagate_ray(ray[check == True], distance[check == True])\n    intersecting_normal = create_ray_from_two_points(\n                                                     sphere[:, 0:3],\n                                                     intersecting_ray[:, 0]\n                                                    )\n    return intersecting_ray, intersecting_normal, distance, check\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.boundary.intersect_w_surface","title":"<code>intersect_w_surface(ray, points)</code>","text":"<p>Definition to find intersection point inbetween a surface and a ray. For more see: http://geomalgorithms.com/a06-_intersect-2.html</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>points</code>           \u2013            <pre><code>       Set of points in X,Y and Z to define a planar surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between starting point of a ray with it's intersection with a planar surface.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_surface(ray, points):\n    \"\"\"\n    Definition to find intersection point inbetween a surface and a ray. For more see: http://geomalgorithms.com/a06-_intersect-2.html\n\n    Parameters\n    ----------\n    ray          : torch.tensor\n                   A vector/ray.\n    points       : torch.tensor\n                   Set of points in X,Y and Z to define a planar surface.\n\n    Returns\n    ----------\n    normal       : torch.tensor\n                   Surface normal at the point of intersection.\n    distance     : float\n                   Distance in between starting point of a ray with it's intersection with a planar surface.\n    \"\"\"\n    normal = get_triangle_normal(points)\n    if len(ray.shape) == 2:\n        ray = ray.unsqueeze(0)\n    if len(points.shape) == 2:\n        points = points.unsqueeze(0)\n    if len(normal.shape) == 2:\n        normal = normal.unsqueeze(0)\n    f = normal[:, 0] - ray[:, 0]\n    distance = (torch.mm(normal[:, 1], f.T) / torch.mm(normal[:, 1], ray[:, 1].T)).T\n    new_normal = torch.zeros_like(ray)\n    new_normal[:, 0] = ray[:, 0] + distance * ray[:, 1]\n    new_normal[:, 1] = normal[:, 1]\n    new_normal = torch.nan_to_num(\n                                  new_normal,\n                                  nan = float('nan'),\n                                  posinf = float('nan'),\n                                  neginf = float('nan')\n                                 )\n    distance = torch.nan_to_num(\n                                distance,\n                                nan = float('nan'),\n                                posinf = float('nan'),\n                                neginf = float('nan')\n                               )\n    return new_normal, distance\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.boundary.intersect_w_surface_batch","title":"<code>intersect_w_surface_batch(ray, triangle)</code>","text":"<p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray (2 x 3). It can also be a list of rays (n x 2 x 3).\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>       Set of points in X,Y and Z to define a planar surface. It can also be a list of triangles (m x 3 x 3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection (m x n x 2 x 3).</p> </li> <li> <code>distance</code> (              <code>tensor</code> )          \u2013            <p>Distance in between starting point of a ray with it's intersection with a planar surface (m x n).</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_surface_batch(ray, triangle):\n    \"\"\"\n    Parameters\n    ----------\n    ray          : torch.tensor\n                   A vector/ray (2 x 3). It can also be a list of rays (n x 2 x 3).\n    triangle     : torch.tensor\n                   Set of points in X,Y and Z to define a planar surface. It can also be a list of triangles (m x 3 x 3).\n\n    Returns\n    ----------\n    normal       : torch.tensor\n                   Surface normal at the point of intersection (m x n x 2 x 3).\n    distance     : torch.tensor\n                   Distance in between starting point of a ray with it's intersection with a planar surface (m x n).\n    \"\"\"\n    normal = get_triangle_normal(triangle)\n    if len(ray.shape) == 2:\n        ray = ray.unsqueeze(0)\n    if len(triangle.shape) == 2:\n        triangle = triangle.unsqueeze(0)\n    if len(normal.shape) == 2:\n        normal = normal.unsqueeze(0)\n\n    f = normal[:, None, 0] - ray[None, :, 0]\n    distance = (torch.bmm(normal[:, None, 1], f.permute(0, 2, 1)).squeeze(1) / torch.mm(normal[:, 1], ray[:, 1].T)).T\n\n    new_normal = torch.zeros((triangle.shape[0], )+ray.shape)\n    new_normal[:, :, 0] = ray[None, :, 0] + (distance[:, :, None] * ray[:, None, 1]).permute(1, 0, 2)\n    new_normal[:, :, 1] = normal[:, None, 1]\n    new_normal = torch.nan_to_num(\n                                  new_normal,\n                                  nan = float('nan'),\n                                  posinf = float('nan'),\n                                  neginf = float('nan')\n                                 )\n    distance = torch.nan_to_num(\n                                distance,\n                                nan = float('nan'),\n                                posinf = float('nan'),\n                                neginf = float('nan')\n                               )\n    return new_normal, distance.T\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.boundary.intersect_w_triangle","title":"<code>intersect_w_triangle(ray, triangle)</code>","text":"<p>Definition to find intersection point of a ray with a triangle. </p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>              A ray [1 x 2 x 3] or a batch of ray [m x 2 x 3].\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>              Set of points in X,Y and Z to define a single triangle [1 x 3 x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection with the surface of triangle. This could also involve surface normals that are not on the triangle. Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between a starting point of a ray and the intersection point with a given triangle. Expected size is [1 x 1] or [m x 1] depending on the input.</p> </li> <li> <code>intersecting_ray</code> (              <code>tensor</code> )          \u2013            <p>Rays that intersect with the triangle plane and on the triangle. Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.</p> </li> <li> <code>intersecting_normal</code> (              <code>tensor</code> )          \u2013            <p>Normals that intersect with the triangle plane and on the triangle. Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.</p> </li> <li> <code>check</code> (              <code>tensor</code> )          \u2013            <p>A list that provides a bool as True or False for each ray used as input. A test to see is a ray could be on the given triangle. Expected size is [1] or [m].</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_triangle(ray, triangle):\n    \"\"\"\n    Definition to find intersection point of a ray with a triangle. \n\n    Parameters\n    ----------\n    ray                 : torch.tensor\n                          A ray [1 x 2 x 3] or a batch of ray [m x 2 x 3].\n    triangle            : torch.tensor\n                          Set of points in X,Y and Z to define a single triangle [1 x 3 x 3].\n\n    Returns\n    ----------\n    normal              : torch.tensor\n                          Surface normal at the point of intersection with the surface of triangle.\n                          This could also involve surface normals that are not on the triangle.\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.\n    distance            : float\n                          Distance in between a starting point of a ray and the intersection point with a given triangle.\n                          Expected size is [1 x 1] or [m x 1] depending on the input.\n    intersecting_ray    : torch.tensor\n                          Rays that intersect with the triangle plane and on the triangle.\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.\n    intersecting_normal : torch.tensor\n                          Normals that intersect with the triangle plane and on the triangle.\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3] depending on the input.\n    check               : torch.tensor\n                          A list that provides a bool as True or False for each ray used as input.\n                          A test to see is a ray could be on the given triangle.\n                          Expected size is [1] or [m].\n    \"\"\"\n    if len(triangle.shape) == 2:\n       triangle = triangle.unsqueeze(0)\n    if len(ray.shape) == 2:\n       ray = ray.unsqueeze(0)\n    normal, distance = intersect_w_surface(ray, triangle)\n    check = is_it_on_triangle(normal[:, 0], triangle)\n    intersecting_ray = ray.unsqueeze(0)\n    intersecting_ray = intersecting_ray.repeat(triangle.shape[0], 1, 1, 1)\n    intersecting_ray = intersecting_ray[check == True]\n    intersecting_normal = normal.unsqueeze(0)\n    intersecting_normal = intersecting_normal.repeat(triangle.shape[0], 1, 1, 1)\n    intersecting_normal = intersecting_normal[check ==  True]\n    return normal, distance, intersecting_ray, intersecting_normal, check\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.boundary.intersect_w_triangle_batch","title":"<code>intersect_w_triangle_batch(ray, triangle)</code>","text":"<p>Definition to find intersection points of rays with triangles. Returns False for each variable if the rays doesn't intersect with given triangles.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       vectors/rays (n x 2 x 3).\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>       Set of points in X,Y and Z to define triangles (m x 3 x 3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>tensor</code> )          \u2013            <p>Surface normal at the point of intersection (m x n x 2 x 3).</p> </li> <li> <code>distance</code> (              <code>List</code> )          \u2013            <p>Distance in between starting point of a ray with it's intersection with a planar surface (m x n).</p> </li> <li> <code>intersect_ray</code> (              <code>List</code> )          \u2013            <p>List of intersecting rays (k x 2 x 3) where k &lt;= n.</p> </li> <li> <code>intersect_normal</code> (              <code>List</code> )          \u2013            <p>List of intersecting normals (k x 2 x 3) where k &lt;= n*m.</p> </li> <li> <code>check</code> (              <code>tensor</code> )          \u2013            <p>Boolean tensor (m x n) indicating whether each ray intersects with a triangle or not.</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def intersect_w_triangle_batch(ray, triangle):\n    \"\"\"\n    Definition to find intersection points of rays with triangles. Returns False for each variable if the rays doesn't intersect with given triangles.\n\n    Parameters\n    ----------\n    ray          : torch.tensor\n                   vectors/rays (n x 2 x 3).\n    triangle     : torch.tensor\n                   Set of points in X,Y and Z to define triangles (m x 3 x 3).\n\n    Returns\n    ----------\n    normal          : torch.tensor\n                      Surface normal at the point of intersection (m x n x 2 x 3).\n    distance        : List\n                      Distance in between starting point of a ray with it's intersection with a planar surface (m x n).\n    intersect_ray   : List\n                      List of intersecting rays (k x 2 x 3) where k &lt;= n.\n    intersect_normal: List\n                      List of intersecting normals (k x 2 x 3) where k &lt;= n*m.\n    check           : torch.tensor\n                      Boolean tensor (m x n) indicating whether each ray intersects with a triangle or not.\n    \"\"\"\n    if len(triangle.shape) == 2:\n       triangle = triangle.unsqueeze(0)\n    if len(ray.shape) == 2:\n       ray = ray.unsqueeze(0)\n\n    normal, distance = intersect_w_surface_batch(ray, triangle)\n\n    check = is_it_on_triangle_batch(normal[:, :, 0], triangle)\n\n    flat_check = check.flatten()\n    flat_normal = normal.view(-1, normal.size(-2), normal.size(-1))\n    flat_ray = ray.repeat(normal.size(0), 1, 1)\n    flat_distance = distance.flatten()\n\n    filtered_normal = torch.masked_select(flat_normal, flat_check.unsqueeze(-1).unsqueeze(-1).repeat(1, 2, 3))\n    filtered_ray = torch.masked_select(flat_ray, flat_check.unsqueeze(-1).unsqueeze(-1).repeat(1, 2, 3))\n    filtered_distnace = torch.masked_select(flat_distance, flat_check)\n\n    check_count = check.sum(dim=1).tolist()\n    split_size_ray_and_normal = [count * 2 * 3 for count in check_count]\n    split_size_distance = [count for count in check_count]\n\n    normal_grouped = torch.split(filtered_normal, split_size_ray_and_normal)\n    ray_grouped = torch.split(filtered_ray, split_size_ray_and_normal)\n    distance_grouped = torch.split(filtered_distnace, split_size_distance)\n\n    intersecting_normal = [g.view(-1, 2, 3) for g in normal_grouped if g.numel() &gt; 0]\n    intersecting_ray = [g.view(-1, 2, 3) for g in ray_grouped if g.numel() &gt; 0]\n    new_distance = [g for g in distance_grouped if g.numel() &gt; 0]\n\n    return normal, new_distance, intersecting_ray, intersecting_normal, check\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.boundary.reflect","title":"<code>reflect(input_ray, normal)</code>","text":"<p>Definition to reflect an incoming ray from a surface defined by a surface normal.  Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.</p> <p>Parameters:</p> <ul> <li> <code>input_ray</code>           \u2013            <pre><code>       A ray or rays.\n       Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> <li> <code>normal</code>           \u2013            <pre><code>       A surface normal(s).\n       Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output_ray</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a reflected ray. Expected size is [1 x 2 x 3] or [m x 2 x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def reflect(input_ray, normal):\n    \"\"\" \n    Definition to reflect an incoming ray from a surface defined by a surface normal. \n    Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.\n\n\n    Parameters\n    ----------\n    input_ray    : torch.tensor\n                   A ray or rays.\n                   Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n    normal       : torch.tensor\n                   A surface normal(s).\n                   Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n\n    Returns\n    ----------\n    output_ray   : torch.tensor\n                   Array that contains starting points and cosines of a reflected ray.\n                   Expected size is [1 x 2 x 3] or [m x 2 x 3].\n    \"\"\"\n    if len(input_ray.shape) == 2:\n        input_ray = input_ray.unsqueeze(0)\n    if len(normal.shape) == 2:\n        normal = normal.unsqueeze(0)\n    mu = 1\n    div = normal[:, 1, 0]**2 + normal[:, 1, 1]**2 + normal[:, 1, 2]**2 + 1e-8\n    a = mu * (input_ray[:, 1, 0] * normal[:, 1, 0] + input_ray[:, 1, 1] * normal[:, 1, 1] + input_ray[:, 1, 2] * normal[:, 1, 2]) / div\n    a = a.unsqueeze(1)\n    n = int(torch.amax(torch.tensor([normal.shape[0], input_ray.shape[0]])))\n    output_ray = torch.zeros((n, 2, 3)).to(input_ray.device)\n    output_ray[:, 0] = normal[:, 0]\n    output_ray[:, 1] = input_ray[:, 1] - 2 * a * normal[:, 1]\n    return output_ray\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.boundary.refract","title":"<code>refract(vector, normvector, n1, n2, error=0.01)</code>","text":"<p>Definition to refract an incoming ray. Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.</p> <p>Parameters:</p> <ul> <li> <code>vector</code>           \u2013            <pre><code>         Incoming ray.\n         Expected size is [2, 3], [1, 2, 3] or [m, 2, 3].\n</code></pre> </li> <li> <code>normvector</code>           \u2013            <pre><code>         Normal vector.\n         Expected size is [2, 3], [1, 2, 3] or [m, 2, 3]].\n</code></pre> </li> <li> <code>n1</code>           \u2013            <pre><code>         Refractive index of the incoming medium.\n</code></pre> </li> <li> <code>n2</code>           \u2013            <pre><code>         Refractive index of the outgoing medium.\n</code></pre> </li> <li> <code>error</code>           \u2013            <pre><code>         Desired error.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Refracted ray. Expected size is [1, 2, 3]</p> </li> </ul> Source code in <code>odak/learn/raytracing/boundary.py</code> <pre><code>def refract(vector, normvector, n1, n2, error = 0.01):\n    \"\"\"\n    Definition to refract an incoming ray.\n    Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.\n\n\n    Parameters\n    ----------\n    vector         : torch.tensor\n                     Incoming ray.\n                     Expected size is [2, 3], [1, 2, 3] or [m, 2, 3].\n    normvector     : torch.tensor\n                     Normal vector.\n                     Expected size is [2, 3], [1, 2, 3] or [m, 2, 3]].\n    n1             : float\n                     Refractive index of the incoming medium.\n    n2             : float\n                     Refractive index of the outgoing medium.\n    error          : float \n                     Desired error.\n\n    Returns\n    -------\n    output         : torch.tensor\n                     Refracted ray.\n                     Expected size is [1, 2, 3]\n    \"\"\"\n    if len(vector.shape) == 2:\n        vector = vector.unsqueeze(0)\n    if len(normvector.shape) == 2:\n        normvector = normvector.unsqueeze(0)\n    mu    = n1 / n2\n    div   = normvector[:, 1, 0] ** 2  + normvector[:, 1, 1] ** 2 + normvector[:, 1, 2] ** 2\n    a     = mu * (vector[:, 1, 0] * normvector[:, 1, 0] + vector[:, 1, 1] * normvector[:, 1, 1] + vector[:, 1, 2] * normvector[:, 1, 2]) / div\n    b     = (mu ** 2 - 1) / div\n    to    = - b * 0.5 / a\n    num   = 0\n    eps   = torch.ones(vector.shape[0], device = vector.device) * error * 2\n    while len(eps[eps &gt; error]) &gt; 0:\n       num   += 1\n       oldto  = to\n       v      = to ** 2 + 2 * a * to + b\n       deltav = 2 * (to + a)\n       to     = to - v / deltav\n       eps    = abs(oldto - to)\n    output = torch.zeros_like(vector)\n    output[:, 0, 0] = normvector[:, 0, 0]\n    output[:, 0, 1] = normvector[:, 0, 1]\n    output[:, 0, 2] = normvector[:, 0, 2]\n    output[:, 1, 0] = mu * vector[:, 1, 0] + to * normvector[:, 1, 0]\n    output[:, 1, 1] = mu * vector[:, 1, 1] + to * normvector[:, 1, 1]\n    output[:, 1, 2] = mu * vector[:, 1, 2] + to * normvector[:, 1, 2]\n    return output\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.detector.detector","title":"<code>detector</code>","text":"<p>A class to represent a detector.</p> Source code in <code>odak/learn/raytracing/detector.py</code> <pre><code>class detector():\n    \"\"\"\n    A class to represent a detector.\n    \"\"\"\n\n\n    def __init__(\n                 self,\n                 colors = 3,\n                 center = torch.tensor([0., 0., 0.]),\n                 tilt = torch.tensor([0., 0., 0.]),\n                 size = torch.tensor([10., 10.]),\n                 resolution = torch.tensor([100, 100]),\n                 device = torch.device('cpu')\n                ):\n        \"\"\"\n        Parameters\n        ----------\n        colors         : int\n                         Number of color channels to register (e.g., RGB).\n        center         : torch.tensor\n                         Center point of the detector [3].\n        tilt           : torch.tensor\n                         Tilt angles of the surface in degrees [3].\n        size           : torch.tensor\n                         Size of the detector [2].\n        resolution     : torch.tensor\n                         Resolution of the detector.\n        device         : torch.device\n                         Device for computation (e.g., cuda, cpu).\n        \"\"\"\n        self.device = device\n        self.colors = colors\n        self.resolution = resolution.to(self.device)\n        self.surface_center = center.to(self.device)\n        self.surface_tilt = tilt.to(self.device)\n        self.size = size.to(self.device)\n        self.pixel_size = torch.tensor([\n                                        self.size[0] / self.resolution[0],\n                                        self.size[1] / self.resolution[1]\n                                       ], device  = self.device)\n        self.pixel_diagonal_size = torch.sqrt(self.pixel_size[0] ** 2 + self.pixel_size[1] ** 2)\n        self.pixel_diagonal_half_size = self.pixel_diagonal_size / 2.\n        self.threshold = torch.nn.Threshold(self.pixel_diagonal_size, 1)\n        self.plane = define_plane(\n                                  point = self.surface_center,\n                                  angles = self.surface_tilt\n                                 )\n        self.pixel_locations, _, _, _ = grid_sample(\n                                                    size = self.size.tolist(),\n                                                    no = self.resolution.tolist(),\n                                                    center = self.surface_center.tolist(),\n                                                    angles = self.surface_tilt.tolist()\n                                                   )\n        self.pixel_locations = self.pixel_locations.to(self.device)\n        self.relu = torch.nn.ReLU()\n        self.clear()\n\n\n    def intersect(self, rays, color = 0):\n        \"\"\"\n        Function to intersect rays with the detector\n\n\n        Parameters\n        ----------\n        rays            : torch.tensor\n                          Rays to be intersected with a detector.\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3].\n        color           : int\n                          Color channel to register.\n\n        Returns\n        -------\n        points          : torch.tensor\n                          Intersection points with the image detector [k x 3].\n        \"\"\"\n        normals, _ = intersect_w_surface(rays, self.plane)\n        points = normals[:, 0]\n        distances_xyz = torch.abs(points.unsqueeze(1) - self.pixel_locations.unsqueeze(0))\n        distances_x = 1e6 * self.relu( - (distances_xyz[:, :, 0] - self.pixel_size[0]))\n        distances_y = 1e6 * self.relu( - (distances_xyz[:, :, 1] - self.pixel_size[1]))\n        hit_x = torch.clamp(distances_x, min = 0., max = 1.)\n        hit_y = torch.clamp(distances_y, min = 0., max = 1.)\n        hit = hit_x * hit_y\n        image = torch.sum(hit, dim = 0)\n        self.image[color] += image.reshape(\n                                           self.image.shape[-2], \n                                           self.image.shape[-1]\n                                          )\n        distances = torch.sum((points.unsqueeze(1) - self.pixel_locations.unsqueeze(0)) ** 2, dim = 2)\n        distance_image = distances\n#        distance_image = distances.reshape(\n#                                           -1,\n#                                           self.image.shape[-2],\n#                                           self.image.shape[-1]\n#                                          )\n        return points, image, distance_image\n\n\n    def get_image(self):\n        \"\"\"\n        Function to return the detector image.\n\n        Returns\n        -------\n        image           : torch.tensor\n                          Detector image.\n        \"\"\"\n        image = (self.image - self.image.min()) / (self.image.max() - self.image.min())\n        return image\n\n\n    def clear(self):\n        \"\"\"\n        Internal function to clear a detector.\n        \"\"\"\n        self.image = torch.zeros(\n\n                                 self.colors,\n                                 self.resolution[0],\n                                 self.resolution[1],\n                                 device = self.device,\n                                )\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.detector.detector.__init__","title":"<code>__init__(colors=3, center=torch.tensor([0.0, 0.0, 0.0]), tilt=torch.tensor([0.0, 0.0, 0.0]), size=torch.tensor([10.0, 10.0]), resolution=torch.tensor([100, 100]), device=torch.device('cpu'))</code>","text":"<p>Parameters:</p> <ul> <li> <code>colors</code>           \u2013            <pre><code>         Number of color channels to register (e.g., RGB).\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>         Center point of the detector [3].\n</code></pre> </li> <li> <code>tilt</code>           \u2013            <pre><code>         Tilt angles of the surface in degrees [3].\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>         Size of the detector [2].\n</code></pre> </li> <li> <code>resolution</code>           \u2013            <pre><code>         Resolution of the detector.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>         Device for computation (e.g., cuda, cpu).\n</code></pre> </li> </ul> Source code in <code>odak/learn/raytracing/detector.py</code> <pre><code>def __init__(\n             self,\n             colors = 3,\n             center = torch.tensor([0., 0., 0.]),\n             tilt = torch.tensor([0., 0., 0.]),\n             size = torch.tensor([10., 10.]),\n             resolution = torch.tensor([100, 100]),\n             device = torch.device('cpu')\n            ):\n    \"\"\"\n    Parameters\n    ----------\n    colors         : int\n                     Number of color channels to register (e.g., RGB).\n    center         : torch.tensor\n                     Center point of the detector [3].\n    tilt           : torch.tensor\n                     Tilt angles of the surface in degrees [3].\n    size           : torch.tensor\n                     Size of the detector [2].\n    resolution     : torch.tensor\n                     Resolution of the detector.\n    device         : torch.device\n                     Device for computation (e.g., cuda, cpu).\n    \"\"\"\n    self.device = device\n    self.colors = colors\n    self.resolution = resolution.to(self.device)\n    self.surface_center = center.to(self.device)\n    self.surface_tilt = tilt.to(self.device)\n    self.size = size.to(self.device)\n    self.pixel_size = torch.tensor([\n                                    self.size[0] / self.resolution[0],\n                                    self.size[1] / self.resolution[1]\n                                   ], device  = self.device)\n    self.pixel_diagonal_size = torch.sqrt(self.pixel_size[0] ** 2 + self.pixel_size[1] ** 2)\n    self.pixel_diagonal_half_size = self.pixel_diagonal_size / 2.\n    self.threshold = torch.nn.Threshold(self.pixel_diagonal_size, 1)\n    self.plane = define_plane(\n                              point = self.surface_center,\n                              angles = self.surface_tilt\n                             )\n    self.pixel_locations, _, _, _ = grid_sample(\n                                                size = self.size.tolist(),\n                                                no = self.resolution.tolist(),\n                                                center = self.surface_center.tolist(),\n                                                angles = self.surface_tilt.tolist()\n                                               )\n    self.pixel_locations = self.pixel_locations.to(self.device)\n    self.relu = torch.nn.ReLU()\n    self.clear()\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.detector.detector.clear","title":"<code>clear()</code>","text":"<p>Internal function to clear a detector.</p> Source code in <code>odak/learn/raytracing/detector.py</code> <pre><code>def clear(self):\n    \"\"\"\n    Internal function to clear a detector.\n    \"\"\"\n    self.image = torch.zeros(\n\n                             self.colors,\n                             self.resolution[0],\n                             self.resolution[1],\n                             device = self.device,\n                            )\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.detector.detector.get_image","title":"<code>get_image()</code>","text":"<p>Function to return the detector image.</p> <p>Returns:</p> <ul> <li> <code>image</code> (              <code>tensor</code> )          \u2013            <p>Detector image.</p> </li> </ul> Source code in <code>odak/learn/raytracing/detector.py</code> <pre><code>def get_image(self):\n    \"\"\"\n    Function to return the detector image.\n\n    Returns\n    -------\n    image           : torch.tensor\n                      Detector image.\n    \"\"\"\n    image = (self.image - self.image.min()) / (self.image.max() - self.image.min())\n    return image\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.detector.detector.intersect","title":"<code>intersect(rays, color=0)</code>","text":"<p>Function to intersect rays with the detector</p> <p>Parameters:</p> <ul> <li> <code>rays</code>           \u2013            <pre><code>          Rays to be intersected with a detector.\n          Expected size is [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> <li> <code>color</code>           \u2013            <pre><code>          Color channel to register.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>points</code> (              <code>tensor</code> )          \u2013            <p>Intersection points with the image detector [k x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/detector.py</code> <pre><code>    def intersect(self, rays, color = 0):\n        \"\"\"\n        Function to intersect rays with the detector\n\n\n        Parameters\n        ----------\n        rays            : torch.tensor\n                          Rays to be intersected with a detector.\n                          Expected size is [1 x 2 x 3] or [m x 2 x 3].\n        color           : int\n                          Color channel to register.\n\n        Returns\n        -------\n        points          : torch.tensor\n                          Intersection points with the image detector [k x 3].\n        \"\"\"\n        normals, _ = intersect_w_surface(rays, self.plane)\n        points = normals[:, 0]\n        distances_xyz = torch.abs(points.unsqueeze(1) - self.pixel_locations.unsqueeze(0))\n        distances_x = 1e6 * self.relu( - (distances_xyz[:, :, 0] - self.pixel_size[0]))\n        distances_y = 1e6 * self.relu( - (distances_xyz[:, :, 1] - self.pixel_size[1]))\n        hit_x = torch.clamp(distances_x, min = 0., max = 1.)\n        hit_y = torch.clamp(distances_y, min = 0., max = 1.)\n        hit = hit_x * hit_y\n        image = torch.sum(hit, dim = 0)\n        self.image[color] += image.reshape(\n                                           self.image.shape[-2], \n                                           self.image.shape[-1]\n                                          )\n        distances = torch.sum((points.unsqueeze(1) - self.pixel_locations.unsqueeze(0)) ** 2, dim = 2)\n        distance_image = distances\n#        distance_image = distances.reshape(\n#                                           -1,\n#                                           self.image.shape[-2],\n#                                           self.image.shape[-1]\n#                                          )\n        return points, image, distance_image\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.mesh.planar_mesh","title":"<code>planar_mesh</code>","text":"Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>class planar_mesh():\n\n\n    def __init__(\n                 self,\n                 size = [1., 1.],\n                 number_of_meshes = [10, 10],\n                 angles = torch.tensor([0., 0., 0.]),\n                 offset = torch.tensor([0., 0., 0.]),\n                 device = torch.device('cpu'),\n                 heights = None\n                ):\n        \"\"\"\n        Definition to generate a plane with meshes.\n\n\n        Parameters\n        -----------\n        number_of_meshes  : torch.tensor\n                            Number of squares over plane.\n                            There are two triangles at each square.\n        size              : torch.tensor\n                            Size of the plane.\n        angles            : torch.tensor\n                            Rotation angles in degrees.\n        offset            : torch.tensor\n                            Offset along XYZ axes.\n                            Expected dimension is [1 x 3] or offset for each triangle [m x 3].\n                            m here refers to `2 * number_of_meshes[0]` times  `number_of_meshes[1]`.\n        device            : torch.device\n                            Computational resource to be used (e.g., cpu, cuda).\n        heights           : torch.tensor\n                            Load surface heights from a tensor.\n        \"\"\"\n        self.device = device\n        self.angles = angles.to(self.device)\n        self.offset = offset.to(self.device)\n        self.size = size.to(self.device)\n        self.number_of_meshes = number_of_meshes.to(self.device)\n        self.init_heights(heights)\n\n\n    def init_heights(self, heights = None):\n        \"\"\"\n        Internal function to initialize a height map.\n        Note that self.heights is a differentiable variable, and can be optimized or learned.\n        See unit test `test/test_learn_ray_detector.py` or `test/test_learn_ray_mesh.py` as examples.\n        \"\"\"\n        if not isinstance(heights, type(None)):\n            self.heights = heights.to(self.device)\n            self.heights.requires_grad = True\n        else:\n            self.heights = torch.zeros(\n                                       (self.number_of_meshes[0], self.number_of_meshes[1], 1),\n                                       requires_grad = True,\n                                       device = self.device,\n                                      )\n        x = torch.linspace(-self.size[0] / 2., self.size[0] / 2., self.number_of_meshes[0], device = self.device) \n        y = torch.linspace(-self.size[1] / 2., self.size[1] / 2., self.number_of_meshes[1], device = self.device)\n        X, Y = torch.meshgrid(x, y, indexing = 'ij')\n        self.X = X.unsqueeze(-1)\n        self.Y = Y.unsqueeze(-1)\n\n\n    def save_heights(self, filename = 'heights.pt'):\n        \"\"\"\n        Function to save heights to a file.\n\n        Parameters\n        ----------\n        filename          : str\n                            Filename.\n        \"\"\"\n        save_torch_tensor(filename, self.heights.detach().clone())\n\n\n    def save_heights_as_PLY(self, filename = 'mesh.ply'):\n        \"\"\"\n        Function to save mesh to a PLY file.\n\n        Parameters\n        ----------\n        filename          : str\n                            Filename.\n        \"\"\"\n        triangles = self.get_triangles()\n        write_PLY(triangles, filename)\n\n\n    def get_squares(self):\n        \"\"\"\n        Internal function to initiate squares over a plane.\n\n        Returns\n        -------\n        squares     : torch.tensor\n                      Squares over a plane.\n                      Expected size is [m x n x 3].\n        \"\"\"\n        squares = torch.cat((\n                             self.X,\n                             self.Y,\n                             self.heights\n                            ), dim = -1)\n        return squares\n\n\n    def get_triangles(self):\n        \"\"\"\n        Internal function to get triangles.\n        \"\"\" \n        squares = self.get_squares()\n        triangles = torch.zeros(2, self.number_of_meshes[0], self.number_of_meshes[1], 3, 3, device = self.device)\n        for i in range(0, self.number_of_meshes[0] - 1):\n            for j in range(0, self.number_of_meshes[1] - 1):\n                first_triangle = torch.cat((\n                                            squares[i + 1, j].unsqueeze(0),\n                                            squares[i + 1, j + 1].unsqueeze(0),\n                                            squares[i, j + 1].unsqueeze(0),\n                                           ), dim = 0)\n                second_triangle = torch.cat((\n                                             squares[i + 1, j].unsqueeze(0),\n                                             squares[i, j + 1].unsqueeze(0),\n                                             squares[i, j].unsqueeze(0),\n                                            ), dim = 0)\n                triangles[0, i, j], _, _, _ = rotate_points(first_triangle, angles = self.angles)\n                triangles[1, i, j], _, _, _ = rotate_points(second_triangle, angles = self.angles)\n        triangles = triangles.view(-1, 3, 3) + self.offset\n        return triangles \n\n\n    def mirror(self, rays):\n        \"\"\"\n        Function to bounce light rays off the meshes.\n\n        Parameters\n        ----------\n        rays              : torch.tensor\n                            Rays to be bounced.\n                            Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n\n        Returns\n        -------\n        reflected_rays    : torch.tensor\n                            Reflected rays.\n                            Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n        reflected_normals : torch.tensor\n                            Reflected normals.\n                            Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n\n        \"\"\"\n        if len(rays.shape) == 2:\n            rays = rays.unsqueeze(0)\n        triangles = self.get_triangles()\n        reflected_rays = torch.empty((0, 2, 3), requires_grad = True, device = self.device)\n        reflected_normals = torch.empty((0, 2, 3), requires_grad = True, device = self.device)\n        for triangle in triangles:\n            _, _, intersecting_rays, intersecting_normals, check = intersect_w_triangle(\n                                                                                        rays,\n                                                                                        triangle\n                                                                                       ) \n            triangle_reflected_rays = reflect(intersecting_rays, intersecting_normals)\n            if triangle_reflected_rays.shape[0] &gt; 0:\n                reflected_rays = torch.cat((\n                                            reflected_rays,\n                                            triangle_reflected_rays\n                                          ))\n                reflected_normals = torch.cat((\n                                               reflected_normals,\n                                               intersecting_normals\n                                              ))\n        return reflected_rays, reflected_normals\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.mesh.planar_mesh.__init__","title":"<code>__init__(size=[1.0, 1.0], number_of_meshes=[10, 10], angles=torch.tensor([0.0, 0.0, 0.0]), offset=torch.tensor([0.0, 0.0, 0.0]), device=torch.device('cpu'), heights=None)</code>","text":"<p>Definition to generate a plane with meshes.</p> <p>Parameters:</p> <ul> <li> <code>number_of_meshes</code>           \u2013            <pre><code>            Number of squares over plane.\n            There are two triangles at each square.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>            Size of the plane.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>            Rotation angles in degrees.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>            Offset along XYZ axes.\n            Expected dimension is [1 x 3] or offset for each triangle [m x 3].\n            m here refers to `2 * number_of_meshes[0]` times  `number_of_meshes[1]`.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>            Computational resource to be used (e.g., cpu, cuda).\n</code></pre> </li> <li> <code>heights</code>           \u2013            <pre><code>            Load surface heights from a tensor.\n</code></pre> </li> </ul> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def __init__(\n             self,\n             size = [1., 1.],\n             number_of_meshes = [10, 10],\n             angles = torch.tensor([0., 0., 0.]),\n             offset = torch.tensor([0., 0., 0.]),\n             device = torch.device('cpu'),\n             heights = None\n            ):\n    \"\"\"\n    Definition to generate a plane with meshes.\n\n\n    Parameters\n    -----------\n    number_of_meshes  : torch.tensor\n                        Number of squares over plane.\n                        There are two triangles at each square.\n    size              : torch.tensor\n                        Size of the plane.\n    angles            : torch.tensor\n                        Rotation angles in degrees.\n    offset            : torch.tensor\n                        Offset along XYZ axes.\n                        Expected dimension is [1 x 3] or offset for each triangle [m x 3].\n                        m here refers to `2 * number_of_meshes[0]` times  `number_of_meshes[1]`.\n    device            : torch.device\n                        Computational resource to be used (e.g., cpu, cuda).\n    heights           : torch.tensor\n                        Load surface heights from a tensor.\n    \"\"\"\n    self.device = device\n    self.angles = angles.to(self.device)\n    self.offset = offset.to(self.device)\n    self.size = size.to(self.device)\n    self.number_of_meshes = number_of_meshes.to(self.device)\n    self.init_heights(heights)\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.mesh.planar_mesh.get_squares","title":"<code>get_squares()</code>","text":"<p>Internal function to initiate squares over a plane.</p> <p>Returns:</p> <ul> <li> <code>squares</code> (              <code>tensor</code> )          \u2013            <p>Squares over a plane. Expected size is [m x n x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def get_squares(self):\n    \"\"\"\n    Internal function to initiate squares over a plane.\n\n    Returns\n    -------\n    squares     : torch.tensor\n                  Squares over a plane.\n                  Expected size is [m x n x 3].\n    \"\"\"\n    squares = torch.cat((\n                         self.X,\n                         self.Y,\n                         self.heights\n                        ), dim = -1)\n    return squares\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.mesh.planar_mesh.get_triangles","title":"<code>get_triangles()</code>","text":"<p>Internal function to get triangles.</p> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def get_triangles(self):\n    \"\"\"\n    Internal function to get triangles.\n    \"\"\" \n    squares = self.get_squares()\n    triangles = torch.zeros(2, self.number_of_meshes[0], self.number_of_meshes[1], 3, 3, device = self.device)\n    for i in range(0, self.number_of_meshes[0] - 1):\n        for j in range(0, self.number_of_meshes[1] - 1):\n            first_triangle = torch.cat((\n                                        squares[i + 1, j].unsqueeze(0),\n                                        squares[i + 1, j + 1].unsqueeze(0),\n                                        squares[i, j + 1].unsqueeze(0),\n                                       ), dim = 0)\n            second_triangle = torch.cat((\n                                         squares[i + 1, j].unsqueeze(0),\n                                         squares[i, j + 1].unsqueeze(0),\n                                         squares[i, j].unsqueeze(0),\n                                        ), dim = 0)\n            triangles[0, i, j], _, _, _ = rotate_points(first_triangle, angles = self.angles)\n            triangles[1, i, j], _, _, _ = rotate_points(second_triangle, angles = self.angles)\n    triangles = triangles.view(-1, 3, 3) + self.offset\n    return triangles \n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.mesh.planar_mesh.init_heights","title":"<code>init_heights(heights=None)</code>","text":"<p>Internal function to initialize a height map. Note that self.heights is a differentiable variable, and can be optimized or learned. See unit test <code>test/test_learn_ray_detector.py</code> or <code>test/test_learn_ray_mesh.py</code> as examples.</p> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def init_heights(self, heights = None):\n    \"\"\"\n    Internal function to initialize a height map.\n    Note that self.heights is a differentiable variable, and can be optimized or learned.\n    See unit test `test/test_learn_ray_detector.py` or `test/test_learn_ray_mesh.py` as examples.\n    \"\"\"\n    if not isinstance(heights, type(None)):\n        self.heights = heights.to(self.device)\n        self.heights.requires_grad = True\n    else:\n        self.heights = torch.zeros(\n                                   (self.number_of_meshes[0], self.number_of_meshes[1], 1),\n                                   requires_grad = True,\n                                   device = self.device,\n                                  )\n    x = torch.linspace(-self.size[0] / 2., self.size[0] / 2., self.number_of_meshes[0], device = self.device) \n    y = torch.linspace(-self.size[1] / 2., self.size[1] / 2., self.number_of_meshes[1], device = self.device)\n    X, Y = torch.meshgrid(x, y, indexing = 'ij')\n    self.X = X.unsqueeze(-1)\n    self.Y = Y.unsqueeze(-1)\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.mesh.planar_mesh.mirror","title":"<code>mirror(rays)</code>","text":"<p>Function to bounce light rays off the meshes.</p> <p>Parameters:</p> <ul> <li> <code>rays</code>           \u2013            <pre><code>            Rays to be bounced.\n            Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>reflected_rays</code> (              <code>tensor</code> )          \u2013            <p>Reflected rays. Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].</p> </li> <li> <code>reflected_normals</code> (              <code>tensor</code> )          \u2013            <p>Reflected normals. Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def mirror(self, rays):\n    \"\"\"\n    Function to bounce light rays off the meshes.\n\n    Parameters\n    ----------\n    rays              : torch.tensor\n                        Rays to be bounced.\n                        Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n\n    Returns\n    -------\n    reflected_rays    : torch.tensor\n                        Reflected rays.\n                        Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n    reflected_normals : torch.tensor\n                        Reflected normals.\n                        Expected size is [2 x 3], [1 x 2 x 3] or [m x 2 x 3].\n\n    \"\"\"\n    if len(rays.shape) == 2:\n        rays = rays.unsqueeze(0)\n    triangles = self.get_triangles()\n    reflected_rays = torch.empty((0, 2, 3), requires_grad = True, device = self.device)\n    reflected_normals = torch.empty((0, 2, 3), requires_grad = True, device = self.device)\n    for triangle in triangles:\n        _, _, intersecting_rays, intersecting_normals, check = intersect_w_triangle(\n                                                                                    rays,\n                                                                                    triangle\n                                                                                   ) \n        triangle_reflected_rays = reflect(intersecting_rays, intersecting_normals)\n        if triangle_reflected_rays.shape[0] &gt; 0:\n            reflected_rays = torch.cat((\n                                        reflected_rays,\n                                        triangle_reflected_rays\n                                      ))\n            reflected_normals = torch.cat((\n                                           reflected_normals,\n                                           intersecting_normals\n                                          ))\n    return reflected_rays, reflected_normals\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.mesh.planar_mesh.save_heights","title":"<code>save_heights(filename='heights.pt')</code>","text":"<p>Function to save heights to a file.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>            Filename.\n</code></pre> </li> </ul> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def save_heights(self, filename = 'heights.pt'):\n    \"\"\"\n    Function to save heights to a file.\n\n    Parameters\n    ----------\n    filename          : str\n                        Filename.\n    \"\"\"\n    save_torch_tensor(filename, self.heights.detach().clone())\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.mesh.planar_mesh.save_heights_as_PLY","title":"<code>save_heights_as_PLY(filename='mesh.ply')</code>","text":"<p>Function to save mesh to a PLY file.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>            Filename.\n</code></pre> </li> </ul> Source code in <code>odak/learn/raytracing/mesh.py</code> <pre><code>def save_heights_as_PLY(self, filename = 'mesh.ply'):\n    \"\"\"\n    Function to save mesh to a PLY file.\n\n    Parameters\n    ----------\n    filename          : str\n                        Filename.\n    \"\"\"\n    triangles = self.get_triangles()\n    write_PLY(triangles, filename)\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.primitives.center_of_triangle","title":"<code>center_of_triangle(triangle)</code>","text":"<p>Definition to calculate center of a triangle.</p> <p>Parameters:</p> <ul> <li> <code>triangle</code>           \u2013            <pre><code>        An array that contains three points defining a triangle (Mx3). \n        It can also parallel process many triangles (NxMx3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>centers</code> (              <code>tensor</code> )          \u2013            <p>Triangle centers.</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def center_of_triangle(triangle):\n    \"\"\"\n    Definition to calculate center of a triangle.\n\n    Parameters\n    ----------\n    triangle      : torch.tensor\n                    An array that contains three points defining a triangle (Mx3). \n                    It can also parallel process many triangles (NxMx3).\n\n    Returns\n    -------\n    centers       : torch.tensor\n                    Triangle centers.\n    \"\"\"\n    if len(triangle.shape) == 2:\n        triangle = triangle.view((1, 3, 3))\n    center = torch.mean(triangle, axis=1)\n    return center\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.primitives.define_circle","title":"<code>define_circle(center, radius, angles)</code>","text":"<p>Definition to describe a circle in a single variable packed form.</p> <p>Parameters:</p> <ul> <li> <code>center</code>           \u2013            <pre><code>  Center of a circle to be defined in 3D space.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>  Radius of a circle to be defined.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>  Angular tilt of a circle represented by rotations about x, y, and z axes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>circle</code> (              <code>list</code> )          \u2013            <p>Single variable packed form.</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def define_circle(center, radius, angles):\n    \"\"\"\n    Definition to describe a circle in a single variable packed form.\n\n    Parameters\n    ----------\n    center  : torch.Tensor\n              Center of a circle to be defined in 3D space.\n    radius  : float\n              Radius of a circle to be defined.\n    angles  : torch.Tensor\n              Angular tilt of a circle represented by rotations about x, y, and z axes.\n\n    Returns\n    ----------\n    circle  : list\n              Single variable packed form.\n    \"\"\"\n    points = define_plane(center, angles=angles)\n    circle = [\n        points,\n        center,\n        torch.tensor([radius])\n    ]\n    return circle\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.primitives.define_plane","title":"<code>define_plane(point, angles=torch.tensor([0.0, 0.0, 0.0]))</code>","text":"<p>Definition to generate a rotation matrix along X axis.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>       A point that is at the center of a plane.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>plane</code> (              <code>tensor</code> )          \u2013            <p>Points defining plane.</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def define_plane(point, angles = torch.tensor([0., 0., 0.])):\n    \"\"\" \n    Definition to generate a rotation matrix along X axis.\n\n    Parameters\n    ----------\n    point        : torch.tensor\n                   A point that is at the center of a plane.\n    angles       : torch.tensor\n                   Rotation angles in degrees.\n\n    Returns\n    ----------\n    plane        : torch.tensor\n                   Points defining plane.\n    \"\"\"\n    plane = torch.tensor([\n                          [10., 10., 0.],\n                          [0., 10., 0.],\n                          [0.,  0., 0.]\n                         ], device = point.device)\n    for i in range(0, plane.shape[0]):\n        plane[i], _, _, _ = rotate_points(plane[i], angles = angles.to(point.device))\n        plane[i] = plane[i] + point\n    return plane\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.primitives.define_plane_mesh","title":"<code>define_plane_mesh(number_of_meshes=[10, 10], size=[1.0, 1.0], angles=torch.tensor([0.0, 0.0, 0.0]), offset=torch.tensor([[0.0, 0.0, 0.0]]))</code>","text":"<p>Definition to generate a plane with meshes.</p> <p>Parameters:</p> <ul> <li> <code>number_of_meshes</code>           \u2013            <pre><code>            Number of squares over plane.\n            There are two triangles at each square.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>            Size of the plane.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>            Rotation angles in degrees.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>            Offset along XYZ axes.\n            Expected dimension is [1 x 3] or offset for each triangle [m x 3].\n            m here refers to `2 * number_of_meshes[0]` times  `number_of_meshes[1]`.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>triangles</code> (              <code>tensor</code> )          \u2013            <p>Triangles [m x 3 x 3], where m is <code>2 * number_of_meshes[0]</code> times  <code>number_of_meshes[1]</code>.</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def define_plane_mesh(\n                      number_of_meshes = [10, 10], \n                      size = [1., 1.], \n                      angles = torch.tensor([0., 0., 0.]), \n                      offset = torch.tensor([[0., 0., 0.]])\n                     ):\n    \"\"\"\n    Definition to generate a plane with meshes.\n\n\n    Parameters\n    -----------\n    number_of_meshes  : torch.tensor\n                        Number of squares over plane.\n                        There are two triangles at each square.\n    size              : list\n                        Size of the plane.\n    angles            : torch.tensor\n                        Rotation angles in degrees.\n    offset            : torch.tensor\n                        Offset along XYZ axes.\n                        Expected dimension is [1 x 3] or offset for each triangle [m x 3].\n                        m here refers to `2 * number_of_meshes[0]` times  `number_of_meshes[1]`. \n\n    Returns\n    -------\n    triangles         : torch.tensor\n                        Triangles [m x 3 x 3], where m is `2 * number_of_meshes[0]` times  `number_of_meshes[1]`.\n    \"\"\"\n    triangles = torch.zeros(2, number_of_meshes[0], number_of_meshes[1], 3, 3)\n    step = [size[0] / number_of_meshes[0], size[1] / number_of_meshes[1]]\n    for i in range(0, number_of_meshes[0] - 1):\n        for j in range(0, number_of_meshes[1] - 1):\n            first_triangle = torch.tensor([\n                                           [       -size[0] / 2. + step[0] * i,       -size[1] / 2. + step[0] * j, 0.],\n                                           [ -size[0] / 2. + step[0] * (i + 1),       -size[1] / 2. + step[0] * j, 0.],\n                                           [       -size[0] / 2. + step[0] * i, -size[1] / 2. + step[0] * (j + 1), 0.]\n                                          ])\n            second_triangle = torch.tensor([\n                                            [ -size[0] / 2. + step[0] * (i + 1), -size[1] / 2. + step[0] * (j + 1), 0.],\n                                            [ -size[0] / 2. + step[0] * (i + 1),       -size[1] / 2. + step[0] * j, 0.],\n                                            [       -size[0] / 2. + step[0] * i, -size[1] / 2. + step[0] * (j + 1), 0.]\n                                           ])\n            triangles[0, i, j], _, _, _ = rotate_points(first_triangle, angles = angles)\n            triangles[1, i, j], _, _, _ = rotate_points(second_triangle, angles = angles)\n    triangles = triangles.view(-1, 3, 3) + offset\n    return triangles\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.primitives.define_sphere","title":"<code>define_sphere(center=torch.tensor([[0.0, 0.0, 0.0]]), radius=torch.tensor([1.0]))</code>","text":"<p>Definition to define a sphere.</p> <p>Parameters:</p> <ul> <li> <code>center</code>           \u2013            <pre><code>      Center of the sphere(s) along XYZ axes.\n      Expected size is [3], [1, 3] or [m, 3].\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of that sphere(s).\n      Expected size is [1], [1, 1], [m] or [m, 1].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>parameters</code> (              <code>tensor</code> )          \u2013            <p>Parameters of defined sphere(s). Expected size is [1, 3] or [m x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def define_sphere(center = torch.tensor([[0., 0., 0.]]), radius = torch.tensor([1.])):\n    \"\"\"\n    Definition to define a sphere.\n\n    Parameters\n    ----------\n    center      : torch.tensor\n                  Center of the sphere(s) along XYZ axes.\n                  Expected size is [3], [1, 3] or [m, 3].\n    radius      : torch.tensor\n                  Radius of that sphere(s).\n                  Expected size is [1], [1, 1], [m] or [m, 1].\n\n    Returns\n    -------\n    parameters  : torch.tensor\n                  Parameters of defined sphere(s).\n                  Expected size is [1, 3] or [m x 3].\n    \"\"\"\n    if len(radius.shape) == 1:\n        radius = radius.unsqueeze(0)\n    if len(center.shape) == 1:\n        center = center.unsqueeze(1)\n    parameters = torch.cat((center, radius), dim = 1)\n    return parameters\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.primitives.is_it_on_triangle","title":"<code>is_it_on_triangle(point_to_check, triangle)</code>","text":"<p>Definition to check if a given point is inside a triangle.  If the given point is inside a defined triangle, this definition returns True. For more details, visit: https://blackpawn.com/texts/pointinpoly/.</p> <p>Parameters:</p> <ul> <li> <code>point_to_check</code>           \u2013            <pre><code>          Point(s) to check.\n          Expected size is [3], [1 x 3] or [m x 3].\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>          Triangle described with three points.\n          Expected size is [3 x 3], [1 x 3 x 3] or [m x 3 x3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Is it on a triangle? Returns NaN if condition not satisfied. Expected size is [1] or [m] depending on the input.</p> </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def is_it_on_triangle(point_to_check, triangle):\n    \"\"\"\n    Definition to check if a given point is inside a triangle. \n    If the given point is inside a defined triangle, this definition returns True.\n    For more details, visit: [https://blackpawn.com/texts/pointinpoly/](https://blackpawn.com/texts/pointinpoly/).\n\n    Parameters\n    ----------\n    point_to_check  : torch.tensor\n                      Point(s) to check.\n                      Expected size is [3], [1 x 3] or [m x 3].\n    triangle        : torch.tensor\n                      Triangle described with three points.\n                      Expected size is [3 x 3], [1 x 3 x 3] or [m x 3 x3].\n\n    Returns\n    -------\n    result          : torch.tensor\n                      Is it on a triangle? Returns NaN if condition not satisfied.\n                      Expected size is [1] or [m] depending on the input.\n    \"\"\"\n    if len(point_to_check.shape) == 1:\n        point_to_check = point_to_check.unsqueeze(0)\n    if len(triangle.shape) == 2:\n        triangle = triangle.unsqueeze(0)\n    v0 = triangle[:, 2] - triangle[:, 0]\n    v1 = triangle[:, 1] - triangle[:, 0]\n    v2 = point_to_check - triangle[:, 0]\n    if len(v0.shape) == 1:\n        v0 = v0.unsqueeze(0)\n    if len(v1.shape) == 1:\n        v1 = v1.unsqueeze(0)\n    if len(v2.shape) == 1:\n        v2 = v2.unsqueeze(0)\n    dot00 = torch.mm(v0, v0.T)\n    dot01 = torch.mm(v0, v1.T)\n    dot02 = torch.mm(v0, v2.T) \n    dot11 = torch.mm(v1, v1.T)\n    dot12 = torch.mm(v1, v2.T)\n    invDenom = 1. / (dot00 * dot11 - dot01 * dot01)\n    u = (dot11 * dot02 - dot01 * dot12) * invDenom\n    v = (dot00 * dot12 - dot01 * dot02) * invDenom\n    result = (u &gt;= 0.) &amp; (v &gt;= 0.) &amp; ((u + v) &lt; 1)\n    return result\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.primitives.is_it_on_triangle_batch","title":"<code>is_it_on_triangle_batch(point_to_check, triangle)</code>","text":"<p>Definition to check if given points are inside triangles. If the given points are inside defined triangles, this definition returns True.</p> <p>Parameters:</p> <ul> <li> <code>point_to_check</code>           \u2013            <pre><code>          Points to check (m x n x 3).\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>          Triangles (m x 3 x 3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>torch.tensor (m x n)</code> )          \u2013            </li> </ul> Source code in <code>odak/learn/raytracing/primitives.py</code> <pre><code>def is_it_on_triangle_batch(point_to_check, triangle):\n    \"\"\"\n    Definition to check if given points are inside triangles. If the given points are inside defined triangles, this definition returns True.\n\n    Parameters\n    ----------\n    point_to_check  : torch.tensor\n                      Points to check (m x n x 3).\n    triangle        : torch.tensor \n                      Triangles (m x 3 x 3).\n\n    Returns\n    ----------\n    result          : torch.tensor (m x n)\n\n    \"\"\"\n    if len(point_to_check.shape) == 1:\n        point_to_check = point_to_check.unsqueeze(0)\n    if len(triangle.shape) == 2:\n        triangle = triangle.unsqueeze(0)\n    v0 = triangle[:, 2] - triangle[:, 0]\n    v1 = triangle[:, 1] - triangle[:, 0]\n    v2 = point_to_check - triangle[:, None, 0]\n    if len(v0.shape) == 1:\n        v0 = v0.unsqueeze(0)\n    if len(v1.shape) == 1:\n        v1 = v1.unsqueeze(0)\n    if len(v2.shape) == 1:\n        v2 = v2.unsqueeze(0)\n\n    dot00 = torch.bmm(v0.unsqueeze(1), v0.unsqueeze(1).permute(0, 2, 1)).squeeze(1)\n    dot01 = torch.bmm(v0.unsqueeze(1), v1.unsqueeze(1).permute(0, 2, 1)).squeeze(1)\n    dot02 = torch.bmm(v0.unsqueeze(1), v2.permute(0, 2, 1)).squeeze(1)\n    dot11 = torch.bmm(v1.unsqueeze(1), v1.unsqueeze(1).permute(0, 2, 1)).squeeze(1)\n    dot12 = torch.bmm(v1.unsqueeze(1), v2.permute(0, 2, 1)).squeeze(1)\n    invDenom = 1. / (dot00 * dot11 - dot01 * dot01)\n    u = (dot11 * dot02 - dot01 * dot12) * invDenom\n    v = (dot00 * dot12 - dot01 * dot02) * invDenom\n    result = (u &gt;= 0.) &amp; (v &gt;= 0.) &amp; ((u + v) &lt; 1)\n\n    return result\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.ray.create_ray","title":"<code>create_ray(xyz, abg, direction=False)</code>","text":"<p>Definition to create a ray.</p> <p>Parameters:</p> <ul> <li> <code>xyz</code>           \u2013            <pre><code>       List that contains X,Y and Z start locations of a ray.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> <li> <code>abg</code>           \u2013            <pre><code>       List that contains angles in degrees with respect to the X,Y and Z axes.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> <li> <code>direction</code>           \u2013            <pre><code>       If set to True, cosines of `abg` is not calculated.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray. Size will be either [1 x 3] or [m x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray(xyz, abg, direction = False):\n    \"\"\"\n    Definition to create a ray.\n\n    Parameters\n    ----------\n    xyz          : torch.tensor\n                   List that contains X,Y and Z start locations of a ray.\n                   Size could be [1 x 3], [3], [m x 3].\n    abg          : torch.tensor\n                   List that contains angles in degrees with respect to the X,Y and Z axes.\n                   Size could be [1 x 3], [3], [m x 3].\n    direction    : bool\n                   If set to True, cosines of `abg` is not calculated.\n\n    Returns\n    ----------\n    ray          : torch.tensor\n                   Array that contains starting points and cosines of a created ray.\n                   Size will be either [1 x 3] or [m x 3].\n    \"\"\"\n    points = xyz\n    angles = abg\n    if len(xyz) == 1:\n        points = xyz.unsqueeze(0)\n    if len(abg) == 1:\n        angles = abg.unsqueeze(0)\n    ray = torch.zeros(points.shape[0], 2, 3, device = points.device)\n    ray[:, 0] = points\n    if direction:\n        ray[:, 1] = abg\n    else:\n        ray[:, 1] = torch.cos(torch.deg2rad(abg))\n    return ray\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.ray.create_ray_from_all_pairs","title":"<code>create_ray_from_all_pairs(x0y0z0, x1y1z1)</code>","text":"<p>Creates rays from all possible pairs of points in x0y0z0 and x1y1z1.</p> <p>Parameters:</p> <ul> <li> <code>x0y0z0</code>           \u2013            <pre><code>       Tensor that contains X, Y, and Z start locations of rays.\n       Size should be [m x 3].\n</code></pre> </li> <li> <code>x1y1z1</code>           \u2013            <pre><code>       Tensor that contains X, Y, and Z end locations of rays.\n       Size should be [n x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rays</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray(s). Size of [n*m x 2 x 3]</p> </li> </ul> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray_from_all_pairs(x0y0z0, x1y1z1):\n    \"\"\"\n    Creates rays from all possible pairs of points in x0y0z0 and x1y1z1.\n\n    Parameters\n    ----------\n    x0y0z0       : torch.tensor\n                   Tensor that contains X, Y, and Z start locations of rays.\n                   Size should be [m x 3].\n    x1y1z1       : torch.tensor\n                   Tensor that contains X, Y, and Z end locations of rays.\n                   Size should be [n x 3].\n\n    Returns\n    ----------\n    rays         : torch.tensor\n                   Array that contains starting points and cosines of a created ray(s). Size of [n*m x 2 x 3]\n    \"\"\"\n\n    if len(x0y0z0.shape) == 1:\n        x0y0z0 = x0y0z0.unsqueeze(0)\n    if len(x1y1z1.shape) == 1:\n        x1y1z1 = x1y1z1.unsqueeze(0)\n\n    m, n = x0y0z0.shape[0], x1y1z1.shape[0]\n    start_points = x0y0z0.unsqueeze(1).expand(-1, n, -1).reshape(-1, 3)\n    end_points = x1y1z1.unsqueeze(0).expand(m, -1, -1).reshape(-1, 3)\n\n    directions = end_points - start_points\n    norms = torch.norm(directions, p=2, dim=1, keepdim=True)\n    norms[norms == 0] = float('nan')\n\n    normalized_directions = directions / norms\n\n    rays = torch.zeros(m * n, 2, 3, device=x0y0z0.device)\n    rays[:, 0, :] = start_points\n    rays[:, 1, :] = normalized_directions\n\n    return rays\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.ray.create_ray_from_grid_w_luminous_angle","title":"<code>create_ray_from_grid_w_luminous_angle(center, size, no, tilt, num_ray_per_light, angle_limit)</code>","text":"<p>Generate a 2D array of lights, each emitting rays within a specified solid angle and tilt.</p> Parameters: <p>center              : torch.tensor                       The center point of the light array, shape [3]. size                : list[int]                       The size of the light array [height, width] no                  : list[int]                       The number of the light arary [number of lights in height , number of lights inwidth] tilt                : torch.tensor                       The tilt angles in degrees along x, y, z axes for the rays, shape [3]. angle_limit         : float                       The maximum angle in degrees from the initial direction vector within which to emit rays. num_rays_per_light  : int                       The number of rays each light should emit.</p> Returns: <p>rays : torch.tensor        Array that contains starting points and cosines of a created ray(s). Size of [n x 2 x 3]</p> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray_from_grid_w_luminous_angle(center, size, no, tilt, num_ray_per_light, angle_limit):\n    \"\"\"\n    Generate a 2D array of lights, each emitting rays within a specified solid angle and tilt.\n\n    Parameters:\n    ----------\n    center              : torch.tensor\n                          The center point of the light array, shape [3].\n    size                : list[int]\n                          The size of the light array [height, width]\n    no                  : list[int]\n                          The number of the light arary [number of lights in height , number of lights inwidth]\n    tilt                : torch.tensor\n                          The tilt angles in degrees along x, y, z axes for the rays, shape [3].\n    angle_limit         : float\n                          The maximum angle in degrees from the initial direction vector within which to emit rays.\n    num_rays_per_light  : int\n                          The number of rays each light should emit.\n\n    Returns:\n    ----------\n    rays : torch.tensor\n           Array that contains starting points and cosines of a created ray(s). Size of [n x 2 x 3]\n    \"\"\"\n\n    samples = torch.zeros((no[0], no[1], 3))\n\n    x = torch.linspace(-size[0] / 2., size[0] / 2., no[0])\n    y = torch.linspace(-size[1] / 2., size[1] / 2., no[1])\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n\n    samples[:, :, 0] = X.detach().clone()\n    samples[:, :, 1] = Y.detach().clone()\n    samples = samples.reshape((no[0]*no[1], 3))\n\n    samples, *_ = rotate_points(samples, angles=tilt)\n\n    samples = samples + center\n    angle_limit = torch.as_tensor(angle_limit)\n    cos_alpha = torch.cos(angle_limit * torch.pi / 180)\n    tilt = tilt * torch.pi / 180\n\n    theta = torch.acos(1 - 2 * torch.rand(num_ray_per_light*samples.size(0)) * (1-cos_alpha))\n    phi = 2 * torch.pi * torch.rand(num_ray_per_light*samples.size(0))  \n\n    directions = torch.stack([\n        torch.sin(theta) * torch.cos(phi),  \n        torch.sin(theta) * torch.sin(phi),  \n        torch.cos(theta)                    \n    ], dim=1)\n\n    c, s = torch.cos(tilt), torch.sin(tilt)\n\n    Rx = torch.tensor([\n        [1, 0, 0],\n        [0, c[0], -s[0]],\n        [0, s[0], c[0]]\n    ])\n\n    Ry = torch.tensor([\n        [c[1], 0, s[1]],\n        [0, 1, 0],\n        [-s[1], 0, c[1]]\n    ])\n\n    Rz = torch.tensor([\n        [c[2], -s[2], 0],\n        [s[2], c[2], 0],\n        [0, 0, 1]\n    ])\n\n    origins = samples.repeat(num_ray_per_light, 1)\n\n    directions = torch.matmul(directions, (Rz@Ry@Rx).T)\n\n\n    rays = torch.zeros(num_ray_per_light*samples.size(0), 2, 3)\n    rays[:, 0, :] = origins\n    rays[:, 1, :] = directions\n\n    return rays\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.ray.create_ray_from_point_w_luminous_angle","title":"<code>create_ray_from_point_w_luminous_angle(origin, num_ray, tilt, angle_limit)</code>","text":"<p>Generate rays from a point, tilted by specific angles along x, y, z axes, within a specified solid angle.</p> Parameters: <p>origin      : torch.tensor               The origin point of the rays, shape [3]. num_rays    : int               The total number of rays to generate. tilt        : torch.tensor               The tilt angles in degrees along x, y, z axes, shape [3]. angle_limit : float               The maximum angle in degrees from the initial direction vector within which to emit rays.</p> Returns: <p>rays : torch.tensor        Array that contains starting points and cosines of a created ray(s). Size of [n x 2 x 3]</p> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray_from_point_w_luminous_angle(origin, num_ray, tilt, angle_limit):\n    \"\"\"\n    Generate rays from a point, tilted by specific angles along x, y, z axes, within a specified solid angle.\n\n    Parameters:\n    ----------\n    origin      : torch.tensor\n                  The origin point of the rays, shape [3].\n    num_rays    : int\n                  The total number of rays to generate.\n    tilt        : torch.tensor\n                  The tilt angles in degrees along x, y, z axes, shape [3].\n    angle_limit : float\n                  The maximum angle in degrees from the initial direction vector within which to emit rays.\n\n    Returns:\n    ----------\n    rays : torch.tensor\n           Array that contains starting points and cosines of a created ray(s). Size of [n x 2 x 3]\n    \"\"\"\n    angle_limit = torch.as_tensor(angle_limit) \n    cos_alpha = torch.cos(angle_limit * torch.pi / 180)\n    tilt = tilt * torch.pi / 180\n\n    theta = torch.acos(1 - 2 * torch.rand(num_ray) * (1-cos_alpha))\n    phi = 2 * torch.pi * torch.rand(num_ray)  \n\n\n    directions = torch.stack([\n        torch.sin(theta) * torch.cos(phi),  \n        torch.sin(theta) * torch.sin(phi),  \n        torch.cos(theta)                    \n    ], dim=1)\n\n    c, s = torch.cos(tilt), torch.sin(tilt)\n\n    Rx = torch.tensor([\n        [1, 0, 0],\n        [0, c[0], -s[0]],\n        [0, s[0], c[0]]\n    ])\n\n    Ry = torch.tensor([\n        [c[1], 0, s[1]],\n        [0, 1, 0],\n        [-s[1], 0, c[1]]\n    ])\n\n    Rz = torch.tensor([\n        [c[2], -s[2], 0],\n        [s[2], c[2], 0],\n        [0, 0, 1]\n    ])\n\n    origins = origin.repeat(num_ray, 1)\n    directions = torch.matmul(directions, (Rz@Ry@Rx).T)\n\n\n    rays = torch.zeros(num_ray, 2, 3)\n    rays[:, 0, :] = origins\n    rays[:, 1, :] = directions\n\n    return rays\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.ray.create_ray_from_two_points","title":"<code>create_ray_from_two_points(x0y0z0, x1y1z1)</code>","text":"<p>Definition to create a ray from two given points. Note that both inputs must match in shape.</p> <p>Parameters:</p> <ul> <li> <code>x0y0z0</code>           \u2013            <pre><code>       List that contains X,Y and Z start locations of a ray.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> <li> <code>x1y1z1</code>           \u2013            <pre><code>       List that contains X,Y and Z ending locations of a ray or batch of rays.\n       Size could be [1 x 3], [3], [m x 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray(s).</p> </li> </ul> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def create_ray_from_two_points(x0y0z0, x1y1z1):\n    \"\"\"\n    Definition to create a ray from two given points. Note that both inputs must match in shape.\n\n    Parameters\n    ----------\n    x0y0z0       : torch.tensor\n                   List that contains X,Y and Z start locations of a ray.\n                   Size could be [1 x 3], [3], [m x 3].\n    x1y1z1       : torch.tensor\n                   List that contains X,Y and Z ending locations of a ray or batch of rays.\n                   Size could be [1 x 3], [3], [m x 3].\n\n    Returns\n    ----------\n    ray          : torch.tensor\n                   Array that contains starting points and cosines of a created ray(s).\n    \"\"\"\n    if len(x0y0z0.shape) == 1:\n        x0y0z0 = x0y0z0.unsqueeze(0)\n    if len(x1y1z1.shape) == 1:\n        x1y1z1 = x1y1z1.unsqueeze(0)\n    xdiff = x1y1z1[:, 0] - x0y0z0[:, 0]\n    ydiff = x1y1z1[:, 1] - x0y0z0[:, 1]\n    zdiff = x1y1z1[:, 2] - x0y0z0[:, 2]\n    s = (xdiff ** 2 + ydiff ** 2 + zdiff ** 2) ** 0.5\n    s[s == 0] = float('nan')\n    cosines = torch.zeros_like(x0y0z0 * x1y1z1)\n    cosines[:, 0] = xdiff / s\n    cosines[:, 1] = ydiff / s\n    cosines[:, 2] = zdiff / s\n    ray = torch.zeros(xdiff.shape[0], 2, 3, device = x0y0z0.device)\n    ray[:, 0] = x0y0z0\n    ray[:, 1] = cosines\n    return ray\n</code></pre>"},{"location":"odak/learn_raytracing/#odak.learn.raytracing.ray.propagate_ray","title":"<code>propagate_ray(ray, distance)</code>","text":"<p>Definition to propagate a ray at a certain given distance.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>     A ray with a size of [2 x 3], [1 x 2 x 3] or a batch of rays with [m x 2 x 3].\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>     Distance with a size of [1], [1, m] or distances with a size of [m], [1, m].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_ray</code> (              <code>tensor</code> )          \u2013            <p>Propagated ray with a size of [1 x 2 x 3] or batch of rays with [m x 2 x 3].</p> </li> </ul> Source code in <code>odak/learn/raytracing/ray.py</code> <pre><code>def propagate_ray(ray, distance):\n    \"\"\"\n    Definition to propagate a ray at a certain given distance.\n\n    Parameters\n    ----------\n    ray        : torch.tensor\n                 A ray with a size of [2 x 3], [1 x 2 x 3] or a batch of rays with [m x 2 x 3].\n    distance   : torch.tensor\n                 Distance with a size of [1], [1, m] or distances with a size of [m], [1, m].\n\n    Returns\n    ----------\n    new_ray    : torch.tensor\n                 Propagated ray with a size of [1 x 2 x 3] or batch of rays with [m x 2 x 3].\n    \"\"\"\n    if len(ray.shape) == 2:\n        ray = ray.unsqueeze(0)\n    if len(distance.shape) == 2:\n        distance = distance.squeeze(-1)\n    new_ray = torch.zeros_like(ray)\n    new_ray[:, 0, 0] = distance * ray[:, 1, 0] + ray[:, 0, 0]\n    new_ray[:, 0, 1] = distance * ray[:, 1, 1] + ray[:, 0, 1]\n    new_ray[:, 0, 2] = distance * ray[:, 1, 2] + ray[:, 0, 2]\n    return new_ray\n</code></pre>"},{"location":"odak/learn_tools/","title":"odak.learn.tools","text":"<p><code>odak.learn.tools</code></p> <p>Provides necessary definitions for general tools used across the library.</p>"},{"location":"odak/learn_tools/#odak.learn.tools.blur_gaussian","title":"<code>blur_gaussian(field, kernel_length=[21, 21], nsigma=[3, 3], padding='same')</code>","text":"<p>A definition to blur a field using a Gaussian kernel.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>        MxN field.\n</code></pre> </li> <li> <code>kernel_length</code>               (<code>list</code>, default:                   <code>[21, 21]</code> )           \u2013            <pre><code>        Length of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>nsigma</code>           \u2013            <pre><code>        Sigma of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>padding</code>           \u2013            <pre><code>        Padding value, see torch.nn.functional.conv2d() for more.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>blurred_field</code> (              <code>tensor</code> )          \u2013            <p>Blurred field.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def blur_gaussian(field, kernel_length = [21, 21], nsigma = [3, 3], padding = 'same'):\n    \"\"\"\n    A definition to blur a field using a Gaussian kernel.\n\n    Parameters\n    ----------\n    field         : torch.tensor\n                    MxN field.\n    kernel_length : list\n                    Length of the Gaussian kernel along X and Y axes.\n    nsigma        : list\n                    Sigma of the Gaussian kernel along X and Y axes.\n    padding       : int or string\n                    Padding value, see torch.nn.functional.conv2d() for more.\n\n    Returns\n    ----------\n    blurred_field : torch.tensor\n                    Blurred field.\n    \"\"\"\n    kernel = generate_2d_gaussian(kernel_length, nsigma).to(field.device)\n    kernel = kernel.unsqueeze(0).unsqueeze(0)\n    if len(field.shape) == 2:\n        field = field.view(1, 1, field.shape[-2], field.shape[-1])\n    blurred_field = torch.nn.functional.conv2d(field, kernel, padding='same')\n    if field.shape[1] == 1:\n        blurred_field = blurred_field.view(\n                                           blurred_field.shape[-2],\n                                           blurred_field.shape[-1]\n                                          )\n    return blurred_field\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.circular_binary_mask","title":"<code>circular_binary_mask(px, py, r)</code>","text":"<p>Definition to generate a 2D circular binary mask.</p> Parameter <p>px           : int                Pixel count in x. py           : int                Pixel count in y. r            : int                Radius of the circle.</p> <p>Returns:</p> <ul> <li> <code>mask</code> (              <code>tensor</code> )          \u2013            <p>Mask [1 x 1 x m x n].</p> </li> </ul> Source code in <code>odak/learn/tools/mask.py</code> <pre><code>def circular_binary_mask(px, py, r):\n    \"\"\"\n    Definition to generate a 2D circular binary mask.\n\n    Parameter\n    ---------\n    px           : int\n                   Pixel count in x.\n    py           : int\n                   Pixel count in y.\n    r            : int\n                   Radius of the circle.\n\n    Returns\n    -------\n    mask         : torch.tensor\n                   Mask [1 x 1 x m x n].\n    \"\"\"\n    x = torch.linspace(-px / 2., px / 2., px)\n    y = torch.linspace(-py / 2., py / 2., py)\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    Z = (X ** 2 + Y ** 2) ** 0.5\n    mask = torch.zeros_like(Z)\n    mask[Z &lt; r] = 1\n    return mask\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.convolve2d","title":"<code>convolve2d(field, kernel)</code>","text":"<p>Definition to convolve a field with a kernel by multiplying in frequency space.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field with MxN shape.\n</code></pre> </li> <li> <code>kernel</code>           \u2013            <pre><code>      Input kernel with MxN shape.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>tensor</code> )          \u2013            <p>Convolved field.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def convolve2d(field, kernel):\n    \"\"\"\n    Definition to convolve a field with a kernel by multiplying in frequency space.\n\n    Parameters\n    ----------\n    field       : torch.tensor\n                  Input field with MxN shape.\n    kernel      : torch.tensor\n                  Input kernel with MxN shape.\n\n    Returns\n    ----------\n    new_field   : torch.tensor\n                  Convolved field.\n    \"\"\"\n    fr = torch.fft.fft2(field)\n    fr2 = torch.fft.fft2(torch.flip(torch.flip(kernel, [1, 0]), [0, 1]))\n    m, n = fr.shape\n    new_field = torch.real(torch.fft.ifft2(fr*fr2))\n    new_field = torch.roll(new_field, shifts=(int(n/2+1), 0), dims=(1, 0))\n    new_field = torch.roll(new_field, shifts=(int(m/2+1), 0), dims=(0, 1))\n    return new_field\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.correlation_2d","title":"<code>correlation_2d(first_tensor, second_tensor)</code>","text":"<p>Definition to calculate the correlation between two tensors.</p> <p>Parameters:</p> <ul> <li> <code>first_tensor</code>           \u2013            <pre><code>        First tensor.\n</code></pre> </li> <li> <code>second_tensor</code>               (<code>tensor</code>)           \u2013            <pre><code>        Second tensor.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>correlation</code> (              <code>tensor</code> )          \u2013            <p>Correlation between the two tensors.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def correlation_2d(first_tensor, second_tensor):\n    \"\"\"\n    Definition to calculate the correlation between two tensors.\n\n    Parameters\n    ----------\n    first_tensor  : torch.tensor\n                    First tensor.\n    second_tensor : torch.tensor\n                    Second tensor.\n\n    Returns\n    ----------\n    correlation   : torch.tensor\n                    Correlation between the two tensors.\n    \"\"\"\n    fft_first_tensor = (torch.fft.fft2(first_tensor))\n    fft_second_tensor = (torch.fft.fft2(second_tensor))\n    conjugate_second_tensor = torch.conj(fft_second_tensor)\n    result = torch.fft.ifftshift(torch.fft.ifft2(fft_first_tensor * conjugate_second_tensor))\n    return result\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.crop_center","title":"<code>crop_center(field, size=None)</code>","text":"<p>Definition to crop the center of a field with 2Mx2N size. The outcome is a MxN array.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field 2M x 2N or K x L x 2M x 2N or K x 2M x 2N x L array.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Dimensions to crop with respect to center of the image (e.g., M x N or 1 x 1 x M x N).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>cropped</code> (              <code>ndarray</code> )          \u2013            <p>Cropped version of the input field.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def crop_center(field, size = None):\n    \"\"\"\n    Definition to crop the center of a field with 2Mx2N size. The outcome is a MxN array.\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field 2M x 2N or K x L x 2M x 2N or K x 2M x 2N x L array.\n    size        : list\n                  Dimensions to crop with respect to center of the image (e.g., M x N or 1 x 1 x M x N).\n\n    Returns\n    ----------\n    cropped     : ndarray\n                  Cropped version of the input field.\n    \"\"\"\n    orig_resolution = field.shape\n    if len(field.shape) &lt; 3:\n        field = field.unsqueeze(0)\n    if len(field.shape) &lt; 4:\n        field = field.unsqueeze(0)\n    permute_flag = False\n    if field.shape[-1] &lt; 5:\n        permute_flag = True\n        field = field.permute(0, 3, 1, 2)\n    if type(size) == type(None):\n        qx = int(field.shape[-2] // 4)\n        qy = int(field.shape[-1] // 4)\n        cropped_padded = field[:, :, qx: qx + field.shape[-2] // 2, qy:qy + field.shape[-1] // 2]\n    else:\n        cx = int(field.shape[-2] // 2)\n        cy = int(field.shape[-1] // 2)\n        hx = int(size[-2] // 2)\n        hy = int(size[-1] // 2)\n        cropped_padded = field[:, :, cx-hx:cx+hx, cy-hy:cy+hy]\n    cropped = cropped_padded\n    if permute_flag:\n        cropped = cropped.permute(0, 2, 3, 1)\n    if len(orig_resolution) == 2:\n        cropped = cropped_padded.squeeze(0).squeeze(0)\n    if len(orig_resolution) == 3:\n        cropped = cropped_padded.squeeze(0)\n    return cropped\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.cross_product","title":"<code>cross_product(vector1, vector2)</code>","text":"<p>Definition to cross product two vectors and return the resultant vector. Used method described under: http://en.wikipedia.org/wiki/Cross_product</p> <p>Parameters:</p> <ul> <li> <code>vector1</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>vector2</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray.</p> </li> </ul> Source code in <code>odak/learn/tools/vector.py</code> <pre><code>def cross_product(vector1, vector2):\n    \"\"\"\n    Definition to cross product two vectors and return the resultant vector. Used method described under: http://en.wikipedia.org/wiki/Cross_product\n\n    Parameters\n    ----------\n    vector1      : torch.tensor\n                   A vector/ray.\n    vector2      : torch.tensor\n                   A vector/ray.\n\n    Returns\n    ----------\n    ray          : torch.tensor\n                   Array that contains starting points and cosines of a created ray.\n    \"\"\"\n    angle = torch.cross(vector1[1].T, vector2[1].T)\n    angle = torch.tensor(angle)\n    ray = torch.tensor([vector1[0], angle], dtype=torch.float32)\n    return ray\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.distance_between_two_points","title":"<code>distance_between_two_points(point1, point2)</code>","text":"<p>Definition to calculate distance between two given points.</p> <p>Parameters:</p> <ul> <li> <code>point1</code>           \u2013            <pre><code>      First point in X,Y,Z.\n</code></pre> </li> <li> <code>point2</code>           \u2013            <pre><code>      Second point in X,Y,Z.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>Tensor</code> )          \u2013            <p>Distance in between given two points.</p> </li> </ul> Source code in <code>odak/learn/tools/vector.py</code> <pre><code>def distance_between_two_points(point1, point2):\n    \"\"\"\n    Definition to calculate distance between two given points.\n\n    Parameters\n    ----------\n    point1      : torch.Tensor\n                  First point in X,Y,Z.\n    point2      : torch.Tensor\n                  Second point in X,Y,Z.\n\n    Returns\n    ----------\n    distance    : torch.Tensor\n                  Distance in between given two points.\n    \"\"\"\n    point1 = torch.tensor(point1) if not isinstance(point1, torch.Tensor) else point1\n    point2 = torch.tensor(point2) if not isinstance(point2, torch.Tensor) else point2\n\n    if len(point1.shape) == 1 and len(point2.shape) == 1:\n        distance = torch.sqrt(torch.sum((point1 - point2) ** 2))\n    elif len(point1.shape) == 2 or len(point2.shape) == 2:\n        distance = torch.sqrt(torch.sum((point1 - point2) ** 2, dim=-1))\n\n    return distance\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.evaluate_3d_gaussians","title":"<code>evaluate_3d_gaussians(points, centers, sigmas, angles)</code>","text":"<p>Evaluate 3D Gaussian functions at given points, with optional rotation.</p> <p>Parameters:</p> <ul> <li> <code>points</code>               (<code>(Tensor, shape[n, 3])</code>)           \u2013            <p>The 3D points at which to evaluate the Gaussians.</p> </li> <li> <code>centers</code>               (<code>(Tensor, shape[n, 3])</code>)           \u2013            <p>The centers of the Gaussians.</p> </li> <li> <code>sigmas</code>               (<code>(Tensor, shape[n, 3])</code>)           \u2013            <p>The standard deviations (spread) of the Gaussians along each axis.</p> </li> <li> <code>angles</code>               (<code>(Tensor, shape[n, 3])</code>)           \u2013            <p>The rotation angles (in radians) for each Gaussian, applied to the points.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>(Tensor, shape[n])</code>           \u2013            <p>The evaluated Gaussian intensities at each point.</p> </li> </ul> Notes <ul> <li>The function first rotates the points according to the given angles and centers.</li> <li>The Gaussian is evaluated as:   .. math::      I = \\frac{1}{\\sigma_x \\sigma_y \\sigma_z (2\\pi)^{3/2}}          \\exp\\left(-\\frac{1}{2}\\sum_{i=1}^3 \\left(\\frac{x_i'}{\\sigma_i}\\right)^2\\right)   where \\(\\(x'\\)\\) are the rotated points.</li> <li>If <code>sigmas</code> has more than one dimension, it is reshaped appropriately.</li> </ul> Source code in <code>odak/learn/tools/function.py</code> <pre><code>def evaluate_3d_gaussians(\n                          points: torch.Tensor,\n                          centers: torch.Tensor,\n                          sigmas: torch.Tensor,\n                          angles: torch.Tensor,\n                         ) -&gt; torch.Tensor:\n    \"\"\"\n    Evaluate 3D Gaussian functions at given points, with optional rotation.\n\n    Parameters\n    ----------\n    points : torch.Tensor, shape [n, 3]\n        The 3D points at which to evaluate the Gaussians.\n    centers : torch.Tensor, shape [n, 3]\n        The centers of the Gaussians.\n    sigmas : torch.Tensor, shape [n, 3]\n        The standard deviations (spread) of the Gaussians along each axis.\n    angles : torch.Tensor, shape [n, 3]\n        The rotation angles (in radians) for each Gaussian, applied to the points.\n\n    Returns\n    -------\n    torch.Tensor, shape [n]\n        The evaluated Gaussian intensities at each point.\n\n    Notes\n    -----\n    - The function first rotates the points according to the given angles and centers.\n    - The Gaussian is evaluated as:\n      .. math::\n         I = \\\\frac{1}{\\\\sigma_x \\\\sigma_y \\\\sigma_z (2\\\\pi)^{3/2}}\n             \\\\exp\\\\left(-\\\\frac{1}{2}\\\\sum_{i=1}^3 \\\\left(\\\\frac{x_i'}{\\\\sigma_i}\\\\right)^2\\\\right)\n      where $\\\\(x'\\\\)$ are the rotated points.\n    - If `sigmas` has more than one dimension, it is reshaped appropriately.\n    \"\"\"\n    points_rotated, _, _, _ = rotate_points(\n                                            point = points,\n                                            angles = angles,\n                                            offset = centers\n                                           )\n    if sigmas.shape[0] &gt; 1:\n        sigmas = sigmas.unsqueeze(1)\n    exponent = torch.sum(-0.5 * (points_rotated / sigmas) ** 2, dim = -1)\n    if len(sigmas.shape) == 3:\n        sigmas = sigmas.squeeze(1)\n    divider = (sigmas[:, 0] * sigmas[:, 1] * sigmas[:, 2]) * (2. * torch.pi) ** (3. / 2.)\n    divider = divider.unsqueeze(-1)\n    exponential = torch.exp(exponent)\n    intensities = exponential / divider\n    return intensities\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.expanduser","title":"<code>expanduser(filename)</code>","text":"<p>Definition to decode filename using namespaces and shortcuts.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>        Filename.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_filename</code> (              <code>str</code> )          \u2013            <p>Filename.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def expanduser(filename):\n    \"\"\"\n    Definition to decode filename using namespaces and shortcuts.\n\n\n    Parameters\n    ----------\n    filename      : str\n                    Filename.\n\n\n    Returns\n    -------\n    new_filename  : str\n                    Filename.\n    \"\"\"\n    new_filename = os.path.expanduser(filename)\n    return new_filename\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.freeze","title":"<code>freeze(model)</code>","text":"<p>A utility function to freeze the parameters of a provided model defined as a Pythonic class. For instance, <code>odak.learn.models.unet</code> is such a model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <pre><code>         Model to be frozen, in other terms `requires_grad` to be set to `False`.\n</code></pre> </li> </ul> Source code in <code>odak/learn/tools/models.py</code> <pre><code>def freeze(model):\n    \"\"\"\n    A utility function to freeze the parameters of a provided model defined as a Pythonic class.\n    For instance, `odak.learn.models.unet` is such a model.\n\n    Parameters\n    ----------\n    model          : torch.nn.modules\n                     Model to be frozen, in other terms `requires_grad` to be set to `False`.                    \n    \"\"\"\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.generate_2d_dirac_delta","title":"<code>generate_2d_dirac_delta(kernel_length=[21, 21], a=[3, 3], mu=[0, 0], theta=0, normalize=False)</code>","text":"<p>Generate 2D Dirac delta function by using Gaussian distribution. Inspired from https://en.wikipedia.org/wiki/Dirac_delta_function</p> <p>Parameters:</p> <ul> <li> <code>kernel_length</code>               (<code>list</code>, default:                   <code>[21, 21]</code> )           \u2013            <pre><code>        Length of the Dirac delta function along X and Y axes.\n</code></pre> </li> <li> <code>a</code>           \u2013            <pre><code>        The scale factor in Gaussian distribution to approximate the Dirac delta function. \n        As a approaches zero, the Gaussian distribution becomes infinitely narrow and tall at the center (x=0), approaching the Dirac delta function.\n</code></pre> </li> <li> <code>mu</code>           \u2013            <pre><code>        Mu of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>theta</code>           \u2013            <pre><code>        The rotation angle of the 2D Dirac delta function.\n</code></pre> </li> <li> <code>normalize</code>           \u2013            <pre><code>        If set True, normalize the output.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>kernel_2d</code> (              <code>tensor</code> )          \u2013            <p>Generated 2D Dirac delta function.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def generate_2d_dirac_delta(\n                            kernel_length = [21, 21],\n                            a = [3, 3],\n                            mu = [0, 0],\n                            theta = 0,\n                            normalize = False\n                           ):\n    \"\"\"\n    Generate 2D Dirac delta function by using Gaussian distribution.\n    Inspired from https://en.wikipedia.org/wiki/Dirac_delta_function\n\n    Parameters\n    ----------\n    kernel_length : list\n                    Length of the Dirac delta function along X and Y axes.\n    a             : list\n                    The scale factor in Gaussian distribution to approximate the Dirac delta function. \n                    As a approaches zero, the Gaussian distribution becomes infinitely narrow and tall at the center (x=0), approaching the Dirac delta function.\n    mu            : list\n                    Mu of the Gaussian kernel along X and Y axes.\n    theta         : float\n                    The rotation angle of the 2D Dirac delta function.\n    normalize     : bool\n                    If set True, normalize the output.\n\n    Returns\n    ----------\n    kernel_2d     : torch.tensor\n                    Generated 2D Dirac delta function.\n    \"\"\"\n    x = torch.linspace(-kernel_length[0] / 2., kernel_length[0] / 2., kernel_length[0])\n    y = torch.linspace(-kernel_length[1] / 2., kernel_length[1] / 2., kernel_length[1])\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    X = X - mu[0]\n    Y = Y - mu[1]\n    theta = torch.as_tensor(theta)\n    X_rot = X * torch.cos(theta) - Y * torch.sin(theta)\n    Y_rot = X * torch.sin(theta) + Y * torch.cos(theta)\n    kernel_2d = (1 / (abs(a[0] * a[1]) * torch.pi)) * torch.exp(-((X_rot / a[0]) ** 2 + (Y_rot / a[1]) ** 2))\n    if normalize:\n        kernel_2d = kernel_2d / kernel_2d.max()\n    return kernel_2d\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.generate_2d_gaussian","title":"<code>generate_2d_gaussian(kernel_length=[21, 21], nsigma=[3, 3], mu=[0, 0], normalize=False)</code>","text":"<p>Generate 2D Gaussian kernel. Inspired from https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy</p> <p>Parameters:</p> <ul> <li> <code>kernel_length</code>               (<code>list</code>, default:                   <code>[21, 21]</code> )           \u2013            <pre><code>        Length of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>nsigma</code>           \u2013            <pre><code>        Sigma of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>mu</code>           \u2013            <pre><code>        Mu of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>normalize</code>           \u2013            <pre><code>        If set True, normalize the output.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>kernel_2d</code> (              <code>tensor</code> )          \u2013            <p>Generated Gaussian kernel.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def generate_2d_gaussian(kernel_length = [21, 21], nsigma = [3, 3], mu = [0, 0], normalize = False):\n    \"\"\"\n    Generate 2D Gaussian kernel. Inspired from https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy\n\n    Parameters\n    ----------\n    kernel_length : list\n                    Length of the Gaussian kernel along X and Y axes.\n    nsigma        : list\n                    Sigma of the Gaussian kernel along X and Y axes.\n    mu            : list\n                    Mu of the Gaussian kernel along X and Y axes.\n    normalize     : bool\n                    If set True, normalize the output.\n\n    Returns\n    ----------\n    kernel_2d     : torch.tensor\n                    Generated Gaussian kernel.\n    \"\"\"\n    x = torch.linspace(-kernel_length[0]/2., kernel_length[0]/2., kernel_length[0])\n    y = torch.linspace(-kernel_length[1]/2., kernel_length[1]/2., kernel_length[1])\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    if nsigma[0] == 0:\n        nsigma[0] = 1e-5\n    if nsigma[1] == 0:\n        nsigma[1] = 1e-5\n    kernel_2d = 1. / (2. * torch.pi * nsigma[0] * nsigma[1]) * torch.exp(-((X - mu[0])**2. / (2. * nsigma[0]**2.) + (Y - mu[1])**2. / (2. * nsigma[1]**2.)))\n    if normalize:\n        kernel_2d = kernel_2d / kernel_2d.max()\n    return kernel_2d\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.get_rotation_matrix","title":"<code>get_rotation_matrix(tilt_angles=[0.0, 0.0, 0.0], tilt_order='XYZ')</code>","text":"<p>Function to generate rotation matrix for given tilt angles and tilt order.</p> <p>Parameters:</p> <ul> <li> <code>tilt_angles</code>           \u2013            <pre><code>             Tilt angles in degrees along XYZ axes.\n</code></pre> </li> <li> <code>tilt_order</code>           \u2013            <pre><code>             Rotation order (e.g., XYZ, XZY, ZXY, YXZ, ZYX).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rotmat</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix.</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def get_rotation_matrix(tilt_angles = [0., 0., 0.], tilt_order = 'XYZ'):\n    \"\"\"\n    Function to generate rotation matrix for given tilt angles and tilt order.\n\n\n    Parameters\n    ----------\n    tilt_angles        : list\n                         Tilt angles in degrees along XYZ axes.\n    tilt_order         : str\n                         Rotation order (e.g., XYZ, XZY, ZXY, YXZ, ZYX).\n\n    Returns\n    -------\n    rotmat             : torch.tensor\n                         Rotation matrix.\n    \"\"\"\n    rotx = rotmatx(tilt_angles[0])\n    roty = rotmaty(tilt_angles[1])\n    rotz = rotmatz(tilt_angles[2])\n    if tilt_order =='XYZ':\n        rotmat = torch.mm(rotz,torch.mm(roty, rotx))\n    elif tilt_order == 'XZY':\n        rotmat = torch.mm(roty,torch.mm(rotz, rotx))\n    elif tilt_order == 'ZXY':\n        rotmat = torch.mm(roty,torch.mm(rotx, rotz))\n    elif tilt_order == 'YXZ':\n        rotmat = torch.mm(rotz,torch.mm(rotx, roty))\n    elif tilt_order == 'ZYX':\n         rotmat = torch.mm(rotx,torch.mm(roty, rotz))\n    return rotmat\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.grid_sample","title":"<code>grid_sample(no=[10, 10], size=[100.0, 100.0], center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate samples over a surface.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Physical size of the surface.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>tensor</code> )          \u2013            <p>Samples generated.</p> </li> <li> <code>rotx</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at X axis.</p> </li> <li> <code>roty</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at Y axis.</p> </li> <li> <code>rotz</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at Z axis.</p> </li> </ul> Source code in <code>odak/learn/tools/sample.py</code> <pre><code>def grid_sample(\n                no = [10, 10],\n                size = [100., 100.], \n                center = [0., 0., 0.], \n                angles = [0., 0., 0.]):\n    \"\"\"\n    Definition to generate samples over a surface.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    size        : list\n                  Physical size of the surface.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    -------\n    samples     : torch.tensor\n                  Samples generated.\n    rotx        : torch.tensor\n                  Rotation matrix at X axis.\n    roty        : torch.tensor\n                  Rotation matrix at Y axis.\n    rotz        : torch.tensor\n                  Rotation matrix at Z axis.\n    \"\"\"\n    center = torch.tensor(center)\n    angles = torch.tensor(angles)\n    size = torch.tensor(size)\n    samples = torch.zeros((no[0], no[1], 3))\n    x = torch.linspace(-size[0] / 2., size[0] / 2., no[0])\n    y = torch.linspace(-size[1] / 2., size[1] / 2., no[1])\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    samples[:, :, 0] = X.detach().clone()\n    samples[:, :, 1] = Y.detach().clone()\n    samples = samples.reshape((samples.shape[0] * samples.shape[1], samples.shape[2]))\n    samples, rotx, roty, rotz = rotate_points(samples, angles = angles, offset = center)\n    return samples, rotx, roty, rotz\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.histogram_loss","title":"<code>histogram_loss(frame, ground_truth, bins=32, limits=[0.0, 1.0])</code>","text":"<p>Function for evaluating a frame against a target using histogram.</p> <p>Parameters:</p> <ul> <li> <code>frame</code>           \u2013            <pre><code>           Input frame [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n</code></pre> </li> <li> <code>ground_truth</code>           \u2013            <pre><code>           Ground truth [1 x 3 x m x n] or  [3 x m x n] or [1 x m x n] or  [m x n].\n</code></pre> </li> <li> <code>bins</code>           \u2013            <pre><code>           Number of bins.\n</code></pre> </li> <li> <code>limits</code>           \u2013            <pre><code>           Limits.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>float</code> )          \u2013            <p>Loss from evaluation.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def histogram_loss(frame, ground_truth, bins = 32, limits = [0., 1.]):\n    \"\"\"\n    Function for evaluating a frame against a target using histogram.\n\n    Parameters\n    ----------\n    frame            : torch.tensor\n                       Input frame [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n    ground_truth     : torch.tensor\n                       Ground truth [1 x 3 x m x n] or  [3 x m x n] or [1 x m x n] or  [m x n].\n    bins             : int\n                       Number of bins.\n    limits           : list\n                       Limits.\n\n    Returns\n    -------\n    loss             : float\n                       Loss from evaluation.\n    \"\"\"\n    if len(frame.shape) == 2:\n        frame = frame.unsqueeze(0).unsqueeze(0)\n    elif len(frame.shape) == 3:\n        frame = frame.unsqueeze(0)\n\n    if len(ground_truth.shape) == 2:\n        ground_truth = ground_truth.unsqueeze(0).unsqueeze(0)\n    elif len(ground_truth.shape) == 3:\n        ground_truth = ground_truth.unsqueeze(0)\n\n    histogram_frame = torch.zeros(frame.shape[1], bins).to(frame.device)\n    histogram_ground_truth = torch.zeros(ground_truth.shape[1], bins).to(frame.device)\n\n    l2 = torch.nn.MSELoss()\n\n    for i in range(frame.shape[1]):\n        histogram_frame[i] = torch.histc(frame[:, i].flatten(), bins=bins, min=limits[0], max=limits[1])\n        histogram_ground_truth[i] = torch.histc(ground_truth[:, i].flatten(), bins=bins, min=limits[0], max=limits[1])\n\n    loss = l2(histogram_frame, histogram_ground_truth)\n\n    return loss\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.load_image","title":"<code>load_image(fn, normalizeby=0.0, torch_style=False)</code>","text":"<p>Definition to load an image from a given location as a torch tensor.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>normalizeby</code>           \u2013            <pre><code>       Value to to normalize images with. Default value of zero will lead to no normalization.\n</code></pre> </li> <li> <code>torch_style</code>           \u2013            <pre><code>       If set True, it will load an image mxnx3 as 3xmxn.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image</code> (              <code>ndarray</code> )          \u2013            <p>Image loaded as a Numpy array.</p> </li> </ul> Source code in <code>odak/learn/tools/file.py</code> <pre><code>def load_image(fn, normalizeby = 0., torch_style = False):\n    \"\"\"\n    Definition to load an image from a given location as a torch tensor.\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    normalizeby  : float or optional\n                   Value to to normalize images with. Default value of zero will lead to no normalization.\n    torch_style  : bool or optional\n                   If set True, it will load an image mxnx3 as 3xmxn.\n\n    Returns\n    -------\n    image        :  ndarray\n                    Image loaded as a Numpy array.\n\n    \"\"\"\n    image = odak.tools.load_image(fn, normalizeby = normalizeby, torch_style = torch_style)\n    image = torch.from_numpy(image).float()\n    return image\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.michelson_contrast","title":"<code>michelson_contrast(image, roi_high, roi_low)</code>","text":"<p>A function to calculate michelson contrast ratio of given region of interests of the image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>        Image to be tested [1 x 3 x m x n] or [3 x m x n] or [m x n].\n</code></pre> </li> <li> <code>roi_high</code>           \u2013            <pre><code>        Corner locations of the roi for high intensity area [m_start, m_end, n_start, n_end].\n</code></pre> </li> <li> <code>roi_low</code>           \u2013            <pre><code>        Corner locations of the roi for low intensity area [m_start, m_end, n_start, n_end].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Michelson contrast for the given regions. [1] or [3] depending on input image.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def michelson_contrast(image, roi_high, roi_low):\n    \"\"\"\n    A function to calculate michelson contrast ratio of given region of interests of the image.\n\n    Parameters\n    ----------\n    image         : torch.tensor\n                    Image to be tested [1 x 3 x m x n] or [3 x m x n] or [m x n].\n    roi_high      : torch.tensor\n                    Corner locations of the roi for high intensity area [m_start, m_end, n_start, n_end].\n    roi_low       : torch.tensor\n                    Corner locations of the roi for low intensity area [m_start, m_end, n_start, n_end].\n\n    Returns\n    -------\n    result        : torch.tensor\n                    Michelson contrast for the given regions. [1] or [3] depending on input image.\n    \"\"\"\n    if len(image.shape) == 2:\n        image = image.unsqueeze(0)\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    region_low = image[:, :, roi_low[0]:roi_low[1], roi_low[2]:roi_low[3]]\n    region_high = image[:, :, roi_high[0]:roi_high[1], roi_high[2]:roi_high[3]]\n    high = torch.mean(region_high, dim = (2, 3))\n    low = torch.mean(region_low, dim = (2, 3))\n    result = (high - low) / (high + low)\n    return result.squeeze(0)\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.multi_scale_total_variation_loss","title":"<code>multi_scale_total_variation_loss(frame, levels=3)</code>","text":"<p>Function for evaluating a frame against a target using multi scale total variation approach. Here, multi scale refers to image pyramid of an input frame, where at each level image resolution is half of the previous level.</p> <p>Parameters:</p> <ul> <li> <code>frame</code>           \u2013            <pre><code>        Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n</code></pre> </li> <li> <code>levels</code>           \u2013            <pre><code>        Number of levels to go in the image pyriamid.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>float</code> )          \u2013            <p>Loss from evaluation.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def multi_scale_total_variation_loss(frame, levels = 3):\n    \"\"\"\n    Function for evaluating a frame against a target using multi scale total variation approach. Here, multi scale refers to image pyramid of an input frame, where at each level image resolution is half of the previous level.\n\n    Parameters\n    ----------\n    frame         : torch.tensor\n                    Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n    levels        : int\n                    Number of levels to go in the image pyriamid.\n\n    Returns\n    -------\n    loss          : float\n                    Loss from evaluation.\n    \"\"\"\n    if len(frame.shape) == 2:\n        frame = frame.unsqueeze(0)\n    if len(frame.shape) == 3:\n        frame = frame.unsqueeze(0)\n    scale = torch.nn.Upsample(scale_factor = 0.5, mode = 'nearest')\n    level = frame\n    loss = 0\n    for i in range(levels):\n        if i != 0:\n           level = scale(level)\n        loss += total_variation_loss(level) \n    return loss\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.quantize","title":"<code>quantize(image_field, bits=8, limits=[0.0, 1.0])</code>","text":"<p>Definition to quantize a image field (0-255, 8 bit) to a certain bits level.</p> <p>Parameters:</p> <ul> <li> <code>image_field</code>               (<code>tensor</code>)           \u2013            <pre><code>      Input image field between any range.\n</code></pre> </li> <li> <code>bits</code>           \u2013            <pre><code>      A value in between one to eight.\n</code></pre> </li> <li> <code>limits</code>           \u2013            <pre><code>      The minimum and maximum of the image_field variable.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>tensor</code> )          \u2013            <p>Quantized image field.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def quantize(image_field, bits = 8, limits = [0., 1.]):\n    \"\"\" \n    Definition to quantize a image field (0-255, 8 bit) to a certain bits level.\n\n    Parameters\n    ----------\n    image_field : torch.tensor\n                  Input image field between any range.\n    bits        : int\n                  A value in between one to eight.\n    limits      : list\n                  The minimum and maximum of the image_field variable.\n\n    Returns\n    ----------\n    new_field   : torch.tensor\n                  Quantized image field.\n    \"\"\"\n    normalized_field = (image_field - limits[0]) / (limits[1] - limits[0])\n    divider = 2 ** bits\n    new_field = normalized_field * divider\n    new_field = new_field.int()\n    return new_field\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.radial_basis_function","title":"<code>radial_basis_function(value, epsilon=0.5)</code>","text":"<p>Function to pass a value into radial basis function with Gaussian description.</p> <p>Parameters:</p> <ul> <li> <code>value</code>           \u2013            <pre><code>           Value(s) to pass to the radial basis function.\n</code></pre> </li> <li> <code>epsilon</code>           \u2013            <pre><code>           Epsilon used in the Gaussian radial basis function (e.g., y=e^(-(epsilon x value)^2).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Output values.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def radial_basis_function(value, epsilon = 0.5):\n    \"\"\"\n    Function to pass a value into radial basis function with Gaussian description.\n\n    Parameters\n    ----------\n    value            : torch.tensor\n                       Value(s) to pass to the radial basis function. \n    epsilon          : float\n                       Epsilon used in the Gaussian radial basis function (e.g., y=e^(-(epsilon x value)^2).\n\n    Returns\n    -------\n    output           : torch.tensor\n                       Output values.\n    \"\"\"\n    output = torch.exp((-(epsilon * value)**2))\n    return output\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.resize","title":"<code>resize(image, multiplier=0.5, mode='nearest')</code>","text":"<p>Definition to resize an image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>        Image with MxNx3 resolution.\n</code></pre> </li> <li> <code>multiplier</code>           \u2013            <pre><code>        Multiplier used in resizing operation (e.g., 0.5 is half size in one axis).\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>        Mode to be used in scaling, nearest, bilinear, etc.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_image</code> (              <code>tensor</code> )          \u2013            <p>Resized image.</p> </li> </ul> Source code in <code>odak/learn/tools/file.py</code> <pre><code>def resize(image, multiplier = 0.5, mode = 'nearest'):\n    \"\"\"\n    Definition to resize an image.\n\n    Parameters\n    ----------\n    image         : torch.tensor\n                    Image with MxNx3 resolution.\n    multiplier    : float\n                    Multiplier used in resizing operation (e.g., 0.5 is half size in one axis).\n    mode          : str\n                    Mode to be used in scaling, nearest, bilinear, etc.\n\n    Returns\n    -------\n    new_image     : torch.tensor\n                    Resized image.\n\n    \"\"\"\n    scale = torch.nn.Upsample(scale_factor = multiplier, mode = mode)\n    new_image = torch.zeros((int(image.shape[0] * multiplier), int(image.shape[1] * multiplier), 3)).to(image.device)\n    for i in range(3):\n        cache = image[:,:,i].unsqueeze(0)\n        cache = cache.unsqueeze(0)\n        new_cache = scale(cache).unsqueeze(0)\n        new_image[:,:,i] = new_cache.unsqueeze(0)\n    return new_image\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.rotate_points","title":"<code>rotate_points(point, angles=torch.tensor([[0, 0, 0]]), mode='XYZ', origin=torch.tensor([[0, 0, 0]]), offset=torch.tensor([[0, 0, 0]]))</code>","text":"<p>Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>       A point with size of [3] or [1, 3] or [m, 3].\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>       Rotation mode determines ordering of the rotations at each axis.\n       There are XYZ,YXZ,ZXY and ZYX modes.\n</code></pre> </li> <li> <code>origin</code>           \u2013            <pre><code>       Reference point for a rotation.\n       Expected size is [3] or [1, 3].\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>       Shift with the given offset.\n       Expected size is [3] or [1, 3] or [m, 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the rotation [1 x 3] or [m x 3].</p> </li> <li> <code>rotx</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along X axis [3 x 3].</p> </li> <li> <code>roty</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along Y axis [3 x 3].</p> </li> <li> <code>rotz</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along Z axis [3 x 3].</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def rotate_points(\n                 point,\n                 angles = torch.tensor([[0, 0, 0]]), \n                 mode='XYZ', \n                 origin = torch.tensor([[0, 0, 0]]), \n                 offset = torch.tensor([[0, 0, 0]])\n                ):\n    \"\"\"\n    Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.\n\n    Parameters\n    ----------\n    point        : torch.tensor\n                   A point with size of [3] or [1, 3] or [m, 3].\n    angles       : torch.tensor\n                   Rotation angles in degrees. \n    mode         : str\n                   Rotation mode determines ordering of the rotations at each axis.\n                   There are XYZ,YXZ,ZXY and ZYX modes.\n    origin       : torch.tensor\n                   Reference point for a rotation.\n                   Expected size is [3] or [1, 3].\n    offset       : torch.tensor\n                   Shift with the given offset.\n                   Expected size is [3] or [1, 3] or [m, 3].\n\n    Returns\n    ----------\n    result       : torch.tensor\n                   Result of the rotation [1 x 3] or [m x 3].\n    rotx         : torch.tensor\n                   Rotation matrix along X axis [3 x 3].\n    roty         : torch.tensor\n                   Rotation matrix along Y axis [3 x 3].\n    rotz         : torch.tensor\n                   Rotation matrix along Z axis [3 x 3].\n    \"\"\"\n    origin = origin.to(point.device)\n    offset = offset.to(point.device)\n    if len(point.shape) == 1:\n        point = point.unsqueeze(0)\n    if len(angles.shape) == 1:\n        angles = angles.unsqueeze(0)\n    rotx = rotmatx(angles[:, 0])\n    roty = rotmaty(angles[:, 1])\n    rotz = rotmatz(angles[:, 2])\n    new_points = (point - origin).T\n    if angles.shape[0] &gt; 1:\n        new_points = new_points.unsqueeze(0)\n        if len(origin.shape) == 2:\n            origin = origin.unsqueeze(1)\n        if len(offset.shape) == 2:\n            offset = offset.unsqueeze(1)\n    if mode == 'XYZ':\n        result = (rotz @ (roty @ (rotx @ new_points))).mT\n    elif mode == 'XZY':\n        result = torch.mm(roty, torch.mm(rotz, torch.mm(rotx, new_points))).T\n    elif mode == 'YXZ':\n        result = torch.mm(rotz, torch.mm(rotx, torch.mm(roty, new_points))).T\n    elif mode == 'ZXY':\n        result = torch.mm(roty, torch.mm(rotx, torch.mm(rotz, new_points))).T\n    elif mode == 'ZYX':\n        result = torch.mm(rotx, torch.mm(roty, torch.mm(rotz, new_points))).T\n    result += origin\n    result += offset\n    return result, rotx, roty, rotz\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.rotmatx","title":"<code>rotmatx(angle)</code>","text":"<p>Definition to generate a rotation matrix along X axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rotx</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along X axis.</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def rotmatx(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along X axis.\n\n    Parameters\n    ----------\n    angle        : torch.tensor\n                   Rotation angles in degrees.\n\n    Returns\n    ----------\n    rotx         : torch.tensor\n                   Rotation matrix along X axis.\n    \"\"\"\n    angle = torch.deg2rad(angle)\n    rotx = torch.zeros(angle.shape[0], 3, 3, device = angle.device)\n    rotx[:, 0, 0] = 1.\n    rotx[:, 1, 1] = torch.cos(angle)\n    rotx[:, 1, 2] = - torch.sin(angle)\n    rotx[:, 2, 1] = torch.sin(angle)\n    rotx[:, 2, 2] = torch.cos(angle)\n    if rotx.shape[0] == 1:\n        rotx = rotx.squeeze(0)\n    return rotx\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.rotmaty","title":"<code>rotmaty(angle)</code>","text":"<p>Definition to generate a rotation matrix along Y axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>roty</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along Y axis.</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def rotmaty(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along Y axis.\n\n    Parameters\n    ----------\n    angle        : torch.tensor\n                   Rotation angles in degrees.\n\n    Returns\n    ----------\n    roty         : torch.tensor\n                   Rotation matrix along Y axis.\n    \"\"\"\n    angle = torch.deg2rad(angle)\n    roty = torch.zeros(angle.shape[0], 3, 3, device = angle.device)\n    roty[:, 0, 0] = torch.cos(angle)\n    roty[:, 0, 2] = torch.sin(angle)\n    roty[:, 1, 1] = 1.\n    roty[:, 2, 0] = - torch.sin(angle)\n    roty[:, 2, 2] = torch.cos(angle)\n    if roty.shape[0] == 1:\n        roty = roty.squeeze(0)\n    return roty\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.rotmatz","title":"<code>rotmatz(angle)</code>","text":"<p>Definition to generate a rotation matrix along Z axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rotz</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along Z axis.</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def rotmatz(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along Z axis.\n\n    Parameters\n    ----------\n    angle        : torch.tensor\n                   Rotation angles in degrees.\n\n    Returns\n    ----------\n    rotz         : torch.tensor\n                   Rotation matrix along Z axis.\n    \"\"\"\n    angle = torch.deg2rad(angle)\n    rotz = torch.zeros(angle.shape[0], 3, 3, device = angle.device)\n    rotz[:, 0, 0] = torch.cos(angle)\n    rotz[:, 0, 1] = - torch.sin(angle)\n    rotz[:, 1, 0] = torch.sin(angle)\n    rotz[:, 1, 1] = torch.cos(angle)\n    rotz[:, 2, 2] = 1.\n    if rotz.shape[0] == 1:\n        rotz = rotz.squeeze(0)\n    return rotz\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.same_side","title":"<code>same_side(p1, p2, a, b)</code>","text":"<p>Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.</p> <p>Parameters:</p> <ul> <li> <code>p1</code>           \u2013            <pre><code>      Point(s) to check.\n</code></pre> </li> <li> <code>p2</code>           \u2013            <pre><code>      This is the point check against.\n</code></pre> </li> <li> <code>a</code>           \u2013            <pre><code>      First point that forms the line.\n</code></pre> </li> <li> <code>b</code>           \u2013            <pre><code>      Second point that forms the line.\n</code></pre> </li> </ul> Source code in <code>odak/learn/tools/vector.py</code> <pre><code>def same_side(p1, p2, a, b):\n    \"\"\"\n    Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.\n\n    Parameters\n    ----------\n    p1          : list\n                  Point(s) to check.\n    p2          : list\n                  This is the point check against.\n    a           : list\n                  First point that forms the line.\n    b           : list\n                  Second point that forms the line.\n    \"\"\"\n    ba = torch.subtract(b, a)\n    p1a = torch.subtract(p1, a)\n    p2a = torch.subtract(p2, a)\n    cp1 = torch.cross(ba, p1a)\n    cp2 = torch.cross(ba, p2a)\n    test = torch.dot(cp1, cp2)\n    if len(p1.shape) &gt; 1:\n        return test &gt;= 0\n    if test &gt;= 0:\n        return True\n    return False\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.save_image","title":"<code>save_image(fn, img, cmin=0, cmax=255, color_depth=8)</code>","text":"<p>Definition to save a torch tensor as an image.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>img</code>           \u2013            <pre><code>       A numpy array with NxMx3 or NxMx1 shapes.\n</code></pre> </li> <li> <code>cmin</code>           \u2013            <pre><code>       Minimum value that will be interpreted as 0 level in the final image.\n</code></pre> </li> <li> <code>cmax</code>           \u2013            <pre><code>       Maximum value that will be interpreted as 255 level in the final image.\n</code></pre> </li> <li> <code>color_depth</code>           \u2013            <pre><code>       Color depth of an image. Default is eight.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if successful.</p> </li> </ul> Source code in <code>odak/learn/tools/file.py</code> <pre><code>def save_image(fn, img, cmin = 0, cmax = 255, color_depth = 8):\n    \"\"\"\n    Definition to save a torch tensor as an image.\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    img          : ndarray\n                   A numpy array with NxMx3 or NxMx1 shapes.\n    cmin         : int\n                   Minimum value that will be interpreted as 0 level in the final image.\n    cmax         : int\n                   Maximum value that will be interpreted as 255 level in the final image.\n    color_depth  : int\n                   Color depth of an image. Default is eight.\n\n\n    Returns\n    ----------\n    bool         :  bool\n                    True if successful.\n\n    \"\"\"\n    if len(img.shape) ==  4:\n        img = img.squeeze(0)\n    if len(img.shape) &gt; 2 and torch.argmin(torch.tensor(img.shape)) == 0:\n        new_img = torch.zeros(img.shape[1], img.shape[2], img.shape[0]).to(img.device)\n        for i in range(img.shape[0]):\n            new_img[:, :, i] = img[i].detach().clone()\n        img = new_img.detach().clone()\n    img = img.cpu().detach().numpy()\n    return odak.tools.save_image(fn, img, cmin = cmin, cmax = cmax, color_depth = color_depth)\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.save_torch_tensor","title":"<code>save_torch_tensor(fn, tensor)</code>","text":"<p>Definition to save a torch tensor.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>tensor</code>           \u2013            <pre><code>       Torch tensor to be saved.\n</code></pre> </li> </ul> Source code in <code>odak/learn/tools/file.py</code> <pre><code>def save_torch_tensor(fn, tensor):\n    \"\"\"\n    Definition to save a torch tensor.\n\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    tensor       : torch.tensor\n                   Torch tensor to be saved.\n    \"\"\" \n    torch.save(tensor, expanduser(fn))\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.spatial_gradient","title":"<code>spatial_gradient(frame)</code>","text":"<p>Function to calculate the spatial gradient of a given frame.</p> <p>Parameters:</p> <ul> <li> <code>frame</code>           \u2013            <pre><code>        Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>diff_x</code> (              <code>float</code> )          \u2013            <p>Spatial gradient along X.</p> </li> <li> <code>diff_y</code> (              <code>float</code> )          \u2013            <p>Spatial gradient along Y.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def spatial_gradient(frame):\n    \"\"\"\n    Function to calculate the spatial gradient of a given frame.\n\n    Parameters\n    ----------\n    frame         : torch.tensor\n                    Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n\n    Returns\n    -------\n    diff_x        : float\n                    Spatial gradient along X.\n    diff_y        : float\n                    Spatial gradient along Y.\n    \"\"\"\n    if len(frame.shape) == 2:\n        frame = frame.unsqueeze(0)\n    if len(frame.shape) == 3:\n        frame = frame.unsqueeze(0)    \n    diff_x = frame[:, :, :, 1:] - frame[:, :, :, :-1]\n    diff_y = frame[:, :, 1:, :] - frame[:, :, :-1, :]\n    return diff_x, diff_y\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.tilt_towards","title":"<code>tilt_towards(location, lookat)</code>","text":"<p>Definition to tilt surface normal of a plane towards a point.</p> <p>Parameters:</p> <ul> <li> <code>location</code>           \u2013            <pre><code>       Center of the plane to be tilted.\n</code></pre> </li> <li> <code>lookat</code>           \u2013            <pre><code>       Tilt towards this point.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>angles</code> (              <code>list</code> )          \u2013            <p>Rotation angles in degrees.</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def tilt_towards(location, lookat):\n    \"\"\"\n    Definition to tilt surface normal of a plane towards a point.\n\n    Parameters\n    ----------\n    location     : list\n                   Center of the plane to be tilted.\n    lookat       : list\n                   Tilt towards this point.\n\n    Returns\n    ----------\n    angles       : list\n                   Rotation angles in degrees.\n    \"\"\"\n    dx = location[0] - lookat[0]\n    dy = location[1] - lookat[1]\n    dz = location[2] - lookat[2]\n    dist = torch.sqrt(torch.tensor(dx ** 2 + dy ** 2 + dz ** 2))\n    phi = torch.atan2(torch.tensor(dy), torch.tensor(dx))\n    theta = torch.arccos(dz / dist)\n    angles = [0, float(torch.rad2deg(theta)), float(torch.rad2deg(phi))]\n    return angles\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.torch_load","title":"<code>torch_load(fn, weights_only=True)</code>","text":"<p>Definition to load a torch files (*.pt).</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>weights_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <pre><code>       See torch.load() for details.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>data</code> (              <code>any</code> )          \u2013            <p>See torch.load() for more.</p> </li> </ul> Source code in <code>odak/learn/tools/file.py</code> <pre><code>def torch_load(fn, weights_only = True):\n    \"\"\"\n    Definition to load a torch files (*.pt).\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    weights_only : bool\n                   See torch.load() for details.\n\n    Returns\n    -------\n    data         : any\n                   See torch.load() for more.\n    \"\"\"  \n    data = torch.load(\n                      expanduser(fn),\n                      weights_only = weights_only\n                     )\n    return data\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.total_variation_loss","title":"<code>total_variation_loss(frame)</code>","text":"<p>Function for evaluating a frame against a target using total variation approach.</p> <p>Parameters:</p> <ul> <li> <code>frame</code>           \u2013            <pre><code>        Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>float</code> )          \u2013            <p>Loss from evaluation.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def total_variation_loss(frame):\n    \"\"\"\n    Function for evaluating a frame against a target using total variation approach.\n\n    Parameters\n    ----------\n    frame         : torch.tensor\n                    Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n\n    Returns\n    -------\n    loss          : float\n                    Loss from evaluation.\n    \"\"\"\n    if len(frame.shape) == 2:\n        frame = frame.unsqueeze(0)\n    if len(frame.shape) == 3:\n        frame = frame.unsqueeze(0)\n    diff_x, diff_y = spatial_gradient(frame)\n    pixel_count = frame.shape[0] * frame.shape[1] * frame.shape[2] * frame.shape[3]\n    loss = ((diff_x ** 2).sum() + (diff_y ** 2).sum()) / pixel_count\n    return loss\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.unfreeze","title":"<code>unfreeze(model)</code>","text":"<p>A utility function to unfreeze the parameters of a provided model defined as a Pythonic class. For instance, <code>odak.learn.models.unet</code> is such a model.</p> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <pre><code>         Model to unfreeze, in other terms `requires_grad` to be set to `True`.\n</code></pre> </li> </ul> Source code in <code>odak/learn/tools/models.py</code> <pre><code>def unfreeze(model):\n    \"\"\"\n    A utility function to unfreeze the parameters of a provided model defined as a Pythonic class.\n    For instance, `odak.learn.models.unet` is such a model.\n\n\n    Parameters\n    ----------\n    model          : torch.nn.modules\n                     Model to unfreeze, in other terms `requires_grad` to be set to `True`.\n    \"\"\"\n    for parameter in model.parameters():\n        parameter.requires_grad = True\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.weber_contrast","title":"<code>weber_contrast(image, roi_high, roi_low)</code>","text":"<p>A function to calculate weber contrast ratio of given region of interests of the image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>        Image to be tested [1 x 3 x m x n] or [3 x m x n] or [1 x m x n] or [m x n].\n</code></pre> </li> <li> <code>roi_high</code>           \u2013            <pre><code>        Corner locations of the roi for high intensity area [m_start, m_end, n_start, n_end].\n</code></pre> </li> <li> <code>roi_low</code>           \u2013            <pre><code>        Corner locations of the roi for low intensity area [m_start, m_end, n_start, n_end].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Weber contrast for given regions. [1] or [3] depending on input image.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def weber_contrast(image, roi_high, roi_low):\n    \"\"\"\n    A function to calculate weber contrast ratio of given region of interests of the image.\n\n    Parameters\n    ----------\n    image         : torch.tensor\n                    Image to be tested [1 x 3 x m x n] or [3 x m x n] or [1 x m x n] or [m x n].\n    roi_high      : torch.tensor\n                    Corner locations of the roi for high intensity area [m_start, m_end, n_start, n_end].\n    roi_low       : torch.tensor\n                    Corner locations of the roi for low intensity area [m_start, m_end, n_start, n_end].\n\n    Returns\n    -------\n    result        : torch.tensor\n                    Weber contrast for given regions. [1] or [3] depending on input image.\n    \"\"\"\n    if len(image.shape) == 2:\n        image = image.unsqueeze(0)\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    region_low = image[:, :, roi_low[0]:roi_low[1], roi_low[2]:roi_low[3]]\n    region_high = image[:, :, roi_high[0]:roi_high[1], roi_high[2]:roi_high[3]]\n    high = torch.mean(region_high, dim = (2, 3))\n    low = torch.mean(region_low, dim = (2, 3))\n    result = (high - low) / low\n    return result.squeeze(0)\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.wrapped_mean_squared_error","title":"<code>wrapped_mean_squared_error(image, ground_truth, reduction='mean')</code>","text":"<p>A function to calculate the wrapped mean squared error between predicted and target angles.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>        Image to be tested [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n</code></pre> </li> <li> <code>ground_truth</code>           \u2013            <pre><code>        Ground truth to be tested [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n</code></pre> </li> <li> <code>reduction</code>           \u2013            <pre><code>        Specifies the reduction to apply to the output: 'mean' (default) or 'sum'.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>wmse</code> (              <code>tensor</code> )          \u2013            <p>The calculated wrapped mean squared error.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def wrapped_mean_squared_error(image, ground_truth, reduction = 'mean'):\n    \"\"\"\n    A function to calculate the wrapped mean squared error between predicted and target angles.\n\n    Parameters\n    ----------\n    image         : torch.tensor\n                    Image to be tested [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n    ground_truth  : torch.tensor\n                    Ground truth to be tested [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n    reduction     : str\n                    Specifies the reduction to apply to the output: 'mean' (default) or 'sum'.\n\n    Returns\n    -------\n    wmse        : torch.tensor\n                  The calculated wrapped mean squared error. \n    \"\"\"\n    sin_diff = torch.sin(image) - torch.sin(ground_truth)\n    cos_diff = torch.cos(image) - torch.cos(ground_truth)\n    loss = (sin_diff**2 + cos_diff**2)\n\n    if reduction == 'mean':\n        return loss.mean()\n    elif reduction == 'sum':\n        return loss.sum()\n    else:\n        raise ValueError(\"Invalid reduction type. Choose 'mean' or 'sum'.\")\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.zero_pad","title":"<code>zero_pad(field, size=None, method='center')</code>","text":"<p>Definition to zero pad a MxN array to 2Mx2N array.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>            Input field MxN or KxJxMxN or KxMxNxJ array.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>            Size to be zeropadded (e.g., [m, n], last two dimensions only).\n</code></pre> </li> <li> <code>method</code>           \u2013            <pre><code>            Zeropad either by placing the content to center or to the left.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field_zero_padded</code> (              <code>ndarray</code> )          \u2013            <p>Zeropadded version of the input field.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def zero_pad(field, size = None, method = 'center'):\n    \"\"\"\n    Definition to zero pad a MxN array to 2Mx2N array.\n\n    Parameters\n    ----------\n    field             : ndarray\n                        Input field MxN or KxJxMxN or KxMxNxJ array.\n    size              : list\n                        Size to be zeropadded (e.g., [m, n], last two dimensions only).\n    method            : str\n                        Zeropad either by placing the content to center or to the left.\n\n    Returns\n    ----------\n    field_zero_padded : ndarray\n                        Zeropadded version of the input field.\n    \"\"\"\n    orig_resolution = field.shape\n    if len(field.shape) &lt; 3:\n        field = field.unsqueeze(0)\n    if len(field.shape) &lt; 4:\n        field = field.unsqueeze(0)\n    permute_flag = False\n    if field.shape[-1] &lt; 5:\n        permute_flag = True\n        field = field.permute(0, 3, 1, 2)\n    if type(size) == type(None):\n        resolution = [field.shape[0], field.shape[1], 2 * field.shape[-2], 2 * field.shape[-1]]\n    else:\n        resolution = [field.shape[0], field.shape[1], size[0], size[1]]\n    field_zero_padded = torch.zeros(resolution, device = field.device, dtype = field.dtype)\n    if method == 'center':\n       start = [\n                resolution[-2] // 2 - field.shape[-2] // 2,\n                resolution[-1] // 2 - field.shape[-1] // 2\n               ]\n       field_zero_padded[\n                         :, :,\n                         start[0] : start[0] + field.shape[-2],\n                         start[1] : start[1] + field.shape[-1]\n                         ] = field\n    elif method == 'left':\n       field_zero_padded[\n                         :, :,\n                         0: field.shape[-2],\n                         0: field.shape[-1]\n                        ] = field\n    if permute_flag == True:\n        field_zero_padded = field_zero_padded.permute(0, 2, 3, 1)\n    if len(orig_resolution) == 2:\n        field_zero_padded = field_zero_padded.squeeze(0).squeeze(0)\n    if len(orig_resolution) == 3:\n        field_zero_padded = field_zero_padded.squeeze(0)\n    return field_zero_padded\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.file.load_image","title":"<code>load_image(fn, normalizeby=0.0, torch_style=False)</code>","text":"<p>Definition to load an image from a given location as a torch tensor.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>normalizeby</code>           \u2013            <pre><code>       Value to to normalize images with. Default value of zero will lead to no normalization.\n</code></pre> </li> <li> <code>torch_style</code>           \u2013            <pre><code>       If set True, it will load an image mxnx3 as 3xmxn.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image</code> (              <code>ndarray</code> )          \u2013            <p>Image loaded as a Numpy array.</p> </li> </ul> Source code in <code>odak/learn/tools/file.py</code> <pre><code>def load_image(fn, normalizeby = 0., torch_style = False):\n    \"\"\"\n    Definition to load an image from a given location as a torch tensor.\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    normalizeby  : float or optional\n                   Value to to normalize images with. Default value of zero will lead to no normalization.\n    torch_style  : bool or optional\n                   If set True, it will load an image mxnx3 as 3xmxn.\n\n    Returns\n    -------\n    image        :  ndarray\n                    Image loaded as a Numpy array.\n\n    \"\"\"\n    image = odak.tools.load_image(fn, normalizeby = normalizeby, torch_style = torch_style)\n    image = torch.from_numpy(image).float()\n    return image\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.file.resize","title":"<code>resize(image, multiplier=0.5, mode='nearest')</code>","text":"<p>Definition to resize an image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>        Image with MxNx3 resolution.\n</code></pre> </li> <li> <code>multiplier</code>           \u2013            <pre><code>        Multiplier used in resizing operation (e.g., 0.5 is half size in one axis).\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>        Mode to be used in scaling, nearest, bilinear, etc.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_image</code> (              <code>tensor</code> )          \u2013            <p>Resized image.</p> </li> </ul> Source code in <code>odak/learn/tools/file.py</code> <pre><code>def resize(image, multiplier = 0.5, mode = 'nearest'):\n    \"\"\"\n    Definition to resize an image.\n\n    Parameters\n    ----------\n    image         : torch.tensor\n                    Image with MxNx3 resolution.\n    multiplier    : float\n                    Multiplier used in resizing operation (e.g., 0.5 is half size in one axis).\n    mode          : str\n                    Mode to be used in scaling, nearest, bilinear, etc.\n\n    Returns\n    -------\n    new_image     : torch.tensor\n                    Resized image.\n\n    \"\"\"\n    scale = torch.nn.Upsample(scale_factor = multiplier, mode = mode)\n    new_image = torch.zeros((int(image.shape[0] * multiplier), int(image.shape[1] * multiplier), 3)).to(image.device)\n    for i in range(3):\n        cache = image[:,:,i].unsqueeze(0)\n        cache = cache.unsqueeze(0)\n        new_cache = scale(cache).unsqueeze(0)\n        new_image[:,:,i] = new_cache.unsqueeze(0)\n    return new_image\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.file.save_image","title":"<code>save_image(fn, img, cmin=0, cmax=255, color_depth=8)</code>","text":"<p>Definition to save a torch tensor as an image.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>img</code>           \u2013            <pre><code>       A numpy array with NxMx3 or NxMx1 shapes.\n</code></pre> </li> <li> <code>cmin</code>           \u2013            <pre><code>       Minimum value that will be interpreted as 0 level in the final image.\n</code></pre> </li> <li> <code>cmax</code>           \u2013            <pre><code>       Maximum value that will be interpreted as 255 level in the final image.\n</code></pre> </li> <li> <code>color_depth</code>           \u2013            <pre><code>       Color depth of an image. Default is eight.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if successful.</p> </li> </ul> Source code in <code>odak/learn/tools/file.py</code> <pre><code>def save_image(fn, img, cmin = 0, cmax = 255, color_depth = 8):\n    \"\"\"\n    Definition to save a torch tensor as an image.\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    img          : ndarray\n                   A numpy array with NxMx3 or NxMx1 shapes.\n    cmin         : int\n                   Minimum value that will be interpreted as 0 level in the final image.\n    cmax         : int\n                   Maximum value that will be interpreted as 255 level in the final image.\n    color_depth  : int\n                   Color depth of an image. Default is eight.\n\n\n    Returns\n    ----------\n    bool         :  bool\n                    True if successful.\n\n    \"\"\"\n    if len(img.shape) ==  4:\n        img = img.squeeze(0)\n    if len(img.shape) &gt; 2 and torch.argmin(torch.tensor(img.shape)) == 0:\n        new_img = torch.zeros(img.shape[1], img.shape[2], img.shape[0]).to(img.device)\n        for i in range(img.shape[0]):\n            new_img[:, :, i] = img[i].detach().clone()\n        img = new_img.detach().clone()\n    img = img.cpu().detach().numpy()\n    return odak.tools.save_image(fn, img, cmin = cmin, cmax = cmax, color_depth = color_depth)\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.file.save_torch_tensor","title":"<code>save_torch_tensor(fn, tensor)</code>","text":"<p>Definition to save a torch tensor.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>tensor</code>           \u2013            <pre><code>       Torch tensor to be saved.\n</code></pre> </li> </ul> Source code in <code>odak/learn/tools/file.py</code> <pre><code>def save_torch_tensor(fn, tensor):\n    \"\"\"\n    Definition to save a torch tensor.\n\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    tensor       : torch.tensor\n                   Torch tensor to be saved.\n    \"\"\" \n    torch.save(tensor, expanduser(fn))\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.file.torch_load","title":"<code>torch_load(fn, weights_only=True)</code>","text":"<p>Definition to load a torch files (*.pt).</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>weights_only</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <pre><code>       See torch.load() for details.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>data</code> (              <code>any</code> )          \u2013            <p>See torch.load() for more.</p> </li> </ul> Source code in <code>odak/learn/tools/file.py</code> <pre><code>def torch_load(fn, weights_only = True):\n    \"\"\"\n    Definition to load a torch files (*.pt).\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    weights_only : bool\n                   See torch.load() for details.\n\n    Returns\n    -------\n    data         : any\n                   See torch.load() for more.\n    \"\"\"  \n    data = torch.load(\n                      expanduser(fn),\n                      weights_only = weights_only\n                     )\n    return data\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.loss.histogram_loss","title":"<code>histogram_loss(frame, ground_truth, bins=32, limits=[0.0, 1.0])</code>","text":"<p>Function for evaluating a frame against a target using histogram.</p> <p>Parameters:</p> <ul> <li> <code>frame</code>           \u2013            <pre><code>           Input frame [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n</code></pre> </li> <li> <code>ground_truth</code>           \u2013            <pre><code>           Ground truth [1 x 3 x m x n] or  [3 x m x n] or [1 x m x n] or  [m x n].\n</code></pre> </li> <li> <code>bins</code>           \u2013            <pre><code>           Number of bins.\n</code></pre> </li> <li> <code>limits</code>           \u2013            <pre><code>           Limits.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>float</code> )          \u2013            <p>Loss from evaluation.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def histogram_loss(frame, ground_truth, bins = 32, limits = [0., 1.]):\n    \"\"\"\n    Function for evaluating a frame against a target using histogram.\n\n    Parameters\n    ----------\n    frame            : torch.tensor\n                       Input frame [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n    ground_truth     : torch.tensor\n                       Ground truth [1 x 3 x m x n] or  [3 x m x n] or [1 x m x n] or  [m x n].\n    bins             : int\n                       Number of bins.\n    limits           : list\n                       Limits.\n\n    Returns\n    -------\n    loss             : float\n                       Loss from evaluation.\n    \"\"\"\n    if len(frame.shape) == 2:\n        frame = frame.unsqueeze(0).unsqueeze(0)\n    elif len(frame.shape) == 3:\n        frame = frame.unsqueeze(0)\n\n    if len(ground_truth.shape) == 2:\n        ground_truth = ground_truth.unsqueeze(0).unsqueeze(0)\n    elif len(ground_truth.shape) == 3:\n        ground_truth = ground_truth.unsqueeze(0)\n\n    histogram_frame = torch.zeros(frame.shape[1], bins).to(frame.device)\n    histogram_ground_truth = torch.zeros(ground_truth.shape[1], bins).to(frame.device)\n\n    l2 = torch.nn.MSELoss()\n\n    for i in range(frame.shape[1]):\n        histogram_frame[i] = torch.histc(frame[:, i].flatten(), bins=bins, min=limits[0], max=limits[1])\n        histogram_ground_truth[i] = torch.histc(ground_truth[:, i].flatten(), bins=bins, min=limits[0], max=limits[1])\n\n    loss = l2(histogram_frame, histogram_ground_truth)\n\n    return loss\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.loss.michelson_contrast","title":"<code>michelson_contrast(image, roi_high, roi_low)</code>","text":"<p>A function to calculate michelson contrast ratio of given region of interests of the image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>        Image to be tested [1 x 3 x m x n] or [3 x m x n] or [m x n].\n</code></pre> </li> <li> <code>roi_high</code>           \u2013            <pre><code>        Corner locations of the roi for high intensity area [m_start, m_end, n_start, n_end].\n</code></pre> </li> <li> <code>roi_low</code>           \u2013            <pre><code>        Corner locations of the roi for low intensity area [m_start, m_end, n_start, n_end].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Michelson contrast for the given regions. [1] or [3] depending on input image.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def michelson_contrast(image, roi_high, roi_low):\n    \"\"\"\n    A function to calculate michelson contrast ratio of given region of interests of the image.\n\n    Parameters\n    ----------\n    image         : torch.tensor\n                    Image to be tested [1 x 3 x m x n] or [3 x m x n] or [m x n].\n    roi_high      : torch.tensor\n                    Corner locations of the roi for high intensity area [m_start, m_end, n_start, n_end].\n    roi_low       : torch.tensor\n                    Corner locations of the roi for low intensity area [m_start, m_end, n_start, n_end].\n\n    Returns\n    -------\n    result        : torch.tensor\n                    Michelson contrast for the given regions. [1] or [3] depending on input image.\n    \"\"\"\n    if len(image.shape) == 2:\n        image = image.unsqueeze(0)\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    region_low = image[:, :, roi_low[0]:roi_low[1], roi_low[2]:roi_low[3]]\n    region_high = image[:, :, roi_high[0]:roi_high[1], roi_high[2]:roi_high[3]]\n    high = torch.mean(region_high, dim = (2, 3))\n    low = torch.mean(region_low, dim = (2, 3))\n    result = (high - low) / (high + low)\n    return result.squeeze(0)\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.loss.multi_scale_total_variation_loss","title":"<code>multi_scale_total_variation_loss(frame, levels=3)</code>","text":"<p>Function for evaluating a frame against a target using multi scale total variation approach. Here, multi scale refers to image pyramid of an input frame, where at each level image resolution is half of the previous level.</p> <p>Parameters:</p> <ul> <li> <code>frame</code>           \u2013            <pre><code>        Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n</code></pre> </li> <li> <code>levels</code>           \u2013            <pre><code>        Number of levels to go in the image pyriamid.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>float</code> )          \u2013            <p>Loss from evaluation.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def multi_scale_total_variation_loss(frame, levels = 3):\n    \"\"\"\n    Function for evaluating a frame against a target using multi scale total variation approach. Here, multi scale refers to image pyramid of an input frame, where at each level image resolution is half of the previous level.\n\n    Parameters\n    ----------\n    frame         : torch.tensor\n                    Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n    levels        : int\n                    Number of levels to go in the image pyriamid.\n\n    Returns\n    -------\n    loss          : float\n                    Loss from evaluation.\n    \"\"\"\n    if len(frame.shape) == 2:\n        frame = frame.unsqueeze(0)\n    if len(frame.shape) == 3:\n        frame = frame.unsqueeze(0)\n    scale = torch.nn.Upsample(scale_factor = 0.5, mode = 'nearest')\n    level = frame\n    loss = 0\n    for i in range(levels):\n        if i != 0:\n           level = scale(level)\n        loss += total_variation_loss(level) \n    return loss\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.loss.radial_basis_function","title":"<code>radial_basis_function(value, epsilon=0.5)</code>","text":"<p>Function to pass a value into radial basis function with Gaussian description.</p> <p>Parameters:</p> <ul> <li> <code>value</code>           \u2013            <pre><code>           Value(s) to pass to the radial basis function.\n</code></pre> </li> <li> <code>epsilon</code>           \u2013            <pre><code>           Epsilon used in the Gaussian radial basis function (e.g., y=e^(-(epsilon x value)^2).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Output values.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def radial_basis_function(value, epsilon = 0.5):\n    \"\"\"\n    Function to pass a value into radial basis function with Gaussian description.\n\n    Parameters\n    ----------\n    value            : torch.tensor\n                       Value(s) to pass to the radial basis function. \n    epsilon          : float\n                       Epsilon used in the Gaussian radial basis function (e.g., y=e^(-(epsilon x value)^2).\n\n    Returns\n    -------\n    output           : torch.tensor\n                       Output values.\n    \"\"\"\n    output = torch.exp((-(epsilon * value)**2))\n    return output\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.loss.spatial_gradient","title":"<code>spatial_gradient(frame)</code>","text":"<p>Function to calculate the spatial gradient of a given frame.</p> <p>Parameters:</p> <ul> <li> <code>frame</code>           \u2013            <pre><code>        Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>diff_x</code> (              <code>float</code> )          \u2013            <p>Spatial gradient along X.</p> </li> <li> <code>diff_y</code> (              <code>float</code> )          \u2013            <p>Spatial gradient along Y.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def spatial_gradient(frame):\n    \"\"\"\n    Function to calculate the spatial gradient of a given frame.\n\n    Parameters\n    ----------\n    frame         : torch.tensor\n                    Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n\n    Returns\n    -------\n    diff_x        : float\n                    Spatial gradient along X.\n    diff_y        : float\n                    Spatial gradient along Y.\n    \"\"\"\n    if len(frame.shape) == 2:\n        frame = frame.unsqueeze(0)\n    if len(frame.shape) == 3:\n        frame = frame.unsqueeze(0)    \n    diff_x = frame[:, :, :, 1:] - frame[:, :, :, :-1]\n    diff_y = frame[:, :, 1:, :] - frame[:, :, :-1, :]\n    return diff_x, diff_y\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.loss.total_variation_loss","title":"<code>total_variation_loss(frame)</code>","text":"<p>Function for evaluating a frame against a target using total variation approach.</p> <p>Parameters:</p> <ul> <li> <code>frame</code>           \u2013            <pre><code>        Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>float</code> )          \u2013            <p>Loss from evaluation.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def total_variation_loss(frame):\n    \"\"\"\n    Function for evaluating a frame against a target using total variation approach.\n\n    Parameters\n    ----------\n    frame         : torch.tensor\n                    Input frame [1 x 3 x m x n] or [3 x m x n] or [m x n].\n\n    Returns\n    -------\n    loss          : float\n                    Loss from evaluation.\n    \"\"\"\n    if len(frame.shape) == 2:\n        frame = frame.unsqueeze(0)\n    if len(frame.shape) == 3:\n        frame = frame.unsqueeze(0)\n    diff_x, diff_y = spatial_gradient(frame)\n    pixel_count = frame.shape[0] * frame.shape[1] * frame.shape[2] * frame.shape[3]\n    loss = ((diff_x ** 2).sum() + (diff_y ** 2).sum()) / pixel_count\n    return loss\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.loss.weber_contrast","title":"<code>weber_contrast(image, roi_high, roi_low)</code>","text":"<p>A function to calculate weber contrast ratio of given region of interests of the image.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>        Image to be tested [1 x 3 x m x n] or [3 x m x n] or [1 x m x n] or [m x n].\n</code></pre> </li> <li> <code>roi_high</code>           \u2013            <pre><code>        Corner locations of the roi for high intensity area [m_start, m_end, n_start, n_end].\n</code></pre> </li> <li> <code>roi_low</code>           \u2013            <pre><code>        Corner locations of the roi for low intensity area [m_start, m_end, n_start, n_end].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Weber contrast for given regions. [1] or [3] depending on input image.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def weber_contrast(image, roi_high, roi_low):\n    \"\"\"\n    A function to calculate weber contrast ratio of given region of interests of the image.\n\n    Parameters\n    ----------\n    image         : torch.tensor\n                    Image to be tested [1 x 3 x m x n] or [3 x m x n] or [1 x m x n] or [m x n].\n    roi_high      : torch.tensor\n                    Corner locations of the roi for high intensity area [m_start, m_end, n_start, n_end].\n    roi_low       : torch.tensor\n                    Corner locations of the roi for low intensity area [m_start, m_end, n_start, n_end].\n\n    Returns\n    -------\n    result        : torch.tensor\n                    Weber contrast for given regions. [1] or [3] depending on input image.\n    \"\"\"\n    if len(image.shape) == 2:\n        image = image.unsqueeze(0)\n    if len(image.shape) == 3:\n        image = image.unsqueeze(0)\n    region_low = image[:, :, roi_low[0]:roi_low[1], roi_low[2]:roi_low[3]]\n    region_high = image[:, :, roi_high[0]:roi_high[1], roi_high[2]:roi_high[3]]\n    high = torch.mean(region_high, dim = (2, 3))\n    low = torch.mean(region_low, dim = (2, 3))\n    result = (high - low) / low\n    return result.squeeze(0)\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.loss.wrapped_mean_squared_error","title":"<code>wrapped_mean_squared_error(image, ground_truth, reduction='mean')</code>","text":"<p>A function to calculate the wrapped mean squared error between predicted and target angles.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>        Image to be tested [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n</code></pre> </li> <li> <code>ground_truth</code>           \u2013            <pre><code>        Ground truth to be tested [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n</code></pre> </li> <li> <code>reduction</code>           \u2013            <pre><code>        Specifies the reduction to apply to the output: 'mean' (default) or 'sum'.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>wmse</code> (              <code>tensor</code> )          \u2013            <p>The calculated wrapped mean squared error.</p> </li> </ul> Source code in <code>odak/learn/tools/loss.py</code> <pre><code>def wrapped_mean_squared_error(image, ground_truth, reduction = 'mean'):\n    \"\"\"\n    A function to calculate the wrapped mean squared error between predicted and target angles.\n\n    Parameters\n    ----------\n    image         : torch.tensor\n                    Image to be tested [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n    ground_truth  : torch.tensor\n                    Ground truth to be tested [1 x 3 x m x n]  or [3 x m x n] or [1 x m x n] or [m x n].\n    reduction     : str\n                    Specifies the reduction to apply to the output: 'mean' (default) or 'sum'.\n\n    Returns\n    -------\n    wmse        : torch.tensor\n                  The calculated wrapped mean squared error. \n    \"\"\"\n    sin_diff = torch.sin(image) - torch.sin(ground_truth)\n    cos_diff = torch.cos(image) - torch.cos(ground_truth)\n    loss = (sin_diff**2 + cos_diff**2)\n\n    if reduction == 'mean':\n        return loss.mean()\n    elif reduction == 'sum':\n        return loss.sum()\n    else:\n        raise ValueError(\"Invalid reduction type. Choose 'mean' or 'sum'.\")\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.matrix.blur_gaussian","title":"<code>blur_gaussian(field, kernel_length=[21, 21], nsigma=[3, 3], padding='same')</code>","text":"<p>A definition to blur a field using a Gaussian kernel.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>        MxN field.\n</code></pre> </li> <li> <code>kernel_length</code>               (<code>list</code>, default:                   <code>[21, 21]</code> )           \u2013            <pre><code>        Length of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>nsigma</code>           \u2013            <pre><code>        Sigma of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>padding</code>           \u2013            <pre><code>        Padding value, see torch.nn.functional.conv2d() for more.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>blurred_field</code> (              <code>tensor</code> )          \u2013            <p>Blurred field.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def blur_gaussian(field, kernel_length = [21, 21], nsigma = [3, 3], padding = 'same'):\n    \"\"\"\n    A definition to blur a field using a Gaussian kernel.\n\n    Parameters\n    ----------\n    field         : torch.tensor\n                    MxN field.\n    kernel_length : list\n                    Length of the Gaussian kernel along X and Y axes.\n    nsigma        : list\n                    Sigma of the Gaussian kernel along X and Y axes.\n    padding       : int or string\n                    Padding value, see torch.nn.functional.conv2d() for more.\n\n    Returns\n    ----------\n    blurred_field : torch.tensor\n                    Blurred field.\n    \"\"\"\n    kernel = generate_2d_gaussian(kernel_length, nsigma).to(field.device)\n    kernel = kernel.unsqueeze(0).unsqueeze(0)\n    if len(field.shape) == 2:\n        field = field.view(1, 1, field.shape[-2], field.shape[-1])\n    blurred_field = torch.nn.functional.conv2d(field, kernel, padding='same')\n    if field.shape[1] == 1:\n        blurred_field = blurred_field.view(\n                                           blurred_field.shape[-2],\n                                           blurred_field.shape[-1]\n                                          )\n    return blurred_field\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.matrix.convolve2d","title":"<code>convolve2d(field, kernel)</code>","text":"<p>Definition to convolve a field with a kernel by multiplying in frequency space.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field with MxN shape.\n</code></pre> </li> <li> <code>kernel</code>           \u2013            <pre><code>      Input kernel with MxN shape.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>tensor</code> )          \u2013            <p>Convolved field.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def convolve2d(field, kernel):\n    \"\"\"\n    Definition to convolve a field with a kernel by multiplying in frequency space.\n\n    Parameters\n    ----------\n    field       : torch.tensor\n                  Input field with MxN shape.\n    kernel      : torch.tensor\n                  Input kernel with MxN shape.\n\n    Returns\n    ----------\n    new_field   : torch.tensor\n                  Convolved field.\n    \"\"\"\n    fr = torch.fft.fft2(field)\n    fr2 = torch.fft.fft2(torch.flip(torch.flip(kernel, [1, 0]), [0, 1]))\n    m, n = fr.shape\n    new_field = torch.real(torch.fft.ifft2(fr*fr2))\n    new_field = torch.roll(new_field, shifts=(int(n/2+1), 0), dims=(1, 0))\n    new_field = torch.roll(new_field, shifts=(int(m/2+1), 0), dims=(0, 1))\n    return new_field\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.matrix.correlation_2d","title":"<code>correlation_2d(first_tensor, second_tensor)</code>","text":"<p>Definition to calculate the correlation between two tensors.</p> <p>Parameters:</p> <ul> <li> <code>first_tensor</code>           \u2013            <pre><code>        First tensor.\n</code></pre> </li> <li> <code>second_tensor</code>               (<code>tensor</code>)           \u2013            <pre><code>        Second tensor.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>correlation</code> (              <code>tensor</code> )          \u2013            <p>Correlation between the two tensors.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def correlation_2d(first_tensor, second_tensor):\n    \"\"\"\n    Definition to calculate the correlation between two tensors.\n\n    Parameters\n    ----------\n    first_tensor  : torch.tensor\n                    First tensor.\n    second_tensor : torch.tensor\n                    Second tensor.\n\n    Returns\n    ----------\n    correlation   : torch.tensor\n                    Correlation between the two tensors.\n    \"\"\"\n    fft_first_tensor = (torch.fft.fft2(first_tensor))\n    fft_second_tensor = (torch.fft.fft2(second_tensor))\n    conjugate_second_tensor = torch.conj(fft_second_tensor)\n    result = torch.fft.ifftshift(torch.fft.ifft2(fft_first_tensor * conjugate_second_tensor))\n    return result\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.matrix.crop_center","title":"<code>crop_center(field, size=None)</code>","text":"<p>Definition to crop the center of a field with 2Mx2N size. The outcome is a MxN array.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field 2M x 2N or K x L x 2M x 2N or K x 2M x 2N x L array.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Dimensions to crop with respect to center of the image (e.g., M x N or 1 x 1 x M x N).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>cropped</code> (              <code>ndarray</code> )          \u2013            <p>Cropped version of the input field.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def crop_center(field, size = None):\n    \"\"\"\n    Definition to crop the center of a field with 2Mx2N size. The outcome is a MxN array.\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field 2M x 2N or K x L x 2M x 2N or K x 2M x 2N x L array.\n    size        : list\n                  Dimensions to crop with respect to center of the image (e.g., M x N or 1 x 1 x M x N).\n\n    Returns\n    ----------\n    cropped     : ndarray\n                  Cropped version of the input field.\n    \"\"\"\n    orig_resolution = field.shape\n    if len(field.shape) &lt; 3:\n        field = field.unsqueeze(0)\n    if len(field.shape) &lt; 4:\n        field = field.unsqueeze(0)\n    permute_flag = False\n    if field.shape[-1] &lt; 5:\n        permute_flag = True\n        field = field.permute(0, 3, 1, 2)\n    if type(size) == type(None):\n        qx = int(field.shape[-2] // 4)\n        qy = int(field.shape[-1] // 4)\n        cropped_padded = field[:, :, qx: qx + field.shape[-2] // 2, qy:qy + field.shape[-1] // 2]\n    else:\n        cx = int(field.shape[-2] // 2)\n        cy = int(field.shape[-1] // 2)\n        hx = int(size[-2] // 2)\n        hy = int(size[-1] // 2)\n        cropped_padded = field[:, :, cx-hx:cx+hx, cy-hy:cy+hy]\n    cropped = cropped_padded\n    if permute_flag:\n        cropped = cropped.permute(0, 2, 3, 1)\n    if len(orig_resolution) == 2:\n        cropped = cropped_padded.squeeze(0).squeeze(0)\n    if len(orig_resolution) == 3:\n        cropped = cropped_padded.squeeze(0)\n    return cropped\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.matrix.generate_2d_dirac_delta","title":"<code>generate_2d_dirac_delta(kernel_length=[21, 21], a=[3, 3], mu=[0, 0], theta=0, normalize=False)</code>","text":"<p>Generate 2D Dirac delta function by using Gaussian distribution. Inspired from https://en.wikipedia.org/wiki/Dirac_delta_function</p> <p>Parameters:</p> <ul> <li> <code>kernel_length</code>               (<code>list</code>, default:                   <code>[21, 21]</code> )           \u2013            <pre><code>        Length of the Dirac delta function along X and Y axes.\n</code></pre> </li> <li> <code>a</code>           \u2013            <pre><code>        The scale factor in Gaussian distribution to approximate the Dirac delta function. \n        As a approaches zero, the Gaussian distribution becomes infinitely narrow and tall at the center (x=0), approaching the Dirac delta function.\n</code></pre> </li> <li> <code>mu</code>           \u2013            <pre><code>        Mu of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>theta</code>           \u2013            <pre><code>        The rotation angle of the 2D Dirac delta function.\n</code></pre> </li> <li> <code>normalize</code>           \u2013            <pre><code>        If set True, normalize the output.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>kernel_2d</code> (              <code>tensor</code> )          \u2013            <p>Generated 2D Dirac delta function.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def generate_2d_dirac_delta(\n                            kernel_length = [21, 21],\n                            a = [3, 3],\n                            mu = [0, 0],\n                            theta = 0,\n                            normalize = False\n                           ):\n    \"\"\"\n    Generate 2D Dirac delta function by using Gaussian distribution.\n    Inspired from https://en.wikipedia.org/wiki/Dirac_delta_function\n\n    Parameters\n    ----------\n    kernel_length : list\n                    Length of the Dirac delta function along X and Y axes.\n    a             : list\n                    The scale factor in Gaussian distribution to approximate the Dirac delta function. \n                    As a approaches zero, the Gaussian distribution becomes infinitely narrow and tall at the center (x=0), approaching the Dirac delta function.\n    mu            : list\n                    Mu of the Gaussian kernel along X and Y axes.\n    theta         : float\n                    The rotation angle of the 2D Dirac delta function.\n    normalize     : bool\n                    If set True, normalize the output.\n\n    Returns\n    ----------\n    kernel_2d     : torch.tensor\n                    Generated 2D Dirac delta function.\n    \"\"\"\n    x = torch.linspace(-kernel_length[0] / 2., kernel_length[0] / 2., kernel_length[0])\n    y = torch.linspace(-kernel_length[1] / 2., kernel_length[1] / 2., kernel_length[1])\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    X = X - mu[0]\n    Y = Y - mu[1]\n    theta = torch.as_tensor(theta)\n    X_rot = X * torch.cos(theta) - Y * torch.sin(theta)\n    Y_rot = X * torch.sin(theta) + Y * torch.cos(theta)\n    kernel_2d = (1 / (abs(a[0] * a[1]) * torch.pi)) * torch.exp(-((X_rot / a[0]) ** 2 + (Y_rot / a[1]) ** 2))\n    if normalize:\n        kernel_2d = kernel_2d / kernel_2d.max()\n    return kernel_2d\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.matrix.generate_2d_gaussian","title":"<code>generate_2d_gaussian(kernel_length=[21, 21], nsigma=[3, 3], mu=[0, 0], normalize=False)</code>","text":"<p>Generate 2D Gaussian kernel. Inspired from https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy</p> <p>Parameters:</p> <ul> <li> <code>kernel_length</code>               (<code>list</code>, default:                   <code>[21, 21]</code> )           \u2013            <pre><code>        Length of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>nsigma</code>           \u2013            <pre><code>        Sigma of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>mu</code>           \u2013            <pre><code>        Mu of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>normalize</code>           \u2013            <pre><code>        If set True, normalize the output.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>kernel_2d</code> (              <code>tensor</code> )          \u2013            <p>Generated Gaussian kernel.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def generate_2d_gaussian(kernel_length = [21, 21], nsigma = [3, 3], mu = [0, 0], normalize = False):\n    \"\"\"\n    Generate 2D Gaussian kernel. Inspired from https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy\n\n    Parameters\n    ----------\n    kernel_length : list\n                    Length of the Gaussian kernel along X and Y axes.\n    nsigma        : list\n                    Sigma of the Gaussian kernel along X and Y axes.\n    mu            : list\n                    Mu of the Gaussian kernel along X and Y axes.\n    normalize     : bool\n                    If set True, normalize the output.\n\n    Returns\n    ----------\n    kernel_2d     : torch.tensor\n                    Generated Gaussian kernel.\n    \"\"\"\n    x = torch.linspace(-kernel_length[0]/2., kernel_length[0]/2., kernel_length[0])\n    y = torch.linspace(-kernel_length[1]/2., kernel_length[1]/2., kernel_length[1])\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    if nsigma[0] == 0:\n        nsigma[0] = 1e-5\n    if nsigma[1] == 0:\n        nsigma[1] = 1e-5\n    kernel_2d = 1. / (2. * torch.pi * nsigma[0] * nsigma[1]) * torch.exp(-((X - mu[0])**2. / (2. * nsigma[0]**2.) + (Y - mu[1])**2. / (2. * nsigma[1]**2.)))\n    if normalize:\n        kernel_2d = kernel_2d / kernel_2d.max()\n    return kernel_2d\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.matrix.quantize","title":"<code>quantize(image_field, bits=8, limits=[0.0, 1.0])</code>","text":"<p>Definition to quantize a image field (0-255, 8 bit) to a certain bits level.</p> <p>Parameters:</p> <ul> <li> <code>image_field</code>               (<code>tensor</code>)           \u2013            <pre><code>      Input image field between any range.\n</code></pre> </li> <li> <code>bits</code>           \u2013            <pre><code>      A value in between one to eight.\n</code></pre> </li> <li> <code>limits</code>           \u2013            <pre><code>      The minimum and maximum of the image_field variable.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>tensor</code> )          \u2013            <p>Quantized image field.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def quantize(image_field, bits = 8, limits = [0., 1.]):\n    \"\"\" \n    Definition to quantize a image field (0-255, 8 bit) to a certain bits level.\n\n    Parameters\n    ----------\n    image_field : torch.tensor\n                  Input image field between any range.\n    bits        : int\n                  A value in between one to eight.\n    limits      : list\n                  The minimum and maximum of the image_field variable.\n\n    Returns\n    ----------\n    new_field   : torch.tensor\n                  Quantized image field.\n    \"\"\"\n    normalized_field = (image_field - limits[0]) / (limits[1] - limits[0])\n    divider = 2 ** bits\n    new_field = normalized_field * divider\n    new_field = new_field.int()\n    return new_field\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.matrix.zero_pad","title":"<code>zero_pad(field, size=None, method='center')</code>","text":"<p>Definition to zero pad a MxN array to 2Mx2N array.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>            Input field MxN or KxJxMxN or KxMxNxJ array.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>            Size to be zeropadded (e.g., [m, n], last two dimensions only).\n</code></pre> </li> <li> <code>method</code>           \u2013            <pre><code>            Zeropad either by placing the content to center or to the left.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field_zero_padded</code> (              <code>ndarray</code> )          \u2013            <p>Zeropadded version of the input field.</p> </li> </ul> Source code in <code>odak/learn/tools/matrix.py</code> <pre><code>def zero_pad(field, size = None, method = 'center'):\n    \"\"\"\n    Definition to zero pad a MxN array to 2Mx2N array.\n\n    Parameters\n    ----------\n    field             : ndarray\n                        Input field MxN or KxJxMxN or KxMxNxJ array.\n    size              : list\n                        Size to be zeropadded (e.g., [m, n], last two dimensions only).\n    method            : str\n                        Zeropad either by placing the content to center or to the left.\n\n    Returns\n    ----------\n    field_zero_padded : ndarray\n                        Zeropadded version of the input field.\n    \"\"\"\n    orig_resolution = field.shape\n    if len(field.shape) &lt; 3:\n        field = field.unsqueeze(0)\n    if len(field.shape) &lt; 4:\n        field = field.unsqueeze(0)\n    permute_flag = False\n    if field.shape[-1] &lt; 5:\n        permute_flag = True\n        field = field.permute(0, 3, 1, 2)\n    if type(size) == type(None):\n        resolution = [field.shape[0], field.shape[1], 2 * field.shape[-2], 2 * field.shape[-1]]\n    else:\n        resolution = [field.shape[0], field.shape[1], size[0], size[1]]\n    field_zero_padded = torch.zeros(resolution, device = field.device, dtype = field.dtype)\n    if method == 'center':\n       start = [\n                resolution[-2] // 2 - field.shape[-2] // 2,\n                resolution[-1] // 2 - field.shape[-1] // 2\n               ]\n       field_zero_padded[\n                         :, :,\n                         start[0] : start[0] + field.shape[-2],\n                         start[1] : start[1] + field.shape[-1]\n                         ] = field\n    elif method == 'left':\n       field_zero_padded[\n                         :, :,\n                         0: field.shape[-2],\n                         0: field.shape[-1]\n                        ] = field\n    if permute_flag == True:\n        field_zero_padded = field_zero_padded.permute(0, 2, 3, 1)\n    if len(orig_resolution) == 2:\n        field_zero_padded = field_zero_padded.squeeze(0).squeeze(0)\n    if len(orig_resolution) == 3:\n        field_zero_padded = field_zero_padded.squeeze(0)\n    return field_zero_padded\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.sample.grid_sample","title":"<code>grid_sample(no=[10, 10], size=[100.0, 100.0], center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate samples over a surface.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Physical size of the surface.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>tensor</code> )          \u2013            <p>Samples generated.</p> </li> <li> <code>rotx</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at X axis.</p> </li> <li> <code>roty</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at Y axis.</p> </li> <li> <code>rotz</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix at Z axis.</p> </li> </ul> Source code in <code>odak/learn/tools/sample.py</code> <pre><code>def grid_sample(\n                no = [10, 10],\n                size = [100., 100.], \n                center = [0., 0., 0.], \n                angles = [0., 0., 0.]):\n    \"\"\"\n    Definition to generate samples over a surface.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    size        : list\n                  Physical size of the surface.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    -------\n    samples     : torch.tensor\n                  Samples generated.\n    rotx        : torch.tensor\n                  Rotation matrix at X axis.\n    roty        : torch.tensor\n                  Rotation matrix at Y axis.\n    rotz        : torch.tensor\n                  Rotation matrix at Z axis.\n    \"\"\"\n    center = torch.tensor(center)\n    angles = torch.tensor(angles)\n    size = torch.tensor(size)\n    samples = torch.zeros((no[0], no[1], 3))\n    x = torch.linspace(-size[0] / 2., size[0] / 2., no[0])\n    y = torch.linspace(-size[1] / 2., size[1] / 2., no[1])\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    samples[:, :, 0] = X.detach().clone()\n    samples[:, :, 1] = Y.detach().clone()\n    samples = samples.reshape((samples.shape[0] * samples.shape[1], samples.shape[2]))\n    samples, rotx, roty, rotz = rotate_points(samples, angles = angles, offset = center)\n    return samples, rotx, roty, rotz\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.transformation.get_rotation_matrix","title":"<code>get_rotation_matrix(tilt_angles=[0.0, 0.0, 0.0], tilt_order='XYZ')</code>","text":"<p>Function to generate rotation matrix for given tilt angles and tilt order.</p> <p>Parameters:</p> <ul> <li> <code>tilt_angles</code>           \u2013            <pre><code>             Tilt angles in degrees along XYZ axes.\n</code></pre> </li> <li> <code>tilt_order</code>           \u2013            <pre><code>             Rotation order (e.g., XYZ, XZY, ZXY, YXZ, ZYX).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rotmat</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix.</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def get_rotation_matrix(tilt_angles = [0., 0., 0.], tilt_order = 'XYZ'):\n    \"\"\"\n    Function to generate rotation matrix for given tilt angles and tilt order.\n\n\n    Parameters\n    ----------\n    tilt_angles        : list\n                         Tilt angles in degrees along XYZ axes.\n    tilt_order         : str\n                         Rotation order (e.g., XYZ, XZY, ZXY, YXZ, ZYX).\n\n    Returns\n    -------\n    rotmat             : torch.tensor\n                         Rotation matrix.\n    \"\"\"\n    rotx = rotmatx(tilt_angles[0])\n    roty = rotmaty(tilt_angles[1])\n    rotz = rotmatz(tilt_angles[2])\n    if tilt_order =='XYZ':\n        rotmat = torch.mm(rotz,torch.mm(roty, rotx))\n    elif tilt_order == 'XZY':\n        rotmat = torch.mm(roty,torch.mm(rotz, rotx))\n    elif tilt_order == 'ZXY':\n        rotmat = torch.mm(roty,torch.mm(rotx, rotz))\n    elif tilt_order == 'YXZ':\n        rotmat = torch.mm(rotz,torch.mm(rotx, roty))\n    elif tilt_order == 'ZYX':\n         rotmat = torch.mm(rotx,torch.mm(roty, rotz))\n    return rotmat\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.transformation.rotate_points","title":"<code>rotate_points(point, angles=torch.tensor([[0, 0, 0]]), mode='XYZ', origin=torch.tensor([[0, 0, 0]]), offset=torch.tensor([[0, 0, 0]]))</code>","text":"<p>Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>       A point with size of [3] or [1, 3] or [m, 3].\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>       Rotation mode determines ordering of the rotations at each axis.\n       There are XYZ,YXZ,ZXY and ZYX modes.\n</code></pre> </li> <li> <code>origin</code>           \u2013            <pre><code>       Reference point for a rotation.\n       Expected size is [3] or [1, 3].\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>       Shift with the given offset.\n       Expected size is [3] or [1, 3] or [m, 3].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the rotation [1 x 3] or [m x 3].</p> </li> <li> <code>rotx</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along X axis [3 x 3].</p> </li> <li> <code>roty</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along Y axis [3 x 3].</p> </li> <li> <code>rotz</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along Z axis [3 x 3].</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def rotate_points(\n                 point,\n                 angles = torch.tensor([[0, 0, 0]]), \n                 mode='XYZ', \n                 origin = torch.tensor([[0, 0, 0]]), \n                 offset = torch.tensor([[0, 0, 0]])\n                ):\n    \"\"\"\n    Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.\n\n    Parameters\n    ----------\n    point        : torch.tensor\n                   A point with size of [3] or [1, 3] or [m, 3].\n    angles       : torch.tensor\n                   Rotation angles in degrees. \n    mode         : str\n                   Rotation mode determines ordering of the rotations at each axis.\n                   There are XYZ,YXZ,ZXY and ZYX modes.\n    origin       : torch.tensor\n                   Reference point for a rotation.\n                   Expected size is [3] or [1, 3].\n    offset       : torch.tensor\n                   Shift with the given offset.\n                   Expected size is [3] or [1, 3] or [m, 3].\n\n    Returns\n    ----------\n    result       : torch.tensor\n                   Result of the rotation [1 x 3] or [m x 3].\n    rotx         : torch.tensor\n                   Rotation matrix along X axis [3 x 3].\n    roty         : torch.tensor\n                   Rotation matrix along Y axis [3 x 3].\n    rotz         : torch.tensor\n                   Rotation matrix along Z axis [3 x 3].\n    \"\"\"\n    origin = origin.to(point.device)\n    offset = offset.to(point.device)\n    if len(point.shape) == 1:\n        point = point.unsqueeze(0)\n    if len(angles.shape) == 1:\n        angles = angles.unsqueeze(0)\n    rotx = rotmatx(angles[:, 0])\n    roty = rotmaty(angles[:, 1])\n    rotz = rotmatz(angles[:, 2])\n    new_points = (point - origin).T\n    if angles.shape[0] &gt; 1:\n        new_points = new_points.unsqueeze(0)\n        if len(origin.shape) == 2:\n            origin = origin.unsqueeze(1)\n        if len(offset.shape) == 2:\n            offset = offset.unsqueeze(1)\n    if mode == 'XYZ':\n        result = (rotz @ (roty @ (rotx @ new_points))).mT\n    elif mode == 'XZY':\n        result = torch.mm(roty, torch.mm(rotz, torch.mm(rotx, new_points))).T\n    elif mode == 'YXZ':\n        result = torch.mm(rotz, torch.mm(rotx, torch.mm(roty, new_points))).T\n    elif mode == 'ZXY':\n        result = torch.mm(roty, torch.mm(rotx, torch.mm(rotz, new_points))).T\n    elif mode == 'ZYX':\n        result = torch.mm(rotx, torch.mm(roty, torch.mm(rotz, new_points))).T\n    result += origin\n    result += offset\n    return result, rotx, roty, rotz\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.transformation.rotmatx","title":"<code>rotmatx(angle)</code>","text":"<p>Definition to generate a rotation matrix along X axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rotx</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along X axis.</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def rotmatx(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along X axis.\n\n    Parameters\n    ----------\n    angle        : torch.tensor\n                   Rotation angles in degrees.\n\n    Returns\n    ----------\n    rotx         : torch.tensor\n                   Rotation matrix along X axis.\n    \"\"\"\n    angle = torch.deg2rad(angle)\n    rotx = torch.zeros(angle.shape[0], 3, 3, device = angle.device)\n    rotx[:, 0, 0] = 1.\n    rotx[:, 1, 1] = torch.cos(angle)\n    rotx[:, 1, 2] = - torch.sin(angle)\n    rotx[:, 2, 1] = torch.sin(angle)\n    rotx[:, 2, 2] = torch.cos(angle)\n    if rotx.shape[0] == 1:\n        rotx = rotx.squeeze(0)\n    return rotx\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.transformation.rotmaty","title":"<code>rotmaty(angle)</code>","text":"<p>Definition to generate a rotation matrix along Y axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>roty</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along Y axis.</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def rotmaty(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along Y axis.\n\n    Parameters\n    ----------\n    angle        : torch.tensor\n                   Rotation angles in degrees.\n\n    Returns\n    ----------\n    roty         : torch.tensor\n                   Rotation matrix along Y axis.\n    \"\"\"\n    angle = torch.deg2rad(angle)\n    roty = torch.zeros(angle.shape[0], 3, 3, device = angle.device)\n    roty[:, 0, 0] = torch.cos(angle)\n    roty[:, 0, 2] = torch.sin(angle)\n    roty[:, 1, 1] = 1.\n    roty[:, 2, 0] = - torch.sin(angle)\n    roty[:, 2, 2] = torch.cos(angle)\n    if roty.shape[0] == 1:\n        roty = roty.squeeze(0)\n    return roty\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.transformation.rotmatz","title":"<code>rotmatz(angle)</code>","text":"<p>Definition to generate a rotation matrix along Z axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rotz</code> (              <code>tensor</code> )          \u2013            <p>Rotation matrix along Z axis.</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def rotmatz(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along Z axis.\n\n    Parameters\n    ----------\n    angle        : torch.tensor\n                   Rotation angles in degrees.\n\n    Returns\n    ----------\n    rotz         : torch.tensor\n                   Rotation matrix along Z axis.\n    \"\"\"\n    angle = torch.deg2rad(angle)\n    rotz = torch.zeros(angle.shape[0], 3, 3, device = angle.device)\n    rotz[:, 0, 0] = torch.cos(angle)\n    rotz[:, 0, 1] = - torch.sin(angle)\n    rotz[:, 1, 0] = torch.sin(angle)\n    rotz[:, 1, 1] = torch.cos(angle)\n    rotz[:, 2, 2] = 1.\n    if rotz.shape[0] == 1:\n        rotz = rotz.squeeze(0)\n    return rotz\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.transformation.tilt_towards","title":"<code>tilt_towards(location, lookat)</code>","text":"<p>Definition to tilt surface normal of a plane towards a point.</p> <p>Parameters:</p> <ul> <li> <code>location</code>           \u2013            <pre><code>       Center of the plane to be tilted.\n</code></pre> </li> <li> <code>lookat</code>           \u2013            <pre><code>       Tilt towards this point.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>angles</code> (              <code>list</code> )          \u2013            <p>Rotation angles in degrees.</p> </li> </ul> Source code in <code>odak/learn/tools/transformation.py</code> <pre><code>def tilt_towards(location, lookat):\n    \"\"\"\n    Definition to tilt surface normal of a plane towards a point.\n\n    Parameters\n    ----------\n    location     : list\n                   Center of the plane to be tilted.\n    lookat       : list\n                   Tilt towards this point.\n\n    Returns\n    ----------\n    angles       : list\n                   Rotation angles in degrees.\n    \"\"\"\n    dx = location[0] - lookat[0]\n    dy = location[1] - lookat[1]\n    dz = location[2] - lookat[2]\n    dist = torch.sqrt(torch.tensor(dx ** 2 + dy ** 2 + dz ** 2))\n    phi = torch.atan2(torch.tensor(dy), torch.tensor(dx))\n    theta = torch.arccos(dz / dist)\n    angles = [0, float(torch.rad2deg(theta)), float(torch.rad2deg(phi))]\n    return angles\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.vector.cross_product","title":"<code>cross_product(vector1, vector2)</code>","text":"<p>Definition to cross product two vectors and return the resultant vector. Used method described under: http://en.wikipedia.org/wiki/Cross_product</p> <p>Parameters:</p> <ul> <li> <code>vector1</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>vector2</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>tensor</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray.</p> </li> </ul> Source code in <code>odak/learn/tools/vector.py</code> <pre><code>def cross_product(vector1, vector2):\n    \"\"\"\n    Definition to cross product two vectors and return the resultant vector. Used method described under: http://en.wikipedia.org/wiki/Cross_product\n\n    Parameters\n    ----------\n    vector1      : torch.tensor\n                   A vector/ray.\n    vector2      : torch.tensor\n                   A vector/ray.\n\n    Returns\n    ----------\n    ray          : torch.tensor\n                   Array that contains starting points and cosines of a created ray.\n    \"\"\"\n    angle = torch.cross(vector1[1].T, vector2[1].T)\n    angle = torch.tensor(angle)\n    ray = torch.tensor([vector1[0], angle], dtype=torch.float32)\n    return ray\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.vector.distance_between_two_points","title":"<code>distance_between_two_points(point1, point2)</code>","text":"<p>Definition to calculate distance between two given points.</p> <p>Parameters:</p> <ul> <li> <code>point1</code>           \u2013            <pre><code>      First point in X,Y,Z.\n</code></pre> </li> <li> <code>point2</code>           \u2013            <pre><code>      Second point in X,Y,Z.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>Tensor</code> )          \u2013            <p>Distance in between given two points.</p> </li> </ul> Source code in <code>odak/learn/tools/vector.py</code> <pre><code>def distance_between_two_points(point1, point2):\n    \"\"\"\n    Definition to calculate distance between two given points.\n\n    Parameters\n    ----------\n    point1      : torch.Tensor\n                  First point in X,Y,Z.\n    point2      : torch.Tensor\n                  Second point in X,Y,Z.\n\n    Returns\n    ----------\n    distance    : torch.Tensor\n                  Distance in between given two points.\n    \"\"\"\n    point1 = torch.tensor(point1) if not isinstance(point1, torch.Tensor) else point1\n    point2 = torch.tensor(point2) if not isinstance(point2, torch.Tensor) else point2\n\n    if len(point1.shape) == 1 and len(point2.shape) == 1:\n        distance = torch.sqrt(torch.sum((point1 - point2) ** 2))\n    elif len(point1.shape) == 2 or len(point2.shape) == 2:\n        distance = torch.sqrt(torch.sum((point1 - point2) ** 2, dim=-1))\n\n    return distance\n</code></pre>"},{"location":"odak/learn_tools/#odak.learn.tools.vector.same_side","title":"<code>same_side(p1, p2, a, b)</code>","text":"<p>Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.</p> <p>Parameters:</p> <ul> <li> <code>p1</code>           \u2013            <pre><code>      Point(s) to check.\n</code></pre> </li> <li> <code>p2</code>           \u2013            <pre><code>      This is the point check against.\n</code></pre> </li> <li> <code>a</code>           \u2013            <pre><code>      First point that forms the line.\n</code></pre> </li> <li> <code>b</code>           \u2013            <pre><code>      Second point that forms the line.\n</code></pre> </li> </ul> Source code in <code>odak/learn/tools/vector.py</code> <pre><code>def same_side(p1, p2, a, b):\n    \"\"\"\n    Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.\n\n    Parameters\n    ----------\n    p1          : list\n                  Point(s) to check.\n    p2          : list\n                  This is the point check against.\n    a           : list\n                  First point that forms the line.\n    b           : list\n                  Second point that forms the line.\n    \"\"\"\n    ba = torch.subtract(b, a)\n    p1a = torch.subtract(p1, a)\n    p2a = torch.subtract(p2, a)\n    cp1 = torch.cross(ba, p1a)\n    cp2 = torch.cross(ba, p2a)\n    test = torch.dot(cp1, cp2)\n    if len(p1.shape) &gt; 1:\n        return test &gt;= 0\n    if test &gt;= 0:\n        return True\n    return False\n</code></pre>"},{"location":"odak/learn_wave/","title":"odak.learn.wave","text":""},{"location":"odak/learn_wave/#odak.learn.wave.classical.angular_spectrum","title":"<code>angular_spectrum(field, k, distance, dx, wavelength, zero_padding=False, aperture=1.0)</code>","text":"<p>A definition to calculate convolution with Angular Spectrum method for beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field [m x n].\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>zero_padding</code>           \u2013            <pre><code>           Zero pad in Fourier domain.\n</code></pre> </li> <li> <code>aperture</code>           \u2013            <pre><code>           Fourier domain aperture (e.g., pinhole in a typical holographic display).\n           The default is one, but an aperture could be as large as input field [m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def angular_spectrum(\n                     field,\n                     k,\n                     distance,\n                     dx,\n                     wavelength,\n                     zero_padding = False,\n                     aperture = 1.\n                    ):\n    \"\"\"\n    A definition to calculate convolution with Angular Spectrum method for beam propagation.\n\n    Parameters\n    ----------\n    field            : torch.complex\n                       Complex field [m x n].\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    zero_padding     : bool\n                       Zero pad in Fourier domain.\n    aperture         : torch.tensor\n                       Fourier domain aperture (e.g., pinhole in a typical holographic display).\n                       The default is one, but an aperture could be as large as input field [m x n].\n\n\n    Returns\n    -------\n    result           : torch.complex\n                       Final complex field (MxN).\n\n    \"\"\"\n    H = get_propagation_kernel(\n                               nu = field.shape[-2], \n                               nv = field.shape[-1], \n                               dx = dx, \n                               wavelength = wavelength, \n                               distance = distance, \n                               propagation_type = 'Angular Spectrum',\n                               device = field.device\n                              )\n    result = custom(field, H, zero_padding = zero_padding, aperture = aperture)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.band_limited_angular_spectrum","title":"<code>band_limited_angular_spectrum(field, k, distance, dx, wavelength, zero_padding=False, aperture=1.0)</code>","text":"<p>A definition to calculate bandlimited angular spectrum based beam propagation. For more  <code>Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673</code>.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           A complex field.\n           The expected size is [m x n].\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>zero_padding</code>           \u2013            <pre><code>           Zero pad in Fourier domain.\n</code></pre> </li> <li> <code>aperture</code>           \u2013            <pre><code>           Fourier domain aperture (e.g., pinhole in a typical holographic display).\n           The default is one, but an aperture could be as large as input field [m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field [m x n].</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def band_limited_angular_spectrum(\n                                  field,\n                                  k,\n                                  distance,\n                                  dx,\n                                  wavelength,\n                                  zero_padding = False,\n                                  aperture = 1.\n                                 ):\n    \"\"\"\n    A definition to calculate bandlimited angular spectrum based beam propagation. For more \n    `Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673`.\n\n    Parameters\n    ----------\n    field            : torch.complex\n                       A complex field.\n                       The expected size is [m x n].\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    zero_padding     : bool\n                       Zero pad in Fourier domain.\n    aperture         : torch.tensor\n                       Fourier domain aperture (e.g., pinhole in a typical holographic display).\n                       The default is one, but an aperture could be as large as input field [m x n].\n\n\n    Returns\n    -------\n    result           : torch.complex\n                       Final complex field [m x n].\n    \"\"\"\n    H = get_propagation_kernel(\n                               nu = field.shape[-2], \n                               nv = field.shape[-1], \n                               dx = dx, \n                               wavelength = wavelength, \n                               distance = distance, \n                               propagation_type = 'Bandlimited Angular Spectrum',\n                               device = field.device\n                              )\n    result = custom(field, H, zero_padding = zero_padding, aperture = aperture)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.custom","title":"<code>custom(field, kernel, zero_padding=False, aperture=1.0)</code>","text":"<p>A definition to calculate convolution based Fresnel approximation for beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field [m x n].\n</code></pre> </li> <li> <code>kernel</code>           \u2013            <pre><code>           Custom complex kernel for beam propagation.\n</code></pre> </li> <li> <code>zero_padding</code>           \u2013            <pre><code>           Zero pad in Fourier domain.\n</code></pre> </li> <li> <code>aperture</code>           \u2013            <pre><code>           Fourier domain aperture (e.g., pinhole in a typical holographic display).\n           The default is one, but an aperture could be as large as input field [m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def custom(\n           field,\n           kernel,\n           zero_padding = False,\n           aperture = 1.\n          ):\n    \"\"\"\n    A definition to calculate convolution based Fresnel approximation for beam propagation.\n\n    Parameters\n    ----------\n    field            : torch.complex\n                       Complex field [m x n].\n    kernel           : torch.complex\n                       Custom complex kernel for beam propagation.\n    zero_padding     : bool\n                       Zero pad in Fourier domain.\n    aperture         : torch.tensor\n                       Fourier domain aperture (e.g., pinhole in a typical holographic display).\n                       The default is one, but an aperture could be as large as input field [m x n].\n\n    Returns\n    -------\n    result           : torch.complex\n                       Final complex field (MxN).\n\n    \"\"\"\n    if type(kernel) == type(None):\n        H = torch.ones(field.shape).to(field.device)\n    else:\n        H = kernel * aperture\n    U1 = torch.fft.fftshift(torch.fft.fft2(field)) * aperture\n    if zero_padding == False:\n        U2 = H * U1\n    elif zero_padding == True:\n        U2 = zero_pad(H * U1)\n    result = torch.fft.ifft2(torch.fft.ifftshift(U2))\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.fraunhofer","title":"<code>fraunhofer(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate light transport usin Fraunhofer approximation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def fraunhofer(\n               field,\n               k,\n               distance,\n               dx,\n               wavelength\n              ):\n    \"\"\"\n    A definition to calculate light transport usin Fraunhofer approximation.\n\n    Parameters\n    ----------\n    field            : torch.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : torch.complex\n                       Final complex field (MxN).\n    \"\"\"\n    nv, nu = field.shape[-1], field.shape[-2]\n    x = torch.linspace(-nv*dx/2, nv*dx/2, nv, dtype=torch.float32)\n    y = torch.linspace(-nu*dx/2, nu*dx/2, nu, dtype=torch.float32)\n    Y, X = torch.meshgrid(y, x, indexing='ij')\n    Z = torch.pow(X, 2) + torch.pow(Y, 2)\n    c = 1. / (1j * wavelength * distance) * torch.exp(1j * k * 0.5 / distance * Z)\n    c = c.to(field.device)\n    result = c * torch.fft.ifftshift(torch.fft.fft2(torch.fft.fftshift(field))) * dx ** 2\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.gerchberg_saxton","title":"<code>gerchberg_saxton(field, n_iterations, distance, dx, wavelength, slm_range=6.28, propagation_type='Transfer Function Fresnel')</code>","text":"<p>Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>slm_range</code>           \u2013            <pre><code>           Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more.\n</code></pre> </li> <li> <code>propagation_type</code>               (<code>str</code>, default:                   <code>'Transfer Function Fresnel'</code> )           \u2013            <pre><code>           Type of the propagation (see odak.learn.wave.propagate_beam).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>hologram</code> (              <code>cfloat</code> )          \u2013            <p>Calculated complex hologram.</p> </li> <li> <code>reconstruction</code> (              <code>cfloat</code> )          \u2013            <p>Calculated reconstruction using calculated hologram.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def gerchberg_saxton(\n                     field,\n                     n_iterations,\n                     distance,\n                     dx,\n                     wavelength,\n                     slm_range = 6.28,\n                     propagation_type = 'Transfer Function Fresnel'\n                    ):\n    \"\"\"\n    Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246.\n\n    Parameters\n    ----------\n    field            : torch.cfloat\n                       Complex field (MxN).\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    slm_range        : float\n                       Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more.\n    propagation_type : str\n                       Type of the propagation (see odak.learn.wave.propagate_beam).\n\n    Returns\n    -------\n    hologram         : torch.cfloat\n                       Calculated complex hologram.\n    reconstruction   : torch.cfloat\n                       Calculated reconstruction using calculated hologram. \n    \"\"\"\n    k = wavenumber(wavelength)\n    reconstruction = field\n    for i in range(n_iterations):\n        hologram = propagate_beam(\n            reconstruction, k, -distance, dx, wavelength, propagation_type)\n        reconstruction = propagate_beam(\n            hologram, k, distance, dx, wavelength, propagation_type)\n        reconstruction = set_amplitude(reconstruction, field)\n    reconstruction = propagate_beam(\n        hologram, k, distance, dx, wavelength, propagation_type)\n    return hologram, reconstruction\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.get_angular_spectrum_kernel","title":"<code>get_angular_spectrum_kernel(nu, nv, dx=8e-06, wavelength=5.15e-07, distance=0.0, device=torch.device('cpu'))</code>","text":"<p>Helper function for odak.learn.wave.angular_spectrum.</p> <p>Parameters:</p> <ul> <li> <code>nu</code>           \u2013            <pre><code>             Resolution at X axis in pixels.\n</code></pre> </li> <li> <code>nv</code>           \u2013            <pre><code>             Resolution at Y axis in pixels.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>             Pixel pitch in meters.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>             Wavelength in meters.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>             Distance in meters.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>             Device, for more see torch.device().\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>H</code> (              <code>float</code> )          \u2013            <p>Complex kernel in Fourier domain.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def get_angular_spectrum_kernel(\n                                nu,\n                                nv,\n                                dx = 8e-6,\n                                wavelength = 515e-9,\n                                distance = 0.,\n                                device = torch.device('cpu')\n                               ):\n    \"\"\"\n    Helper function for odak.learn.wave.angular_spectrum.\n\n    Parameters\n    ----------\n    nu                 : int\n                         Resolution at X axis in pixels.\n    nv                 : int\n                         Resolution at Y axis in pixels.\n    dx                 : float\n                         Pixel pitch in meters.\n    wavelength         : float\n                         Wavelength in meters.\n    distance           : float\n                         Distance in meters.\n    device             : torch.device\n                         Device, for more see torch.device().\n\n\n    Returns\n    -------\n    H                  : float\n                         Complex kernel in Fourier domain.\n    \"\"\"\n    distance = torch.tensor([distance]).to(device)\n    fx = torch.linspace(-1. / 2. / dx, 1. / 2. / dx, nu, dtype = torch.float32, device = device)\n    fy = torch.linspace(-1. / 2. / dx, 1. / 2. / dx, nv, dtype = torch.float32, device = device)\n    FY, FX = torch.meshgrid(fx, fy, indexing='ij')\n    H = torch.exp(1j  * distance * (2 * (torch.pi * (1 / wavelength) * torch.sqrt(1. - (wavelength * FX) ** 2 - (wavelength * FY) ** 2))))\n    H = H.to(device)\n    return H\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.get_band_limited_angular_spectrum_kernel","title":"<code>get_band_limited_angular_spectrum_kernel(nu, nv, dx=8e-06, wavelength=5.15e-07, distance=0.0, device=torch.device('cpu'))</code>","text":"<p>Helper function for odak.learn.wave.band_limited_angular_spectrum.</p> <p>Parameters:</p> <ul> <li> <code>nu</code>           \u2013            <pre><code>             Resolution at X axis in pixels.\n</code></pre> </li> <li> <code>nv</code>           \u2013            <pre><code>             Resolution at Y axis in pixels.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>             Pixel pitch in meters.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>             Wavelength in meters.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>             Distance in meters.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>             Device, for more see torch.device().\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>H</code> (              <code>complex64</code> )          \u2013            <p>Complex kernel in Fourier domain.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def get_band_limited_angular_spectrum_kernel(\n                                             nu,\n                                             nv,\n                                             dx = 8e-6,\n                                             wavelength = 515e-9,\n                                             distance = 0.,\n                                             device = torch.device('cpu')\n                                            ):\n    \"\"\"\n    Helper function for odak.learn.wave.band_limited_angular_spectrum.\n\n    Parameters\n    ----------\n    nu                 : int\n                         Resolution at X axis in pixels.\n    nv                 : int\n                         Resolution at Y axis in pixels.\n    dx                 : float\n                         Pixel pitch in meters.\n    wavelength         : float\n                         Wavelength in meters.\n    distance           : float\n                         Distance in meters.\n    device             : torch.device\n                         Device, for more see torch.device().\n\n\n    Returns\n    -------\n    H                  : torch.complex64\n                         Complex kernel in Fourier domain.\n    \"\"\"\n    x = dx * float(nu)\n    y = dx * float(nv)\n    fx = torch.linspace(\n                        -1 / (2 * dx) + 0.5 / (2 * x),\n                         1 / (2 * dx) - 0.5 / (2 * x),\n                         nu,\n                         dtype = torch.float32,\n                         device = device\n                        )\n    fy = torch.linspace(\n                        -1 / (2 * dx) + 0.5 / (2 * y),\n                        1 / (2 * dx) - 0.5 / (2 * y),\n                        nv,\n                        dtype = torch.float32,\n                        device = device\n                       )\n    FY, FX = torch.meshgrid(fx, fy, indexing='ij')\n    HH_exp = 2 * torch.pi * torch.sqrt(1 / wavelength ** 2 - (FX ** 2 + FY ** 2))\n    distance = torch.tensor([distance], device = device)\n    H_exp = torch.mul(HH_exp, distance)\n    fx_max = 1 / torch.sqrt((2 * distance * (1 / x))**2 + 1) / wavelength\n    fy_max = 1 / torch.sqrt((2 * distance * (1 / y))**2 + 1) / wavelength\n    H_filter = ((torch.abs(FX) &lt; fx_max) &amp; (torch.abs(FY) &lt; fy_max)).clone().detach()\n    H = generate_complex_field(H_filter, H_exp)\n    return H\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.get_impulse_response_fresnel_kernel","title":"<code>get_impulse_response_fresnel_kernel(nu, nv, dx=8e-06, wavelength=5.15e-07, distance=0.0, device=torch.device('cpu'), scale=1, aperture_samples=[20, 20, 5, 5])</code>","text":"<p>Helper function for odak.learn.wave.impulse_response_fresnel.</p> <p>Parameters:</p> <ul> <li> <code>nu</code>           \u2013            <pre><code>             Resolution at X axis in pixels.\n</code></pre> </li> <li> <code>nv</code>           \u2013            <pre><code>             Resolution at Y axis in pixels.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>             Pixel pitch in meters.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>             Wavelength in meters.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>             Distance in meters.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>             Device, for more see torch.device().\n</code></pre> </li> <li> <code>scale</code>           \u2013            <pre><code>             Scale with respect to nu and nv (e.g., scale = 2 leads to  2 x nu and 2 x nv resolution for H).\n</code></pre> </li> <li> <code>aperture_samples</code>           \u2013            <pre><code>             Number of samples to represent a rectangular pixel. First two is for XY of hologram plane pixels, and second two is for image plane pixels.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>H</code> (              <code>complex64</code> )          \u2013            <p>Complex kernel in Fourier domain.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def get_impulse_response_fresnel_kernel(\n                                        nu,\n                                        nv,\n                                        dx = 8e-6,\n                                        wavelength = 515e-9,\n                                        distance = 0.,\n                                        device = torch.device('cpu'),\n                                        scale = 1,\n                                        aperture_samples = [20, 20, 5, 5]\n                                       ):\n    \"\"\"\n    Helper function for odak.learn.wave.impulse_response_fresnel.\n\n    Parameters\n    ----------\n    nu                 : int\n                         Resolution at X axis in pixels.\n    nv                 : int\n                         Resolution at Y axis in pixels.\n    dx                 : float\n                         Pixel pitch in meters.\n    wavelength         : float\n                         Wavelength in meters.\n    distance           : float\n                         Distance in meters.\n    device             : torch.device\n                         Device, for more see torch.device().\n    scale              : int\n                         Scale with respect to nu and nv (e.g., scale = 2 leads to  2 x nu and 2 x nv resolution for H).\n    aperture_samples   : list\n                         Number of samples to represent a rectangular pixel. First two is for XY of hologram plane pixels, and second two is for image plane pixels.\n\n    Returns\n    -------\n    H                  : torch.complex64\n                         Complex kernel in Fourier domain.\n    \"\"\"\n    k = wavenumber(wavelength)\n    distance = torch.as_tensor(distance, device = device)\n    length_x, length_y = (torch.tensor(dx * nu, device = device), torch.tensor(dx * nv, device = device))\n    x = torch.linspace(- length_x / 2., length_x / 2., nu * scale, device = device)\n    y = torch.linspace(- length_y / 2., length_y / 2., nv * scale, device = device)\n    X, Y = torch.meshgrid(x, y, indexing = 'ij')\n    wxs = torch.linspace(- dx / 2., dx / 2., aperture_samples[0], device = device)\n    wys = torch.linspace(- dx / 2., dx / 2., aperture_samples[1], device = device)\n    h = torch.zeros(nu * scale, nv * scale, dtype = torch.complex64, device = device)\n    pxs = torch.linspace(- dx / 2., dx / 2., aperture_samples[2], device = device)\n    pys = torch.linspace(- dx / 2., dx / 2., aperture_samples[3], device = device)\n    for wx in tqdm(wxs):\n        for wy in wys:\n            for px in pxs:\n                for py in pys:\n                    r = (X + px - wx) ** 2 + (Y + py - wy) ** 2\n                    h += 1. / (1j * wavelength * distance) * torch.exp(1j * k / (2 * distance) * r) \n    H = torch.fft.fftshift(torch.fft.fft2(torch.fft.fftshift(h))) * dx ** 2 / aperture_samples[0] / aperture_samples[1] / aperture_samples[2] / aperture_samples[3]\n    return H\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.get_incoherent_angular_spectrum_kernel","title":"<code>get_incoherent_angular_spectrum_kernel(nu, nv, dx=8e-06, wavelength=5.15e-07, distance=0.0, device=torch.device('cpu'))</code>","text":"<p>Helper function for odak.learn.wave.angular_spectrum.</p> <p>Parameters:</p> <ul> <li> <code>nu</code>           \u2013            <pre><code>             Resolution at X axis in pixels.\n</code></pre> </li> <li> <code>nv</code>           \u2013            <pre><code>             Resolution at Y axis in pixels.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>             Pixel pitch in meters.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>             Wavelength in meters.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>             Distance in meters.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>             Device, for more see torch.device().\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>H</code> (              <code>float</code> )          \u2013            <p>Complex kernel in Fourier domain.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def get_incoherent_angular_spectrum_kernel(\n                                           nu,\n                                           nv,\n                                           dx = 8e-6,\n                                           wavelength = 515e-9,\n                                           distance = 0.,\n                                           device = torch.device('cpu')\n                                          ):\n    \"\"\"\n    Helper function for odak.learn.wave.angular_spectrum.\n\n    Parameters\n    ----------\n    nu                 : int\n                         Resolution at X axis in pixels.\n    nv                 : int\n                         Resolution at Y axis in pixels.\n    dx                 : float\n                         Pixel pitch in meters.\n    wavelength         : float\n                         Wavelength in meters.\n    distance           : float\n                         Distance in meters.\n    device             : torch.device\n                         Device, for more see torch.device().\n\n\n    Returns\n    -------\n    H                  : float\n                         Complex kernel in Fourier domain.\n    \"\"\"\n    distance = torch.tensor([distance]).to(device)\n    fx = torch.linspace(-1. / 2. / dx, 1. / 2. / dx, nu, dtype = torch.float32, device = device)\n    fy = torch.linspace(-1. / 2. / dx, 1. / 2. / dx, nv, dtype = torch.float32, device = device)\n    FY, FX = torch.meshgrid(fx, fy, indexing='ij')\n    H = torch.exp(1j  * distance * (2 * (torch.pi * (1 / wavelength) * torch.sqrt(1. - (wavelength * FX) ** 2 - (wavelength * FY) ** 2))))\n    H_ptime = correlation_2d(H, H)\n    H = H_ptime.to(device)\n    return H\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.get_light_kernels","title":"<code>get_light_kernels(wavelengths, distances, pixel_pitches, resolution=[1080, 1920], resolution_factor=1, samples=[50, 50, 5, 5], propagation_type='Bandlimited Angular Spectrum', kernel_type='spatial', device=torch.device('cpu'))</code>","text":"<p>Utility function to request a tensor filled with light transport kernels according to the given optical configurations.</p> <p>Parameters:</p> <ul> <li> <code>wavelengths</code>           \u2013            <pre><code>             A list of wavelengths.\n</code></pre> </li> <li> <code>distances</code>           \u2013            <pre><code>             A list of propagation distances.\n</code></pre> </li> <li> <code>pixel_pitches</code>           \u2013            <pre><code>             A list of pixel_pitches.\n</code></pre> </li> <li> <code>resolution</code>           \u2013            <pre><code>             Resolution of the light transport kernel.\n</code></pre> </li> <li> <code>resolution_factor</code>           \u2013            <pre><code>             If `Impulse Response Fresnel` propagation is used, this resolution factor could be set larger than one leading to higher resolution light transport kernels than the provided native `resolution`. For more, see odak.learn.wave.get_impulse_response_kernel().\n</code></pre> </li> <li> <code>samples</code>           \u2013            <pre><code>             If `Impulse Response Fresnel` propagation is used, these sample counts will be used to calculate the light transport kernel. For more, see odak.learn.wave.get_impulse_response_kernel().\n</code></pre> </li> <li> <code>propagation_type</code>           \u2013            <pre><code>             Propagation type. For more, see odak.learn.wave.propagate_beam().\n</code></pre> </li> <li> <code>kernel_type</code>           \u2013            <pre><code>             If set to `spatial`, light transport kernels will be provided in space. But if set to `fourier`, these kernels will be provided in the Fourier domain.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>             Device used for computation (i.e., cpu, cuda).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>light_kernels_amplitude</code> (              <code>tensor</code> )          \u2013            <p>Amplitudes of the light kernels generated [w x d x p x m x n].</p> </li> <li> <code>light_kernels_phase</code> (              <code>tensor</code> )          \u2013            <p>Phases of the light kernels generated [w x d x p x m x n].</p> </li> <li> <code>light_kernels_complex</code> (              <code>tensor</code> )          \u2013            <p>Complex light kernels generated [w x d x p x m x n].</p> </li> <li> <code>light_parameters</code> (              <code>tensor</code> )          \u2013            <p>Parameters of each pixel in light_kernels* [w x d x p x m x n x 5].  Last dimension contains, wavelengths, distances, pixel pitches, X and Y locations in order.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def get_light_kernels(\n                      wavelengths,\n                      distances,\n                      pixel_pitches,\n                      resolution = [1080, 1920],\n                      resolution_factor = 1,\n                      samples = [50, 50, 5, 5],\n                      propagation_type = 'Bandlimited Angular Spectrum',\n                      kernel_type = 'spatial',\n                      device = torch.device('cpu')\n                     ):\n    \"\"\"\n    Utility function to request a tensor filled with light transport kernels according to the given optical configurations.\n\n    Parameters\n    ----------\n    wavelengths        : list\n                         A list of wavelengths.\n    distances          : list\n                         A list of propagation distances.\n    pixel_pitches      : list\n                         A list of pixel_pitches.\n    resolution         : list\n                         Resolution of the light transport kernel.\n    resolution_factor  : int\n                         If `Impulse Response Fresnel` propagation is used, this resolution factor could be set larger than one leading to higher resolution light transport kernels than the provided native `resolution`. For more, see odak.learn.wave.get_impulse_response_kernel().\n    samples            : list\n                         If `Impulse Response Fresnel` propagation is used, these sample counts will be used to calculate the light transport kernel. For more, see odak.learn.wave.get_impulse_response_kernel().\n    propagation_type   : str\n                         Propagation type. For more, see odak.learn.wave.propagate_beam().\n    kernel_type        : str\n                         If set to `spatial`, light transport kernels will be provided in space. But if set to `fourier`, these kernels will be provided in the Fourier domain.\n    device             : torch.device\n                         Device used for computation (i.e., cpu, cuda).\n\n    Returns\n    -------\n    light_kernels_amplitude : torch.tensor\n                              Amplitudes of the light kernels generated [w x d x p x m x n].\n    light_kernels_phase     : torch.tensor\n                              Phases of the light kernels generated [w x d x p x m x n].\n    light_kernels_complex   : torch.tensor\n                              Complex light kernels generated [w x d x p x m x n].\n    light_parameters        : torch.tensor\n                              Parameters of each pixel in light_kernels* [w x d x p x m x n x 5].  Last dimension contains, wavelengths, distances, pixel pitches, X and Y locations in order.\n    \"\"\"\n    if propagation_type != 'Impulse Response Fresnel' and propagation_type != 'Seperable Impulse Response Fresnel':\n        resolution_factor = 1\n    light_kernels_complex = torch.zeros(            \n                                        len(wavelengths),\n                                        len(distances),\n                                        len(pixel_pitches),\n                                        resolution[0] * resolution_factor,\n                                        resolution[1] * resolution_factor,\n                                        dtype = torch.complex64,\n                                        device = device\n                                       )\n    light_parameters = torch.zeros(\n                                   len(wavelengths),\n                                   len(distances),\n                                   len(pixel_pitches),\n                                   resolution[0] * resolution_factor,\n                                   resolution[1] * resolution_factor,\n                                   5,\n                                   dtype = torch.float32,\n                                   device = device\n                                  )\n    for wavelength_id, distance_id, pixel_pitch_id in itertools.product(\n                                                                        range(len(wavelengths)),\n                                                                        range(len(distances)),\n                                                                        range(len(pixel_pitches)),\n                                                                       ):\n        pixel_pitch = pixel_pitches[pixel_pitch_id]\n        wavelength = wavelengths[wavelength_id]\n        distance = distances[distance_id]\n        kernel_fourier = get_propagation_kernel(\n                                                nu = resolution[0],\n                                                nv = resolution[1],\n                                                dx = pixel_pitch,\n                                                wavelength = wavelength,\n                                                distance = distance,\n                                                device = device,\n                                                propagation_type = propagation_type,\n                                                scale = resolution_factor,\n                                                samples = samples\n                                               )\n        if kernel_type == 'spatial':\n            kernel = torch.fft.ifftshift(torch.fft.ifft2(kernel_fourier))\n        elif kernel_type == 'fourier':\n            kernel = kernel_fourier\n        else:\n            logging.warning('Unknown kernel type requested.')\n            raise ValueError('Unknown kernel type requested.')\n        kernel_amplitude = calculate_amplitude(kernel)\n        kernel_phase = calculate_phase(kernel) % (2 * torch.pi)\n        light_kernels_complex[wavelength_id, distance_id, pixel_pitch_id] = kernel\n        light_parameters[wavelength_id, distance_id, pixel_pitch_id, :, :, 0] = wavelength\n        light_parameters[wavelength_id, distance_id, pixel_pitch_id, :, :, 1] = distance\n        light_parameters[wavelength_id, distance_id, pixel_pitch_id, :, :, 2] = pixel_pitch\n        lims = [\n                resolution[0] // 2 * pixel_pitch,\n                resolution[1] // 2 * pixel_pitch \n               ]\n        x = torch.linspace(-lims[0], lims[0], resolution[0] * resolution_factor, device = device)\n        y = torch.linspace(-lims[1], lims[1], resolution[1] * resolution_factor, device = device)        \n        X, Y = torch.meshgrid(x, y, indexing = 'ij')\n        light_parameters[wavelength_id, distance_id, pixel_pitch_id, :, :, 3] = X\n        light_parameters[wavelength_id, distance_id, pixel_pitch_id, :, :, 4] = Y\n    light_kernels_amplitude = calculate_amplitude(light_kernels_complex)\n    light_kernels_phase = calculate_phase(light_kernels_complex) % (2. * torch.pi)\n    return light_kernels_amplitude, light_kernels_phase, light_kernels_complex, light_parameters\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.get_point_wise_impulse_response_fresnel_kernel","title":"<code>get_point_wise_impulse_response_fresnel_kernel(aperture_points, aperture_field, target_points, resolution, resolution_factor=1, wavelength=5.15e-07, distance=0.0, randomization=False, device=torch.device('cpu'))</code>","text":"<p>This function is a freeform point spread function calculation routine for an aperture defined with a complex field, <code>aperture_field</code>, and locations in space, <code>aperture_points</code>. The point spread function is calculated over provided points, <code>target_points</code>. The final result is reshaped to follow the provided <code>resolution</code>.</p> <p>Parameters:</p> <ul> <li> <code>aperture_points</code>           \u2013            <pre><code>                   Points representing an aperture in Euler space (XYZ) [m x 3].\n</code></pre> </li> <li> <code>aperture_field</code>           \u2013            <pre><code>                   Complex field for each point provided by `aperture_points` [1 x m].\n</code></pre> </li> <li> <code>target_points</code>           \u2013            <pre><code>                   Target points where the propagated field will be calculated [n x 1].\n</code></pre> </li> <li> <code>resolution</code>           \u2013            <pre><code>                   Final resolution that the propagated field will be reshaped [X x Y].\n</code></pre> </li> <li> <code>resolution_factor</code>           \u2013            <pre><code>                   Scale with respect to `resolution` (e.g., scale = 2 leads to `2 x resolution` for the final complex field.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>                   Wavelength in meters.\n</code></pre> </li> <li> <code>randomization</code>           \u2013            <pre><code>                   If set `True`, this will help generate a noisy response roughly approximating a real life case, where imperfections occur.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>                   Distance in meters.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>h</code> (              <code>float</code> )          \u2013            <p>Complex field in spatial domain.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def get_point_wise_impulse_response_fresnel_kernel(\n                                                   aperture_points,\n                                                   aperture_field,\n                                                   target_points,\n                                                   resolution,\n                                                   resolution_factor = 1,\n                                                   wavelength = 515e-9,\n                                                   distance = 0.,\n                                                   randomization = False,\n                                                   device = torch.device('cpu')\n                                                  ):\n    \"\"\"\n    This function is a freeform point spread function calculation routine for an aperture defined with a complex field, `aperture_field`, and locations in space, `aperture_points`.\n    The point spread function is calculated over provided points, `target_points`.\n    The final result is reshaped to follow the provided `resolution`.\n\n    Parameters\n    ----------\n    aperture_points          : torch.tensor\n                               Points representing an aperture in Euler space (XYZ) [m x 3].\n    aperture_field           : torch.tensor\n                               Complex field for each point provided by `aperture_points` [1 x m].\n    target_points            : torch.tensor\n                               Target points where the propagated field will be calculated [n x 1].\n    resolution               : list\n                               Final resolution that the propagated field will be reshaped [X x Y].\n    resolution_factor        : int\n                               Scale with respect to `resolution` (e.g., scale = 2 leads to `2 x resolution` for the final complex field.\n    wavelength               : float\n                               Wavelength in meters.\n    randomization            : bool\n                               If set `True`, this will help generate a noisy response roughly approximating a real life case, where imperfections occur.\n    distance                 : float\n                               Distance in meters.\n\n    Returns\n    -------\n    h                        : float\n                               Complex field in spatial domain.\n    \"\"\"\n    device = aperture_field.device\n    k = wavenumber(wavelength)\n    if randomization:\n        pp = [\n              aperture_points[:, 0].max() - aperture_points[:, 0].min(),\n              aperture_points[:, 1].max() - aperture_points[:, 1].min()\n             ]\n        target_points[:, 0] = target_points[:, 0] - torch.randn(target_points[:, 0].shape) * pp[0]\n        target_points[:, 1] = target_points[:, 1] - torch.randn(target_points[:, 1].shape) * pp[1]\n    deltaX = aperture_points[:, 0].unsqueeze(0) - target_points[:, 0].unsqueeze(-1)\n    deltaY = aperture_points[:, 1].unsqueeze(0) - target_points[:, 1].unsqueeze(-1)\n    r = deltaX ** 2 + deltaY ** 2\n    h = torch.exp(1j * k / (2 * distance) * r) * aperture_field\n    h = torch.sum(h, dim = 1).reshape(resolution[0] * resolution_factor, resolution[1] * resolution_factor)\n    h = 1. / (1j * wavelength * distance) * h\n    return h\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.get_propagation_kernel","title":"<code>get_propagation_kernel(nu, nv, dx=8e-06, wavelength=5.15e-07, distance=0.0, device=torch.device('cpu'), propagation_type='Bandlimited Angular Spectrum', scale=1, samples=[20, 20, 5, 5])</code>","text":"<p>Get propagation kernel for the propagation type.</p> <p>Parameters:</p> <ul> <li> <code>nu</code>           \u2013            <pre><code>             Resolution at X axis in pixels.\n</code></pre> </li> <li> <code>nv</code>           \u2013            <pre><code>             Resolution at Y axis in pixels.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>             Pixel pitch in meters.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>             Wavelength in meters.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>             Distance in meters.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>             Device, for more see torch.device().\n</code></pre> </li> <li> <code>propagation_type</code>           \u2013            <pre><code>             Propagation type.\n             The options are `Angular Spectrum`, `Bandlimited Angular Spectrum` and `Transfer Function Fresnel`.\n</code></pre> </li> <li> <code>scale</code>           \u2013            <pre><code>             Scale factor for scaled beam propagation.\n</code></pre> </li> <li> <code>samples</code>           \u2013            <pre><code>             When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for a hologram pixel and second two is for an image plane pixel.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>kernel</code> (              <code>tensor</code> )          \u2013            <p>Complex kernel for the given propagation type.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def get_propagation_kernel(\n                           nu, \n                           nv, \n                           dx = 8e-6, \n                           wavelength = 515e-9, \n                           distance = 0., \n                           device = torch.device('cpu'), \n                           propagation_type = 'Bandlimited Angular Spectrum', \n                           scale = 1,\n                           samples = [20, 20, 5, 5]\n                          ):\n    \"\"\"\n    Get propagation kernel for the propagation type.\n\n    Parameters\n    ----------\n    nu                 : int\n                         Resolution at X axis in pixels.\n    nv                 : int\n                         Resolution at Y axis in pixels.\n    dx                 : float\n                         Pixel pitch in meters.\n    wavelength         : float\n                         Wavelength in meters.\n    distance           : float\n                         Distance in meters.\n    device             : torch.device\n                         Device, for more see torch.device().\n    propagation_type   : str\n                         Propagation type.\n                         The options are `Angular Spectrum`, `Bandlimited Angular Spectrum` and `Transfer Function Fresnel`.\n    scale              : int\n                         Scale factor for scaled beam propagation.\n    samples            : list\n                         When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for a hologram pixel and second two is for an image plane pixel.\n\n\n    Returns\n    -------\n    kernel             : torch.tensor\n                         Complex kernel for the given propagation type.\n    \"\"\"                                                      \n    logging.warning('Requested propagation kernel size for {} method with {} m distance, {} m pixel pitch, {} m wavelength, {} x {} resolutions, x{} scale and {} samples.'.format(propagation_type, distance, dx, wavelength, nu, nv, scale, samples))\n    if propagation_type == 'Bandlimited Angular Spectrum':\n        kernel = get_band_limited_angular_spectrum_kernel(\n                                                          nu = nu,\n                                                          nv = nv,\n                                                          dx = dx,\n                                                          wavelength = wavelength,\n                                                          distance = distance,\n                                                          device = device\n                                                         )\n    elif propagation_type == 'Angular Spectrum':\n        kernel = get_angular_spectrum_kernel(\n                                             nu = nu,\n                                             nv = nv,\n                                             dx = dx,\n                                             wavelength = wavelength,\n                                             distance = distance,\n                                             device = device\n                                            )\n    elif propagation_type == 'Transfer Function Fresnel':\n        kernel = get_transfer_function_fresnel_kernel(\n                                                      nu = nu,\n                                                      nv = nv,\n                                                      dx = dx,\n                                                      wavelength = wavelength,\n                                                      distance = distance,\n                                                      device = device\n                                                     )\n    elif propagation_type == 'Impulse Response Fresnel':\n        kernel = get_impulse_response_fresnel_kernel(\n                                                     nu = nu, \n                                                     nv = nv, \n                                                     dx = dx, \n                                                     wavelength = wavelength,\n                                                     distance = distance,\n                                                     device =  device,\n                                                     scale = scale,\n                                                     aperture_samples = samples\n                                                    )\n    elif propagation_type == 'Incoherent Angular Spectrum':\n        kernel = get_incoherent_angular_spectrum_kernel(\n                                                        nu = nu,\n                                                        nv = nv, \n                                                        dx = dx, \n                                                        wavelength = wavelength, \n                                                        distance = distance,\n                                                        device = device\n                                                       )\n    elif propagation_type == 'Seperable Impulse Response Fresnel':\n        kernel, _, _, _ = get_seperable_impulse_response_fresnel_kernel(\n                                                                        nu = nu,\n                                                                        nv = nv,\n                                                                        dx = dx,\n                                                                        wavelength = wavelength,\n                                                                        distance = distance,\n                                                                        device = device,\n                                                                        scale = scale,\n                                                                        aperture_samples = samples\n                                                                       )\n    else:\n        logging.warning('Propagation type not recognized')\n        assert True == False\n    return kernel\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.get_seperable_impulse_response_fresnel_kernel","title":"<code>get_seperable_impulse_response_fresnel_kernel(nu, nv, dx=3.74e-06, wavelength=5.15e-07, distance=0.0, scale=1, aperture_samples=[50, 50, 5, 5], device=torch.device('cpu'))</code>","text":"<p>Returns impulse response fresnel kernel in separable form.</p> <p>Parameters:</p> <ul> <li> <code>nu</code>           \u2013            <pre><code>             Resolution at X axis in pixels.\n</code></pre> </li> <li> <code>nv</code>           \u2013            <pre><code>             Resolution at Y axis in pixels.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>             Pixel pitch in meters.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>             Wavelength in meters.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>             Distance in meters.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>             Device, for more see torch.device().\n</code></pre> </li> <li> <code>scale</code>           \u2013            <pre><code>             Scale with respect to nu and nv (e.g., scale = 2 leads to  2 x nu and 2 x nv resolution for H).\n</code></pre> </li> <li> <code>aperture_samples</code>           \u2013            <pre><code>             Number of samples to represent a rectangular pixel. First two is for XY of hologram plane pixels, and second two is for image plane pixels.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>H</code> (              <code>complex64</code> )          \u2013            <p>Complex kernel in Fourier domain.</p> </li> <li> <code>h</code> (              <code>complex64</code> )          \u2013            <p>Complex kernel in spatial domain.</p> </li> <li> <code>h_x</code> (              <code>complex64</code> )          \u2013            <p>1D complex kernel in spatial domain along X axis.</p> </li> <li> <code>h_y</code> (              <code>complex64</code> )          \u2013            <p>1D complex kernel in spatial domain along Y axis.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def get_seperable_impulse_response_fresnel_kernel(\n                                                  nu,\n                                                  nv,\n                                                  dx = 3.74e-6,\n                                                  wavelength = 515e-9,\n                                                  distance = 0.,\n                                                  scale = 1,\n                                                  aperture_samples = [50, 50, 5, 5],\n                                                  device = torch.device('cpu')\n                                                 ):\n    \"\"\"\n    Returns impulse response fresnel kernel in separable form.\n\n    Parameters\n    ----------\n    nu                 : int\n                         Resolution at X axis in pixels.\n    nv                 : int\n                         Resolution at Y axis in pixels.\n    dx                 : float\n                         Pixel pitch in meters.\n    wavelength         : float\n                         Wavelength in meters.\n    distance           : float\n                         Distance in meters.\n    device             : torch.device\n                         Device, for more see torch.device().\n    scale              : int\n                         Scale with respect to nu and nv (e.g., scale = 2 leads to  2 x nu and 2 x nv resolution for H).\n    aperture_samples   : list\n                         Number of samples to represent a rectangular pixel. First two is for XY of hologram plane pixels, and second two is for image plane pixels.\n\n    Returns\n    -------\n    H                  : torch.complex64\n                         Complex kernel in Fourier domain.\n    h                  : torch.complex64\n                         Complex kernel in spatial domain.\n    h_x                : torch.complex64\n                         1D complex kernel in spatial domain along X axis.\n    h_y                : torch.complex64\n                         1D complex kernel in spatial domain along Y axis.\n    \"\"\"\n    k = wavenumber(wavelength)\n    distance = torch.as_tensor(distance, device = device)\n    length_x, length_y = (\n                          torch.tensor(dx * nu, device = device),\n                          torch.tensor(dx * nv, device = device)\n                         )\n    x = torch.linspace(- length_x / 2., length_x / 2., nu * scale, device = device)\n    y = torch.linspace(- length_y / 2., length_y / 2., nv * scale, device = device)\n    wxs = torch.linspace(- dx / 2., dx / 2., aperture_samples[0], device = device).unsqueeze(0).unsqueeze(0)\n    wys = torch.linspace(- dx / 2., dx / 2., aperture_samples[1], device = device).unsqueeze(0).unsqueeze(-1)\n    pxs = torch.linspace(- dx / 2., dx / 2., aperture_samples[2], device = device).unsqueeze(0).unsqueeze(-1)\n    pys = torch.linspace(- dx / 2., dx / 2., aperture_samples[3], device = device).unsqueeze(0).unsqueeze(0)\n    wxs = (wxs - pxs).reshape(1, -1).unsqueeze(-1)\n    wys = (wys - pys).reshape(1, -1).unsqueeze(1)\n\n    X = x.unsqueeze(-1).unsqueeze(-1)\n    Y = y[y.shape[0] // 2].unsqueeze(-1).unsqueeze(-1)\n    r_x = (X + wxs) ** 2\n    r_y = (Y + wys) ** 2\n    r = r_x + r_y\n    h_x = torch.exp(1j * k / (2 * distance) * r)\n    h_x = torch.sum(h_x, axis = (1, 2))\n\n    if nu != nv:\n        X = x[x.shape[0] // 2].unsqueeze(-1).unsqueeze(-1)\n        Y = y.unsqueeze(-1).unsqueeze(-1)\n        r_x = (X + wxs) ** 2\n        r_y = (Y + wys) ** 2\n        r = r_x + r_y\n        h_y = torch.exp(1j * k * r / (2 * distance))\n        h_y = torch.sum(h_y, axis = (1, 2))\n    else:\n        h_y = h_x.detach().clone()\n    h = torch.exp(1j * k * distance) / (1j * wavelength * distance) * h_x.unsqueeze(1) * h_y.unsqueeze(0)\n    H = torch.fft.fftshift(torch.fft.fft2(torch.fft.fftshift(h))) * dx ** 2 / aperture_samples[0] / aperture_samples[1] / aperture_samples[2] / aperture_samples[3]\n    return H, h, h_x, h_y\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.get_transfer_function_fresnel_kernel","title":"<code>get_transfer_function_fresnel_kernel(nu, nv, dx=8e-06, wavelength=5.15e-07, distance=0.0, device=torch.device('cpu'))</code>","text":"<p>Helper function for odak.learn.wave.transfer_function_fresnel.</p> <p>Parameters:</p> <ul> <li> <code>nu</code>           \u2013            <pre><code>             Resolution at X axis in pixels.\n</code></pre> </li> <li> <code>nv</code>           \u2013            <pre><code>             Resolution at Y axis in pixels.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>             Pixel pitch in meters.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>             Wavelength in meters.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>             Distance in meters.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>             Device, for more see torch.device().\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>H</code> (              <code>complex64</code> )          \u2013            <p>Complex kernel in Fourier domain.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def get_transfer_function_fresnel_kernel(\n                                         nu,\n                                         nv,\n                                         dx = 8e-6,\n                                         wavelength = 515e-9,\n                                         distance = 0.,\n                                         device = torch.device('cpu')\n                                        ):\n    \"\"\"\n    Helper function for odak.learn.wave.transfer_function_fresnel.\n\n    Parameters\n    ----------\n    nu                 : int\n                         Resolution at X axis in pixels.\n    nv                 : int\n                         Resolution at Y axis in pixels.\n    dx                 : float\n                         Pixel pitch in meters.\n    wavelength         : float\n                         Wavelength in meters.\n    distance           : float\n                         Distance in meters.\n    device             : torch.device\n                         Device, for more see torch.device().\n\n\n    Returns\n    -------\n    H                  : torch.complex64\n                         Complex kernel in Fourier domain.\n    \"\"\"\n    distance = torch.tensor([distance]).to(device)\n    fx = torch.linspace(-1. / 2. /dx, 1. / 2. /dx, nu, dtype = torch.float32, device = device)\n    fy = torch.linspace(-1. / 2. /dx, 1. / 2. /dx, nv, dtype = torch.float32, device = device)\n    FY, FX = torch.meshgrid(fx, fy, indexing = 'ij')\n    k = wavenumber(wavelength)\n    H = torch.exp(-1j * distance * (k - torch.pi * wavelength * (FX ** 2 + FY ** 2)))\n    return H\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.impulse_response_fresnel","title":"<code>impulse_response_fresnel(field, k, distance, dx, wavelength, zero_padding=False, aperture=1.0, scale=1, samples=[20, 20, 5, 5])</code>","text":"<p>A definition to calculate convolution based Fresnel approximation for beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>zero_padding</code>           \u2013            <pre><code>           Zero pad in Fourier domain.\n</code></pre> </li> <li> <code>aperture</code>           \u2013            <pre><code>           Fourier domain aperture (e.g., pinhole in a typical holographic display).\n           The default is one, but an aperture could be as large as input field [m x n].\n</code></pre> </li> <li> <code>scale</code>           \u2013            <pre><code>           Resolution factor to scale generated kernel.\n</code></pre> </li> <li> <code>samples</code>           \u2013            <pre><code>           When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for hologram plane pixel and the last two is for image plane pixel.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def impulse_response_fresnel(\n                             field,\n                             k,\n                             distance,\n                             dx,\n                             wavelength,\n                             zero_padding = False,\n                             aperture = 1.,\n                             scale = 1,\n                             samples = [20, 20, 5, 5]\n                            ):\n    \"\"\"\n    A definition to calculate convolution based Fresnel approximation for beam propagation.\n\n    Parameters\n    ----------\n    field            : torch.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    zero_padding     : bool\n                       Zero pad in Fourier domain.\n    aperture         : torch.tensor\n                       Fourier domain aperture (e.g., pinhole in a typical holographic display).\n                       The default is one, but an aperture could be as large as input field [m x n].\n    scale            : int\n                       Resolution factor to scale generated kernel.\n    samples          : list\n                       When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for hologram plane pixel and the last two is for image plane pixel.\n\n    Returns\n    -------\n    result           : torch.complex\n                       Final complex field (MxN).\n\n    \"\"\"\n    H = get_propagation_kernel(\n                               nu = field.shape[-2], \n                               nv = field.shape[-1], \n                               dx = dx, \n                               wavelength = wavelength, \n                               distance = distance, \n                               propagation_type = 'Impulse Response Fresnel',\n                               device = field.device,\n                               scale = scale,\n                               samples = samples\n                              )\n    if scale &gt; 1:\n        field_amplitude = calculate_amplitude(field)\n        field_phase = calculate_phase(field)\n        field_scale_amplitude = torch.zeros(field.shape[-2] * scale, field.shape[-1] * scale, device = field.device)\n        field_scale_phase = torch.zeros_like(field_scale_amplitude)\n        field_scale_amplitude[::scale, ::scale] = field_amplitude\n        field_scale_phase[::scale, ::scale] = field_phase\n        field_scale = generate_complex_field(field_scale_amplitude, field_scale_phase)\n    else:\n        field_scale = field\n    result = custom(field_scale, H, zero_padding = zero_padding, aperture = aperture)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.incoherent_angular_spectrum","title":"<code>incoherent_angular_spectrum(field, k, distance, dx, wavelength, zero_padding=False, aperture=1.0)</code>","text":"<p>A definition to calculate incoherent beam propagation with Angular Spectrum method.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field [m x n].\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>zero_padding</code>           \u2013            <pre><code>           Zero pad in Fourier domain.\n</code></pre> </li> <li> <code>aperture</code>           \u2013            <pre><code>           Fourier domain aperture (e.g., pinhole in a typical holographic display).\n           The default is one, but an aperture could be as large as input field [m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field [m x n].</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def incoherent_angular_spectrum(\n                                field,\n                                k,\n                                distance,\n                                dx,\n                                wavelength,\n                                zero_padding = False,\n                                aperture = 1.\n                               ):\n    \"\"\"\n    A definition to calculate incoherent beam propagation with Angular Spectrum method.\n\n    Parameters\n    ----------\n    field            : torch.complex\n                       Complex field [m x n].\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    zero_padding     : bool\n                       Zero pad in Fourier domain.\n    aperture         : torch.tensor\n                       Fourier domain aperture (e.g., pinhole in a typical holographic display).\n                       The default is one, but an aperture could be as large as input field [m x n].\n\n\n    Returns\n    -------\n    result           : torch.complex\n                       Final complex field [m x n].\n    \"\"\"\n    H = get_propagation_kernel(\n                               nu = field.shape[-2], \n                               nv = field.shape[-1], \n                               dx = dx, \n                               wavelength = wavelength, \n                               distance = distance, \n                               propagation_type = 'Incoherent Angular Spectrum',\n                               device = field.device\n                              )\n    result = custom(field, H, zero_padding = zero_padding, aperture = aperture)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.point_wise","title":"<code>point_wise(target, wavelength, distance, dx, device, lens_size=401)</code>","text":"<p>Naive point-wise hologram calculation method. For more information, refer to Maimone, Andrew, Andreas Georgiou, and Joel S. Kollin. \"Holographic near-eye displays for virtual and augmented reality.\" ACM Transactions on Graphics (TOG) 36.4 (2017): 1-16.</p> <p>Parameters:</p> <ul> <li> <code>target</code>           \u2013            <pre><code>           float input target to be converted into a hologram (Target should be in range of 0 and 1).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>           Device type (cuda or cpu)`.\n</code></pre> </li> <li> <code>lens_size</code>           \u2013            <pre><code>           Size of lens for masking sub holograms(in pixels).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>hologram</code> (              <code>cfloat</code> )          \u2013            <p>Calculated complex hologram.</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def point_wise(\n               target,\n               wavelength,\n               distance,\n               dx,\n               device,\n               lens_size=401\n              ):\n    \"\"\"\n    Naive point-wise hologram calculation method. For more information, refer to Maimone, Andrew, Andreas Georgiou, and Joel S. Kollin. \"Holographic near-eye displays for virtual and augmented reality.\" ACM Transactions on Graphics (TOG) 36.4 (2017): 1-16.\n\n    Parameters\n    ----------\n    target           : torch.float\n                       float input target to be converted into a hologram (Target should be in range of 0 and 1).\n    wavelength       : float\n                       Wavelength of the electric field.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    device           : torch.device\n                       Device type (cuda or cpu)`.\n    lens_size        : int\n                       Size of lens for masking sub holograms(in pixels).\n\n    Returns\n    -------\n    hologram         : torch.cfloat\n                       Calculated complex hologram.\n    \"\"\"\n    target = zero_pad(target)\n    nx, ny = target.shape\n    k = wavenumber(wavelength)\n    ones = torch.ones(target.shape, requires_grad=False).to(device)\n    x = torch.linspace(-nx/2, nx/2, nx).to(device)\n    y = torch.linspace(-ny/2, ny/2, ny).to(device)\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    Z = (X**2+Y**2)**0.5\n    mask = (torch.abs(Z) &lt;= lens_size)\n    mask[mask &gt; 1] = 1\n    fz = quadratic_phase_function(nx, ny, k, focal=-distance, dx=dx).to(device)\n    A = torch.nan_to_num(target**0.5, nan=0.0)\n    fz = mask*fz\n    FA = torch.fft.fft2(torch.fft.fftshift(A))\n    FFZ = torch.fft.fft2(torch.fft.fftshift(fz))\n    H = torch.mul(FA, FFZ)\n    hologram = torch.fft.ifftshift(torch.fft.ifft2(H))\n    hologram = crop_center(hologram)\n    return hologram\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.propagate_beam","title":"<code>propagate_beam(field, k, distance, dx, wavelength, propagation_type='Bandlimited Angular Spectrum', kernel=None, zero_padding=[True, False, True], aperture=1.0, scale=1, samples=[20, 20, 5, 5])</code>","text":"<p>Definitions for various beam propagation methods mostly in accordence with \"Computational Fourier Optics\" by David Vuelz.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field [m x n].\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>propagation_type</code>               (<code>str</code>, default:                   <code>'Bandlimited Angular Spectrum'</code> )           \u2013            <pre><code>           Type of the propagation.\n           The options are Impulse Response Fresnel, Transfer Function Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, Fraunhofer.\n</code></pre> </li> <li> <code>kernel</code>           \u2013            <pre><code>           Custom complex kernel.\n</code></pre> </li> <li> <code>zero_padding</code>           \u2013            <pre><code>           Zero padding the input field if the first item in the list set True.\n           Zero padding in the Fourier domain if the second item in the list set to True.\n           Cropping the result with half resolution if the third item in the list is set to true.\n           Note that in Fraunhofer propagation, setting the second item True or False will have no effect.\n</code></pre> </li> <li> <code>aperture</code>           \u2013            <pre><code>           Aperture at Fourier domain default:[2m x 2n], otherwise depends on `zero_padding`.\n           If provided as a floating point 1, there will be no aperture in Fourier domain.\n</code></pre> </li> <li> <code>scale</code>           \u2013            <pre><code>           Resolution factor to scale generated kernel.\n</code></pre> </li> <li> <code>samples</code>           \u2013            <pre><code>           When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for a hologram pixel and second two is for an image plane pixel.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field [m x n].</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def propagate_beam(\n                   field,\n                   k,\n                   distance,\n                   dx,\n                   wavelength,\n                   propagation_type='Bandlimited Angular Spectrum',\n                   kernel = None,\n                   zero_padding = [True, False, True],\n                   aperture = 1.,\n                   scale = 1,\n                   samples = [20, 20, 5, 5]\n                  ):\n    \"\"\"\n    Definitions for various beam propagation methods mostly in accordence with \"Computational Fourier Optics\" by David Vuelz.\n\n    Parameters\n    ----------\n    field            : torch.complex\n                       Complex field [m x n].\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    propagation_type : str\n                       Type of the propagation.\n                       The options are Impulse Response Fresnel, Transfer Function Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, Fraunhofer.\n    kernel           : torch.complex\n                       Custom complex kernel.\n    zero_padding     : list\n                       Zero padding the input field if the first item in the list set True.\n                       Zero padding in the Fourier domain if the second item in the list set to True.\n                       Cropping the result with half resolution if the third item in the list is set to true.\n                       Note that in Fraunhofer propagation, setting the second item True or False will have no effect.\n    aperture         : torch.tensor\n                       Aperture at Fourier domain default:[2m x 2n], otherwise depends on `zero_padding`.\n                       If provided as a floating point 1, there will be no aperture in Fourier domain.\n    scale            : int\n                       Resolution factor to scale generated kernel.\n    samples          : list\n                       When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for a hologram pixel and second two is for an image plane pixel.\n\n    Returns\n    -------\n    result           : torch.complex\n                       Final complex field [m x n].\n    \"\"\"\n    if zero_padding[0]:\n        field = zero_pad(field)\n    if propagation_type == 'Angular Spectrum':\n        result = angular_spectrum(\n                                  field = field,\n                                  k = k,\n                                  distance = distance,\n                                  dx = dx,\n                                  wavelength = wavelength,\n                                  zero_padding = zero_padding[1],\n                                  aperture = aperture\n                                 )\n    elif propagation_type == 'Bandlimited Angular Spectrum':\n        result = band_limited_angular_spectrum(\n                                               field = field,\n                                               k = k,\n                                               distance = distance,\n                                               dx = dx,\n                                               wavelength = wavelength,\n                                               zero_padding = zero_padding[1],\n                                               aperture = aperture\n                                              )\n    elif propagation_type == 'Impulse Response Fresnel':\n        result = impulse_response_fresnel(\n                                          field = field,\n                                          k = k,\n                                          distance = distance,\n                                          dx = dx,\n                                          wavelength = wavelength,\n                                          zero_padding = zero_padding[1],\n                                          aperture = aperture,\n                                          scale = scale,\n                                          samples = samples\n                                         )\n    elif propagation_type == 'Seperable Impulse Response Fresnel':\n        result = seperable_impulse_response_fresnel(\n                                                    field = field,\n                                                    k = k,\n                                                    distance = distance,\n                                                    dx = dx,\n                                                    wavelength = wavelength,\n                                                    zero_padding = zero_padding[1],\n                                                    aperture = aperture,\n                                                    scale = scale,\n                                                    samples = samples\n                                                   )\n    elif propagation_type == 'Transfer Function Fresnel':\n        result = transfer_function_fresnel(\n                                           field = field,\n                                           k = k,\n                                           distance = distance,\n                                           dx = dx,\n                                           wavelength = wavelength,\n                                           zero_padding = zero_padding[1],\n                                           aperture = aperture\n                                          )\n    elif propagation_type == 'custom':\n        result = custom(\n                        field = field,\n                        kernel = kernel,\n                        zero_padding = zero_padding[1],\n                        aperture = aperture\n                       )\n    elif propagation_type == 'Fraunhofer':\n        result = fraunhofer(\n                            field = field,\n                            k = k,\n                            distance = distance,\n                            dx = dx,\n                            wavelength = wavelength\n                           )\n    elif propagation_type == 'Incoherent Angular Spectrum':\n        result = incoherent_angular_spectrum(\n                                             field = field,\n                                             k = k,\n                                             distance = distance,\n                                             dx = dx,\n                                             wavelength = wavelength,\n                                             zero_padding = zero_padding[1],\n                                             aperture = aperture\n                                            )\n    else:\n        logging.warning('Propagation type not recognized')\n        assert True == False\n    if zero_padding[2]:\n        result = crop_center(result)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.seperable_impulse_response_fresnel","title":"<code>seperable_impulse_response_fresnel(field, k, distance, dx, wavelength, zero_padding=False, aperture=1.0, scale=1, samples=[20, 20, 5, 5])</code>","text":"<p>A definition to calculate convolution based Fresnel approximation for beam propagation for a rectangular aperture using the seperable property.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>zero_padding</code>           \u2013            <pre><code>           Zero pad in Fourier domain.\n</code></pre> </li> <li> <code>aperture</code>           \u2013            <pre><code>           Fourier domain aperture (e.g., pinhole in a typical holographic display).\n           The default is one, but an aperture could be as large as input field [m x n].\n</code></pre> </li> <li> <code>scale</code>           \u2013            <pre><code>           Resolution factor to scale generated kernel.\n</code></pre> </li> <li> <code>samples</code>           \u2013            <pre><code>           When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for hologram plane pixel and the last two is for image plane pixel.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def seperable_impulse_response_fresnel(\n                                       field,\n                                       k,\n                                       distance,\n                                       dx,\n                                       wavelength,\n                                       zero_padding = False,\n                                       aperture = 1.,\n                                       scale = 1,\n                                       samples = [20, 20, 5, 5]\n                                      ):\n    \"\"\"\n    A definition to calculate convolution based Fresnel approximation for beam propagation for a rectangular aperture using the seperable property.\n\n    Parameters\n    ----------\n    field            : torch.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    zero_padding     : bool\n                       Zero pad in Fourier domain.\n    aperture         : torch.tensor\n                       Fourier domain aperture (e.g., pinhole in a typical holographic display).\n                       The default is one, but an aperture could be as large as input field [m x n].\n    scale            : int\n                       Resolution factor to scale generated kernel.\n    samples          : list\n                       When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for hologram plane pixel and the last two is for image plane pixel.\n\n    Returns\n    -------\n    result           : torch.complex\n                       Final complex field (MxN).\n\n    \"\"\"\n    H = get_propagation_kernel(\n                               nu = field.shape[-2], \n                               nv = field.shape[-1], \n                               dx = dx, \n                               wavelength = wavelength, \n                               distance = distance, \n                               propagation_type = 'Seperable Impulse Response Fresnel',\n                               device = field.device,\n                               scale = scale,\n                               samples = samples\n                              )\n    if scale &gt; 1:\n        field_amplitude = calculate_amplitude(field)\n        field_phase = calculate_phase(field)\n        field_scale_amplitude = torch.zeros(field.shape[-2] * scale, field.shape[-1] * scale, device = field.device)\n        field_scale_phase = torch.zeros_like(field_scale_amplitude)\n        field_scale_amplitude[::scale, ::scale] = field_amplitude\n        field_scale_phase[::scale, ::scale] = field_phase\n        field_scale = generate_complex_field(field_scale_amplitude, field_scale_phase)\n    else:\n        field_scale = field\n    result = custom(field_scale, H, zero_padding = zero_padding, aperture = aperture)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.shift_w_double_phase","title":"<code>shift_w_double_phase(phase, depth_shift, pixel_pitch, wavelength, propagation_type='Transfer Function Fresnel', kernel_length=4, sigma=0.5, amplitude=None)</code>","text":"<p>Shift a phase-only hologram by propagating the complex hologram and double phase principle. Coded following in here and Shi, L., Li, B., Kim, C., Kellnhofer, P., &amp; Matusik, W. (2021). Towards real-time photorealistic 3D holography with deep neural networks. Nature, 591(7849), 234-239.</p> <p>Parameters:</p> <ul> <li> <code>phase</code>           \u2013            <pre><code>           Phase value of a phase-only hologram.\n</code></pre> </li> <li> <code>depth_shift</code>           \u2013            <pre><code>           Distance in meters.\n</code></pre> </li> <li> <code>pixel_pitch</code>           \u2013            <pre><code>           Pixel pitch size in meters.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of light.\n</code></pre> </li> <li> <code>propagation_type</code>               (<code>str</code>, default:                   <code>'Transfer Function Fresnel'</code> )           \u2013            <pre><code>           Beam propagation type. For more see odak.learn.wave.propagate_beam().\n</code></pre> </li> <li> <code>kernel_length</code>           \u2013            <pre><code>           Kernel length for the Gaussian blur kernel.\n</code></pre> </li> <li> <code>sigma</code>           \u2013            <pre><code>           Standard deviation for the Gaussian blur kernel.\n</code></pre> </li> <li> <code>amplitude</code>           \u2013            <pre><code>           Amplitude value of a complex hologram.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def shift_w_double_phase(\n                         phase,\n                         depth_shift,\n                         pixel_pitch,\n                         wavelength,\n                         propagation_type = 'Transfer Function Fresnel',\n                         kernel_length = 4,\n                         sigma = 0.5,\n                         amplitude = None\n                        ):\n    \"\"\"\n    Shift a phase-only hologram by propagating the complex hologram and double phase principle. Coded following in [here](https://github.com/liangs111/tensor_holography/blob/6fdb26561a4e554136c579fa57788bb5fc3cac62/optics.py#L131-L207) and Shi, L., Li, B., Kim, C., Kellnhofer, P., &amp; Matusik, W. (2021). Towards real-time photorealistic 3D holography with deep neural networks. Nature, 591(7849), 234-239.\n\n    Parameters\n    ----------\n    phase            : torch.tensor\n                       Phase value of a phase-only hologram.\n    depth_shift      : float\n                       Distance in meters.\n    pixel_pitch      : float\n                       Pixel pitch size in meters.\n    wavelength       : float\n                       Wavelength of light.\n    propagation_type : str\n                       Beam propagation type. For more see odak.learn.wave.propagate_beam().\n    kernel_length    : int\n                       Kernel length for the Gaussian blur kernel.\n    sigma            : float\n                       Standard deviation for the Gaussian blur kernel.\n    amplitude        : torch.tensor\n                       Amplitude value of a complex hologram.\n    \"\"\"\n    if type(amplitude) == type(None):\n        amplitude = torch.ones_like(phase)\n    hologram = generate_complex_field(amplitude, phase)\n    k = wavenumber(wavelength)\n    hologram_padded = zero_pad(hologram)\n    shifted_field_padded = propagate_beam(\n                                          hologram_padded,\n                                          k,\n                                          depth_shift,\n                                          pixel_pitch,\n                                          wavelength,\n                                          propagation_type\n                                         )\n    shifted_field = crop_center(shifted_field_padded)\n    phase_shift = torch.exp(torch.tensor([-2 * torch.pi * depth_shift / wavelength]).to(phase.device))\n    shift = torch.cos(phase_shift) + 1j * torch.sin(phase_shift)\n    shifted_complex_hologram = shifted_field * shift\n\n    if kernel_length &gt; 0 and sigma &gt;0:\n        blur_kernel = generate_2d_gaussian(\n                                           [kernel_length, kernel_length],\n                                           [sigma, sigma]\n                                          ).to(phase.device)\n        blur_kernel = blur_kernel.unsqueeze(0)\n        blur_kernel = blur_kernel.unsqueeze(0)\n        field_imag = torch.imag(shifted_complex_hologram)\n        field_real = torch.real(shifted_complex_hologram)\n        field_imag = field_imag.unsqueeze(0)\n        field_imag = field_imag.unsqueeze(0)\n        field_real = field_real.unsqueeze(0)\n        field_real = field_real.unsqueeze(0)\n        field_imag = torch.nn.functional.conv2d(field_imag, blur_kernel, padding='same')\n        field_real = torch.nn.functional.conv2d(field_real, blur_kernel, padding='same')\n        shifted_complex_hologram = torch.complex(field_real, field_imag)\n        shifted_complex_hologram = shifted_complex_hologram.squeeze(0)\n        shifted_complex_hologram = shifted_complex_hologram.squeeze(0)\n\n    shifted_amplitude = calculate_amplitude(shifted_complex_hologram)\n    shifted_amplitude = shifted_amplitude / torch.amax(shifted_amplitude, [0,1])\n\n    shifted_phase = calculate_phase(shifted_complex_hologram)\n    phase_zero_mean = shifted_phase - torch.mean(shifted_phase)\n\n    phase_offset = torch.arccos(shifted_amplitude)\n    phase_low = phase_zero_mean - phase_offset\n    phase_high = phase_zero_mean + phase_offset\n\n    phase_only = torch.zeros_like(phase)\n    phase_only[0::2, 0::2] = phase_low[0::2, 0::2]\n    phase_only[0::2, 1::2] = phase_high[0::2, 1::2]\n    phase_only[1::2, 0::2] = phase_high[1::2, 0::2]\n    phase_only[1::2, 1::2] = phase_low[1::2, 1::2]\n    return phase_only\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.stochastic_gradient_descent","title":"<code>stochastic_gradient_descent(target, wavelength, distance, pixel_pitch, propagation_type='Bandlimited Angular Spectrum', n_iteration=100, loss_function=None, learning_rate=0.1)</code>","text":"<p>Definition to generate phase and reconstruction from target image via stochastic gradient descent.</p> <p>Parameters:</p> <ul> <li> <code>target</code>           \u2013            <pre><code>                    Target field amplitude [m x n].\n                    Keep the target values between zero and one.\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>                    Set if the converted array requires gradient.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>                    Hologram plane distance wrt SLM plane.\n</code></pre> </li> <li> <code>pixel_pitch</code>           \u2013            <pre><code>                    SLM pixel pitch in meters.\n</code></pre> </li> <li> <code>propagation_type</code>           \u2013            <pre><code>                    Type of the propagation (see odak.learn.wave.propagate_beam()).\n</code></pre> </li> <li> <code>n_iteration</code>           \u2013            <pre><code>                    Number of iteration.\n</code></pre> </li> <li> <code>loss_function</code>           \u2013            <pre><code>                    If none it is set to be l2 loss.\n</code></pre> </li> <li> <code>learning_rate</code>           \u2013            <pre><code>                    Learning rate.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>hologram</code> (              <code>Tensor</code> )          \u2013            <p>Phase only hologram as torch array</p> </li> <li> <code>reconstruction_intensity</code> (              <code>Tensor</code> )          \u2013            <p>Reconstruction as torch array</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def stochastic_gradient_descent(\n                                target,\n                                wavelength,\n                                distance,\n                                pixel_pitch,\n                                propagation_type = 'Bandlimited Angular Spectrum',\n                                n_iteration = 100,\n                                loss_function = None,\n                                learning_rate = 0.1\n                               ):\n    \"\"\"\n    Definition to generate phase and reconstruction from target image via stochastic gradient descent.\n\n    Parameters\n    ----------\n    target                    : torch.Tensor\n                                Target field amplitude [m x n].\n                                Keep the target values between zero and one.\n    wavelength                : double\n                                Set if the converted array requires gradient.\n    distance                  : double\n                                Hologram plane distance wrt SLM plane.\n    pixel_pitch               : float\n                                SLM pixel pitch in meters.\n    propagation_type          : str\n                                Type of the propagation (see odak.learn.wave.propagate_beam()).\n    n_iteration:              : int\n                                Number of iteration.\n    loss_function:            : function\n                                If none it is set to be l2 loss.\n    learning_rate             : float\n                                Learning rate.\n\n    Returns\n    -------\n    hologram                  : torch.Tensor\n                                Phase only hologram as torch array\n\n    reconstruction_intensity  : torch.Tensor\n                                Reconstruction as torch array\n\n    \"\"\"\n    phase = torch.randn_like(target, requires_grad = True)\n    k = wavenumber(wavelength)\n    optimizer = torch.optim.Adam([phase], lr = learning_rate)\n    if type(loss_function) == type(None):\n        loss_function = torch.nn.MSELoss()\n    t = tqdm(range(n_iteration), leave = False, dynamic_ncols = True)\n    for i in t:\n        optimizer.zero_grad()\n        hologram = generate_complex_field(1., phase)\n        reconstruction = propagate_beam(\n                                        hologram, \n                                        k, \n                                        distance, \n                                        pixel_pitch, \n                                        wavelength, \n                                        propagation_type, \n                                        zero_padding = [True, False, True]\n                                       )\n        reconstruction_intensity = calculate_amplitude(reconstruction) ** 2\n        loss = loss_function(reconstruction_intensity, target)\n        description = \"Loss:{:.4f}\".format(loss.item())\n        loss.backward(retain_graph = True)\n        optimizer.step()\n        t.set_description(description)\n    logging.warning(description)\n    torch.no_grad()\n    hologram = generate_complex_field(1., phase)\n    reconstruction = propagate_beam(\n                                    hologram, \n                                    k, \n                                    distance, \n                                    pixel_pitch, \n                                    wavelength, \n                                    propagation_type, \n                                    zero_padding = [True, False, True]\n                                   )\n    return hologram, reconstruction\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.classical.transfer_function_fresnel","title":"<code>transfer_function_fresnel(field, k, distance, dx, wavelength, zero_padding=False, aperture=1.0)</code>","text":"<p>A definition to calculate convolution based Fresnel approximation for beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>zero_padding</code>           \u2013            <pre><code>           Zero pad in Fourier domain.\n</code></pre> </li> <li> <code>aperture</code>           \u2013            <pre><code>           Fourier domain aperture (e.g., pinhole in a typical holographic display).\n           The default is one, but an aperture could be as large as input field [m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/learn/wave/classical.py</code> <pre><code>def transfer_function_fresnel(\n                              field,\n                              k,\n                              distance,\n                              dx,\n                              wavelength,\n                              zero_padding = False,\n                              aperture = 1.\n                             ):\n    \"\"\"\n    A definition to calculate convolution based Fresnel approximation for beam propagation.\n\n    Parameters\n    ----------\n    field            : torch.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    zero_padding     : bool\n                       Zero pad in Fourier domain.\n    aperture         : torch.tensor\n                       Fourier domain aperture (e.g., pinhole in a typical holographic display).\n                       The default is one, but an aperture could be as large as input field [m x n].\n\n\n    Returns\n    -------\n    result           : torch.complex\n                       Final complex field (MxN).\n\n    \"\"\"\n    H = get_propagation_kernel(\n                               nu = field.shape[-2], \n                               nv = field.shape[-1], \n                               dx = dx, \n                               wavelength = wavelength, \n                               distance = distance, \n                               propagation_type = 'Transfer Function Fresnel',\n                               device = field.device\n                              )\n    result = custom(field, H, zero_padding = zero_padding, aperture = aperture)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.lens.blazed_grating","title":"<code>blazed_grating(nx, ny, levels=2, axis='x')</code>","text":"<p>A defininition to generate a blazed grating (also known as ramp grating). For more consult de Blas, Mario Garc\u00eda, et al. \"High resolution 2D beam steerer made from cascaded 1D liquid crystal phase gratings.\" Scientific Reports 12.1 (2022): 5145 and Igasaki, Yasunori, et al. \"High efficiency electrically-addressable phase-only spatial light modulator.\" optical review 6 (1999): 339-344.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>       Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>       Size of the output along Y.\n</code></pre> </li> <li> <code>levels</code>           \u2013            <pre><code>       Number of pixels.\n</code></pre> </li> <li> <code>axis</code>           \u2013            <pre><code>       Axis of glazed grating. It could be `x` or `y`.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/lens.py</code> <pre><code>def blazed_grating(nx, ny, levels = 2, axis = 'x'):\n    \"\"\"\n    A defininition to generate a blazed grating (also known as ramp grating). For more consult de Blas, Mario Garc\u00eda, et al. \"High resolution 2D beam steerer made from cascaded 1D liquid crystal phase gratings.\" Scientific Reports 12.1 (2022): 5145 and Igasaki, Yasunori, et al. \"High efficiency electrically-addressable phase-only spatial light modulator.\" optical review 6 (1999): 339-344.\n\n\n    Parameters\n    ----------\n    nx           : int\n                   Size of the output along X.\n    ny           : int\n                   Size of the output along Y.\n    levels       : int\n                   Number of pixels.\n    axis         : str\n                   Axis of glazed grating. It could be `x` or `y`.\n\n    \"\"\"\n    if levels &lt; 2:\n        levels = 2\n    x = (torch.abs(torch.arange(-nx, 0)) % levels) / levels * (2 * np.pi)\n    y = (torch.abs(torch.arange(-ny, 0)) % levels) / levels * (2 * np.pi)\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    if axis == 'x':\n        blazed_grating = torch.exp(1j * X)\n    elif axis == 'y':\n        blazed_grating = torch.exp(1j * Y)\n    return blazed_grating\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.lens.linear_grating","title":"<code>linear_grating(nx, ny, every=2, add=None, axis='x')</code>","text":"<p>A definition to generate a linear grating. This could also be interpreted as two levels blazed grating. For more on blazed gratings see odak.learn.wave.blazed_grating() function.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>     Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>     Size of the output along Y.\n</code></pre> </li> <li> <code>every</code>           \u2013            <pre><code>     Add the add value at every given number.\n</code></pre> </li> <li> <code>add</code>           \u2013            <pre><code>     Angle to be added.\n</code></pre> </li> <li> <code>axis</code>           \u2013            <pre><code>     Axis eiter X,Y or both.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field</code> (              <code>tensor</code> )          \u2013            <p>Linear grating term.</p> </li> </ul> Source code in <code>odak/learn/wave/lens.py</code> <pre><code>def linear_grating(nx, ny, every = 2, add = None, axis = 'x'):\n    \"\"\"\n    A definition to generate a linear grating. This could also be interpreted as two levels blazed grating. For more on blazed gratings see odak.learn.wave.blazed_grating() function.\n\n    Parameters\n    ----------\n    nx         : int\n                 Size of the output along X.\n    ny         : int\n                 Size of the output along Y.\n    every      : int\n                 Add the add value at every given number.\n    add        : float\n                 Angle to be added.\n    axis       : string\n                 Axis eiter X,Y or both.\n\n    Returns\n    ----------\n    field      : torch.tensor\n                 Linear grating term.\n    \"\"\"\n    if isinstance(add, type(None)):\n        add = np.pi\n    grating = torch.zeros((nx, ny), dtype=torch.complex64)\n    if axis == 'x':\n        grating[::every, :] = torch.exp(torch.tensor(1j*add))\n    if axis == 'y':\n        grating[:, ::every] = torch.exp(torch.tensor(1j*add))\n    if axis == 'xy':\n        checker = np.indices((nx, ny)).sum(axis=0) % every\n        checker = torch.from_numpy(checker)\n        checker += 1\n        checker = checker % 2\n        grating = torch.exp(1j*checker*add)\n    return grating\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.lens.prism_grating","title":"<code>prism_grating(nx, ny, k, angle, dx=0.001, axis='x', phase_offset=0.0)</code>","text":"<p>A definition to generate 2D phase function that represents a prism. See Goodman's Introduction to Fourier Optics book or Engstr\u00f6m, David, et al. \"Improved beam steering accuracy of a single beam with a 1D phase-only spatial light modulator.\" Optics express 16.22 (2008): 18275-18287. for more.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>       Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>       Size of the output along Y.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>       See odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>angle</code>           \u2013            <pre><code>       Tilt angle of the prism in degrees.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>       Pixel pitch.\n</code></pre> </li> <li> <code>axis</code>           \u2013            <pre><code>       Axis of the prism.\n</code></pre> </li> <li> <code>phase_offset</code>               (<code>float</code>, default:                   <code>0.0</code> )           \u2013            <pre><code>       Phase offset in angles. Default is zero.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>prism</code> (              <code>tensor</code> )          \u2013            <p>Generated phase function for a prism.</p> </li> </ul> Source code in <code>odak/learn/wave/lens.py</code> <pre><code>def prism_grating(nx, ny, k, angle, dx = 0.001, axis = 'x', phase_offset = 0.):\n    \"\"\"\n    A definition to generate 2D phase function that represents a prism. See Goodman's Introduction to Fourier Optics book or Engstr\u00f6m, David, et al. \"Improved beam steering accuracy of a single beam with a 1D phase-only spatial light modulator.\" Optics express 16.22 (2008): 18275-18287. for more.\n\n    Parameters\n    ----------\n    nx           : int\n                   Size of the output along X.\n    ny           : int\n                   Size of the output along Y.\n    k            : odak.wave.wavenumber\n                   See odak.wave.wavenumber for more.\n    angle        : float\n                   Tilt angle of the prism in degrees.\n    dx           : float\n                   Pixel pitch.\n    axis         : str\n                   Axis of the prism.\n    phase_offset : float\n                   Phase offset in angles. Default is zero.\n\n    Returns\n    ----------\n    prism        : torch.tensor\n                   Generated phase function for a prism.\n    \"\"\"\n    angle = torch.deg2rad(torch.tensor([angle]))\n    phase_offset = torch.deg2rad(torch.tensor([phase_offset]))\n    x = torch.arange(0, nx) * dx\n    y = torch.arange(0, ny) * dx\n    X, Y = torch.meshgrid(x, y, indexing='ij')\n    if axis == 'y':\n        phase = k * torch.sin(angle) * Y + phase_offset\n        prism = torch.exp(-1j * phase)\n    elif axis == 'x':\n        phase = k * torch.sin(angle) * X + phase_offset\n        prism = torch.exp(-1j * phase)\n    return prism\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.lens.quadratic_phase_function","title":"<code>quadratic_phase_function(nx, ny, k, focal=0.4, dx=0.001, offset=[0, 0])</code>","text":"<p>A definition to generate 2D quadratic phase function, which is typically use to represent lenses.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>     Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>     Size of the output along Y.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>     See odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>focal</code>           \u2013            <pre><code>     Focal length of the quadratic phase function.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>     Pixel pitch.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>     Deviation from the center along X and Y axes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>qpf</code> (              <code>tensor</code> )          \u2013            <p>Generated quadratic phase function.</p> </li> </ul> Source code in <code>odak/learn/wave/lens.py</code> <pre><code>def quadratic_phase_function(nx, ny, k, focal=0.4, dx=0.001, offset=[0, 0]):\n    \"\"\" \n    A definition to generate 2D quadratic phase function, which is typically use to represent lenses.\n\n    Parameters\n    ----------\n    nx         : int\n                 Size of the output along X.\n    ny         : int\n                 Size of the output along Y.\n    k          : odak.wave.wavenumber\n                 See odak.wave.wavenumber for more.\n    focal      : float\n                 Focal length of the quadratic phase function.\n    dx         : float\n                 Pixel pitch.\n    offset     : list\n                 Deviation from the center along X and Y axes.\n\n    Returns\n    -------\n    qpf        : torch.tensor\n                 Generated quadratic phase function.\n    \"\"\"\n    size = [nx, ny]\n    x = torch.linspace(-size[0] * dx / 2, size[0] * dx / 2, size[0]) - offset[1] * dx\n    y = torch.linspace(-size[1] * dx / 2, size[1] * dx / 2, size[1]) - offset[0] * dx\n    X, Y = torch.meshgrid(x, y, indexing = 'ij')\n    Z = X ** 2 + Y ** 2\n    qpf = torch.exp(-0.5j * k / focal * Z)\n    return qpf\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.multiplane_loss","title":"<code>multiplane_loss</code>","text":"<p>Loss function for computing loss in multiplanar images. Unlike, previous methods, this loss function accounts for defocused parts of an image.</p> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>class multiplane_loss():\n    \"\"\"\n    Loss function for computing loss in multiplanar images. Unlike, previous methods, this loss function accounts for defocused parts of an image.\n    \"\"\"\n\n    def __init__(self, target_image, target_depth, blur_ratio = 0.25, \n                 target_blur_size = 10, number_of_planes = 4, weights = [1., 2.1, 0.6], \n                 multiplier = 1., scheme = 'defocus', reduction = 'mean', device = torch.device('cpu')):\n        \"\"\"\n        Parameters\n        ----------\n        target_image      : torch.tensor\n                            Color target image [3 x m x n].\n        target_depth      : torch.tensor\n                            Monochrome target depth, same resolution as target_image.\n        target_blur_size  : int\n                            Maximum target blur size.\n        blur_ratio        : float\n                            Blur ratio, a value between zero and one.\n        number_of_planes  : int\n                            Number of planes.\n        weights           : list\n                            Weights of the loss function.\n        multiplier        : float\n                            Multiplier to multipy with targets.\n        scheme            : str\n                            The type of the loss, `naive` without defocus or `defocus` with defocus.\n        reduction         : str\n                            Reduction can either be 'mean', 'none' or 'sum'. For more see: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n        device            : torch.device\n                            Device to be used (e.g., cuda, cpu, opencl).\n        \"\"\"\n        self.device = device\n        self.target_image     = target_image.float().to(self.device)\n        self.target_depth     = target_depth.float().to(self.device)\n        self.target_blur_size = target_blur_size\n        if self.target_blur_size % 2 == 0:\n            self.target_blur_size += 1\n        self.number_of_planes = number_of_planes\n        self.multiplier       = multiplier\n        self.weights          = weights\n        self.reduction        = reduction\n        self.blur_ratio       = blur_ratio\n        self.set_targets()\n        if scheme == 'defocus':\n            self.add_defocus_blur()\n        self.loss_function = torch.nn.MSELoss(reduction = self.reduction)\n\n    def get_targets(self):\n        \"\"\"\n        Returns\n        -------\n        targets           : torch.tensor\n                            Returns a copy of the targets.\n        target_depth      : torch.tensor\n                            Returns a copy of the normalized quantized depth map.\n\n        \"\"\"\n        divider = self.number_of_planes - 1\n        if divider == 0:\n            divider = 1\n        return self.targets.detach().clone(), self.focus_target.detach().clone(), self.target_depth.detach().clone() / divider\n\n\n    def set_targets(self):\n        \"\"\"\n        Internal function for slicing the depth into planes without considering defocus. Users can query the results with get_targets() within the same class.\n        \"\"\"\n        self.target_depth = self.target_depth * (self.number_of_planes - 1)\n        self.target_depth = torch.round(self.target_depth, decimals = 0)\n        self.targets      = torch.zeros(\n                                        self.number_of_planes,\n                                        self.target_image.shape[0],\n                                        self.target_image.shape[1],\n                                        self.target_image.shape[2],\n                                        requires_grad = False,\n                                        device = self.device\n                                       )\n        self.focus_target = torch.zeros_like(self.target_image, requires_grad = False)\n        self.masks        = torch.zeros_like(self.targets)\n        for i in range(self.number_of_planes):\n            for ch in range(self.target_image.shape[0]):\n                mask_zeros = torch.zeros_like(self.target_image[ch], dtype = torch.int)\n                mask_ones = torch.ones_like(self.target_image[ch], dtype = torch.int)\n                mask = torch.where(self.target_depth == i, mask_ones, mask_zeros)\n                new_target = self.target_image[ch] * mask\n                self.focus_target = self.focus_target + new_target.squeeze(0).squeeze(0).detach().clone()\n                self.targets[i, ch] = new_target.squeeze(0).squeeze(0)\n                self.masks[i, ch] = mask.detach().clone() \n\n\n    def add_defocus_blur(self):\n        \"\"\"\n        Internal function for adding defocus blur to the multiplane targets. Users can query the results with get_targets() within the same class.\n        \"\"\"\n        kernel_length = [self.target_blur_size, self.target_blur_size ]\n        for ch in range(self.target_image.shape[0]):\n            targets_cache = self.targets[:, ch].detach().clone()\n            target = torch.sum(targets_cache, axis = 0)\n            for i in range(self.number_of_planes):\n                defocus = torch.zeros_like(targets_cache[i])\n                for j in range(self.number_of_planes):\n                    nsigma = [int(abs(i - j) * self.blur_ratio), int(abs(i -j) * self.blur_ratio)]\n                    if torch.sum(targets_cache[j]) &gt; 0:\n                        if i == j:\n                            nsigma = [0., 0.]\n                        kernel = generate_2d_gaussian(kernel_length, nsigma).to(self.device)\n                        kernel = kernel / torch.sum(kernel)\n                        kernel = kernel.unsqueeze(0).unsqueeze(0)\n                        target_current = target.detach().clone().unsqueeze(0).unsqueeze(0)\n                        defocus_plane = torch.nn.functional.conv2d(target_current, kernel, padding = 'same')\n                        defocus_plane = defocus_plane.view(defocus_plane.shape[-2], defocus_plane.shape[-1])\n                        defocus = defocus + defocus_plane * torch.abs(self.masks[j, ch])\n                self.targets[i, ch] = defocus\n        self.targets = self.targets.detach().clone() * self.multiplier\n\n\n    def __call__(self, image, target, plane_id = None, inject_noise = False, noise_ratio = 1e-3):\n        \"\"\"\n        Calculates the multiplane loss against a given target.\n\n        Parameters\n        ----------\n        image         : torch.tensor\n                        Image to compare with a target [3 x m x n].\n        target        : torch.tensor\n                        Target image for comparison [3 x m x n].\n        plane_id      : int\n                        Number of the plane under test.\n        inject_noise  : bool\n                        When True, noise is added on the targets at the given `noise_ratio`.\n        noise_ratio   : float\n                        Noise ratio.\n\n        Returns\n        -------\n        loss          : torch.tensor\n                        Computed loss.\n        \"\"\"\n        l2 = self.weights[0] * self.loss_function(image, target)\n        if isinstance(plane_id, type(None)):\n            mask = self.masks\n        else:\n            mask= self.masks[plane_id, :]\n        if inject_noise:\n            target = target + torch.randn_like(target) * noise_ratio * (target.max() - target.min())\n        l2_mask = self.weights[1] * self.loss_function(image * mask, target * mask)\n        l2_cor = self.weights[2] * self.loss_function(image * target, target * target)\n        loss = l2 + l2_mask + l2_cor\n        return loss\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.multiplane_loss.__call__","title":"<code>__call__(image, target, plane_id=None, inject_noise=False, noise_ratio=0.001)</code>","text":"<p>Calculates the multiplane loss against a given target.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>        Image to compare with a target [3 x m x n].\n</code></pre> </li> <li> <code>target</code>           \u2013            <pre><code>        Target image for comparison [3 x m x n].\n</code></pre> </li> <li> <code>plane_id</code>           \u2013            <pre><code>        Number of the plane under test.\n</code></pre> </li> <li> <code>inject_noise</code>           \u2013            <pre><code>        When True, noise is added on the targets at the given `noise_ratio`.\n</code></pre> </li> <li> <code>noise_ratio</code>           \u2013            <pre><code>        Noise ratio.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>tensor</code> )          \u2013            <p>Computed loss.</p> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def __call__(self, image, target, plane_id = None, inject_noise = False, noise_ratio = 1e-3):\n    \"\"\"\n    Calculates the multiplane loss against a given target.\n\n    Parameters\n    ----------\n    image         : torch.tensor\n                    Image to compare with a target [3 x m x n].\n    target        : torch.tensor\n                    Target image for comparison [3 x m x n].\n    plane_id      : int\n                    Number of the plane under test.\n    inject_noise  : bool\n                    When True, noise is added on the targets at the given `noise_ratio`.\n    noise_ratio   : float\n                    Noise ratio.\n\n    Returns\n    -------\n    loss          : torch.tensor\n                    Computed loss.\n    \"\"\"\n    l2 = self.weights[0] * self.loss_function(image, target)\n    if isinstance(plane_id, type(None)):\n        mask = self.masks\n    else:\n        mask= self.masks[plane_id, :]\n    if inject_noise:\n        target = target + torch.randn_like(target) * noise_ratio * (target.max() - target.min())\n    l2_mask = self.weights[1] * self.loss_function(image * mask, target * mask)\n    l2_cor = self.weights[2] * self.loss_function(image * target, target * target)\n    loss = l2 + l2_mask + l2_cor\n    return loss\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.multiplane_loss.__init__","title":"<code>__init__(target_image, target_depth, blur_ratio=0.25, target_blur_size=10, number_of_planes=4, weights=[1.0, 2.1, 0.6], multiplier=1.0, scheme='defocus', reduction='mean', device=torch.device('cpu'))</code>","text":"<p>Parameters:</p> <ul> <li> <code>target_image</code>           \u2013            <pre><code>            Color target image [3 x m x n].\n</code></pre> </li> <li> <code>target_depth</code>           \u2013            <pre><code>            Monochrome target depth, same resolution as target_image.\n</code></pre> </li> <li> <code>target_blur_size</code>           \u2013            <pre><code>            Maximum target blur size.\n</code></pre> </li> <li> <code>blur_ratio</code>           \u2013            <pre><code>            Blur ratio, a value between zero and one.\n</code></pre> </li> <li> <code>number_of_planes</code>           \u2013            <pre><code>            Number of planes.\n</code></pre> </li> <li> <code>weights</code>           \u2013            <pre><code>            Weights of the loss function.\n</code></pre> </li> <li> <code>multiplier</code>           \u2013            <pre><code>            Multiplier to multipy with targets.\n</code></pre> </li> <li> <code>scheme</code>           \u2013            <pre><code>            The type of the loss, `naive` without defocus or `defocus` with defocus.\n</code></pre> </li> <li> <code>reduction</code>           \u2013            <pre><code>            Reduction can either be 'mean', 'none' or 'sum'. For more see: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>            Device to be used (e.g., cuda, cpu, opencl).\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def __init__(self, target_image, target_depth, blur_ratio = 0.25, \n             target_blur_size = 10, number_of_planes = 4, weights = [1., 2.1, 0.6], \n             multiplier = 1., scheme = 'defocus', reduction = 'mean', device = torch.device('cpu')):\n    \"\"\"\n    Parameters\n    ----------\n    target_image      : torch.tensor\n                        Color target image [3 x m x n].\n    target_depth      : torch.tensor\n                        Monochrome target depth, same resolution as target_image.\n    target_blur_size  : int\n                        Maximum target blur size.\n    blur_ratio        : float\n                        Blur ratio, a value between zero and one.\n    number_of_planes  : int\n                        Number of planes.\n    weights           : list\n                        Weights of the loss function.\n    multiplier        : float\n                        Multiplier to multipy with targets.\n    scheme            : str\n                        The type of the loss, `naive` without defocus or `defocus` with defocus.\n    reduction         : str\n                        Reduction can either be 'mean', 'none' or 'sum'. For more see: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n    device            : torch.device\n                        Device to be used (e.g., cuda, cpu, opencl).\n    \"\"\"\n    self.device = device\n    self.target_image     = target_image.float().to(self.device)\n    self.target_depth     = target_depth.float().to(self.device)\n    self.target_blur_size = target_blur_size\n    if self.target_blur_size % 2 == 0:\n        self.target_blur_size += 1\n    self.number_of_planes = number_of_planes\n    self.multiplier       = multiplier\n    self.weights          = weights\n    self.reduction        = reduction\n    self.blur_ratio       = blur_ratio\n    self.set_targets()\n    if scheme == 'defocus':\n        self.add_defocus_blur()\n    self.loss_function = torch.nn.MSELoss(reduction = self.reduction)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.multiplane_loss.add_defocus_blur","title":"<code>add_defocus_blur()</code>","text":"<p>Internal function for adding defocus blur to the multiplane targets. Users can query the results with get_targets() within the same class.</p> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def add_defocus_blur(self):\n    \"\"\"\n    Internal function for adding defocus blur to the multiplane targets. Users can query the results with get_targets() within the same class.\n    \"\"\"\n    kernel_length = [self.target_blur_size, self.target_blur_size ]\n    for ch in range(self.target_image.shape[0]):\n        targets_cache = self.targets[:, ch].detach().clone()\n        target = torch.sum(targets_cache, axis = 0)\n        for i in range(self.number_of_planes):\n            defocus = torch.zeros_like(targets_cache[i])\n            for j in range(self.number_of_planes):\n                nsigma = [int(abs(i - j) * self.blur_ratio), int(abs(i -j) * self.blur_ratio)]\n                if torch.sum(targets_cache[j]) &gt; 0:\n                    if i == j:\n                        nsigma = [0., 0.]\n                    kernel = generate_2d_gaussian(kernel_length, nsigma).to(self.device)\n                    kernel = kernel / torch.sum(kernel)\n                    kernel = kernel.unsqueeze(0).unsqueeze(0)\n                    target_current = target.detach().clone().unsqueeze(0).unsqueeze(0)\n                    defocus_plane = torch.nn.functional.conv2d(target_current, kernel, padding = 'same')\n                    defocus_plane = defocus_plane.view(defocus_plane.shape[-2], defocus_plane.shape[-1])\n                    defocus = defocus + defocus_plane * torch.abs(self.masks[j, ch])\n            self.targets[i, ch] = defocus\n    self.targets = self.targets.detach().clone() * self.multiplier\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.multiplane_loss.get_targets","title":"<code>get_targets()</code>","text":"<p>Returns:</p> <ul> <li> <code>targets</code> (              <code>tensor</code> )          \u2013            <p>Returns a copy of the targets.</p> </li> <li> <code>target_depth</code> (              <code>tensor</code> )          \u2013            <p>Returns a copy of the normalized quantized depth map.</p> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def get_targets(self):\n    \"\"\"\n    Returns\n    -------\n    targets           : torch.tensor\n                        Returns a copy of the targets.\n    target_depth      : torch.tensor\n                        Returns a copy of the normalized quantized depth map.\n\n    \"\"\"\n    divider = self.number_of_planes - 1\n    if divider == 0:\n        divider = 1\n    return self.targets.detach().clone(), self.focus_target.detach().clone(), self.target_depth.detach().clone() / divider\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.multiplane_loss.set_targets","title":"<code>set_targets()</code>","text":"<p>Internal function for slicing the depth into planes without considering defocus. Users can query the results with get_targets() within the same class.</p> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def set_targets(self):\n    \"\"\"\n    Internal function for slicing the depth into planes without considering defocus. Users can query the results with get_targets() within the same class.\n    \"\"\"\n    self.target_depth = self.target_depth * (self.number_of_planes - 1)\n    self.target_depth = torch.round(self.target_depth, decimals = 0)\n    self.targets      = torch.zeros(\n                                    self.number_of_planes,\n                                    self.target_image.shape[0],\n                                    self.target_image.shape[1],\n                                    self.target_image.shape[2],\n                                    requires_grad = False,\n                                    device = self.device\n                                   )\n    self.focus_target = torch.zeros_like(self.target_image, requires_grad = False)\n    self.masks        = torch.zeros_like(self.targets)\n    for i in range(self.number_of_planes):\n        for ch in range(self.target_image.shape[0]):\n            mask_zeros = torch.zeros_like(self.target_image[ch], dtype = torch.int)\n            mask_ones = torch.ones_like(self.target_image[ch], dtype = torch.int)\n            mask = torch.where(self.target_depth == i, mask_ones, mask_zeros)\n            new_target = self.target_image[ch] * mask\n            self.focus_target = self.focus_target + new_target.squeeze(0).squeeze(0).detach().clone()\n            self.targets[i, ch] = new_target.squeeze(0).squeeze(0)\n            self.masks[i, ch] = mask.detach().clone() \n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.perceptual_multiplane_loss","title":"<code>perceptual_multiplane_loss</code>","text":"<p>Perceptual loss function for computing loss in multiplanar images. Unlike, previous methods, this loss function accounts for defocused parts of an image.</p> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>class perceptual_multiplane_loss():\n    \"\"\"\n    Perceptual loss function for computing loss in multiplanar images. Unlike, previous methods, this loss function accounts for defocused parts of an image.\n    \"\"\"\n\n    def __init__(self, target_image, target_depth, blur_ratio = 0.25, \n                 target_blur_size = 10, number_of_planes = 4, multiplier = 1., scheme = 'defocus', \n                 base_loss_weights = {'base_l2_loss': 1., 'loss_l2_mask': 1., 'loss_l2_cor': 1., 'base_l1_loss': 1., 'loss_l1_mask': 1., 'loss_l1_cor': 1.},\n                 additional_loss_weights = {'cvvdp': 1.}, reduction = 'mean', return_components = False, device = torch.device('cpu')):\n        \"\"\"\n        Parameters\n        ----------\n        target_image            : torch.tensor\n                                    Color target image [3 x m x n].\n        target_depth            : torch.tensor\n                                    Monochrome target depth, same resolution as target_image.\n        target_blur_size        : int\n                                    Maximum target blur size.\n        blur_ratio              : float\n                                    Blur ratio, a value between zero and one.\n        number_of_planes        : int\n                                    Number of planes.\n        multiplier              : float\n                                    Multiplier to multipy with targets.\n        scheme                  : str\n                                    The type of the loss, `naive` without defocus or `defocus` with defocus.\n        base_loss_weights       : list\n                                    Weights of the base loss functions. Default is {'base_l2_loss': 1., 'loss_l2_mask': 1., 'loss_l2_cor': 1., 'base_l1_loss': 1., 'loss_l1_mask': 1., 'loss_l1_cor': 1.}.\n        additional_loss_weights : dict\n                                    Additional loss terms and their weights (e.g., {'cvvdp': 1.}). Supported loss terms are 'cvvdp', 'fvvdp', 'lpips', 'psnr', 'ssim', 'msssim'.\n        reduction               : str\n                                    Reduction can either be 'mean', 'none' or 'sum'. For more see: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n        return_components       : bool\n                                    If True (False by default), returns the components of the loss as a dict.\n        device                  : torch.device\n                                    Device to be used (e.g., cuda, cpu, opencl).\n        \"\"\"\n        self.device = device\n        self.target_image     = target_image.float().to(self.device)\n        self.target_depth     = target_depth.float().to(self.device)\n        self.target_blur_size = target_blur_size\n        if self.target_blur_size % 2 == 0:\n            self.target_blur_size += 1\n        self.number_of_planes = number_of_planes\n        self.multiplier       = multiplier\n        self.reduction        = reduction\n        if self.reduction == 'none' and len(list(additional_loss_weights.keys())) &gt; 0:\n            logging.warning(\"Reduction cannot be 'none' for additional loss functions. Changing reduction to 'mean'.\")\n            self.reduction = 'mean'\n        self.blur_ratio       = blur_ratio\n        self.set_targets()\n        if scheme == 'defocus':\n            self.add_defocus_blur()\n        self.base_loss_weights = base_loss_weights\n        self.additional_loss_weights = additional_loss_weights\n        self.return_components = return_components\n        self.l1_loss_fn = torch.nn.L1Loss(reduction = self.reduction)\n        self.l2_loss_fn = torch.nn.MSELoss(reduction = self.reduction)\n        for key in self.additional_loss_weights.keys():\n            if self.additional_loss_weights[key]:\n                if key == 'cvvdp':\n                    self.cvvdp = CVVDP(device = device)\n                if key == 'fvvdp':\n                    self.fvvdp = FVVDP()\n                if key == 'lpips':\n                    self.lpips = LPIPS()\n                if key == 'psnr':\n                    self.psnr = PSNR()\n                if key == 'ssim':\n                    self.ssim = SSIM()\n                if key == 'msssim':\n                    self.msssim = MSSSIM()\n\n    def get_targets(self):\n        \"\"\"\n        Returns\n        -------\n        targets           : torch.tensor\n                            Returns a copy of the targets.\n        target_depth      : torch.tensor\n                            Returns a copy of the normalized quantized depth map.\n\n        \"\"\"\n        divider = self.number_of_planes - 1\n        if divider == 0:\n            divider = 1\n        return self.targets.detach().clone(), self.focus_target.detach().clone(), self.target_depth.detach().clone() / divider\n\n\n    def set_targets(self):\n        \"\"\"\n        Internal function for slicing the depth into planes without considering defocus. Users can query the results with get_targets() within the same class.\n        \"\"\"\n        self.target_depth = self.target_depth * (self.number_of_planes - 1)\n        self.target_depth = torch.round(self.target_depth, decimals = 0)\n        self.targets      = torch.zeros(\n                                        self.number_of_planes,\n                                        self.target_image.shape[0],\n                                        self.target_image.shape[1],\n                                        self.target_image.shape[2],\n                                        requires_grad = False,\n                                        device = self.device\n                                       )\n        self.focus_target = torch.zeros_like(self.target_image, requires_grad = False)\n        self.masks        = torch.zeros_like(self.targets)\n        for i in range(self.number_of_planes):\n            for ch in range(self.target_image.shape[0]):\n                mask_zeros = torch.zeros_like(self.target_image[ch], dtype = torch.int)\n                mask_ones = torch.ones_like(self.target_image[ch], dtype = torch.int)\n                mask = torch.where(self.target_depth == i, mask_ones, mask_zeros)\n                new_target = self.target_image[ch] * mask\n                self.focus_target = self.focus_target + new_target.squeeze(0).squeeze(0).detach().clone()\n                self.targets[i, ch] = new_target.squeeze(0).squeeze(0)\n                self.masks[i, ch] = mask.detach().clone() \n\n\n    def add_defocus_blur(self):\n        \"\"\"\n        Internal function for adding defocus blur to the multiplane targets. Users can query the results with get_targets() within the same class.\n        \"\"\"\n        kernel_length = [self.target_blur_size, self.target_blur_size ]\n        for ch in range(self.target_image.shape[0]):\n            targets_cache = self.targets[:, ch].detach().clone()\n            target = torch.sum(targets_cache, axis = 0)\n            for i in range(self.number_of_planes):\n                defocus = torch.zeros_like(targets_cache[i])\n                for j in range(self.number_of_planes):\n                    nsigma = [int(abs(i - j) * self.blur_ratio), int(abs(i -j) * self.blur_ratio)]\n                    if torch.sum(targets_cache[j]) &gt; 0:\n                        if i == j:\n                            nsigma = [0., 0.]\n                        kernel = generate_2d_gaussian(kernel_length, nsigma).to(self.device)\n                        kernel = kernel / torch.sum(kernel)\n                        kernel = kernel.unsqueeze(0).unsqueeze(0)\n                        target_current = target.detach().clone().unsqueeze(0).unsqueeze(0)\n                        defocus_plane = torch.nn.functional.conv2d(target_current, kernel, padding = 'same')\n                        defocus_plane = defocus_plane.view(defocus_plane.shape[-2], defocus_plane.shape[-1])\n                        defocus = defocus + defocus_plane * torch.abs(self.masks[j, ch])\n                self.targets[i, ch] = defocus\n        self.targets = self.targets.detach().clone() * self.multiplier\n\n\n    def __call__(self, image, target, plane_id = None, inject_noise = False, noise_ratio = 1e-3):\n        \"\"\"\n        Calculates the multiplane loss against a given target.\n\n        Parameters\n        ----------\n        image         : torch.tensor\n                        Image to compare with a target [3 x m x n].\n        target        : torch.tensor\n                        Target image for comparison [3 x m x n].\n        plane_id      : int\n                        Number of the plane under test.\n        inject_noise  : bool\n                        When True, noise is added on the targets at the given `noise_ratio`.\n        noise_ratio   : float\n                        Noise ratio.\n\n\n        Returns\n        -------\n        loss          : torch.tensor\n                        Computed loss.\n        \"\"\"\n        loss_components = {}\n        if isinstance(plane_id, type(None)):\n            mask = self.masks\n        else:\n            mask= self.masks[plane_id, :]\n        if inject_noise:\n            target = target + torch.randn_like(target) * noise_ratio * (target.max() - target.min())\n        l2 = self.base_loss_weights['base_l2_loss'] * self.l2_loss_fn(image, target)\n        l2_mask = self.base_loss_weights['loss_l2_mask'] * self.l2_loss_fn(image * mask, target * mask)\n        l2_cor = self.base_loss_weights['loss_l2_cor'] * self.l2_loss_fn(image * target, target * target)\n        loss_components['l2'] = l2\n        loss_components['l2_mask'] = l2_mask\n        loss_components['l2_cor'] = l2_cor\n        loss = l2 + l2_mask + l2_cor\n\n        l1 = self.base_loss_weights['base_l1_loss'] * self.l1_loss_fn(image, target)\n        l1_mask = self.base_loss_weights['loss_l1_mask'] * self.l1_loss_fn(image * mask, target * mask)\n        l1_cor = self.base_loss_weights['loss_l1_cor'] * self.l1_loss_fn(image * target, target * target)\n        loss_components['l1'] = l1\n        loss_components['l1_mask'] = l1_mask\n        loss_components['l1_cor'] = l1_cor\n        loss += l1 + l1_mask + l1_cor\n\n        for key in self.additional_loss_weights.keys():\n            if self.additional_loss_weights[key]:\n                if key == 'cvvdp':\n                    loss_cvvdp = self.additional_loss_weights['cvvdp'] * self.cvvdp(image, target)\n                    loss_components['cvvdp'] = loss_cvvdp\n                    loss += loss_cvvdp\n                if key == 'fvvdp':\n                    loss_fvvdp = self.additional_loss_weights['fvvdp'] * self.fvvdp(image, target)\n                    loss_components['fvvdp'] = loss_fvvdp\n                    loss += loss_fvvdp\n                if key == 'lpips':\n                    loss_lpips = self.additional_loss_weights['lpips'] * self.lpips(image, target)\n                    loss_components['lpips'] = loss_lpips\n                    loss += loss_lpips\n                if key == 'psnr':\n                    loss_psnr = self.additional_loss_weights['psnr'] * self.psnr(image, target)\n                    loss_components['psnr'] = loss_psnr\n                    loss += loss_psnr\n                if key == 'ssim':\n                    loss_ssim = self.additional_loss_weights['ssim'] * self.ssim(image, target)\n                    loss_components['ssim'] = loss_ssim\n                    loss += loss_ssim\n                if key == 'msssim':\n                    loss_msssim = self.additional_loss_weights['msssim'] * self.msssim(image, target)\n                    loss_components['msssim'] = loss_msssim\n                    loss += loss_msssim\n        if self.return_components:\n            return loss, loss_components\n        return loss\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.perceptual_multiplane_loss.__call__","title":"<code>__call__(image, target, plane_id=None, inject_noise=False, noise_ratio=0.001)</code>","text":"<p>Calculates the multiplane loss against a given target.</p> <p>Parameters:</p> <ul> <li> <code>image</code>           \u2013            <pre><code>        Image to compare with a target [3 x m x n].\n</code></pre> </li> <li> <code>target</code>           \u2013            <pre><code>        Target image for comparison [3 x m x n].\n</code></pre> </li> <li> <code>plane_id</code>           \u2013            <pre><code>        Number of the plane under test.\n</code></pre> </li> <li> <code>inject_noise</code>           \u2013            <pre><code>        When True, noise is added on the targets at the given `noise_ratio`.\n</code></pre> </li> <li> <code>noise_ratio</code>           \u2013            <pre><code>        Noise ratio.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss</code> (              <code>tensor</code> )          \u2013            <p>Computed loss.</p> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def __call__(self, image, target, plane_id = None, inject_noise = False, noise_ratio = 1e-3):\n    \"\"\"\n    Calculates the multiplane loss against a given target.\n\n    Parameters\n    ----------\n    image         : torch.tensor\n                    Image to compare with a target [3 x m x n].\n    target        : torch.tensor\n                    Target image for comparison [3 x m x n].\n    plane_id      : int\n                    Number of the plane under test.\n    inject_noise  : bool\n                    When True, noise is added on the targets at the given `noise_ratio`.\n    noise_ratio   : float\n                    Noise ratio.\n\n\n    Returns\n    -------\n    loss          : torch.tensor\n                    Computed loss.\n    \"\"\"\n    loss_components = {}\n    if isinstance(plane_id, type(None)):\n        mask = self.masks\n    else:\n        mask= self.masks[plane_id, :]\n    if inject_noise:\n        target = target + torch.randn_like(target) * noise_ratio * (target.max() - target.min())\n    l2 = self.base_loss_weights['base_l2_loss'] * self.l2_loss_fn(image, target)\n    l2_mask = self.base_loss_weights['loss_l2_mask'] * self.l2_loss_fn(image * mask, target * mask)\n    l2_cor = self.base_loss_weights['loss_l2_cor'] * self.l2_loss_fn(image * target, target * target)\n    loss_components['l2'] = l2\n    loss_components['l2_mask'] = l2_mask\n    loss_components['l2_cor'] = l2_cor\n    loss = l2 + l2_mask + l2_cor\n\n    l1 = self.base_loss_weights['base_l1_loss'] * self.l1_loss_fn(image, target)\n    l1_mask = self.base_loss_weights['loss_l1_mask'] * self.l1_loss_fn(image * mask, target * mask)\n    l1_cor = self.base_loss_weights['loss_l1_cor'] * self.l1_loss_fn(image * target, target * target)\n    loss_components['l1'] = l1\n    loss_components['l1_mask'] = l1_mask\n    loss_components['l1_cor'] = l1_cor\n    loss += l1 + l1_mask + l1_cor\n\n    for key in self.additional_loss_weights.keys():\n        if self.additional_loss_weights[key]:\n            if key == 'cvvdp':\n                loss_cvvdp = self.additional_loss_weights['cvvdp'] * self.cvvdp(image, target)\n                loss_components['cvvdp'] = loss_cvvdp\n                loss += loss_cvvdp\n            if key == 'fvvdp':\n                loss_fvvdp = self.additional_loss_weights['fvvdp'] * self.fvvdp(image, target)\n                loss_components['fvvdp'] = loss_fvvdp\n                loss += loss_fvvdp\n            if key == 'lpips':\n                loss_lpips = self.additional_loss_weights['lpips'] * self.lpips(image, target)\n                loss_components['lpips'] = loss_lpips\n                loss += loss_lpips\n            if key == 'psnr':\n                loss_psnr = self.additional_loss_weights['psnr'] * self.psnr(image, target)\n                loss_components['psnr'] = loss_psnr\n                loss += loss_psnr\n            if key == 'ssim':\n                loss_ssim = self.additional_loss_weights['ssim'] * self.ssim(image, target)\n                loss_components['ssim'] = loss_ssim\n                loss += loss_ssim\n            if key == 'msssim':\n                loss_msssim = self.additional_loss_weights['msssim'] * self.msssim(image, target)\n                loss_components['msssim'] = loss_msssim\n                loss += loss_msssim\n    if self.return_components:\n        return loss, loss_components\n    return loss\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.perceptual_multiplane_loss.__init__","title":"<code>__init__(target_image, target_depth, blur_ratio=0.25, target_blur_size=10, number_of_planes=4, multiplier=1.0, scheme='defocus', base_loss_weights={'base_l2_loss': 1.0, 'loss_l2_mask': 1.0, 'loss_l2_cor': 1.0, 'base_l1_loss': 1.0, 'loss_l1_mask': 1.0, 'loss_l1_cor': 1.0}, additional_loss_weights={'cvvdp': 1.0}, reduction='mean', return_components=False, device=torch.device('cpu'))</code>","text":"<p>Parameters:</p> <ul> <li> <code>target_image</code>           \u2013            <pre><code>                    Color target image [3 x m x n].\n</code></pre> </li> <li> <code>target_depth</code>           \u2013            <pre><code>                    Monochrome target depth, same resolution as target_image.\n</code></pre> </li> <li> <code>target_blur_size</code>           \u2013            <pre><code>                    Maximum target blur size.\n</code></pre> </li> <li> <code>blur_ratio</code>           \u2013            <pre><code>                    Blur ratio, a value between zero and one.\n</code></pre> </li> <li> <code>number_of_planes</code>           \u2013            <pre><code>                    Number of planes.\n</code></pre> </li> <li> <code>multiplier</code>           \u2013            <pre><code>                    Multiplier to multipy with targets.\n</code></pre> </li> <li> <code>scheme</code>           \u2013            <pre><code>                    The type of the loss, `naive` without defocus or `defocus` with defocus.\n</code></pre> </li> <li> <code>base_loss_weights</code>           \u2013            <pre><code>                    Weights of the base loss functions. Default is {'base_l2_loss': 1., 'loss_l2_mask': 1., 'loss_l2_cor': 1., 'base_l1_loss': 1., 'loss_l1_mask': 1., 'loss_l1_cor': 1.}.\n</code></pre> </li> <li> <code>additional_loss_weights</code>               (<code>dict</code>, default:                   <code>{'cvvdp': 1.0}</code> )           \u2013            <pre><code>                    Additional loss terms and their weights (e.g., {'cvvdp': 1.}). Supported loss terms are 'cvvdp', 'fvvdp', 'lpips', 'psnr', 'ssim', 'msssim'.\n</code></pre> </li> <li> <code>reduction</code>           \u2013            <pre><code>                    Reduction can either be 'mean', 'none' or 'sum'. For more see: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n</code></pre> </li> <li> <code>return_components</code>           \u2013            <pre><code>                    If True (False by default), returns the components of the loss as a dict.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>                    Device to be used (e.g., cuda, cpu, opencl).\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def __init__(self, target_image, target_depth, blur_ratio = 0.25, \n             target_blur_size = 10, number_of_planes = 4, multiplier = 1., scheme = 'defocus', \n             base_loss_weights = {'base_l2_loss': 1., 'loss_l2_mask': 1., 'loss_l2_cor': 1., 'base_l1_loss': 1., 'loss_l1_mask': 1., 'loss_l1_cor': 1.},\n             additional_loss_weights = {'cvvdp': 1.}, reduction = 'mean', return_components = False, device = torch.device('cpu')):\n    \"\"\"\n    Parameters\n    ----------\n    target_image            : torch.tensor\n                                Color target image [3 x m x n].\n    target_depth            : torch.tensor\n                                Monochrome target depth, same resolution as target_image.\n    target_blur_size        : int\n                                Maximum target blur size.\n    blur_ratio              : float\n                                Blur ratio, a value between zero and one.\n    number_of_planes        : int\n                                Number of planes.\n    multiplier              : float\n                                Multiplier to multipy with targets.\n    scheme                  : str\n                                The type of the loss, `naive` without defocus or `defocus` with defocus.\n    base_loss_weights       : list\n                                Weights of the base loss functions. Default is {'base_l2_loss': 1., 'loss_l2_mask': 1., 'loss_l2_cor': 1., 'base_l1_loss': 1., 'loss_l1_mask': 1., 'loss_l1_cor': 1.}.\n    additional_loss_weights : dict\n                                Additional loss terms and their weights (e.g., {'cvvdp': 1.}). Supported loss terms are 'cvvdp', 'fvvdp', 'lpips', 'psnr', 'ssim', 'msssim'.\n    reduction               : str\n                                Reduction can either be 'mean', 'none' or 'sum'. For more see: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n    return_components       : bool\n                                If True (False by default), returns the components of the loss as a dict.\n    device                  : torch.device\n                                Device to be used (e.g., cuda, cpu, opencl).\n    \"\"\"\n    self.device = device\n    self.target_image     = target_image.float().to(self.device)\n    self.target_depth     = target_depth.float().to(self.device)\n    self.target_blur_size = target_blur_size\n    if self.target_blur_size % 2 == 0:\n        self.target_blur_size += 1\n    self.number_of_planes = number_of_planes\n    self.multiplier       = multiplier\n    self.reduction        = reduction\n    if self.reduction == 'none' and len(list(additional_loss_weights.keys())) &gt; 0:\n        logging.warning(\"Reduction cannot be 'none' for additional loss functions. Changing reduction to 'mean'.\")\n        self.reduction = 'mean'\n    self.blur_ratio       = blur_ratio\n    self.set_targets()\n    if scheme == 'defocus':\n        self.add_defocus_blur()\n    self.base_loss_weights = base_loss_weights\n    self.additional_loss_weights = additional_loss_weights\n    self.return_components = return_components\n    self.l1_loss_fn = torch.nn.L1Loss(reduction = self.reduction)\n    self.l2_loss_fn = torch.nn.MSELoss(reduction = self.reduction)\n    for key in self.additional_loss_weights.keys():\n        if self.additional_loss_weights[key]:\n            if key == 'cvvdp':\n                self.cvvdp = CVVDP(device = device)\n            if key == 'fvvdp':\n                self.fvvdp = FVVDP()\n            if key == 'lpips':\n                self.lpips = LPIPS()\n            if key == 'psnr':\n                self.psnr = PSNR()\n            if key == 'ssim':\n                self.ssim = SSIM()\n            if key == 'msssim':\n                self.msssim = MSSSIM()\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.perceptual_multiplane_loss.add_defocus_blur","title":"<code>add_defocus_blur()</code>","text":"<p>Internal function for adding defocus blur to the multiplane targets. Users can query the results with get_targets() within the same class.</p> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def add_defocus_blur(self):\n    \"\"\"\n    Internal function for adding defocus blur to the multiplane targets. Users can query the results with get_targets() within the same class.\n    \"\"\"\n    kernel_length = [self.target_blur_size, self.target_blur_size ]\n    for ch in range(self.target_image.shape[0]):\n        targets_cache = self.targets[:, ch].detach().clone()\n        target = torch.sum(targets_cache, axis = 0)\n        for i in range(self.number_of_planes):\n            defocus = torch.zeros_like(targets_cache[i])\n            for j in range(self.number_of_planes):\n                nsigma = [int(abs(i - j) * self.blur_ratio), int(abs(i -j) * self.blur_ratio)]\n                if torch.sum(targets_cache[j]) &gt; 0:\n                    if i == j:\n                        nsigma = [0., 0.]\n                    kernel = generate_2d_gaussian(kernel_length, nsigma).to(self.device)\n                    kernel = kernel / torch.sum(kernel)\n                    kernel = kernel.unsqueeze(0).unsqueeze(0)\n                    target_current = target.detach().clone().unsqueeze(0).unsqueeze(0)\n                    defocus_plane = torch.nn.functional.conv2d(target_current, kernel, padding = 'same')\n                    defocus_plane = defocus_plane.view(defocus_plane.shape[-2], defocus_plane.shape[-1])\n                    defocus = defocus + defocus_plane * torch.abs(self.masks[j, ch])\n            self.targets[i, ch] = defocus\n    self.targets = self.targets.detach().clone() * self.multiplier\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.perceptual_multiplane_loss.get_targets","title":"<code>get_targets()</code>","text":"<p>Returns:</p> <ul> <li> <code>targets</code> (              <code>tensor</code> )          \u2013            <p>Returns a copy of the targets.</p> </li> <li> <code>target_depth</code> (              <code>tensor</code> )          \u2013            <p>Returns a copy of the normalized quantized depth map.</p> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def get_targets(self):\n    \"\"\"\n    Returns\n    -------\n    targets           : torch.tensor\n                        Returns a copy of the targets.\n    target_depth      : torch.tensor\n                        Returns a copy of the normalized quantized depth map.\n\n    \"\"\"\n    divider = self.number_of_planes - 1\n    if divider == 0:\n        divider = 1\n    return self.targets.detach().clone(), self.focus_target.detach().clone(), self.target_depth.detach().clone() / divider\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.perceptual_multiplane_loss.set_targets","title":"<code>set_targets()</code>","text":"<p>Internal function for slicing the depth into planes without considering defocus. Users can query the results with get_targets() within the same class.</p> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def set_targets(self):\n    \"\"\"\n    Internal function for slicing the depth into planes without considering defocus. Users can query the results with get_targets() within the same class.\n    \"\"\"\n    self.target_depth = self.target_depth * (self.number_of_planes - 1)\n    self.target_depth = torch.round(self.target_depth, decimals = 0)\n    self.targets      = torch.zeros(\n                                    self.number_of_planes,\n                                    self.target_image.shape[0],\n                                    self.target_image.shape[1],\n                                    self.target_image.shape[2],\n                                    requires_grad = False,\n                                    device = self.device\n                                   )\n    self.focus_target = torch.zeros_like(self.target_image, requires_grad = False)\n    self.masks        = torch.zeros_like(self.targets)\n    for i in range(self.number_of_planes):\n        for ch in range(self.target_image.shape[0]):\n            mask_zeros = torch.zeros_like(self.target_image[ch], dtype = torch.int)\n            mask_ones = torch.ones_like(self.target_image[ch], dtype = torch.int)\n            mask = torch.where(self.target_depth == i, mask_ones, mask_zeros)\n            new_target = self.target_image[ch] * mask\n            self.focus_target = self.focus_target + new_target.squeeze(0).squeeze(0).detach().clone()\n            self.targets[i, ch] = new_target.squeeze(0).squeeze(0)\n            self.masks[i, ch] = mask.detach().clone() \n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.phase_gradient","title":"<code>phase_gradient</code>","text":"<p>               Bases: <code>Module</code></p> <p>The class 'phase_gradient' provides a regularization function to measure the variation(Gradient or Laplace) of the phase of the complex amplitude. </p> <p>This implements a convolution of the phase with a kernel.</p> <p>The kernel is a simple 3 by 3 Laplacian kernel here, but you can also try other edge detection methods.</p> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>class phase_gradient(nn.Module):\n\n    \"\"\"\n    The class 'phase_gradient' provides a regularization function to measure the variation(Gradient or Laplace) of the phase of the complex amplitude. \n\n    This implements a convolution of the phase with a kernel.\n\n    The kernel is a simple 3 by 3 Laplacian kernel here, but you can also try other edge detection methods.\n    \"\"\"\n\n\n    def __init__(self, kernel = None, loss = nn.MSELoss(), device = torch.device(\"cpu\")):\n        \"\"\"\n        Parameters\n        ----------\n        kernel                  : torch.tensor\n                                    Convolution filter kernel, 3 by 3 Laplacian kernel by default.\n        loss                    : torch.nn.Module\n                                    loss function, L2 Loss by default.\n        \"\"\"\n        super(phase_gradient, self).__init__()\n        self.device = device\n        self.loss = loss\n        if kernel == None:\n            self.kernel = torch.tensor([[[[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]]], dtype=torch.float32) / 8\n        else:\n            if len(kernel.shape) == 4:\n                self.kernel = kernel\n            else:\n                self.kernel = kernel.reshape((1, 1, kernel.shape[0], kernel.shape[1]))\n        self.kernel = Variable(self.kernel.to(self.device))\n\n\n    def forward(self, phase):\n        \"\"\"\n        Calculates the phase gradient Loss.\n\n        Parameters\n        ----------\n        phase                  : torch.tensor\n                                    Phase of the complex amplitude.\n\n        Returns\n        -------\n\n        loss_value              : torch.tensor\n                                    The computed loss.\n        \"\"\"\n\n        if len(phase.shape) == 2:\n            phase = phase.reshape((1, 1, phase.shape[0], phase.shape[1]))\n        edge_detect = self.functional_conv2d(phase)\n        loss_value = self.loss(edge_detect, torch.zeros_like(edge_detect))\n        return loss_value\n\n\n    def functional_conv2d(self, phase):\n        \"\"\"\n        Calculates the gradient of the phase.\n\n        Parameters\n        ----------\n        phase                  : torch.tensor\n                                    Phase of the complex amplitude.\n\n        Returns\n        -------\n\n        edge_detect              : torch.tensor\n                                    The computed phase gradient.\n        \"\"\"\n        edge_detect = F.conv2d(phase, self.kernel, padding = self.kernel.shape[-1] // 2)\n        return edge_detect\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.phase_gradient.__init__","title":"<code>__init__(kernel=None, loss=nn.MSELoss(), device=torch.device('cpu'))</code>","text":"<p>Parameters:</p> <ul> <li> <code>kernel</code>           \u2013            <pre><code>                    Convolution filter kernel, 3 by 3 Laplacian kernel by default.\n</code></pre> </li> <li> <code>loss</code>           \u2013            <pre><code>                    loss function, L2 Loss by default.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def __init__(self, kernel = None, loss = nn.MSELoss(), device = torch.device(\"cpu\")):\n    \"\"\"\n    Parameters\n    ----------\n    kernel                  : torch.tensor\n                                Convolution filter kernel, 3 by 3 Laplacian kernel by default.\n    loss                    : torch.nn.Module\n                                loss function, L2 Loss by default.\n    \"\"\"\n    super(phase_gradient, self).__init__()\n    self.device = device\n    self.loss = loss\n    if kernel == None:\n        self.kernel = torch.tensor([[[[-1, -1, -1], [-1, 8, -1], [-1, -1, -1]]]], dtype=torch.float32) / 8\n    else:\n        if len(kernel.shape) == 4:\n            self.kernel = kernel\n        else:\n            self.kernel = kernel.reshape((1, 1, kernel.shape[0], kernel.shape[1]))\n    self.kernel = Variable(self.kernel.to(self.device))\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.phase_gradient.forward","title":"<code>forward(phase)</code>","text":"<p>Calculates the phase gradient Loss.</p> <p>Parameters:</p> <ul> <li> <code>phase</code>           \u2013            <pre><code>                    Phase of the complex amplitude.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss_value</code> (              <code>tensor</code> )          \u2013            <p>The computed loss.</p> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def forward(self, phase):\n    \"\"\"\n    Calculates the phase gradient Loss.\n\n    Parameters\n    ----------\n    phase                  : torch.tensor\n                                Phase of the complex amplitude.\n\n    Returns\n    -------\n\n    loss_value              : torch.tensor\n                                The computed loss.\n    \"\"\"\n\n    if len(phase.shape) == 2:\n        phase = phase.reshape((1, 1, phase.shape[0], phase.shape[1]))\n    edge_detect = self.functional_conv2d(phase)\n    loss_value = self.loss(edge_detect, torch.zeros_like(edge_detect))\n    return loss_value\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.phase_gradient.functional_conv2d","title":"<code>functional_conv2d(phase)</code>","text":"<p>Calculates the gradient of the phase.</p> <p>Parameters:</p> <ul> <li> <code>phase</code>           \u2013            <pre><code>                    Phase of the complex amplitude.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>edge_detect</code> (              <code>tensor</code> )          \u2013            <p>The computed phase gradient.</p> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def functional_conv2d(self, phase):\n    \"\"\"\n    Calculates the gradient of the phase.\n\n    Parameters\n    ----------\n    phase                  : torch.tensor\n                                Phase of the complex amplitude.\n\n    Returns\n    -------\n\n    edge_detect              : torch.tensor\n                                The computed phase gradient.\n    \"\"\"\n    edge_detect = F.conv2d(phase, self.kernel, padding = self.kernel.shape[-1] // 2)\n    return edge_detect\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.speckle_contrast","title":"<code>speckle_contrast</code>","text":"<p>               Bases: <code>Module</code></p> <p>The class 'speckle_contrast' provides a regularization function to measure the speckle contrast of the intensity of the complex amplitude using C=sigma/mean. Where C is the speckle contrast, mean and sigma are mean and standard deviation of the intensity.</p> <p>We refer to the following paper:</p> <p>Kim et al.(2020). Light source optimization for partially coherent holographic displays with consideration of speckle contrast, resolution, and depth of field. Scientific Reports. 10. 18832. 10.1038/s41598-020-75947-0.</p> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>class speckle_contrast(nn.Module):\n\n    \"\"\"\n    The class 'speckle_contrast' provides a regularization function to measure the speckle contrast of the intensity of the complex amplitude using C=sigma/mean. Where C is the speckle contrast, mean and sigma are mean and standard deviation of the intensity.\n\n    We refer to the following paper:\n\n    Kim et al.(2020). Light source optimization for partially coherent holographic displays with consideration of speckle contrast, resolution, and depth of field. Scientific Reports. 10. 18832. 10.1038/s41598-020-75947-0. \n    \"\"\"\n\n\n    def __init__(self, kernel_size = 11, step_size = (1, 1), loss = nn.MSELoss(), device=torch.device(\"cpu\")):\n        \"\"\"\n        Parameters\n        ----------\n        kernel_size             : torch.tensor\n                                    Convolution filter kernel size, 11 by 11 average kernel by default.\n        step_size               : tuple\n                                    Convolution stride in height and width direction.\n        loss                    : torch.nn.Module\n                                    loss function, L2 Loss by default.\n        \"\"\"\n        super(speckle_contrast, self).__init__()\n        self.device = device\n        self.loss = loss\n        self.step_size = step_size\n        self.kernel_size = kernel_size\n        self.kernel = torch.ones((1, 1, self.kernel_size, self.kernel_size)) / (self.kernel_size ** 2)\n        self.kernel = Variable(self.kernel.type(torch.FloatTensor).to(self.device))\n\n\n    def forward(self, intensity):\n        \"\"\"\n        Calculates the speckle contrast Loss.\n\n        Parameters\n        ----------\n        intensity               : torch.tensor\n                                    intensity of the complex amplitude.\n\n        Returns\n        -------\n\n        loss_value              : torch.tensor\n                                    The computed loss.\n        \"\"\"\n\n        if len(intensity.shape) == 2:\n            intensity = intensity.reshape((1, 1, intensity.shape[0], intensity.shape[1]))\n        Speckle_C = self.functional_conv2d(intensity)\n        loss_value = self.loss(Speckle_C, torch.zeros_like(Speckle_C))\n        return loss_value\n\n\n    def functional_conv2d(self, intensity):\n        \"\"\"\n        Calculates the speckle contrast of the intensity.\n\n        Parameters\n        ----------\n        intensity                : torch.tensor\n                                    Intensity of the complex field.\n\n        Returns\n        -------\n\n        Speckle_C               : torch.tensor\n                                    The computed speckle contrast.\n        \"\"\"\n        mean = F.conv2d(intensity, self.kernel, stride = self.step_size)\n        var = torch.sqrt(F.conv2d(torch.pow(intensity, 2), self.kernel, stride = self.step_size) - torch.pow(mean, 2))\n        Speckle_C = var / mean\n        return Speckle_C\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.speckle_contrast.__init__","title":"<code>__init__(kernel_size=11, step_size=(1, 1), loss=nn.MSELoss(), device=torch.device('cpu'))</code>","text":"<p>Parameters:</p> <ul> <li> <code>kernel_size</code>           \u2013            <pre><code>                    Convolution filter kernel size, 11 by 11 average kernel by default.\n</code></pre> </li> <li> <code>step_size</code>           \u2013            <pre><code>                    Convolution stride in height and width direction.\n</code></pre> </li> <li> <code>loss</code>           \u2013            <pre><code>                    loss function, L2 Loss by default.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def __init__(self, kernel_size = 11, step_size = (1, 1), loss = nn.MSELoss(), device=torch.device(\"cpu\")):\n    \"\"\"\n    Parameters\n    ----------\n    kernel_size             : torch.tensor\n                                Convolution filter kernel size, 11 by 11 average kernel by default.\n    step_size               : tuple\n                                Convolution stride in height and width direction.\n    loss                    : torch.nn.Module\n                                loss function, L2 Loss by default.\n    \"\"\"\n    super(speckle_contrast, self).__init__()\n    self.device = device\n    self.loss = loss\n    self.step_size = step_size\n    self.kernel_size = kernel_size\n    self.kernel = torch.ones((1, 1, self.kernel_size, self.kernel_size)) / (self.kernel_size ** 2)\n    self.kernel = Variable(self.kernel.type(torch.FloatTensor).to(self.device))\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.speckle_contrast.forward","title":"<code>forward(intensity)</code>","text":"<p>Calculates the speckle contrast Loss.</p> <p>Parameters:</p> <ul> <li> <code>intensity</code>           \u2013            <pre><code>                    intensity of the complex amplitude.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>loss_value</code> (              <code>tensor</code> )          \u2013            <p>The computed loss.</p> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def forward(self, intensity):\n    \"\"\"\n    Calculates the speckle contrast Loss.\n\n    Parameters\n    ----------\n    intensity               : torch.tensor\n                                intensity of the complex amplitude.\n\n    Returns\n    -------\n\n    loss_value              : torch.tensor\n                                The computed loss.\n    \"\"\"\n\n    if len(intensity.shape) == 2:\n        intensity = intensity.reshape((1, 1, intensity.shape[0], intensity.shape[1]))\n    Speckle_C = self.functional_conv2d(intensity)\n    loss_value = self.loss(Speckle_C, torch.zeros_like(Speckle_C))\n    return loss_value\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.loss.speckle_contrast.functional_conv2d","title":"<code>functional_conv2d(intensity)</code>","text":"<p>Calculates the speckle contrast of the intensity.</p> <p>Parameters:</p> <ul> <li> <code>intensity</code>           \u2013            <pre><code>                    Intensity of the complex field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>Speckle_C</code> (              <code>tensor</code> )          \u2013            <p>The computed speckle contrast.</p> </li> </ul> Source code in <code>odak/learn/wave/loss.py</code> <pre><code>def functional_conv2d(self, intensity):\n    \"\"\"\n    Calculates the speckle contrast of the intensity.\n\n    Parameters\n    ----------\n    intensity                : torch.tensor\n                                Intensity of the complex field.\n\n    Returns\n    -------\n\n    Speckle_C               : torch.tensor\n                                The computed speckle contrast.\n    \"\"\"\n    mean = F.conv2d(intensity, self.kernel, stride = self.step_size)\n    var = torch.sqrt(F.conv2d(torch.pow(intensity, 2), self.kernel, stride = self.step_size) - torch.pow(mean, 2))\n    Speckle_C = var / mean\n    return Speckle_C\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.channel_gate","title":"<code>channel_gate</code>","text":"<p>               Bases: <code>Module</code></p> <p>Channel attention module with various pooling strategies. This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class channel_gate(torch.nn.Module):\n    \"\"\"\n    Channel attention module with various pooling strategies.\n    This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).\n    \"\"\"\n    def __init__(\n                 self, \n                 gate_channels, \n                 reduction_ratio = 16, \n                 pool_types = ['avg', 'max']\n                ):\n        \"\"\"\n        Initializes the channel gate module.\n\n        Parameters\n        ----------\n        gate_channels   : int\n                          Number of channels of the input feature map.\n        reduction_ratio : int\n                          Reduction ratio for the intermediate layer.\n        pool_types      : list\n                          List of pooling operations to apply.\n        \"\"\"\n        super().__init__()\n        self.gate_channels = gate_channels\n        hidden_channels = gate_channels // reduction_ratio\n        if hidden_channels == 0:\n            hidden_channels = 1\n        self.mlp = torch.nn.Sequential(\n                                       convolutional_block_attention.Flatten(),\n                                       torch.nn.Linear(gate_channels, hidden_channels),\n                                       torch.nn.ReLU(),\n                                       torch.nn.Linear(hidden_channels, gate_channels)\n                                      )\n        self.pool_types = pool_types\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the ChannelGate module.\n\n        Applies channel-wise attention to the input tensor.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the ChannelGate module.\n\n        Returns\n        -------\n        output       : torch.tensor\n                       Output tensor after applying channel attention.\n        \"\"\"\n        channel_att_sum = None\n        for pool_type in self.pool_types:\n            if pool_type == 'avg':\n                pool = torch.nn.functional.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n            elif pool_type == 'max':\n                pool = torch.nn.functional.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n            channel_att_raw = self.mlp(pool)\n            channel_att_sum = channel_att_raw if channel_att_sum is None else channel_att_sum + channel_att_raw\n        scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n        output = x * scale\n        return output\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.channel_gate.__init__","title":"<code>__init__(gate_channels, reduction_ratio=16, pool_types=['avg', 'max'])</code>","text":"<p>Initializes the channel gate module.</p> <p>Parameters:</p> <ul> <li> <code>gate_channels</code>           \u2013            <pre><code>          Number of channels of the input feature map.\n</code></pre> </li> <li> <code>reduction_ratio</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <pre><code>          Reduction ratio for the intermediate layer.\n</code></pre> </li> <li> <code>pool_types</code>           \u2013            <pre><code>          List of pooling operations to apply.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self, \n             gate_channels, \n             reduction_ratio = 16, \n             pool_types = ['avg', 'max']\n            ):\n    \"\"\"\n    Initializes the channel gate module.\n\n    Parameters\n    ----------\n    gate_channels   : int\n                      Number of channels of the input feature map.\n    reduction_ratio : int\n                      Reduction ratio for the intermediate layer.\n    pool_types      : list\n                      List of pooling operations to apply.\n    \"\"\"\n    super().__init__()\n    self.gate_channels = gate_channels\n    hidden_channels = gate_channels // reduction_ratio\n    if hidden_channels == 0:\n        hidden_channels = 1\n    self.mlp = torch.nn.Sequential(\n                                   convolutional_block_attention.Flatten(),\n                                   torch.nn.Linear(gate_channels, hidden_channels),\n                                   torch.nn.ReLU(),\n                                   torch.nn.Linear(hidden_channels, gate_channels)\n                                  )\n    self.pool_types = pool_types\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.channel_gate.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the ChannelGate module.</p> <p>Applies channel-wise attention to the input tensor.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the ChannelGate module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying channel attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the ChannelGate module.\n\n    Applies channel-wise attention to the input tensor.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the ChannelGate module.\n\n    Returns\n    -------\n    output       : torch.tensor\n                   Output tensor after applying channel attention.\n    \"\"\"\n    channel_att_sum = None\n    for pool_type in self.pool_types:\n        if pool_type == 'avg':\n            pool = torch.nn.functional.avg_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n        elif pool_type == 'max':\n            pool = torch.nn.functional.max_pool2d(x, (x.size(2), x.size(3)), stride=(x.size(2), x.size(3)))\n        channel_att_raw = self.mlp(pool)\n        channel_att_sum = channel_att_raw if channel_att_sum is None else channel_att_sum + channel_att_raw\n    scale = torch.sigmoid(channel_att_sum).unsqueeze(2).unsqueeze(3).expand_as(x)\n    output = x * scale\n    return output\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.convolution_layer","title":"<code>convolution_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A convolution layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class convolution_layer(torch.nn.Module):\n    \"\"\"\n    A convolution layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 bias = False,\n                 stride = 1,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A convolutional layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        layers = [\n            torch.nn.Conv2d(\n                            input_channels,\n                            output_channels,\n                            kernel_size = kernel_size,\n                            stride = stride,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           )\n        ]\n        if normalization:\n            layers.append(torch.nn.BatchNorm2d(output_channels))\n        if activation:\n            layers.append(activation)\n        self.model = torch.nn.Sequential(*layers)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        result = self.model(x)\n        return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.convolution_layer.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, bias=False, stride=1, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A convolutional layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             bias = False,\n             stride = 1,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A convolutional layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    layers = [\n        torch.nn.Conv2d(\n                        input_channels,\n                        output_channels,\n                        kernel_size = kernel_size,\n                        stride = stride,\n                        padding = kernel_size // 2,\n                        bias = bias\n                       )\n    ]\n    if normalization:\n        layers.append(torch.nn.BatchNorm2d(output_channels))\n    if activation:\n        layers.append(activation)\n    self.model = torch.nn.Sequential(*layers)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.convolution_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    result = self.model(x)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.convolutional_block_attention","title":"<code>convolutional_block_attention</code>","text":"<p>               Bases: <code>Module</code></p> <p>Convolutional Block Attention Module (CBAM) class.  This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class convolutional_block_attention(torch.nn.Module):\n    \"\"\"\n    Convolutional Block Attention Module (CBAM) class. \n    This class is heavily inspired https://github.com/Jongchan/attention-module/commit/e4ee180f1335c09db14d39a65d97c8ca3d1f7b16 (MIT License).\n    \"\"\"\n    def __init__(\n                 self, \n                 gate_channels, \n                 reduction_ratio = 16, \n                 pool_types = ['avg', 'max'], \n                 no_spatial = False\n                ):\n        \"\"\"\n        Initializes the convolutional block attention module.\n\n        Parameters\n        ----------\n        gate_channels   : int\n                          Number of channels of the input feature map.\n        reduction_ratio : int\n                          Reduction ratio for the channel attention.\n        pool_types      : list\n                          List of pooling operations to apply for channel attention.\n        no_spatial      : bool\n                          If True, spatial attention is not applied.\n        \"\"\"\n        super(convolutional_block_attention, self).__init__()\n        self.channel_gate = channel_gate(gate_channels, reduction_ratio, pool_types)\n        self.no_spatial = no_spatial\n        if not no_spatial:\n            self.spatial_gate = spatial_gate()\n\n\n    class Flatten(torch.nn.Module):\n        \"\"\"\n        Flattens the input tensor to a 2D matrix.\n        \"\"\"\n        def forward(self, x):\n            return x.view(x.size(0), -1)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the convolutional block attention module.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the CBAM module.\n\n        Returns\n        -------\n        x_out        : torch.tensor\n                       Output tensor after applying channel and spatial attention.\n        \"\"\"\n        x_out = self.channel_gate(x)\n        if not self.no_spatial:\n            x_out = self.spatial_gate(x_out)\n        return x_out\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.convolutional_block_attention.Flatten","title":"<code>Flatten</code>","text":"<p>               Bases: <code>Module</code></p> <p>Flattens the input tensor to a 2D matrix.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class Flatten(torch.nn.Module):\n    \"\"\"\n    Flattens the input tensor to a 2D matrix.\n    \"\"\"\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.convolutional_block_attention.__init__","title":"<code>__init__(gate_channels, reduction_ratio=16, pool_types=['avg', 'max'], no_spatial=False)</code>","text":"<p>Initializes the convolutional block attention module.</p> <p>Parameters:</p> <ul> <li> <code>gate_channels</code>           \u2013            <pre><code>          Number of channels of the input feature map.\n</code></pre> </li> <li> <code>reduction_ratio</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <pre><code>          Reduction ratio for the channel attention.\n</code></pre> </li> <li> <code>pool_types</code>           \u2013            <pre><code>          List of pooling operations to apply for channel attention.\n</code></pre> </li> <li> <code>no_spatial</code>           \u2013            <pre><code>          If True, spatial attention is not applied.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self, \n             gate_channels, \n             reduction_ratio = 16, \n             pool_types = ['avg', 'max'], \n             no_spatial = False\n            ):\n    \"\"\"\n    Initializes the convolutional block attention module.\n\n    Parameters\n    ----------\n    gate_channels   : int\n                      Number of channels of the input feature map.\n    reduction_ratio : int\n                      Reduction ratio for the channel attention.\n    pool_types      : list\n                      List of pooling operations to apply for channel attention.\n    no_spatial      : bool\n                      If True, spatial attention is not applied.\n    \"\"\"\n    super(convolutional_block_attention, self).__init__()\n    self.channel_gate = channel_gate(gate_channels, reduction_ratio, pool_types)\n    self.no_spatial = no_spatial\n    if not no_spatial:\n        self.spatial_gate = spatial_gate()\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.convolutional_block_attention.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the convolutional block attention module.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the CBAM module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>x_out</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying channel and spatial attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the convolutional block attention module.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the CBAM module.\n\n    Returns\n    -------\n    x_out        : torch.tensor\n                   Output tensor after applying channel and spatial attention.\n    \"\"\"\n    x_out = self.channel_gate(x)\n    if not self.no_spatial:\n        x_out = self.spatial_gate(x_out)\n    return x_out\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.double_convolution","title":"<code>double_convolution</code>","text":"<p>               Bases: <code>Module</code></p> <p>A double convolution layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class double_convolution(torch.nn.Module):\n    \"\"\"\n    A double convolution layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 mid_channels = None,\n                 output_channels = 2,\n                 kernel_size = 3, \n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        Double convolution model.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels    : int\n                          Number of channels in the hidden layer between two convolutions.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        if isinstance(mid_channels, type(None)):\n            mid_channels = output_channels\n        self.activation = activation\n        self.model = torch.nn.Sequential(\n                                         convolution_layer(\n                                                           input_channels = input_channels,\n                                                           output_channels = mid_channels,\n                                                           kernel_size = kernel_size,\n                                                           bias = bias,\n                                                           normalization = normalization,\n                                                           activation = self.activation\n                                                          ),\n                                         convolution_layer(\n                                                           input_channels = mid_channels,\n                                                           output_channels = output_channels,\n                                                           kernel_size = kernel_size,\n                                                           bias = bias,\n                                                           normalization = normalization,\n                                                           activation = self.activation\n                                                          )\n                                        )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        result = self.model(x)\n        return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.double_convolution.__init__","title":"<code>__init__(input_channels=2, mid_channels=None, output_channels=2, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>Double convolution model.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of channels in the hidden layer between two convolutions.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             mid_channels = None,\n             output_channels = 2,\n             kernel_size = 3, \n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    Double convolution model.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels    : int\n                      Number of channels in the hidden layer between two convolutions.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    if isinstance(mid_channels, type(None)):\n        mid_channels = output_channels\n    self.activation = activation\n    self.model = torch.nn.Sequential(\n                                     convolution_layer(\n                                                       input_channels = input_channels,\n                                                       output_channels = mid_channels,\n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       normalization = normalization,\n                                                       activation = self.activation\n                                                      ),\n                                     convolution_layer(\n                                                       input_channels = mid_channels,\n                                                       output_channels = output_channels,\n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       normalization = normalization,\n                                                       activation = self.activation\n                                                      )\n                                    )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.double_convolution.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    result = self.model(x)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.downsample_layer","title":"<code>downsample_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A downscaling component followed by a double convolution.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class downsample_layer(torch.nn.Module):\n    \"\"\"\n    A downscaling component followed by a double convolution.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.maxpool_conv = torch.nn.Sequential(\n                                                torch.nn.MaxPool2d(2),\n                                                double_convolution(\n                                                                   input_channels = input_channels,\n                                                                   mid_channels = output_channels,\n                                                                   output_channels = output_channels,\n                                                                   kernel_size = kernel_size,\n                                                                   bias = bias,\n                                                                   normalization = normalization,\n                                                                   activation = activation\n                                                                  )\n                                               )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x              : torch.tensor\n                         First input data.\n\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        result = self.maxpool_conv(x)\n        return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.downsample_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 3,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.maxpool_conv = torch.nn.Sequential(\n                                            torch.nn.MaxPool2d(2),\n                                            double_convolution(\n                                                               input_channels = input_channels,\n                                                               mid_channels = output_channels,\n                                                               output_channels = output_channels,\n                                                               kernel_size = kernel_size,\n                                                               bias = bias,\n                                                               normalization = normalization,\n                                                               activation = activation\n                                                              )\n                                           )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.downsample_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x              : torch.tensor\n                     First input data.\n\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    result = self.maxpool_conv(x)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.focal_surface_light_propagation","title":"<code>focal_surface_light_propagation</code>","text":"<p>               Bases: <code>Module</code></p> <p>focal_surface_light_propagation model.</p> References <p>Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit}. \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions.\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24),December,2024.</p> Source code in <code>odak/learn/wave/models.py</code> <pre><code>class focal_surface_light_propagation(torch.nn.Module):\n    \"\"\"\n    focal_surface_light_propagation model.\n\n    References\n    ----------\n\n    Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit}. \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions.\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24),December,2024.\n    \"\"\"\n    def __init__(\n                 self,\n                 depth = 3,\n                 dimensions = 8,\n                 input_channels = 6,\n                 out_channels = 6,\n                 kernel_size = 3,\n                 bias = True,\n                 device = torch.device('cpu'),\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        Initializes the focal surface light propagation model.\n\n        Parameters\n        ----------\n        depth             : int\n                            Number of downsampling and upsampling layers.\n        dimensions        : int\n                            Number of dimensions/features in the model.\n        input_channels    : int\n                            Number of input channels.\n        out_channels      : int\n                            Number of output channels.\n        kernel_size       : int\n                            Size of the convolution kernel.\n        bias              : bool\n                            If True, allows convolutional layers to learn a bias term.\n        device            : torch.device\n                            Default device is CPU.\n        activation        : torch.nn.Module\n                            Activation function (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n        \"\"\"\n        super().__init__()\n        self.depth = depth\n        self.device = device\n        self.sv_kernel_generation = spatially_varying_kernel_generation_model(\n            depth = depth,\n            dimensions = dimensions,\n            input_channels = input_channels + 1,  # +1 to account for an extra channel\n            kernel_size = kernel_size,\n            bias = bias,\n            activation = activation\n        )\n        self.light_propagation = spatially_adaptive_unet(\n            depth = depth,\n            dimensions = dimensions,\n            input_channels = input_channels,\n            out_channels = out_channels,\n            kernel_size = kernel_size,\n            bias = bias,\n            activation = activation\n        )\n\n\n    def forward(self, focal_surface, phase_only_hologram):\n        \"\"\"\n        Forward pass through the model.\n\n        Parameters\n        ----------\n        focal_surface         : torch.Tensor\n                                Input focal surface.\n        phase_only_hologram   : torch.Tensor\n                                Input phase-only hologram.\n\n        Returns\n        ----------\n        result                : torch.Tensor\n                                Output tensor after light propagation.\n        \"\"\"\n        input_field = self.generate_input_field(phase_only_hologram)\n        sv_kernel = self.sv_kernel_generation(focal_surface, input_field)\n        output_field = self.light_propagation(sv_kernel, input_field)\n        final = (output_field[:, 0:3, :, :] + 1j * output_field[:, 3:6, :, :])\n        result = calculate_amplitude(final) ** 2\n        return result\n\n\n    def generate_input_field(self, phase_only_hologram):\n        \"\"\"\n        Generates an input field by combining the real and imaginary parts.\n\n        Parameters\n        ----------\n        phase_only_hologram   : torch.Tensor\n                                Input phase-only hologram.\n\n        Returns\n        ----------\n        input_field           : torch.Tensor\n                                Concatenated real and imaginary parts of the complex field.\n        \"\"\"\n        [b, c, h, w] = phase_only_hologram.size()\n        input_phase = phase_only_hologram * 2 * np.pi\n        hologram_amplitude = torch.ones(b, c, h, w, requires_grad = False).to(self.device)\n        field = generate_complex_field(hologram_amplitude, input_phase)\n        input_field = torch.cat((field.real, field.imag), dim = 1)\n        return input_field\n\n\n    def load_weights(self, weight_filename, key_mapping_filename):\n        \"\"\"\n        Function to load weights for this multi-layer perceptron from a file.\n\n        Parameters\n        ----------\n        weight_filename      : str\n                               Path to the old model's weight file.\n        key_mapping_filename : str\n                               Path to the JSON file containing the key mappings.\n        \"\"\"\n        # Load old model weights\n        old_model_weights = torch.load(weight_filename, map_location = self.device,weights_only=True)\n\n        # Load key mappings from JSON file\n        with open(key_mapping_filename, 'r') as json_file:\n            key_mappings = json.load(json_file)\n\n        # Extract the key mappings for sv_kernel_generation and light_prop\n        sv_kernel_generation_key_mapping = key_mappings['sv_kernel_generation_key_mapping']\n        light_prop_key_mapping = key_mappings['light_prop_key_mapping']\n\n        # Initialize new state dicts\n        sv_kernel_generation_new_state_dict = {}\n        light_prop_new_state_dict = {}\n\n        # Map and load sv_kernel_generation_model weights\n        for old_key, value in old_model_weights.items():\n            if old_key in sv_kernel_generation_key_mapping:\n                # Map the old key to the new key\n                new_key = sv_kernel_generation_key_mapping[old_key]\n                sv_kernel_generation_new_state_dict[new_key] = value\n\n        self.sv_kernel_generation.to(self.device)\n        self.sv_kernel_generation.load_state_dict(sv_kernel_generation_new_state_dict)\n\n        # Map and load light_prop model weights\n        for old_key, value in old_model_weights.items():\n            if old_key in light_prop_key_mapping:\n                # Map the old key to the new key\n                new_key = light_prop_key_mapping[old_key]\n                light_prop_new_state_dict[new_key] = value\n        self.light_propagation.to(self.device)\n        self.light_propagation.load_state_dict(light_prop_new_state_dict)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.focal_surface_light_propagation.__init__","title":"<code>__init__(depth=3, dimensions=8, input_channels=6, out_channels=6, kernel_size=3, bias=True, device=torch.device('cpu'), activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>Initializes the focal surface light propagation model.</p> <p>Parameters:</p> <ul> <li> <code>depth</code>           \u2013            <pre><code>            Number of downsampling and upsampling layers.\n</code></pre> </li> <li> <code>dimensions</code>           \u2013            <pre><code>            Number of dimensions/features in the model.\n</code></pre> </li> <li> <code>input_channels</code>           \u2013            <pre><code>            Number of input channels.\n</code></pre> </li> <li> <code>out_channels</code>           \u2013            <pre><code>            Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>            Size of the convolution kernel.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>            If True, allows convolutional layers to learn a bias term.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>            Default device is CPU.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>            Activation function (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/models.py</code> <pre><code>def __init__(\n             self,\n             depth = 3,\n             dimensions = 8,\n             input_channels = 6,\n             out_channels = 6,\n             kernel_size = 3,\n             bias = True,\n             device = torch.device('cpu'),\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    Initializes the focal surface light propagation model.\n\n    Parameters\n    ----------\n    depth             : int\n                        Number of downsampling and upsampling layers.\n    dimensions        : int\n                        Number of dimensions/features in the model.\n    input_channels    : int\n                        Number of input channels.\n    out_channels      : int\n                        Number of output channels.\n    kernel_size       : int\n                        Size of the convolution kernel.\n    bias              : bool\n                        If True, allows convolutional layers to learn a bias term.\n    device            : torch.device\n                        Default device is CPU.\n    activation        : torch.nn.Module\n                        Activation function (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n    \"\"\"\n    super().__init__()\n    self.depth = depth\n    self.device = device\n    self.sv_kernel_generation = spatially_varying_kernel_generation_model(\n        depth = depth,\n        dimensions = dimensions,\n        input_channels = input_channels + 1,  # +1 to account for an extra channel\n        kernel_size = kernel_size,\n        bias = bias,\n        activation = activation\n    )\n    self.light_propagation = spatially_adaptive_unet(\n        depth = depth,\n        dimensions = dimensions,\n        input_channels = input_channels,\n        out_channels = out_channels,\n        kernel_size = kernel_size,\n        bias = bias,\n        activation = activation\n    )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.focal_surface_light_propagation.forward","title":"<code>forward(focal_surface, phase_only_hologram)</code>","text":"<p>Forward pass through the model.</p> <p>Parameters:</p> <ul> <li> <code>focal_surface</code>           \u2013            <pre><code>                Input focal surface.\n</code></pre> </li> <li> <code>phase_only_hologram</code>           \u2013            <pre><code>                Input phase-only hologram.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>Tensor</code> )          \u2013            <p>Output tensor after light propagation.</p> </li> </ul> Source code in <code>odak/learn/wave/models.py</code> <pre><code>def forward(self, focal_surface, phase_only_hologram):\n    \"\"\"\n    Forward pass through the model.\n\n    Parameters\n    ----------\n    focal_surface         : torch.Tensor\n                            Input focal surface.\n    phase_only_hologram   : torch.Tensor\n                            Input phase-only hologram.\n\n    Returns\n    ----------\n    result                : torch.Tensor\n                            Output tensor after light propagation.\n    \"\"\"\n    input_field = self.generate_input_field(phase_only_hologram)\n    sv_kernel = self.sv_kernel_generation(focal_surface, input_field)\n    output_field = self.light_propagation(sv_kernel, input_field)\n    final = (output_field[:, 0:3, :, :] + 1j * output_field[:, 3:6, :, :])\n    result = calculate_amplitude(final) ** 2\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.focal_surface_light_propagation.generate_input_field","title":"<code>generate_input_field(phase_only_hologram)</code>","text":"<p>Generates an input field by combining the real and imaginary parts.</p> <p>Parameters:</p> <ul> <li> <code>phase_only_hologram</code>           \u2013            <pre><code>                Input phase-only hologram.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>input_field</code> (              <code>Tensor</code> )          \u2013            <p>Concatenated real and imaginary parts of the complex field.</p> </li> </ul> Source code in <code>odak/learn/wave/models.py</code> <pre><code>def generate_input_field(self, phase_only_hologram):\n    \"\"\"\n    Generates an input field by combining the real and imaginary parts.\n\n    Parameters\n    ----------\n    phase_only_hologram   : torch.Tensor\n                            Input phase-only hologram.\n\n    Returns\n    ----------\n    input_field           : torch.Tensor\n                            Concatenated real and imaginary parts of the complex field.\n    \"\"\"\n    [b, c, h, w] = phase_only_hologram.size()\n    input_phase = phase_only_hologram * 2 * np.pi\n    hologram_amplitude = torch.ones(b, c, h, w, requires_grad = False).to(self.device)\n    field = generate_complex_field(hologram_amplitude, input_phase)\n    input_field = torch.cat((field.real, field.imag), dim = 1)\n    return input_field\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.focal_surface_light_propagation.load_weights","title":"<code>load_weights(weight_filename, key_mapping_filename)</code>","text":"<p>Function to load weights for this multi-layer perceptron from a file.</p> <p>Parameters:</p> <ul> <li> <code>weight_filename</code>           \u2013            <pre><code>               Path to the old model's weight file.\n</code></pre> </li> <li> <code>key_mapping_filename</code>               (<code>str</code>)           \u2013            <pre><code>               Path to the JSON file containing the key mappings.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/models.py</code> <pre><code>def load_weights(self, weight_filename, key_mapping_filename):\n    \"\"\"\n    Function to load weights for this multi-layer perceptron from a file.\n\n    Parameters\n    ----------\n    weight_filename      : str\n                           Path to the old model's weight file.\n    key_mapping_filename : str\n                           Path to the JSON file containing the key mappings.\n    \"\"\"\n    # Load old model weights\n    old_model_weights = torch.load(weight_filename, map_location = self.device,weights_only=True)\n\n    # Load key mappings from JSON file\n    with open(key_mapping_filename, 'r') as json_file:\n        key_mappings = json.load(json_file)\n\n    # Extract the key mappings for sv_kernel_generation and light_prop\n    sv_kernel_generation_key_mapping = key_mappings['sv_kernel_generation_key_mapping']\n    light_prop_key_mapping = key_mappings['light_prop_key_mapping']\n\n    # Initialize new state dicts\n    sv_kernel_generation_new_state_dict = {}\n    light_prop_new_state_dict = {}\n\n    # Map and load sv_kernel_generation_model weights\n    for old_key, value in old_model_weights.items():\n        if old_key in sv_kernel_generation_key_mapping:\n            # Map the old key to the new key\n            new_key = sv_kernel_generation_key_mapping[old_key]\n            sv_kernel_generation_new_state_dict[new_key] = value\n\n    self.sv_kernel_generation.to(self.device)\n    self.sv_kernel_generation.load_state_dict(sv_kernel_generation_new_state_dict)\n\n    # Map and load light_prop model weights\n    for old_key, value in old_model_weights.items():\n        if old_key in light_prop_key_mapping:\n            # Map the old key to the new key\n            new_key = light_prop_key_mapping[old_key]\n            light_prop_new_state_dict[new_key] = value\n    self.light_propagation.to(self.device)\n    self.light_propagation.load_state_dict(light_prop_new_state_dict)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.global_feature_module","title":"<code>global_feature_module</code>","text":"<p>               Bases: <code>Module</code></p> <p>A global feature layer that processes global features from input channels and applies them to another input tensor via learned transformations.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class global_feature_module(torch.nn.Module):\n    \"\"\"\n    A global feature layer that processes global features from input channels and\n    applies them to another input tensor via learned transformations.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 mid_channels,\n                 output_channels,\n                 kernel_size,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A global feature layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels  : int\n                          Number of mid channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool\n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.transformations_1 = global_transformations(input_channels, output_channels)\n        self.global_features_1 = double_convolution(\n                                                    input_channels = input_channels,\n                                                    mid_channels = mid_channels,\n                                                    output_channels = output_channels,\n                                                    kernel_size = kernel_size,\n                                                    bias = bias,\n                                                    normalization = normalization,\n                                                    activation = activation\n                                                   )\n        self.global_features_2 = double_convolution(\n                                                    input_channels = input_channels,\n                                                    mid_channels = mid_channels,\n                                                    output_channels = output_channels,\n                                                    kernel_size = kernel_size,\n                                                    bias = bias,\n                                                    normalization = normalization,\n                                                    activation = activation\n                                                   )\n        self.transformations_2 = global_transformations(input_channels, output_channels)\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        global_tensor_1 = self.transformations_1(x1, x2)\n        y1 = self.global_features_1(global_tensor_1)\n        y2 = self.global_features_2(y1)\n        global_tensor_2 = self.transformations_2(y1, y2)\n        return global_tensor_2\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.global_feature_module.__init__","title":"<code>__init__(input_channels, mid_channels, output_channels, kernel_size, bias=False, normalization=False, activation=torch.nn.ReLU())</code>","text":"<p>A global feature layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of mid channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             mid_channels,\n             output_channels,\n             kernel_size,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A global feature layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels  : int\n                      Number of mid channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool\n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.transformations_1 = global_transformations(input_channels, output_channels)\n    self.global_features_1 = double_convolution(\n                                                input_channels = input_channels,\n                                                mid_channels = mid_channels,\n                                                output_channels = output_channels,\n                                                kernel_size = kernel_size,\n                                                bias = bias,\n                                                normalization = normalization,\n                                                activation = activation\n                                               )\n    self.global_features_2 = double_convolution(\n                                                input_channels = input_channels,\n                                                mid_channels = mid_channels,\n                                                output_channels = output_channels,\n                                                kernel_size = kernel_size,\n                                                bias = bias,\n                                                normalization = normalization,\n                                                activation = activation\n                                               )\n    self.transformations_2 = global_transformations(input_channels, output_channels)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.global_feature_module.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    global_tensor_1 = self.transformations_1(x1, x2)\n    y1 = self.global_features_1(global_tensor_1)\n    y2 = self.global_features_2(y1)\n    global_tensor_2 = self.transformations_2(y1, y2)\n    return global_tensor_2\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.global_transformations","title":"<code>global_transformations</code>","text":"<p>               Bases: <code>Module</code></p> <p>A global feature layer that processes global features from input channels and applies learned transformations to another input tensor.</p> <p>This implementation is adapted from RSGUnet: https://github.com/MTLab/rsgunet_image_enhance.</p> <p>Reference: J. Huang, P. Zhu, M. Geng et al. \"Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.\"</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class global_transformations(torch.nn.Module):\n    \"\"\"\n    A global feature layer that processes global features from input channels and\n    applies learned transformations to another input tensor.\n\n    This implementation is adapted from RSGUnet:\n    https://github.com/MTLab/rsgunet_image_enhance.\n\n    Reference:\n    J. Huang, P. Zhu, M. Geng et al. \"Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.\"\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels\n                ):\n        \"\"\"\n        A global feature layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        \"\"\"\n        super().__init__()\n        self.global_feature_1 = torch.nn.Sequential(\n            torch.nn.Linear(input_channels, output_channels),\n            torch.nn.LeakyReLU(0.2, inplace = True),\n        )\n        self.global_feature_2 = torch.nn.Sequential(\n            torch.nn.Linear(output_channels, output_channels),\n            torch.nn.LeakyReLU(0.2, inplace = True)\n        )\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.\n        \"\"\"\n        y = torch.mean(x2, dim = (2, 3))\n        y1 = self.global_feature_1(y)\n        y2 = self.global_feature_2(y1)\n        y1 = y1.unsqueeze(2).unsqueeze(3)\n        y2 = y2.unsqueeze(2).unsqueeze(3)\n        result = x1 * y1 + y2\n        return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.global_transformations.__init__","title":"<code>__init__(input_channels, output_channels)</code>","text":"<p>A global feature layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels\n            ):\n    \"\"\"\n    A global feature layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    \"\"\"\n    super().__init__()\n    self.global_feature_1 = torch.nn.Sequential(\n        torch.nn.Linear(input_channels, output_channels),\n        torch.nn.LeakyReLU(0.2, inplace = True),\n    )\n    self.global_feature_2 = torch.nn.Sequential(\n        torch.nn.Linear(output_channels, output_channels),\n        torch.nn.LeakyReLU(0.2, inplace = True)\n    )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.global_transformations.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.\n    \"\"\"\n    y = torch.mean(x2, dim = (2, 3))\n    y1 = self.global_feature_1(y)\n    y2 = self.global_feature_2(y1)\n    y1 = y1.unsqueeze(2).unsqueeze(3)\n    y2 = y2.unsqueeze(2).unsqueeze(3)\n    result = x1 * y1 + y2\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.holobeam_multiholo","title":"<code>holobeam_multiholo</code>","text":"<p>               Bases: <code>Module</code></p> <p>The learned holography model used in the paper, Ak\u015fit, Kaan, and Yuta Itoh. \"HoloBeam: Paper-Thin Near-Eye Displays.\" In 2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR), pp. 581-591. IEEE, 2023.</p> <p>Parameters:</p> <ul> <li> <code>n_input</code>           \u2013            <pre><code>            Number of channels in the input.\n</code></pre> </li> <li> <code>n_hidden</code>           \u2013            <pre><code>            Number of channels in the hidden layers.\n</code></pre> </li> <li> <code>n_output</code>           \u2013            <pre><code>            Number of channels in the output layer.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>            Default device is CPU.\n</code></pre> </li> <li> <code>reduction</code>           \u2013            <pre><code>            Reduction used for torch.nn.MSELoss and torch.nn.L1Loss. The default is 'sum'.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/models.py</code> <pre><code>class holobeam_multiholo(torch.nn.Module):\n    \"\"\"\n    The learned holography model used in the paper, Ak\u015fit, Kaan, and Yuta Itoh. \"HoloBeam: Paper-Thin Near-Eye Displays.\" In 2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR), pp. 581-591. IEEE, 2023.\n\n\n    Parameters\n    ----------\n    n_input           : int\n                        Number of channels in the input.\n    n_hidden          : int\n                        Number of channels in the hidden layers.\n    n_output          : int\n                        Number of channels in the output layer.\n    device            : torch.device\n                        Default device is CPU.\n    reduction         : str\n                        Reduction used for torch.nn.MSELoss and torch.nn.L1Loss. The default is 'sum'.\n    \"\"\"\n    def __init__(\n                 self,\n                 n_input = 1,\n                 n_hidden = 16,\n                 n_output = 2,\n                 device = torch.device('cpu'),\n                 reduction = 'sum'\n                ):\n        super(holobeam_multiholo, self).__init__()\n        torch.random.seed()\n        self.device = device\n        self.reduction = reduction\n        self.l2 = torch.nn.MSELoss(reduction = self.reduction)\n        self.l1 = torch.nn.L1Loss(reduction = self.reduction)\n        self.n_input = n_input\n        self.n_hidden = n_hidden\n        self.n_output = n_output\n        self.network = unet(\n                            dimensions = self.n_hidden,\n                            input_channels = self.n_input,\n                            output_channels = self.n_output\n                           ).to(self.device)\n\n\n    def forward(self, x, test = False):\n        \"\"\"\n        Internal function representing the forward model.\n        \"\"\"\n        if test:\n            torch.no_grad()\n        y = self.network.forward(x) \n        phase_low = y[:, 0].unsqueeze(1)\n        phase_high = y[:, 1].unsqueeze(1)\n        phase_only = torch.zeros_like(phase_low)\n        phase_only[:, :, 0::2, 0::2] = phase_low[:, :,  0::2, 0::2]\n        phase_only[:, :, 1::2, 1::2] = phase_low[:, :, 1::2, 1::2]\n        phase_only[:, :, 0::2, 1::2] = phase_high[:, :, 0::2, 1::2]\n        phase_only[:, :, 1::2, 0::2] = phase_high[:, :, 1::2, 0::2]\n        return phase_only\n\n\n    def evaluate(self, input_data, ground_truth, weights = [1., 0.1]):\n        \"\"\"\n        Internal function for evaluating.\n        \"\"\"\n        loss = weights[0] * self.l2(input_data, ground_truth) + weights[1] * self.l1(input_data, ground_truth)\n        return loss\n\n\n    def fit(self, dataloader, number_of_epochs = 100, learning_rate = 1e-5, directory = './output', save_at_every = 100):\n        \"\"\"\n        Function to train the weights of the multi layer perceptron.\n\n        Parameters\n        ----------\n        dataloader       : torch.utils.data.DataLoader\n                           Data loader.\n        number_of_epochs : int\n                           Number of epochs.\n        learning_rate    : float\n                           Learning rate of the optimizer.\n        directory        : str\n                           Output directory.\n        save_at_every    : int\n                           Save the model at every given epoch count.\n        \"\"\"\n        t_epoch = tqdm(range(number_of_epochs), leave=False, dynamic_ncols = True)\n        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n        for i in t_epoch:\n            epoch_loss = 0.\n            t_data = tqdm(dataloader, leave=False, dynamic_ncols = True)\n            for j, data in enumerate(t_data):\n                self.optimizer.zero_grad()\n                images, holograms = data\n                estimates = self.forward(images)\n                loss = self.evaluate(estimates, holograms)\n                loss.backward(retain_graph=True)\n                self.optimizer.step()\n                description = 'Loss:{:.4f}'.format(loss.item())\n                t_data.set_description(description)\n                epoch_loss += float(loss.item()) / dataloader.__len__()\n            description = 'Epoch Loss:{:.4f}'.format(epoch_loss)\n            t_epoch.set_description(description)\n            if i % save_at_every == 0:\n                self.save_weights(filename='{}/weights_{:04d}.pt'.format(directory, i))\n        self.save_weights(filename='{}/weights.pt'.format(directory))\n        print(description)\n\n\n    def save_weights(self, filename = './weights.pt'):\n        \"\"\"\n        Function to save the current weights of the multi layer perceptron to a file.\n        Parameters\n        ----------\n        filename        : str\n                          Filename.\n        \"\"\"\n        torch.save(self.network.state_dict(), os.path.expanduser(filename))\n\n\n    def load_weights(self, filename = './weights.pt'):\n        \"\"\"\n        Function to load weights for this multi layer perceptron from a file.\n        Parameters\n        ----------\n        filename        : str\n                          Filename.\n        \"\"\"\n        self.network.load_state_dict(torch.load(os.path.expanduser(filename)))\n        self.network.eval()\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.holobeam_multiholo.evaluate","title":"<code>evaluate(input_data, ground_truth, weights=[1.0, 0.1])</code>","text":"<p>Internal function for evaluating.</p> Source code in <code>odak/learn/wave/models.py</code> <pre><code>def evaluate(self, input_data, ground_truth, weights = [1., 0.1]):\n    \"\"\"\n    Internal function for evaluating.\n    \"\"\"\n    loss = weights[0] * self.l2(input_data, ground_truth) + weights[1] * self.l1(input_data, ground_truth)\n    return loss\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.holobeam_multiholo.fit","title":"<code>fit(dataloader, number_of_epochs=100, learning_rate=1e-05, directory='./output', save_at_every=100)</code>","text":"<p>Function to train the weights of the multi layer perceptron.</p> <p>Parameters:</p> <ul> <li> <code>dataloader</code>           \u2013            <pre><code>           Data loader.\n</code></pre> </li> <li> <code>number_of_epochs</code>               (<code>int</code>, default:                   <code>100</code> )           \u2013            <pre><code>           Number of epochs.\n</code></pre> </li> <li> <code>learning_rate</code>           \u2013            <pre><code>           Learning rate of the optimizer.\n</code></pre> </li> <li> <code>directory</code>           \u2013            <pre><code>           Output directory.\n</code></pre> </li> <li> <code>save_at_every</code>           \u2013            <pre><code>           Save the model at every given epoch count.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/models.py</code> <pre><code>def fit(self, dataloader, number_of_epochs = 100, learning_rate = 1e-5, directory = './output', save_at_every = 100):\n    \"\"\"\n    Function to train the weights of the multi layer perceptron.\n\n    Parameters\n    ----------\n    dataloader       : torch.utils.data.DataLoader\n                       Data loader.\n    number_of_epochs : int\n                       Number of epochs.\n    learning_rate    : float\n                       Learning rate of the optimizer.\n    directory        : str\n                       Output directory.\n    save_at_every    : int\n                       Save the model at every given epoch count.\n    \"\"\"\n    t_epoch = tqdm(range(number_of_epochs), leave=False, dynamic_ncols = True)\n    self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n    for i in t_epoch:\n        epoch_loss = 0.\n        t_data = tqdm(dataloader, leave=False, dynamic_ncols = True)\n        for j, data in enumerate(t_data):\n            self.optimizer.zero_grad()\n            images, holograms = data\n            estimates = self.forward(images)\n            loss = self.evaluate(estimates, holograms)\n            loss.backward(retain_graph=True)\n            self.optimizer.step()\n            description = 'Loss:{:.4f}'.format(loss.item())\n            t_data.set_description(description)\n            epoch_loss += float(loss.item()) / dataloader.__len__()\n        description = 'Epoch Loss:{:.4f}'.format(epoch_loss)\n        t_epoch.set_description(description)\n        if i % save_at_every == 0:\n            self.save_weights(filename='{}/weights_{:04d}.pt'.format(directory, i))\n    self.save_weights(filename='{}/weights.pt'.format(directory))\n    print(description)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.holobeam_multiholo.forward","title":"<code>forward(x, test=False)</code>","text":"<p>Internal function representing the forward model.</p> Source code in <code>odak/learn/wave/models.py</code> <pre><code>def forward(self, x, test = False):\n    \"\"\"\n    Internal function representing the forward model.\n    \"\"\"\n    if test:\n        torch.no_grad()\n    y = self.network.forward(x) \n    phase_low = y[:, 0].unsqueeze(1)\n    phase_high = y[:, 1].unsqueeze(1)\n    phase_only = torch.zeros_like(phase_low)\n    phase_only[:, :, 0::2, 0::2] = phase_low[:, :,  0::2, 0::2]\n    phase_only[:, :, 1::2, 1::2] = phase_low[:, :, 1::2, 1::2]\n    phase_only[:, :, 0::2, 1::2] = phase_high[:, :, 0::2, 1::2]\n    phase_only[:, :, 1::2, 0::2] = phase_high[:, :, 1::2, 0::2]\n    return phase_only\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.holobeam_multiholo.load_weights","title":"<code>load_weights(filename='./weights.pt')</code>","text":"<p>Function to load weights for this multi layer perceptron from a file.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>          Filename.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/models.py</code> <pre><code>def load_weights(self, filename = './weights.pt'):\n    \"\"\"\n    Function to load weights for this multi layer perceptron from a file.\n    Parameters\n    ----------\n    filename        : str\n                      Filename.\n    \"\"\"\n    self.network.load_state_dict(torch.load(os.path.expanduser(filename)))\n    self.network.eval()\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.holobeam_multiholo.save_weights","title":"<code>save_weights(filename='./weights.pt')</code>","text":"<p>Function to save the current weights of the multi layer perceptron to a file.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>          Filename.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/models.py</code> <pre><code>def save_weights(self, filename = './weights.pt'):\n    \"\"\"\n    Function to save the current weights of the multi layer perceptron to a file.\n    Parameters\n    ----------\n    filename        : str\n                      Filename.\n    \"\"\"\n    torch.save(self.network.state_dict(), os.path.expanduser(filename))\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.multi_layer_perceptron","title":"<code>multi_layer_perceptron</code>","text":"<p>               Bases: <code>Module</code></p> <p>A multi-layer perceptron model.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class multi_layer_perceptron(torch.nn.Module):\n    \"\"\"\n    A multi-layer perceptron model.\n    \"\"\"\n\n    def __init__(self,\n                 dimensions,\n                 activation = torch.nn.ReLU(),\n                 bias = False,\n                 model_type = 'conventional',\n                 siren_multiplier = 1.,\n                 input_multiplier = None\n                ):\n        \"\"\"\n        Parameters\n        ----------\n        dimensions        : list\n                            List of integers representing the dimensions of each layer (e.g., [2, 10, 1], where the first layer has two channels and last one has one channel.).\n        activation        : torch.nn\n                            Nonlinear activation function.\n                            Default is `torch.nn.ReLU()`.\n        bias              : bool\n                            If set to True, linear layers will include biases.\n        siren_multiplier  : float\n                            When using `SIREN` model type, this parameter functions as a hyperparameter.\n                            The original SIREN work uses 30.\n                            You can bypass this parameter by providing input that are not normalized and larger then one.\n        input_multiplier  : float\n                            Initial value of the input multiplier before the very first layer.\n        model_type        : str\n                            Model type: `conventional`, `swish`, `SIREN`, `FILM SIREN`, `Gaussian`.\n                            `conventional` refers to a standard multi layer perceptron.\n                            For `SIREN,` see: Sitzmann, Vincent, et al. \"Implicit neural representations with periodic activation functions.\" Advances in neural information processing systems 33 (2020): 7462-7473.\n                            For `Swish,` see: Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for activation functions.\" arXiv preprint arXiv:1710.05941 (2017). \n                            For `FILM SIREN,` see: Chan, Eric R., et al. \"pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n                            For `Gaussian,` see: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n        \"\"\"\n        super(multi_layer_perceptron, self).__init__()\n        self.activation = activation\n        self.bias = bias\n        self.model_type = model_type\n        self.layers = torch.nn.ModuleList()\n        self.siren_multiplier = siren_multiplier\n        self.dimensions = dimensions\n        for i in range(len(self.dimensions) - 1):\n            self.layers.append(torch.nn.Linear(self.dimensions[i], self.dimensions[i + 1], bias = self.bias))\n        if not isinstance(input_multiplier, type(None)):\n            self.input_multiplier = torch.nn.ParameterList()\n            self.input_multiplier.append(torch.nn.Parameter(torch.ones(1, self.dimensions[0]) * input_multiplier))\n        if self.model_type == 'FILM SIREN':\n            self.alpha = torch.nn.ParameterList()\n            for j in self.dimensions[1::]:\n                self.alpha.append(torch.nn.Parameter(torch.randn(2, 1, j)))\n        if self.model_type == 'Gaussian':\n            self.alpha = torch.nn.ParameterList()\n            for j in self.dimensions[1::]:\n                self.alpha.append(torch.nn.Parameter(torch.randn(1, 1, j)))\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        if hasattr(self, 'input_multiplier'):\n            result = x * self.input_multiplier[0]\n        else:\n            result = x\n        for layer_id, layer in enumerate(self.layers):\n            result = layer(result)\n            if self.model_type == 'conventional' and layer_id != len(self.layers) -1:\n                result = self.activation(result)\n            elif self.model_type == 'swish' and layer_id != len(self.layers) - 1:\n                result = swish(result)\n            elif self.model_type == 'SIREN' and layer_id != len(self.layers) - 1:\n                result = torch.sin(result * self.siren_multiplier)\n            elif self.model_type == 'FILM SIREN' and layer_id != len(self.layers) - 1:\n                result = torch.sin(self.alpha[layer_id][0] * result + self.alpha[layer_id][1])\n            elif self.model_type == 'Gaussian' and layer_id != len(self.layers) - 1: \n                result = gaussian(result, self.alpha[layer_id][0])\n        return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.multi_layer_perceptron.__init__","title":"<code>__init__(dimensions, activation=torch.nn.ReLU(), bias=False, model_type='conventional', siren_multiplier=1.0, input_multiplier=None)</code>","text":"<p>Parameters:</p> <ul> <li> <code>dimensions</code>           \u2013            <pre><code>            List of integers representing the dimensions of each layer (e.g., [2, 10, 1], where the first layer has two channels and last one has one channel.).\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>            Nonlinear activation function.\n            Default is `torch.nn.ReLU()`.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>            If set to True, linear layers will include biases.\n</code></pre> </li> <li> <code>siren_multiplier</code>           \u2013            <pre><code>            When using `SIREN` model type, this parameter functions as a hyperparameter.\n            The original SIREN work uses 30.\n            You can bypass this parameter by providing input that are not normalized and larger then one.\n</code></pre> </li> <li> <code>input_multiplier</code>           \u2013            <pre><code>            Initial value of the input multiplier before the very first layer.\n</code></pre> </li> <li> <code>model_type</code>           \u2013            <pre><code>            Model type: `conventional`, `swish`, `SIREN`, `FILM SIREN`, `Gaussian`.\n            `conventional` refers to a standard multi layer perceptron.\n            For `SIREN,` see: Sitzmann, Vincent, et al. \"Implicit neural representations with periodic activation functions.\" Advances in neural information processing systems 33 (2020): 7462-7473.\n            For `Swish,` see: Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for activation functions.\" arXiv preprint arXiv:1710.05941 (2017). \n            For `FILM SIREN,` see: Chan, Eric R., et al. \"pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n            For `Gaussian,` see: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(self,\n             dimensions,\n             activation = torch.nn.ReLU(),\n             bias = False,\n             model_type = 'conventional',\n             siren_multiplier = 1.,\n             input_multiplier = None\n            ):\n    \"\"\"\n    Parameters\n    ----------\n    dimensions        : list\n                        List of integers representing the dimensions of each layer (e.g., [2, 10, 1], where the first layer has two channels and last one has one channel.).\n    activation        : torch.nn\n                        Nonlinear activation function.\n                        Default is `torch.nn.ReLU()`.\n    bias              : bool\n                        If set to True, linear layers will include biases.\n    siren_multiplier  : float\n                        When using `SIREN` model type, this parameter functions as a hyperparameter.\n                        The original SIREN work uses 30.\n                        You can bypass this parameter by providing input that are not normalized and larger then one.\n    input_multiplier  : float\n                        Initial value of the input multiplier before the very first layer.\n    model_type        : str\n                        Model type: `conventional`, `swish`, `SIREN`, `FILM SIREN`, `Gaussian`.\n                        `conventional` refers to a standard multi layer perceptron.\n                        For `SIREN,` see: Sitzmann, Vincent, et al. \"Implicit neural representations with periodic activation functions.\" Advances in neural information processing systems 33 (2020): 7462-7473.\n                        For `Swish,` see: Ramachandran, Prajit, Barret Zoph, and Quoc V. Le. \"Searching for activation functions.\" arXiv preprint arXiv:1710.05941 (2017). \n                        For `FILM SIREN,` see: Chan, Eric R., et al. \"pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021.\n                        For `Gaussian,` see: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n    \"\"\"\n    super(multi_layer_perceptron, self).__init__()\n    self.activation = activation\n    self.bias = bias\n    self.model_type = model_type\n    self.layers = torch.nn.ModuleList()\n    self.siren_multiplier = siren_multiplier\n    self.dimensions = dimensions\n    for i in range(len(self.dimensions) - 1):\n        self.layers.append(torch.nn.Linear(self.dimensions[i], self.dimensions[i + 1], bias = self.bias))\n    if not isinstance(input_multiplier, type(None)):\n        self.input_multiplier = torch.nn.ParameterList()\n        self.input_multiplier.append(torch.nn.Parameter(torch.ones(1, self.dimensions[0]) * input_multiplier))\n    if self.model_type == 'FILM SIREN':\n        self.alpha = torch.nn.ParameterList()\n        for j in self.dimensions[1::]:\n            self.alpha.append(torch.nn.Parameter(torch.randn(2, 1, j)))\n    if self.model_type == 'Gaussian':\n        self.alpha = torch.nn.ParameterList()\n        for j in self.dimensions[1::]:\n            self.alpha.append(torch.nn.Parameter(torch.randn(1, 1, j)))\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.multi_layer_perceptron.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    if hasattr(self, 'input_multiplier'):\n        result = x * self.input_multiplier[0]\n    else:\n        result = x\n    for layer_id, layer in enumerate(self.layers):\n        result = layer(result)\n        if self.model_type == 'conventional' and layer_id != len(self.layers) -1:\n            result = self.activation(result)\n        elif self.model_type == 'swish' and layer_id != len(self.layers) - 1:\n            result = swish(result)\n        elif self.model_type == 'SIREN' and layer_id != len(self.layers) - 1:\n            result = torch.sin(result * self.siren_multiplier)\n        elif self.model_type == 'FILM SIREN' and layer_id != len(self.layers) - 1:\n            result = torch.sin(self.alpha[layer_id][0] * result + self.alpha[layer_id][1])\n        elif self.model_type == 'Gaussian' and layer_id != len(self.layers) - 1: \n            result = gaussian(result, self.alpha[layer_id][0])\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.non_local_layer","title":"<code>non_local_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Self-Attention Layer [zi = Wzyi + xi] (non-local block : ref https://arxiv.org/abs/1711.07971)</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class non_local_layer(torch.nn.Module):\n    \"\"\"\n    Self-Attention Layer [zi = Wzyi + xi] (non-local block : ref https://arxiv.org/abs/1711.07971)\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 1024,\n                 bottleneck_channels = 512,\n                 kernel_size = 1,\n                 bias = False,\n                ):\n        \"\"\"\n\n        Parameters\n        ----------\n        input_channels      : int\n                              Number of input channels.\n        bottleneck_channels : int\n                              Number of middle channels.\n        kernel_size         : int\n                              Kernel size.\n        bias                : bool \n                              Set to True to let convolutional layers have bias term.\n        \"\"\"\n        super(non_local_layer, self).__init__()\n        self.input_channels = input_channels\n        self.bottleneck_channels = bottleneck_channels\n        self.g = torch.nn.Conv2d(\n                                 self.input_channels, \n                                 self.bottleneck_channels,\n                                 kernel_size = kernel_size,\n                                 padding = kernel_size // 2,\n                                 bias = bias\n                                )\n        self.W_z = torch.nn.Sequential(\n                                       torch.nn.Conv2d(\n                                                       self.bottleneck_channels,\n                                                       self.input_channels, \n                                                       kernel_size = kernel_size,\n                                                       bias = bias,\n                                                       padding = kernel_size // 2\n                                                      ),\n                                       torch.nn.BatchNorm2d(self.input_channels)\n                                      )\n        torch.nn.init.constant_(self.W_z[1].weight, 0)   \n        torch.nn.init.constant_(self.W_z[1].bias, 0)\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model [zi = Wzyi + xi]\n\n        Parameters\n        ----------\n        x               : torch.tensor\n                          First input data.                       \n\n\n        Returns\n        ----------\n        z               : torch.tensor\n                          Estimated output.\n        \"\"\"\n        batch_size, channels, height, width = x.size()\n        theta = x.view(batch_size, channels, -1).permute(0, 2, 1)\n        phi = x.view(batch_size, channels, -1).permute(0, 2, 1)\n        g = self.g(x).view(batch_size, self.bottleneck_channels, -1).permute(0, 2, 1)\n        attn = torch.bmm(theta, phi.transpose(1, 2)) / (height * width)\n        attn = torch.nn.functional.softmax(attn, dim=-1)\n        y = torch.bmm(attn, g).permute(0, 2, 1).contiguous().view(batch_size, self.bottleneck_channels, height, width)\n        W_y = self.W_z(y)\n        z = W_y + x\n        return z\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.non_local_layer.__init__","title":"<code>__init__(input_channels=1024, bottleneck_channels=512, kernel_size=1, bias=False)</code>","text":"<p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>              Number of input channels.\n</code></pre> </li> <li> <code>bottleneck_channels</code>               (<code>int</code>, default:                   <code>512</code> )           \u2013            <pre><code>              Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>              Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>              Set to True to let convolutional layers have bias term.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 1024,\n             bottleneck_channels = 512,\n             kernel_size = 1,\n             bias = False,\n            ):\n    \"\"\"\n\n    Parameters\n    ----------\n    input_channels      : int\n                          Number of input channels.\n    bottleneck_channels : int\n                          Number of middle channels.\n    kernel_size         : int\n                          Kernel size.\n    bias                : bool \n                          Set to True to let convolutional layers have bias term.\n    \"\"\"\n    super(non_local_layer, self).__init__()\n    self.input_channels = input_channels\n    self.bottleneck_channels = bottleneck_channels\n    self.g = torch.nn.Conv2d(\n                             self.input_channels, \n                             self.bottleneck_channels,\n                             kernel_size = kernel_size,\n                             padding = kernel_size // 2,\n                             bias = bias\n                            )\n    self.W_z = torch.nn.Sequential(\n                                   torch.nn.Conv2d(\n                                                   self.bottleneck_channels,\n                                                   self.input_channels, \n                                                   kernel_size = kernel_size,\n                                                   bias = bias,\n                                                   padding = kernel_size // 2\n                                                  ),\n                                   torch.nn.BatchNorm2d(self.input_channels)\n                                  )\n    torch.nn.init.constant_(self.W_z[1].weight, 0)   \n    torch.nn.init.constant_(self.W_z[1].bias, 0)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.non_local_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model [zi = Wzyi + xi]</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>          First input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>z</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model [zi = Wzyi + xi]\n\n    Parameters\n    ----------\n    x               : torch.tensor\n                      First input data.                       \n\n\n    Returns\n    ----------\n    z               : torch.tensor\n                      Estimated output.\n    \"\"\"\n    batch_size, channels, height, width = x.size()\n    theta = x.view(batch_size, channels, -1).permute(0, 2, 1)\n    phi = x.view(batch_size, channels, -1).permute(0, 2, 1)\n    g = self.g(x).view(batch_size, self.bottleneck_channels, -1).permute(0, 2, 1)\n    attn = torch.bmm(theta, phi.transpose(1, 2)) / (height * width)\n    attn = torch.nn.functional.softmax(attn, dim=-1)\n    y = torch.bmm(attn, g).permute(0, 2, 1).contiguous().view(batch_size, self.bottleneck_channels, height, width)\n    W_y = self.W_z(y)\n    z = W_y + x\n    return z\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.normalization","title":"<code>normalization</code>","text":"<p>               Bases: <code>Module</code></p> <p>A normalization layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class normalization(torch.nn.Module):\n    \"\"\"\n    A normalization layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 dim = 1,\n                ):\n        \"\"\"\n        Normalization layer.\n\n\n        Parameters\n        ----------\n        dim             : int\n                          Dimension (axis) to normalize.\n        \"\"\"\n        super().__init__()\n        self.k = torch.nn.Parameter(torch.ones(1, dim, 1, 1))\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n        var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n        mean = torch.mean(x, dim = 1, keepdim = True)\n        result =  (x - mean) * (var + eps).rsqrt() * self.k\n        return result \n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.normalization.__init__","title":"<code>__init__(dim=1)</code>","text":"<p>Normalization layer.</p> <p>Parameters:</p> <ul> <li> <code>dim</code>           \u2013            <pre><code>          Dimension (axis) to normalize.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             dim = 1,\n            ):\n    \"\"\"\n    Normalization layer.\n\n\n    Parameters\n    ----------\n    dim             : int\n                      Dimension (axis) to normalize.\n    \"\"\"\n    super().__init__()\n    self.k = torch.nn.Parameter(torch.ones(1, dim, 1, 1))\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.normalization.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    eps = 1e-5 if x.dtype == torch.float32 else 1e-3\n    var = torch.var(x, dim = 1, unbiased = False, keepdim = True)\n    mean = torch.mean(x, dim = 1, keepdim = True)\n    result =  (x - mean) * (var + eps).rsqrt() * self.k\n    return result \n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.positional_encoder","title":"<code>positional_encoder</code>","text":"<p>               Bases: <code>Module</code></p> <p>A positional encoder module. This implementation follows this specific work: <code>Martin-Brualla, Ricardo, Noha Radwan, Mehdi SM Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. \"Nerf in the wild: Neural radiance fields for unconstrained photo collections.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7210-7219. 2021.</code>.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class positional_encoder(torch.nn.Module):\n    \"\"\"\n    A positional encoder module.\n    This implementation follows this specific work: `Martin-Brualla, Ricardo, Noha Radwan, Mehdi SM Sajjadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel Duckworth. \"Nerf in the wild: Neural radiance fields for unconstrained photo collections.\" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 7210-7219. 2021.`.\n    \"\"\"\n\n    def __init__(self, L):\n        \"\"\"\n        A positional encoder module.\n\n        Parameters\n        ----------\n        L                   : int\n                              Positional encoding level.\n        \"\"\"\n        super(positional_encoder, self).__init__()\n        self.L = L\n\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x               : torch.tensor\n                          Input data [b x n], where `b` is batch size, `n` is the feature size.\n\n        Returns\n        ----------\n        result          : torch.tensor\n                          Result of the forward operation.\n        \"\"\"\n        freqs = 2 ** torch.arange(self.L, device = x.device)\n        freqs = freqs.view(1, 1, -1)\n        results_cos = torch.cos(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n        results_sin = torch.sin(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n        results = torch.cat((x, results_cos, results_sin), dim = 1)\n        return results\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.positional_encoder.__init__","title":"<code>__init__(L)</code>","text":"<p>A positional encoder module.</p> <p>Parameters:</p> <ul> <li> <code>L</code>           \u2013            <pre><code>              Positional encoding level.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(self, L):\n    \"\"\"\n    A positional encoder module.\n\n    Parameters\n    ----------\n    L                   : int\n                          Positional encoding level.\n    \"\"\"\n    super(positional_encoder, self).__init__()\n    self.L = L\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.positional_encoder.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>          Input data [b x n], where `b` is batch size, `n` is the feature size.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x               : torch.tensor\n                      Input data [b x n], where `b` is batch size, `n` is the feature size.\n\n    Returns\n    ----------\n    result          : torch.tensor\n                      Result of the forward operation.\n    \"\"\"\n    freqs = 2 ** torch.arange(self.L, device = x.device)\n    freqs = freqs.view(1, 1, -1)\n    results_cos = torch.cos(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n    results_sin = torch.sin(x.unsqueeze(-1) * freqs).reshape(x.shape[0], -1)\n    results = torch.cat((x, results_cos, results_sin), dim = 1)\n    return results\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.residual_attention_layer","title":"<code>residual_attention_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A residual block with an attention layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class residual_attention_layer(torch.nn.Module):\n    \"\"\"\n    A residual block with an attention layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 1,\n                 bias = False,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        An attention layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int or optioal\n                          Number of input channels.\n        output_channels : int or optional\n                          Number of middle channels.\n        kernel_size     : int or optional\n                          Kernel size.\n        bias            : bool or optional\n                          Set to True to let convolutional layers have bias term.\n        activation      : torch.nn or optional\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.activation = activation\n        self.convolution0 = torch.nn.Sequential(\n                                                torch.nn.Conv2d(\n                                                                input_channels,\n                                                                output_channels,\n                                                                kernel_size = kernel_size,\n                                                                padding = kernel_size // 2,\n                                                                bias = bias\n                                                               ),\n                                                torch.nn.BatchNorm2d(output_channels)\n                                               )\n        self.convolution1 = torch.nn.Sequential(\n                                                torch.nn.Conv2d(\n                                                                input_channels,\n                                                                output_channels,\n                                                                kernel_size = kernel_size,\n                                                                padding = kernel_size // 2,\n                                                                bias = bias\n                                                               ),\n                                                torch.nn.BatchNorm2d(output_channels)\n                                               )\n        self.final_layer = torch.nn.Sequential(\n                                               self.activation,\n                                               torch.nn.Conv2d(\n                                                               output_channels,\n                                                               output_channels,\n                                                               kernel_size = kernel_size,\n                                                               padding = kernel_size // 2,\n                                                               bias = bias\n                                                              )\n                                              )\n\n\n    def forward(self, x0, x1):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x0             : torch.tensor\n                         First input data.\n\n        x1             : torch.tensor\n                         Seconnd input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        y0 = self.convolution0(x0)\n        y1 = self.convolution1(x1)\n        y2 = torch.add(y0, y1)\n        result = self.final_layer(y2) * x0\n        return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.residual_attention_layer.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=1, bias=False, activation=torch.nn.ReLU())</code>","text":"<p>An attention layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int or optional</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 1,\n             bias = False,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    An attention layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int or optioal\n                      Number of input channels.\n    output_channels : int or optional\n                      Number of middle channels.\n    kernel_size     : int or optional\n                      Kernel size.\n    bias            : bool or optional\n                      Set to True to let convolutional layers have bias term.\n    activation      : torch.nn or optional\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.activation = activation\n    self.convolution0 = torch.nn.Sequential(\n                                            torch.nn.Conv2d(\n                                                            input_channels,\n                                                            output_channels,\n                                                            kernel_size = kernel_size,\n                                                            padding = kernel_size // 2,\n                                                            bias = bias\n                                                           ),\n                                            torch.nn.BatchNorm2d(output_channels)\n                                           )\n    self.convolution1 = torch.nn.Sequential(\n                                            torch.nn.Conv2d(\n                                                            input_channels,\n                                                            output_channels,\n                                                            kernel_size = kernel_size,\n                                                            padding = kernel_size // 2,\n                                                            bias = bias\n                                                           ),\n                                            torch.nn.BatchNorm2d(output_channels)\n                                           )\n    self.final_layer = torch.nn.Sequential(\n                                           self.activation,\n                                           torch.nn.Conv2d(\n                                                           output_channels,\n                                                           output_channels,\n                                                           kernel_size = kernel_size,\n                                                           padding = kernel_size // 2,\n                                                           bias = bias\n                                                          )\n                                          )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.residual_attention_layer.forward","title":"<code>forward(x0, x1)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x0</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x1</code>           \u2013            <pre><code>         Seconnd input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x0, x1):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x0             : torch.tensor\n                     First input data.\n\n    x1             : torch.tensor\n                     Seconnd input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    y0 = self.convolution0(x0)\n    y1 = self.convolution1(x1)\n    y2 = torch.add(y0, y1)\n    result = self.final_layer(y2) * x0\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.residual_layer","title":"<code>residual_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>A residual layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class residual_layer(torch.nn.Module):\n    \"\"\"\n    A residual layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 mid_channels = 16,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = True,\n                 activation = torch.nn.ReLU()\n                ):\n        \"\"\"\n        A convolutional layer class.\n\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        mid_channels    : int\n                          Number of middle channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super().__init__()\n        self.activation = activation\n        self.convolution = double_convolution(\n                                              input_channels,\n                                              mid_channels = mid_channels,\n                                              output_channels = input_channels,\n                                              kernel_size = kernel_size,\n                                              normalization = normalization,\n                                              bias = bias,\n                                              activation = activation\n                                             )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        x0 = self.convolution(x)\n        return x + x0\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.residual_layer.__init__","title":"<code>__init__(input_channels=2, mid_channels=16, kernel_size=3, bias=False, normalization=True, activation=torch.nn.ReLU())</code>","text":"<p>A convolutional layer class.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>mid_channels</code>           \u2013            <pre><code>          Number of middle channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             mid_channels = 16,\n             kernel_size = 3,\n             bias = False,\n             normalization = True,\n             activation = torch.nn.ReLU()\n            ):\n    \"\"\"\n    A convolutional layer class.\n\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    mid_channels    : int\n                      Number of middle channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super().__init__()\n    self.activation = activation\n    self.convolution = double_convolution(\n                                          input_channels,\n                                          mid_channels = mid_channels,\n                                          output_channels = input_channels,\n                                          kernel_size = kernel_size,\n                                          normalization = normalization,\n                                          bias = bias,\n                                          activation = activation\n                                         )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.residual_layer.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    x0 = self.convolution(x)\n    return x + x0\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatial_gate","title":"<code>spatial_gate</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatial attention module that applies a convolution layer after channel pooling. This class is heavily inspired by https://github.com/Jongchan/attention-module/blob/master/MODELS/cbam.py.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatial_gate(torch.nn.Module):\n    \"\"\"\n    Spatial attention module that applies a convolution layer after channel pooling.\n    This class is heavily inspired by https://github.com/Jongchan/attention-module/blob/master/MODELS/cbam.py.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initializes the spatial gate module.\n        \"\"\"\n        super().__init__()\n        kernel_size = 7\n        self.spatial = convolution_layer(2, 1, kernel_size, bias = False, activation = torch.nn.Identity())\n\n\n    def channel_pool(self, x):\n        \"\"\"\n        Applies max and average pooling on the channels.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input tensor.\n\n        Returns\n        -------\n        output        : torch.tensor\n                        Output tensor.\n        \"\"\"\n        max_pool = torch.max(x, 1)[0].unsqueeze(1)\n        avg_pool = torch.mean(x, 1).unsqueeze(1)\n        output = torch.cat((max_pool, avg_pool), dim=1)\n        return output\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the SpatialGate module.\n\n        Applies spatial attention to the input tensor.\n\n        Parameters\n        ----------\n        x            : torch.tensor\n                       Input tensor to the SpatialGate module.\n\n        Returns\n        -------\n        scaled_x     : torch.tensor\n                       Output tensor after applying spatial attention.\n        \"\"\"\n        x_compress = self.channel_pool(x)\n        x_out = self.spatial(x_compress)\n        scale = torch.sigmoid(x_out)\n        scaled_x = x * scale\n        return scaled_x\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatial_gate.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the spatial gate module.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes the spatial gate module.\n    \"\"\"\n    super().__init__()\n    kernel_size = 7\n    self.spatial = convolution_layer(2, 1, kernel_size, bias = False, activation = torch.nn.Identity())\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatial_gate.channel_pool","title":"<code>channel_pool(x)</code>","text":"<p>Applies max and average pooling on the channels.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input tensor.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Output tensor.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def channel_pool(self, x):\n    \"\"\"\n    Applies max and average pooling on the channels.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input tensor.\n\n    Returns\n    -------\n    output        : torch.tensor\n                    Output tensor.\n    \"\"\"\n    max_pool = torch.max(x, 1)[0].unsqueeze(1)\n    avg_pool = torch.mean(x, 1).unsqueeze(1)\n    output = torch.cat((max_pool, avg_pool), dim=1)\n    return output\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatial_gate.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the SpatialGate module.</p> <p>Applies spatial attention to the input tensor.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input tensor to the SpatialGate module.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>scaled_x</code> (              <code>tensor</code> )          \u2013            <p>Output tensor after applying spatial attention.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward pass of the SpatialGate module.\n\n    Applies spatial attention to the input tensor.\n\n    Parameters\n    ----------\n    x            : torch.tensor\n                   Input tensor to the SpatialGate module.\n\n    Returns\n    -------\n    scaled_x     : torch.tensor\n                   Output tensor after applying spatial attention.\n    \"\"\"\n    x_compress = self.channel_pool(x)\n    x_out = self.spatial(x_compress)\n    scale = torch.sigmoid(x_out)\n    scaled_x = x * scale\n    return scaled_x\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_adaptive_convolution","title":"<code>spatially_adaptive_convolution</code>","text":"<p>               Bases: <code>Module</code></p> <p>A spatially adaptive convolution layer.</p> References <p>C. Zheng et al. \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions.\" C. Xu et al. \"Squeezesegv3: Spatially-adaptive Convolution for Efficient Point-Cloud Segmentation.\" C. Zheng et al. \"Windowing Decomposition Convolutional Neural Network for Image Enhancement.\"</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatially_adaptive_convolution(torch.nn.Module):\n    \"\"\"\n    A spatially adaptive convolution layer.\n\n    References\n    ----------\n\n    C. Zheng et al. \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions.\"\n    C. Xu et al. \"Squeezesegv3: Spatially-adaptive Convolution for Efficient Point-Cloud Segmentation.\"\n    C. Zheng et al. \"Windowing Decomposition Convolutional Neural Network for Image Enhancement.\"\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 stride = 1,\n                 padding = 1,\n                 bias = False,\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        Initializes a spatially adaptive convolution layer.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Size of the convolution kernel.\n        stride          : int\n                          Stride of the convolution.\n        padding         : int\n                          Padding added to both sides of the input.\n        bias            : bool\n                          If True, includes a bias term in the convolution.\n        activation      : torch.nn.Module\n                          Activation function to apply. If None, no activation is applied.\n        \"\"\"\n        super(spatially_adaptive_convolution, self).__init__()\n        self.kernel_size = kernel_size\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.stride = stride\n        self.padding = padding\n        self.standard_convolution = torch.nn.Conv2d(\n                                                    in_channels = input_channels,\n                                                    out_channels = self.output_channels,\n                                                    kernel_size = kernel_size,\n                                                    stride = stride,\n                                                    padding = padding,\n                                                    bias = bias\n                                                   )\n        self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n        self.activation = activation\n\n\n    def forward(self, x, sv_kernel_feature):\n        \"\"\"\n        Forward pass for the spatially adaptive convolution layer.\n\n        Parameters\n        ----------\n        x                  : torch.tensor\n                            Input data tensor.\n                            Dimension: (1, C, H, W)\n        sv_kernel_feature   : torch.tensor\n                            Spatially varying kernel features.\n                            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n        Returns\n        -------\n        sa_output          : torch.tensor\n                            Estimated output tensor.\n                            Dimension: (1, output_channels, H_out, W_out)\n        \"\"\"\n        # Pad input and sv_kernel_feature if necessary\n        if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n                -2) * self.stride != x.size(-2):\n            diffY = sv_kernel_feature.size(-2) % self.stride\n            diffX = sv_kernel_feature.size(-1) % self.stride\n            sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                            diffY // 2, diffY - diffY // 2))\n            diffY = x.size(-2) % self.stride\n            diffX = x.size(-1) % self.stride\n            x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                            diffY // 2, diffY - diffY // 2))\n\n        # Unfold the input tensor for matrix multiplication\n        input_feature = torch.nn.functional.unfold(\n                                                   x,\n                                                   kernel_size = (self.kernel_size, self.kernel_size),\n                                                   stride = self.stride,\n                                                   padding = self.padding\n                                                  )\n\n        # Resize sv_kernel_feature to match the input feature\n        sv_kernel = sv_kernel_feature.reshape(\n                                              1,\n                                              self.input_channels * self.kernel_size * self.kernel_size,\n                                              (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                             )\n\n        # Resize weight to match the input channels and kernel size\n        si_kernel = self.weight.reshape(\n                                        self.weight_output_channels,\n                                        self.input_channels * self.kernel_size * self.kernel_size\n                                       )\n\n        # Apply spatially varying kernels\n        sv_feature = input_feature * sv_kernel\n\n        # Perform matrix multiplication\n        sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                                1, self.weight_output_channels,\n                                                                (x.size(-2) // self.stride),\n                                                                (x.size(-1) // self.stride)\n                                                               )\n        return sa_output\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_adaptive_convolution.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, stride=1, padding=1, bias=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>Initializes a spatially adaptive convolution layer.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Size of the convolution kernel.\n</code></pre> </li> <li> <code>stride</code>           \u2013            <pre><code>          Stride of the convolution.\n</code></pre> </li> <li> <code>padding</code>           \u2013            <pre><code>          Padding added to both sides of the input.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          If True, includes a bias term in the convolution.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Activation function to apply. If None, no activation is applied.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             stride = 1,\n             padding = 1,\n             bias = False,\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    Initializes a spatially adaptive convolution layer.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Size of the convolution kernel.\n    stride          : int\n                      Stride of the convolution.\n    padding         : int\n                      Padding added to both sides of the input.\n    bias            : bool\n                      If True, includes a bias term in the convolution.\n    activation      : torch.nn.Module\n                      Activation function to apply. If None, no activation is applied.\n    \"\"\"\n    super(spatially_adaptive_convolution, self).__init__()\n    self.kernel_size = kernel_size\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.stride = stride\n    self.padding = padding\n    self.standard_convolution = torch.nn.Conv2d(\n                                                in_channels = input_channels,\n                                                out_channels = self.output_channels,\n                                                kernel_size = kernel_size,\n                                                stride = stride,\n                                                padding = padding,\n                                                bias = bias\n                                               )\n    self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n    self.activation = activation\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_adaptive_convolution.forward","title":"<code>forward(x, sv_kernel_feature)</code>","text":"<p>Forward pass for the spatially adaptive convolution layer.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>            Input data tensor.\n            Dimension: (1, C, H, W)\n</code></pre> </li> <li> <code>sv_kernel_feature</code>           \u2013            <pre><code>            Spatially varying kernel features.\n            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>sa_output</code> (              <code>tensor</code> )          \u2013            <p>Estimated output tensor. Dimension: (1, output_channels, H_out, W_out)</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x, sv_kernel_feature):\n    \"\"\"\n    Forward pass for the spatially adaptive convolution layer.\n\n    Parameters\n    ----------\n    x                  : torch.tensor\n                        Input data tensor.\n                        Dimension: (1, C, H, W)\n    sv_kernel_feature   : torch.tensor\n                        Spatially varying kernel features.\n                        Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n    Returns\n    -------\n    sa_output          : torch.tensor\n                        Estimated output tensor.\n                        Dimension: (1, output_channels, H_out, W_out)\n    \"\"\"\n    # Pad input and sv_kernel_feature if necessary\n    if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n            -2) * self.stride != x.size(-2):\n        diffY = sv_kernel_feature.size(-2) % self.stride\n        diffX = sv_kernel_feature.size(-1) % self.stride\n        sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                        diffY // 2, diffY - diffY // 2))\n        diffY = x.size(-2) % self.stride\n        diffX = x.size(-1) % self.stride\n        x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                        diffY // 2, diffY - diffY // 2))\n\n    # Unfold the input tensor for matrix multiplication\n    input_feature = torch.nn.functional.unfold(\n                                               x,\n                                               kernel_size = (self.kernel_size, self.kernel_size),\n                                               stride = self.stride,\n                                               padding = self.padding\n                                              )\n\n    # Resize sv_kernel_feature to match the input feature\n    sv_kernel = sv_kernel_feature.reshape(\n                                          1,\n                                          self.input_channels * self.kernel_size * self.kernel_size,\n                                          (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                         )\n\n    # Resize weight to match the input channels and kernel size\n    si_kernel = self.weight.reshape(\n                                    self.weight_output_channels,\n                                    self.input_channels * self.kernel_size * self.kernel_size\n                                   )\n\n    # Apply spatially varying kernels\n    sv_feature = input_feature * sv_kernel\n\n    # Perform matrix multiplication\n    sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                            1, self.weight_output_channels,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n    return sa_output\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_adaptive_module","title":"<code>spatially_adaptive_module</code>","text":"<p>               Bases: <code>Module</code></p> <p>A spatially adaptive module that combines learned spatially adaptive convolutions.</p> References <p>Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class spatially_adaptive_module(torch.nn.Module):\n    \"\"\"\n    A spatially adaptive module that combines learned spatially adaptive convolutions.\n\n    References\n    ----------\n\n    Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels = 2,\n                 output_channels = 2,\n                 kernel_size = 3,\n                 stride = 1,\n                 padding = 1,\n                 bias = False,\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        Initializes a spatially adaptive module.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Size of the convolution kernel.\n        stride          : int\n                          Stride of the convolution.\n        padding         : int\n                          Padding added to both sides of the input.\n        bias            : bool\n                          If True, includes a bias term in the convolution.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        \"\"\"\n        super(spatially_adaptive_module, self).__init__()\n        self.kernel_size = kernel_size\n        self.input_channels = input_channels\n        self.output_channels = output_channels\n        self.stride = stride\n        self.padding = padding\n        self.weight_output_channels = self.output_channels - 1\n        self.standard_convolution = torch.nn.Conv2d(\n                                                    in_channels = input_channels,\n                                                    out_channels = self.weight_output_channels,\n                                                    kernel_size = kernel_size,\n                                                    stride = stride,\n                                                    padding = padding,\n                                                    bias = bias\n                                                   )\n        self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n        self.activation = activation\n\n\n    def forward(self, x, sv_kernel_feature):\n        \"\"\"\n        Forward pass for the spatially adaptive module.\n\n        Parameters\n        ----------\n        x                  : torch.tensor\n                            Input data tensor.\n                            Dimension: (1, C, H, W)\n        sv_kernel_feature   : torch.tensor\n                            Spatially varying kernel features.\n                            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n        Returns\n        -------\n        output             : torch.tensor\n                            Combined output tensor from standard and spatially adaptive convolutions.\n                            Dimension: (1, output_channels, H_out, W_out)\n        \"\"\"\n        # Pad input and sv_kernel_feature if necessary\n        if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n                -2) * self.stride != x.size(-2):\n            diffY = sv_kernel_feature.size(-2) % self.stride\n            diffX = sv_kernel_feature.size(-1) % self.stride\n            sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                            diffY // 2, diffY - diffY // 2))\n            diffY = x.size(-2) % self.stride\n            diffX = x.size(-1) % self.stride\n            x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                            diffY // 2, diffY - diffY // 2))\n\n        # Unfold the input tensor for matrix multiplication\n        input_feature = torch.nn.functional.unfold(\n                                                   x,\n                                                   kernel_size = (self.kernel_size, self.kernel_size),\n                                                   stride = self.stride,\n                                                   padding = self.padding\n                                                  )\n\n        # Resize sv_kernel_feature to match the input feature\n        sv_kernel = sv_kernel_feature.reshape(\n                                              1,\n                                              self.input_channels * self.kernel_size * self.kernel_size,\n                                              (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                             )\n\n        # Apply sv_kernel to the input_feature\n        sv_feature = input_feature * sv_kernel\n\n        # Original spatially varying convolution output\n        sv_output = torch.sum(sv_feature, dim = 1).reshape(\n                                                           1,\n                                                            1,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n\n        # Reshape weight for spatially adaptive convolution\n        si_kernel = self.weight.reshape(\n                                        self.weight_output_channels,\n                                        self.input_channels * self.kernel_size * self.kernel_size\n                                       )\n\n        # Apply si_kernel on sv convolution output\n        sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                                1, self.weight_output_channels,\n                                                                (x.size(-2) // self.stride),\n                                                                (x.size(-1) // self.stride)\n                                                               )\n\n        # Combine the outputs and apply activation function\n        output = self.activation(torch.cat((sv_output, sa_output), dim = 1))\n        return output\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_adaptive_module.__init__","title":"<code>__init__(input_channels=2, output_channels=2, kernel_size=3, stride=1, padding=1, bias=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>Initializes a spatially adaptive module.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>, default:                   <code>2</code> )           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Size of the convolution kernel.\n</code></pre> </li> <li> <code>stride</code>           \u2013            <pre><code>          Stride of the convolution.\n</code></pre> </li> <li> <code>padding</code>           \u2013            <pre><code>          Padding added to both sides of the input.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          If True, includes a bias term in the convolution.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels = 2,\n             output_channels = 2,\n             kernel_size = 3,\n             stride = 1,\n             padding = 1,\n             bias = False,\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    Initializes a spatially adaptive module.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Size of the convolution kernel.\n    stride          : int\n                      Stride of the convolution.\n    padding         : int\n                      Padding added to both sides of the input.\n    bias            : bool\n                      If True, includes a bias term in the convolution.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    \"\"\"\n    super(spatially_adaptive_module, self).__init__()\n    self.kernel_size = kernel_size\n    self.input_channels = input_channels\n    self.output_channels = output_channels\n    self.stride = stride\n    self.padding = padding\n    self.weight_output_channels = self.output_channels - 1\n    self.standard_convolution = torch.nn.Conv2d(\n                                                in_channels = input_channels,\n                                                out_channels = self.weight_output_channels,\n                                                kernel_size = kernel_size,\n                                                stride = stride,\n                                                padding = padding,\n                                                bias = bias\n                                               )\n    self.weight = torch.nn.Parameter(data = self.standard_convolution.weight, requires_grad = True)\n    self.activation = activation\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_adaptive_module.forward","title":"<code>forward(x, sv_kernel_feature)</code>","text":"<p>Forward pass for the spatially adaptive module.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>            Input data tensor.\n            Dimension: (1, C, H, W)\n</code></pre> </li> <li> <code>sv_kernel_feature</code>           \u2013            <pre><code>            Spatially varying kernel features.\n            Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output</code> (              <code>tensor</code> )          \u2013            <p>Combined output tensor from standard and spatially adaptive convolutions. Dimension: (1, output_channels, H_out, W_out)</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x, sv_kernel_feature):\n    \"\"\"\n    Forward pass for the spatially adaptive module.\n\n    Parameters\n    ----------\n    x                  : torch.tensor\n                        Input data tensor.\n                        Dimension: (1, C, H, W)\n    sv_kernel_feature   : torch.tensor\n                        Spatially varying kernel features.\n                        Dimension: (1, C_i * kernel_size * kernel_size, H, W)\n\n    Returns\n    -------\n    output             : torch.tensor\n                        Combined output tensor from standard and spatially adaptive convolutions.\n                        Dimension: (1, output_channels, H_out, W_out)\n    \"\"\"\n    # Pad input and sv_kernel_feature if necessary\n    if sv_kernel_feature.size(-1) * self.stride != x.size(-1) or sv_kernel_feature.size(\n            -2) * self.stride != x.size(-2):\n        diffY = sv_kernel_feature.size(-2) % self.stride\n        diffX = sv_kernel_feature.size(-1) % self.stride\n        sv_kernel_feature = torch.nn.functional.pad(sv_kernel_feature, (diffX // 2, diffX - diffX // 2,\n                                                                        diffY // 2, diffY - diffY // 2))\n        diffY = x.size(-2) % self.stride\n        diffX = x.size(-1) % self.stride\n        x = torch.nn.functional.pad(x, (diffX // 2, diffX - diffX // 2,\n                                        diffY // 2, diffY - diffY // 2))\n\n    # Unfold the input tensor for matrix multiplication\n    input_feature = torch.nn.functional.unfold(\n                                               x,\n                                               kernel_size = (self.kernel_size, self.kernel_size),\n                                               stride = self.stride,\n                                               padding = self.padding\n                                              )\n\n    # Resize sv_kernel_feature to match the input feature\n    sv_kernel = sv_kernel_feature.reshape(\n                                          1,\n                                          self.input_channels * self.kernel_size * self.kernel_size,\n                                          (x.size(-2) // self.stride) * (x.size(-1) // self.stride)\n                                         )\n\n    # Apply sv_kernel to the input_feature\n    sv_feature = input_feature * sv_kernel\n\n    # Original spatially varying convolution output\n    sv_output = torch.sum(sv_feature, dim = 1).reshape(\n                                                       1,\n                                                        1,\n                                                        (x.size(-2) // self.stride),\n                                                        (x.size(-1) // self.stride)\n                                                       )\n\n    # Reshape weight for spatially adaptive convolution\n    si_kernel = self.weight.reshape(\n                                    self.weight_output_channels,\n                                    self.input_channels * self.kernel_size * self.kernel_size\n                                   )\n\n    # Apply si_kernel on sv convolution output\n    sa_output = torch.matmul(si_kernel, sv_feature).reshape(\n                                                            1, self.weight_output_channels,\n                                                            (x.size(-2) // self.stride),\n                                                            (x.size(-1) // self.stride)\n                                                           )\n\n    # Combine the outputs and apply activation function\n    output = self.activation(torch.cat((sv_output, sa_output), dim = 1))\n    return output\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_adaptive_unet","title":"<code>spatially_adaptive_unet</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatially varying U-Net model based on spatially adaptive convolution.</p> References <p>Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class spatially_adaptive_unet(torch.nn.Module):\n    \"\"\"\n    Spatially varying U-Net model based on spatially adaptive convolution.\n\n    References\n    ----------\n\n    Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, and Kaan Ak\u015fit, \"Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions,\" SIGGRAPH Asia 2024 Technical Communications (SA Technical Communications '24), December, 2024.\n    \"\"\"\n    def __init__(\n                 self,\n                 depth=3,\n                 dimensions=8,\n                 input_channels=6,\n                 out_channels=6,\n                 kernel_size=3,\n                 bias=True,\n                 normalization=False,\n                 activation=torch.nn.LeakyReLU(0.2, inplace=True)\n                ):\n        \"\"\"\n        U-Net model.\n\n        Parameters\n        ----------\n        depth          : int\n                         Number of upsampling and downsampling layers.\n        dimensions     : int\n                         Number of dimensions.\n        input_channels : int\n                         Number of input channels.\n        out_channels   : int\n                         Number of output channels.\n        bias           : bool\n                         Set to True to let convolutional layers learn a bias term.\n        normalization  : bool\n                         If True, adds a Batch Normalization layer after the convolutional layer.\n        activation     : torch.nn\n                         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n        \"\"\"\n        super().__init__()\n        self.depth = depth\n        self.out_channels = out_channels\n        self.inc = convolution_layer(\n                                     input_channels=input_channels,\n                                     output_channels=dimensions,\n                                     kernel_size=kernel_size,\n                                     bias=bias,\n                                     normalization=normalization,\n                                     activation=activation\n                                    )\n\n        self.encoder = torch.nn.ModuleList()\n        for i in range(self.depth + 1):  # Downsampling layers\n            down_in_channels = dimensions * (2 ** i)\n            down_out_channels = 2 * down_in_channels\n            pooling_layer = torch.nn.AvgPool2d(2)\n            double_convolution_layer = double_convolution(\n                                                          input_channels=down_in_channels,\n                                                          mid_channels=down_in_channels,\n                                                          output_channels=down_in_channels,\n                                                          kernel_size=kernel_size,\n                                                          bias=bias,\n                                                          normalization=normalization,\n                                                          activation=activation\n                                                         )\n            sam = spatially_adaptive_module(\n                                            input_channels=down_in_channels,\n                                            output_channels=down_out_channels,\n                                            kernel_size=kernel_size,\n                                            bias=bias,\n                                            activation=activation\n                                           )\n            self.encoder.append(torch.nn.ModuleList([pooling_layer, double_convolution_layer, sam]))\n        self.global_feature_module = torch.nn.ModuleList()\n        double_convolution_layer = double_convolution(\n                                                      input_channels=dimensions * (2 ** (depth + 1)),\n                                                      mid_channels=dimensions * (2 ** (depth + 1)),\n                                                      output_channels=dimensions * (2 ** (depth + 1)),\n                                                      kernel_size=kernel_size,\n                                                      bias=bias,\n                                                      normalization=normalization,\n                                                      activation=activation\n                                                     )\n        global_feature_layer = global_feature_module(\n                                                     input_channels=dimensions * (2 ** (depth + 1)),\n                                                     mid_channels=dimensions * (2 ** (depth + 1)),\n                                                     output_channels=dimensions * (2 ** (depth + 1)),\n                                                     kernel_size=kernel_size,\n                                                     bias=bias,\n                                                     activation=torch.nn.LeakyReLU(0.2, inplace=True)\n                                                    )\n        self.global_feature_module.append(torch.nn.ModuleList([double_convolution_layer, global_feature_layer]))\n        self.decoder = torch.nn.ModuleList()\n        for i in range(depth, -1, -1):\n            up_in_channels = dimensions * (2 ** (i + 1))\n            up_mid_channels = up_in_channels // 2\n            if i == 0:\n                up_out_channels = self.out_channels\n                upsample_layer = upsample_convtranspose2d_layer(\n                                                                input_channels=up_in_channels,\n                                                                output_channels=up_mid_channels,\n                                                                kernel_size=2,\n                                                                stride=2,\n                                                                bias=bias,\n                                                               )\n                conv_layer = torch.nn.Sequential(\n                    convolution_layer(\n                                      input_channels=up_mid_channels,\n                                      output_channels=up_mid_channels,\n                                      kernel_size=kernel_size,\n                                      bias=bias,\n                                      normalization=normalization,\n                                      activation=activation,\n                                     ),\n                    convolution_layer(\n                                      input_channels=up_mid_channels,\n                                      output_channels=up_out_channels,\n                                      kernel_size=1,\n                                      bias=bias,\n                                      normalization=normalization,\n                                      activation=None,\n                                     )\n                )\n                self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n            else:\n                up_out_channels = up_in_channels // 2\n                upsample_layer = upsample_convtranspose2d_layer(\n                                                                input_channels=up_in_channels,\n                                                                output_channels=up_mid_channels,\n                                                                kernel_size=2,\n                                                                stride=2,\n                                                                bias=bias,\n                                                               )\n                conv_layer = double_convolution(\n                                                input_channels=up_mid_channels,\n                                                mid_channels=up_mid_channels,\n                                                output_channels=up_out_channels,\n                                                kernel_size=kernel_size,\n                                                bias=bias,\n                                                normalization=normalization,\n                                                activation=activation,\n                                               )\n                self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n\n\n    def forward(self, sv_kernel, field):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        sv_kernel : list of torch.tensor\n                    Learned spatially varying kernels.\n                    Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                    where C_i, H_i, and W_i represent the channel, height, and width\n                    of each feature at a certain scale.\n\n        field     : torch.tensor\n                    Input field data.\n                    Dimension: (1, 6, H, W)\n\n        Returns\n        -------\n        target_field : torch.tensor\n                       Estimated output.\n                       Dimension: (1, 6, H, W)\n        \"\"\"\n        x = self.inc(field)\n        downsampling_outputs = [x]\n        for i, down_layer in enumerate(self.encoder):\n            x_down = down_layer[0](downsampling_outputs[-1])\n            downsampling_outputs.append(x_down)\n            sam_output = down_layer[2](x_down + down_layer[1](x_down), sv_kernel[self.depth - i])\n            downsampling_outputs.append(sam_output)\n        global_feature = self.global_feature_module[0][0](downsampling_outputs[-1])\n        global_feature = self.global_feature_module[0][1](downsampling_outputs[-1], global_feature)\n        downsampling_outputs.append(global_feature)\n        x_up = downsampling_outputs[-1]\n        for i, up_layer in enumerate(self.decoder):\n            x_up = up_layer[0](x_up, downsampling_outputs[2 * (self.depth - i)])\n            x_up = up_layer[1](x_up)\n        result = x_up\n        return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_adaptive_unet.__init__","title":"<code>__init__(depth=3, dimensions=8, input_channels=6, out_channels=6, kernel_size=3, bias=True, normalization=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>U-Net model.</p> <p>Parameters:</p> <ul> <li> <code>depth</code>           \u2013            <pre><code>         Number of upsampling and downsampling layers.\n</code></pre> </li> <li> <code>dimensions</code>           \u2013            <pre><code>         Number of dimensions.\n</code></pre> </li> <li> <code>input_channels</code>               (<code>int</code>, default:                   <code>6</code> )           \u2013            <pre><code>         Number of input channels.\n</code></pre> </li> <li> <code>out_channels</code>           \u2013            <pre><code>         Number of output channels.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>         Set to True to let convolutional layers learn a bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>         If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(\n             self,\n             depth=3,\n             dimensions=8,\n             input_channels=6,\n             out_channels=6,\n             kernel_size=3,\n             bias=True,\n             normalization=False,\n             activation=torch.nn.LeakyReLU(0.2, inplace=True)\n            ):\n    \"\"\"\n    U-Net model.\n\n    Parameters\n    ----------\n    depth          : int\n                     Number of upsampling and downsampling layers.\n    dimensions     : int\n                     Number of dimensions.\n    input_channels : int\n                     Number of input channels.\n    out_channels   : int\n                     Number of output channels.\n    bias           : bool\n                     Set to True to let convolutional layers learn a bias term.\n    normalization  : bool\n                     If True, adds a Batch Normalization layer after the convolutional layer.\n    activation     : torch.nn\n                     Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n    \"\"\"\n    super().__init__()\n    self.depth = depth\n    self.out_channels = out_channels\n    self.inc = convolution_layer(\n                                 input_channels=input_channels,\n                                 output_channels=dimensions,\n                                 kernel_size=kernel_size,\n                                 bias=bias,\n                                 normalization=normalization,\n                                 activation=activation\n                                )\n\n    self.encoder = torch.nn.ModuleList()\n    for i in range(self.depth + 1):  # Downsampling layers\n        down_in_channels = dimensions * (2 ** i)\n        down_out_channels = 2 * down_in_channels\n        pooling_layer = torch.nn.AvgPool2d(2)\n        double_convolution_layer = double_convolution(\n                                                      input_channels=down_in_channels,\n                                                      mid_channels=down_in_channels,\n                                                      output_channels=down_in_channels,\n                                                      kernel_size=kernel_size,\n                                                      bias=bias,\n                                                      normalization=normalization,\n                                                      activation=activation\n                                                     )\n        sam = spatially_adaptive_module(\n                                        input_channels=down_in_channels,\n                                        output_channels=down_out_channels,\n                                        kernel_size=kernel_size,\n                                        bias=bias,\n                                        activation=activation\n                                       )\n        self.encoder.append(torch.nn.ModuleList([pooling_layer, double_convolution_layer, sam]))\n    self.global_feature_module = torch.nn.ModuleList()\n    double_convolution_layer = double_convolution(\n                                                  input_channels=dimensions * (2 ** (depth + 1)),\n                                                  mid_channels=dimensions * (2 ** (depth + 1)),\n                                                  output_channels=dimensions * (2 ** (depth + 1)),\n                                                  kernel_size=kernel_size,\n                                                  bias=bias,\n                                                  normalization=normalization,\n                                                  activation=activation\n                                                 )\n    global_feature_layer = global_feature_module(\n                                                 input_channels=dimensions * (2 ** (depth + 1)),\n                                                 mid_channels=dimensions * (2 ** (depth + 1)),\n                                                 output_channels=dimensions * (2 ** (depth + 1)),\n                                                 kernel_size=kernel_size,\n                                                 bias=bias,\n                                                 activation=torch.nn.LeakyReLU(0.2, inplace=True)\n                                                )\n    self.global_feature_module.append(torch.nn.ModuleList([double_convolution_layer, global_feature_layer]))\n    self.decoder = torch.nn.ModuleList()\n    for i in range(depth, -1, -1):\n        up_in_channels = dimensions * (2 ** (i + 1))\n        up_mid_channels = up_in_channels // 2\n        if i == 0:\n            up_out_channels = self.out_channels\n            upsample_layer = upsample_convtranspose2d_layer(\n                                                            input_channels=up_in_channels,\n                                                            output_channels=up_mid_channels,\n                                                            kernel_size=2,\n                                                            stride=2,\n                                                            bias=bias,\n                                                           )\n            conv_layer = torch.nn.Sequential(\n                convolution_layer(\n                                  input_channels=up_mid_channels,\n                                  output_channels=up_mid_channels,\n                                  kernel_size=kernel_size,\n                                  bias=bias,\n                                  normalization=normalization,\n                                  activation=activation,\n                                 ),\n                convolution_layer(\n                                  input_channels=up_mid_channels,\n                                  output_channels=up_out_channels,\n                                  kernel_size=1,\n                                  bias=bias,\n                                  normalization=normalization,\n                                  activation=None,\n                                 )\n            )\n            self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n        else:\n            up_out_channels = up_in_channels // 2\n            upsample_layer = upsample_convtranspose2d_layer(\n                                                            input_channels=up_in_channels,\n                                                            output_channels=up_mid_channels,\n                                                            kernel_size=2,\n                                                            stride=2,\n                                                            bias=bias,\n                                                           )\n            conv_layer = double_convolution(\n                                            input_channels=up_mid_channels,\n                                            mid_channels=up_mid_channels,\n                                            output_channels=up_out_channels,\n                                            kernel_size=kernel_size,\n                                            bias=bias,\n                                            normalization=normalization,\n                                            activation=activation,\n                                           )\n            self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_adaptive_unet.forward","title":"<code>forward(sv_kernel, field)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>sv_kernel</code>               (<code>list of torch.tensor</code>)           \u2013            <pre><code>    Learned spatially varying kernels.\n    Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n    where C_i, H_i, and W_i represent the channel, height, and width\n    of each feature at a certain scale.\n</code></pre> </li> <li> <code>field</code>           \u2013            <pre><code>    Input field data.\n    Dimension: (1, 6, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>target_field</code> (              <code>tensor</code> )          \u2013            <p>Estimated output. Dimension: (1, 6, H, W)</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, sv_kernel, field):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    sv_kernel : list of torch.tensor\n                Learned spatially varying kernels.\n                Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                where C_i, H_i, and W_i represent the channel, height, and width\n                of each feature at a certain scale.\n\n    field     : torch.tensor\n                Input field data.\n                Dimension: (1, 6, H, W)\n\n    Returns\n    -------\n    target_field : torch.tensor\n                   Estimated output.\n                   Dimension: (1, 6, H, W)\n    \"\"\"\n    x = self.inc(field)\n    downsampling_outputs = [x]\n    for i, down_layer in enumerate(self.encoder):\n        x_down = down_layer[0](downsampling_outputs[-1])\n        downsampling_outputs.append(x_down)\n        sam_output = down_layer[2](x_down + down_layer[1](x_down), sv_kernel[self.depth - i])\n        downsampling_outputs.append(sam_output)\n    global_feature = self.global_feature_module[0][0](downsampling_outputs[-1])\n    global_feature = self.global_feature_module[0][1](downsampling_outputs[-1], global_feature)\n    downsampling_outputs.append(global_feature)\n    x_up = downsampling_outputs[-1]\n    for i, up_layer in enumerate(self.decoder):\n        x_up = up_layer[0](x_up, downsampling_outputs[2 * (self.depth - i)])\n        x_up = up_layer[1](x_up)\n    result = x_up\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_varying_kernel_generation_model","title":"<code>spatially_varying_kernel_generation_model</code>","text":"<p>               Bases: <code>Module</code></p> <p>Spatially_varying_kernel_generation_model revised from RSGUnet: https://github.com/MTLab/rsgunet_image_enhance.</p> <p>Refer to: J. Huang, P. Zhu, M. Geng et al. Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class spatially_varying_kernel_generation_model(torch.nn.Module):\n    \"\"\"\n    Spatially_varying_kernel_generation_model revised from RSGUnet:\n    https://github.com/MTLab/rsgunet_image_enhance.\n\n    Refer to:\n    J. Huang, P. Zhu, M. Geng et al. Range Scaling Global U-Net for Perceptual Image Enhancement on Mobile Devices.\n    \"\"\"\n\n    def __init__(\n                 self,\n                 depth = 3,\n                 dimensions = 8,\n                 input_channels = 7,\n                 kernel_size = 3,\n                 bias = True,\n                 normalization = False,\n                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                ):\n        \"\"\"\n        U-Net model.\n\n        Parameters\n        ----------\n        depth          : int\n                         Number of upsampling and downsampling layers.\n        dimensions     : int\n                         Number of dimensions.\n        input_channels : int\n                         Number of input channels.\n        bias           : bool\n                         Set to True to let convolutional layers learn a bias term.\n        normalization  : bool\n                         If True, adds a Batch Normalization layer after the convolutional layer.\n        activation     : torch.nn\n                         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n        \"\"\"\n        super().__init__()\n        self.depth = depth\n        self.inc = convolution_layer(\n                                     input_channels = input_channels,\n                                     output_channels = dimensions,\n                                     kernel_size = kernel_size,\n                                     bias = bias,\n                                     normalization = normalization,\n                                     activation = activation\n                                    )\n        self.encoder = torch.nn.ModuleList()\n        for i in range(depth + 1):  # downsampling layers\n            if i == 0:\n                in_channels = dimensions * (2 ** i)\n                out_channels = dimensions * (2 ** i)\n            elif i == depth:\n                in_channels = dimensions * (2 ** (i - 1))\n                out_channels = dimensions * (2 ** (i - 1))\n            else:\n                in_channels = dimensions * (2 ** (i - 1))\n                out_channels = 2 * in_channels\n            pooling_layer = torch.nn.AvgPool2d(2)\n            double_convolution_layer = double_convolution(\n                                                          input_channels = in_channels,\n                                                          mid_channels = in_channels,\n                                                          output_channels = out_channels,\n                                                          kernel_size = kernel_size,\n                                                          bias = bias,\n                                                          normalization = normalization,\n                                                          activation = activation\n                                                         )\n            self.encoder.append(pooling_layer)\n            self.encoder.append(double_convolution_layer)\n        self.spatially_varying_feature = torch.nn.ModuleList()  # for kernel generation\n        for i in range(depth, -1, -1):\n            if i == 1:\n                svf_in_channels = dimensions + 2 ** (self.depth + i) + 1\n            else:\n                svf_in_channels = 2 ** (self.depth + i) + 1\n            svf_out_channels = (2 ** (self.depth + i)) * (kernel_size * kernel_size)\n            svf_mid_channels = dimensions * (2 ** (self.depth - 1))\n            spatially_varying_kernel_generation = torch.nn.ModuleList()\n            for j in range(i, -1, -1):\n                pooling_layer = torch.nn.AvgPool2d(2 ** (j + 1))\n                spatially_varying_kernel_generation.append(pooling_layer)\n            kernel_generation_block = torch.nn.Sequential(\n                torch.nn.Conv2d(\n                                in_channels = svf_in_channels,\n                                out_channels = svf_mid_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               ),\n                activation,\n                torch.nn.Conv2d(\n                                in_channels = svf_mid_channels,\n                                out_channels = svf_mid_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               ),\n                activation,\n                torch.nn.Conv2d(\n                                in_channels = svf_mid_channels,\n                                out_channels = svf_out_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               ),\n            )\n            spatially_varying_kernel_generation.append(kernel_generation_block)\n            self.spatially_varying_feature.append(spatially_varying_kernel_generation)\n        self.decoder = torch.nn.ModuleList()\n        global_feature_layer = global_feature_module(  # global feature layer\n                                                     input_channels = dimensions * (2 ** (depth - 1)),\n                                                     mid_channels = dimensions * (2 ** (depth - 1)),\n                                                     output_channels = dimensions * (2 ** (depth - 1)),\n                                                     kernel_size = kernel_size,\n                                                     bias = bias,\n                                                     activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                                                    )\n        self.decoder.append(global_feature_layer)\n        for i in range(depth, 0, -1):\n            if i == 2:\n                up_in_channels = (dimensions // 2) * (2 ** i)\n                up_out_channels = up_in_channels\n                up_mid_channels = up_in_channels\n            elif i == 1:\n                up_in_channels = dimensions * 2\n                up_out_channels = dimensions\n                up_mid_channels = up_out_channels\n            else:\n                up_in_channels = (dimensions // 2) * (2 ** i)\n                up_out_channels = up_in_channels // 2\n                up_mid_channels = up_in_channels\n            upsample_layer = upsample_convtranspose2d_layer(\n                                                            input_channels = up_in_channels,\n                                                            output_channels = up_mid_channels,\n                                                            kernel_size = 2,\n                                                            stride = 2,\n                                                            bias = bias,\n                                                           )\n            conv_layer = double_convolution(\n                                            input_channels = up_mid_channels,\n                                            output_channels = up_out_channels,\n                                            kernel_size = kernel_size,\n                                            bias = bias,\n                                            normalization = normalization,\n                                            activation = activation,\n                                           )\n            self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n\n\n    def forward(self, focal_surface, field):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        focal_surface : torch.tensor\n                        Input focal surface data.\n                        Dimension: (1, 1, H, W)\n\n        field         : torch.tensor\n                        Input field data.\n                        Dimension: (1, 6, H, W)\n\n        Returns\n        -------\n        sv_kernel : list of torch.tensor\n                    Learned spatially varying kernels.\n                    Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                    where C_i, H_i, and W_i represent the channel, height, and width\n                    of each feature at a certain scale.\n        \"\"\"\n        x = self.inc(torch.cat((focal_surface, field), dim = 1))\n        downsampling_outputs = [focal_surface]\n        downsampling_outputs.append(x)\n        for i, down_layer in enumerate(self.encoder):\n            x_down = down_layer(downsampling_outputs[-1])\n            downsampling_outputs.append(x_down)\n        sv_kernels = []\n        for i, (up_layer, svf_layer) in enumerate(zip(self.decoder, self.spatially_varying_feature)):\n            if i == 0:\n                global_feature = up_layer(downsampling_outputs[-2], downsampling_outputs[-1])\n                downsampling_outputs[-1] = global_feature\n                sv_feature = [global_feature, downsampling_outputs[0]]\n                for j in range(self.depth - i + 1):\n                    sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                    if j &gt; 0:\n                        sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n                sv_feature = [sv_feature[0], sv_feature[1], sv_feature[4], sv_feature[2],\n                              sv_feature[3]]\n                sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n                sv_kernels.append(sv_kernel)\n            else:\n                x_up = up_layer[0](downsampling_outputs[-1],\n                                   downsampling_outputs[2 * (self.depth + 1 - i) + 1])\n                x_up = up_layer[1](x_up)\n                downsampling_outputs[-1] = x_up\n                sv_feature = [x_up, downsampling_outputs[0]]\n                for j in range(self.depth - i + 1):\n                    sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                    if j &gt; 0:\n                        sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n                if i == 1:\n                    sv_feature = [sv_feature[0], sv_feature[1], sv_feature[3], sv_feature[2]]\n                sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n                sv_kernels.append(sv_kernel)\n        return sv_kernels\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_varying_kernel_generation_model.__init__","title":"<code>__init__(depth=3, dimensions=8, input_channels=7, kernel_size=3, bias=True, normalization=False, activation=torch.nn.LeakyReLU(0.2, inplace=True))</code>","text":"<p>U-Net model.</p> <p>Parameters:</p> <ul> <li> <code>depth</code>           \u2013            <pre><code>         Number of upsampling and downsampling layers.\n</code></pre> </li> <li> <code>dimensions</code>           \u2013            <pre><code>         Number of dimensions.\n</code></pre> </li> <li> <code>input_channels</code>               (<code>int</code>, default:                   <code>7</code> )           \u2013            <pre><code>         Number of input channels.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>         Set to True to let convolutional layers learn a bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>         If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>         Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(\n             self,\n             depth = 3,\n             dimensions = 8,\n             input_channels = 7,\n             kernel_size = 3,\n             bias = True,\n             normalization = False,\n             activation = torch.nn.LeakyReLU(0.2, inplace = True)\n            ):\n    \"\"\"\n    U-Net model.\n\n    Parameters\n    ----------\n    depth          : int\n                     Number of upsampling and downsampling layers.\n    dimensions     : int\n                     Number of dimensions.\n    input_channels : int\n                     Number of input channels.\n    bias           : bool\n                     Set to True to let convolutional layers learn a bias term.\n    normalization  : bool\n                     If True, adds a Batch Normalization layer after the convolutional layer.\n    activation     : torch.nn\n                     Non-linear activation layer (e.g., torch.nn.ReLU(), torch.nn.Sigmoid()).\n    \"\"\"\n    super().__init__()\n    self.depth = depth\n    self.inc = convolution_layer(\n                                 input_channels = input_channels,\n                                 output_channels = dimensions,\n                                 kernel_size = kernel_size,\n                                 bias = bias,\n                                 normalization = normalization,\n                                 activation = activation\n                                )\n    self.encoder = torch.nn.ModuleList()\n    for i in range(depth + 1):  # downsampling layers\n        if i == 0:\n            in_channels = dimensions * (2 ** i)\n            out_channels = dimensions * (2 ** i)\n        elif i == depth:\n            in_channels = dimensions * (2 ** (i - 1))\n            out_channels = dimensions * (2 ** (i - 1))\n        else:\n            in_channels = dimensions * (2 ** (i - 1))\n            out_channels = 2 * in_channels\n        pooling_layer = torch.nn.AvgPool2d(2)\n        double_convolution_layer = double_convolution(\n                                                      input_channels = in_channels,\n                                                      mid_channels = in_channels,\n                                                      output_channels = out_channels,\n                                                      kernel_size = kernel_size,\n                                                      bias = bias,\n                                                      normalization = normalization,\n                                                      activation = activation\n                                                     )\n        self.encoder.append(pooling_layer)\n        self.encoder.append(double_convolution_layer)\n    self.spatially_varying_feature = torch.nn.ModuleList()  # for kernel generation\n    for i in range(depth, -1, -1):\n        if i == 1:\n            svf_in_channels = dimensions + 2 ** (self.depth + i) + 1\n        else:\n            svf_in_channels = 2 ** (self.depth + i) + 1\n        svf_out_channels = (2 ** (self.depth + i)) * (kernel_size * kernel_size)\n        svf_mid_channels = dimensions * (2 ** (self.depth - 1))\n        spatially_varying_kernel_generation = torch.nn.ModuleList()\n        for j in range(i, -1, -1):\n            pooling_layer = torch.nn.AvgPool2d(2 ** (j + 1))\n            spatially_varying_kernel_generation.append(pooling_layer)\n        kernel_generation_block = torch.nn.Sequential(\n            torch.nn.Conv2d(\n                            in_channels = svf_in_channels,\n                            out_channels = svf_mid_channels,\n                            kernel_size = kernel_size,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           ),\n            activation,\n            torch.nn.Conv2d(\n                            in_channels = svf_mid_channels,\n                            out_channels = svf_mid_channels,\n                            kernel_size = kernel_size,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           ),\n            activation,\n            torch.nn.Conv2d(\n                            in_channels = svf_mid_channels,\n                            out_channels = svf_out_channels,\n                            kernel_size = kernel_size,\n                            padding = kernel_size // 2,\n                            bias = bias\n                           ),\n        )\n        spatially_varying_kernel_generation.append(kernel_generation_block)\n        self.spatially_varying_feature.append(spatially_varying_kernel_generation)\n    self.decoder = torch.nn.ModuleList()\n    global_feature_layer = global_feature_module(  # global feature layer\n                                                 input_channels = dimensions * (2 ** (depth - 1)),\n                                                 mid_channels = dimensions * (2 ** (depth - 1)),\n                                                 output_channels = dimensions * (2 ** (depth - 1)),\n                                                 kernel_size = kernel_size,\n                                                 bias = bias,\n                                                 activation = torch.nn.LeakyReLU(0.2, inplace = True)\n                                                )\n    self.decoder.append(global_feature_layer)\n    for i in range(depth, 0, -1):\n        if i == 2:\n            up_in_channels = (dimensions // 2) * (2 ** i)\n            up_out_channels = up_in_channels\n            up_mid_channels = up_in_channels\n        elif i == 1:\n            up_in_channels = dimensions * 2\n            up_out_channels = dimensions\n            up_mid_channels = up_out_channels\n        else:\n            up_in_channels = (dimensions // 2) * (2 ** i)\n            up_out_channels = up_in_channels // 2\n            up_mid_channels = up_in_channels\n        upsample_layer = upsample_convtranspose2d_layer(\n                                                        input_channels = up_in_channels,\n                                                        output_channels = up_mid_channels,\n                                                        kernel_size = 2,\n                                                        stride = 2,\n                                                        bias = bias,\n                                                       )\n        conv_layer = double_convolution(\n                                        input_channels = up_mid_channels,\n                                        output_channels = up_out_channels,\n                                        kernel_size = kernel_size,\n                                        bias = bias,\n                                        normalization = normalization,\n                                        activation = activation,\n                                       )\n        self.decoder.append(torch.nn.ModuleList([upsample_layer, conv_layer]))\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.spatially_varying_kernel_generation_model.forward","title":"<code>forward(focal_surface, field)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>focal_surface</code>               (<code>tensor</code>)           \u2013            <pre><code>        Input focal surface data.\n        Dimension: (1, 1, H, W)\n</code></pre> </li> <li> <code>field</code>           \u2013            <pre><code>        Input field data.\n        Dimension: (1, 6, H, W)\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>sv_kernel</code> (              <code>list of torch.tensor</code> )          \u2013            <p>Learned spatially varying kernels. Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i), where C_i, H_i, and W_i represent the channel, height, and width of each feature at a certain scale.</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, focal_surface, field):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    focal_surface : torch.tensor\n                    Input focal surface data.\n                    Dimension: (1, 1, H, W)\n\n    field         : torch.tensor\n                    Input field data.\n                    Dimension: (1, 6, H, W)\n\n    Returns\n    -------\n    sv_kernel : list of torch.tensor\n                Learned spatially varying kernels.\n                Dimension of each element in the list: (1, C_i * kernel_size * kernel_size, H_i, W_i),\n                where C_i, H_i, and W_i represent the channel, height, and width\n                of each feature at a certain scale.\n    \"\"\"\n    x = self.inc(torch.cat((focal_surface, field), dim = 1))\n    downsampling_outputs = [focal_surface]\n    downsampling_outputs.append(x)\n    for i, down_layer in enumerate(self.encoder):\n        x_down = down_layer(downsampling_outputs[-1])\n        downsampling_outputs.append(x_down)\n    sv_kernels = []\n    for i, (up_layer, svf_layer) in enumerate(zip(self.decoder, self.spatially_varying_feature)):\n        if i == 0:\n            global_feature = up_layer(downsampling_outputs[-2], downsampling_outputs[-1])\n            downsampling_outputs[-1] = global_feature\n            sv_feature = [global_feature, downsampling_outputs[0]]\n            for j in range(self.depth - i + 1):\n                sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                if j &gt; 0:\n                    sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n            sv_feature = [sv_feature[0], sv_feature[1], sv_feature[4], sv_feature[2],\n                          sv_feature[3]]\n            sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n            sv_kernels.append(sv_kernel)\n        else:\n            x_up = up_layer[0](downsampling_outputs[-1],\n                               downsampling_outputs[2 * (self.depth + 1 - i) + 1])\n            x_up = up_layer[1](x_up)\n            downsampling_outputs[-1] = x_up\n            sv_feature = [x_up, downsampling_outputs[0]]\n            for j in range(self.depth - i + 1):\n                sv_feature[1] = svf_layer[self.depth - i](sv_feature[1])\n                if j &gt; 0:\n                    sv_feature.append(svf_layer[j](downsampling_outputs[2 * j]))\n            if i == 1:\n                sv_feature = [sv_feature[0], sv_feature[1], sv_feature[3], sv_feature[2]]\n            sv_kernel = svf_layer[-1](torch.cat(sv_feature, dim = 1))\n            sv_kernels.append(sv_kernel)\n    return sv_kernels\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.unet","title":"<code>unet</code>","text":"<p>               Bases: <code>Module</code></p> <p>A U-Net model, heavily inspired from <code>https://github.com/milesial/Pytorch-UNet/tree/master/unet</code> and more can be read from Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer International Publishing, 2015.</p> Source code in <code>odak/learn/models/models.py</code> <pre><code>class unet(torch.nn.Module):\n    \"\"\"\n    A U-Net model, heavily inspired from `https://github.com/milesial/Pytorch-UNet/tree/master/unet` and more can be read from Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. \"U-net: Convolutional networks for biomedical image segmentation.\" Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer International Publishing, 2015.\n    \"\"\"\n\n    def __init__(\n                 self, \n                 depth = 4,\n                 dimensions = 64, \n                 input_channels = 2, \n                 output_channels = 1, \n                 bilinear = False,\n                 kernel_size = 3,\n                 bias = False,\n                 activation = torch.nn.ReLU(inplace = True),\n                ):\n        \"\"\"\n        U-Net model.\n\n        Parameters\n        ----------\n        depth             : int\n                            Number of upsampling and downsampling\n        dimensions        : int\n                            Number of dimensions.\n        input_channels    : int\n                            Number of input channels.\n        output_channels   : int\n                            Number of output channels.\n        bilinear          : bool\n                            Uses bilinear upsampling in upsampling layers when set True.\n        bias              : bool\n                            Set True to let convolutional layers learn a bias term.\n        activation        : torch.nn\n                            Non-linear activation layer to be used (e.g., torch.nn.ReLU(), torch.nn.Sigmoid().\n        \"\"\"\n        super(unet, self).__init__()\n        self.inc = double_convolution(\n                                      input_channels = input_channels,\n                                      mid_channels = dimensions,\n                                      output_channels = dimensions,\n                                      kernel_size = kernel_size,\n                                      bias = bias,\n                                      activation = activation\n                                     )      \n\n        self.downsampling_layers = torch.nn.ModuleList()\n        self.upsampling_layers = torch.nn.ModuleList()\n        for i in range(depth): # downsampling layers\n            in_channels = dimensions * (2 ** i)\n            out_channels = dimensions * (2 ** (i + 1))\n            down_layer = downsample_layer(in_channels,\n                                            out_channels,\n                                            kernel_size=kernel_size,\n                                            bias=bias,\n                                            activation=activation\n                                            )\n            self.downsampling_layers.append(down_layer)      \n\n        for i in range(depth - 1, -1, -1):  # upsampling layers\n            up_in_channels = dimensions * (2 ** (i + 1))  \n            up_out_channels = dimensions * (2 ** i) \n            up_layer = upsample_layer(up_in_channels, up_out_channels, kernel_size=kernel_size, bias=bias, activation=activation, bilinear=bilinear)\n            self.upsampling_layers.append(up_layer)\n        self.outc = torch.nn.Conv2d(\n                                    dimensions, \n                                    output_channels,\n                                    kernel_size = kernel_size,\n                                    padding = kernel_size // 2,\n                                    bias = bias\n                                   )\n\n\n    def forward(self, x):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x             : torch.tensor\n                        Input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Estimated output.      \n        \"\"\"\n        downsampling_outputs = [self.inc(x)]\n        for down_layer in self.downsampling_layers:\n            x_down = down_layer(downsampling_outputs[-1])\n            downsampling_outputs.append(x_down)\n        x_up = downsampling_outputs[-1]\n        for i, up_layer in enumerate((self.upsampling_layers)):\n            x_up = up_layer(x_up, downsampling_outputs[-(i + 2)])       \n        result = self.outc(x_up)\n        return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.unet.__init__","title":"<code>__init__(depth=4, dimensions=64, input_channels=2, output_channels=1, bilinear=False, kernel_size=3, bias=False, activation=torch.nn.ReLU(inplace=True))</code>","text":"<p>U-Net model.</p> <p>Parameters:</p> <ul> <li> <code>depth</code>           \u2013            <pre><code>            Number of upsampling and downsampling\n</code></pre> </li> <li> <code>dimensions</code>           \u2013            <pre><code>            Number of dimensions.\n</code></pre> </li> <li> <code>input_channels</code>           \u2013            <pre><code>            Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>           \u2013            <pre><code>            Number of output channels.\n</code></pre> </li> <li> <code>bilinear</code>           \u2013            <pre><code>            Uses bilinear upsampling in upsampling layers when set True.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>            Set True to let convolutional layers learn a bias term.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>            Non-linear activation layer to be used (e.g., torch.nn.ReLU(), torch.nn.Sigmoid().\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def __init__(\n             self, \n             depth = 4,\n             dimensions = 64, \n             input_channels = 2, \n             output_channels = 1, \n             bilinear = False,\n             kernel_size = 3,\n             bias = False,\n             activation = torch.nn.ReLU(inplace = True),\n            ):\n    \"\"\"\n    U-Net model.\n\n    Parameters\n    ----------\n    depth             : int\n                        Number of upsampling and downsampling\n    dimensions        : int\n                        Number of dimensions.\n    input_channels    : int\n                        Number of input channels.\n    output_channels   : int\n                        Number of output channels.\n    bilinear          : bool\n                        Uses bilinear upsampling in upsampling layers when set True.\n    bias              : bool\n                        Set True to let convolutional layers learn a bias term.\n    activation        : torch.nn\n                        Non-linear activation layer to be used (e.g., torch.nn.ReLU(), torch.nn.Sigmoid().\n    \"\"\"\n    super(unet, self).__init__()\n    self.inc = double_convolution(\n                                  input_channels = input_channels,\n                                  mid_channels = dimensions,\n                                  output_channels = dimensions,\n                                  kernel_size = kernel_size,\n                                  bias = bias,\n                                  activation = activation\n                                 )      \n\n    self.downsampling_layers = torch.nn.ModuleList()\n    self.upsampling_layers = torch.nn.ModuleList()\n    for i in range(depth): # downsampling layers\n        in_channels = dimensions * (2 ** i)\n        out_channels = dimensions * (2 ** (i + 1))\n        down_layer = downsample_layer(in_channels,\n                                        out_channels,\n                                        kernel_size=kernel_size,\n                                        bias=bias,\n                                        activation=activation\n                                        )\n        self.downsampling_layers.append(down_layer)      \n\n    for i in range(depth - 1, -1, -1):  # upsampling layers\n        up_in_channels = dimensions * (2 ** (i + 1))  \n        up_out_channels = dimensions * (2 ** i) \n        up_layer = upsample_layer(up_in_channels, up_out_channels, kernel_size=kernel_size, bias=bias, activation=activation, bilinear=bilinear)\n        self.upsampling_layers.append(up_layer)\n    self.outc = torch.nn.Conv2d(\n                                dimensions, \n                                output_channels,\n                                kernel_size = kernel_size,\n                                padding = kernel_size // 2,\n                                bias = bias\n                               )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.unet.forward","title":"<code>forward(x)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>        Input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Estimated output.</p> </li> </ul> Source code in <code>odak/learn/models/models.py</code> <pre><code>def forward(self, x):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x             : torch.tensor\n                    Input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Estimated output.      \n    \"\"\"\n    downsampling_outputs = [self.inc(x)]\n    for down_layer in self.downsampling_layers:\n        x_down = down_layer(downsampling_outputs[-1])\n        downsampling_outputs.append(x_down)\n    x_up = downsampling_outputs[-1]\n    for i, up_layer in enumerate((self.upsampling_layers)):\n        x_up = up_layer(x_up, downsampling_outputs[-(i + 2)])       \n    result = self.outc(x_up)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.upsample_convtranspose2d_layer","title":"<code>upsample_convtranspose2d_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>An upsampling convtranspose2d layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class upsample_convtranspose2d_layer(torch.nn.Module):\n    \"\"\"\n    An upsampling convtranspose2d layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 2,\n                 stride = 2,\n                 bias = False,\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool\n                          Set to True to let convolutional layers have bias term.\n        \"\"\"\n        super().__init__()\n        self.up = torch.nn.ConvTranspose2d(\n                                           in_channels = input_channels,\n                                           out_channels = output_channels,\n                                           bias = bias,\n                                           kernel_size = kernel_size,\n                                           stride = stride\n                                          )\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Result of the forward operation\n        \"\"\"\n        x1 = self.up(x1)\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                          diffY // 2, diffY - diffY // 2])\n        result = x1 + x2\n        return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.upsample_convtranspose2d_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=2, stride=2, bias=False)</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 2,\n             stride = 2,\n             bias = False,\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool\n                      Set to True to let convolutional layers have bias term.\n    \"\"\"\n    super().__init__()\n    self.up = torch.nn.ConvTranspose2d(\n                                       in_channels = input_channels,\n                                       out_channels = output_channels,\n                                       bias = bias,\n                                       kernel_size = kernel_size,\n                                       stride = stride\n                                      )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.upsample_convtranspose2d_layer.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Result of the forward operation\n    \"\"\"\n    x1 = self.up(x1)\n    diffY = x2.size()[2] - x1.size()[2]\n    diffX = x2.size()[3] - x1.size()[3]\n    x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                      diffY // 2, diffY - diffY // 2])\n    result = x1 + x2\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.upsample_layer","title":"<code>upsample_layer</code>","text":"<p>               Bases: <code>Module</code></p> <p>An upsampling convolutional layer.</p> Source code in <code>odak/learn/models/components.py</code> <pre><code>class upsample_layer(torch.nn.Module):\n    \"\"\"\n    An upsampling convolutional layer.\n    \"\"\"\n    def __init__(\n                 self,\n                 input_channels,\n                 output_channels,\n                 kernel_size = 3,\n                 bias = False,\n                 normalization = False,\n                 activation = torch.nn.ReLU(),\n                 bilinear = True\n                ):\n        \"\"\"\n        A downscaling component with a double convolution.\n\n        Parameters\n        ----------\n        input_channels  : int\n                          Number of input channels.\n        output_channels : int\n                          Number of output channels.\n        kernel_size     : int\n                          Kernel size.\n        bias            : bool \n                          Set to True to let convolutional layers have bias term.\n        normalization   : bool                \n                          If True, adds a Batch Normalization layer after the convolutional layer.\n        activation      : torch.nn\n                          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n        bilinear        : bool\n                          If set to True, bilinear sampling is used.\n        \"\"\"\n        super(upsample_layer, self).__init__()\n        if bilinear:\n            self.up = torch.nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n            self.conv = double_convolution(\n                                           input_channels = input_channels + output_channels,\n                                           mid_channels = input_channels // 2,\n                                           output_channels = output_channels,\n                                           kernel_size = kernel_size,\n                                           normalization = normalization,\n                                           bias = bias,\n                                           activation = activation\n                                          )\n        else:\n            self.up = torch.nn.ConvTranspose2d(input_channels , input_channels // 2, kernel_size = 2, stride = 2)\n            self.conv = double_convolution(\n                                           input_channels = input_channels,\n                                           mid_channels = output_channels,\n                                           output_channels = output_channels,\n                                           kernel_size = kernel_size,\n                                           normalization = normalization,\n                                           bias = bias,\n                                           activation = activation\n                                          )\n\n\n    def forward(self, x1, x2):\n        \"\"\"\n        Forward model.\n\n        Parameters\n        ----------\n        x1             : torch.tensor\n                         First input data.\n        x2             : torch.tensor\n                         Second input data.\n\n\n        Returns\n        ----------\n        result        : torch.tensor\n                        Result of the forward operation\n        \"\"\" \n        x1 = self.up(x1)\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                          diffY // 2, diffY - diffY // 2])\n        x = torch.cat([x2, x1], dim = 1)\n        result = self.conv(x)\n        return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.upsample_layer.__init__","title":"<code>__init__(input_channels, output_channels, kernel_size=3, bias=False, normalization=False, activation=torch.nn.ReLU(), bilinear=True)</code>","text":"<p>A downscaling component with a double convolution.</p> <p>Parameters:</p> <ul> <li> <code>input_channels</code>           \u2013            <pre><code>          Number of input channels.\n</code></pre> </li> <li> <code>output_channels</code>               (<code>int</code>)           \u2013            <pre><code>          Number of output channels.\n</code></pre> </li> <li> <code>kernel_size</code>           \u2013            <pre><code>          Kernel size.\n</code></pre> </li> <li> <code>bias</code>           \u2013            <pre><code>          Set to True to let convolutional layers have bias term.\n</code></pre> </li> <li> <code>normalization</code>           \u2013            <pre><code>          If True, adds a Batch Normalization layer after the convolutional layer.\n</code></pre> </li> <li> <code>activation</code>           \u2013            <pre><code>          Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n</code></pre> </li> <li> <code>bilinear</code>           \u2013            <pre><code>          If set to True, bilinear sampling is used.\n</code></pre> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def __init__(\n             self,\n             input_channels,\n             output_channels,\n             kernel_size = 3,\n             bias = False,\n             normalization = False,\n             activation = torch.nn.ReLU(),\n             bilinear = True\n            ):\n    \"\"\"\n    A downscaling component with a double convolution.\n\n    Parameters\n    ----------\n    input_channels  : int\n                      Number of input channels.\n    output_channels : int\n                      Number of output channels.\n    kernel_size     : int\n                      Kernel size.\n    bias            : bool \n                      Set to True to let convolutional layers have bias term.\n    normalization   : bool                \n                      If True, adds a Batch Normalization layer after the convolutional layer.\n    activation      : torch.nn\n                      Nonlinear activation layer to be used. If None, uses torch.nn.ReLU().\n    bilinear        : bool\n                      If set to True, bilinear sampling is used.\n    \"\"\"\n    super(upsample_layer, self).__init__()\n    if bilinear:\n        self.up = torch.nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n        self.conv = double_convolution(\n                                       input_channels = input_channels + output_channels,\n                                       mid_channels = input_channels // 2,\n                                       output_channels = output_channels,\n                                       kernel_size = kernel_size,\n                                       normalization = normalization,\n                                       bias = bias,\n                                       activation = activation\n                                      )\n    else:\n        self.up = torch.nn.ConvTranspose2d(input_channels , input_channels // 2, kernel_size = 2, stride = 2)\n        self.conv = double_convolution(\n                                       input_channels = input_channels,\n                                       mid_channels = output_channels,\n                                       output_channels = output_channels,\n                                       kernel_size = kernel_size,\n                                       normalization = normalization,\n                                       bias = bias,\n                                       activation = activation\n                                      )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.upsample_layer.forward","title":"<code>forward(x1, x2)</code>","text":"<p>Forward model.</p> <p>Parameters:</p> <ul> <li> <code>x1</code>           \u2013            <pre><code>         First input data.\n</code></pre> </li> <li> <code>x2</code>           \u2013            <pre><code>         Second input data.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>tensor</code> )          \u2013            <p>Result of the forward operation</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def forward(self, x1, x2):\n    \"\"\"\n    Forward model.\n\n    Parameters\n    ----------\n    x1             : torch.tensor\n                     First input data.\n    x2             : torch.tensor\n                     Second input data.\n\n\n    Returns\n    ----------\n    result        : torch.tensor\n                    Result of the forward operation\n    \"\"\" \n    x1 = self.up(x1)\n    diffY = x2.size()[2] - x1.size()[2]\n    diffX = x2.size()[3] - x1.size()[3]\n    x1 = torch.nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n                                      diffY // 2, diffY - diffY // 2])\n    x = torch.cat([x2, x1], dim = 1)\n    result = self.conv(x)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.gaussian","title":"<code>gaussian(x, multiplier=1.0)</code>","text":"<p>A Gaussian non-linear activation. For more details: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>       Input data.\n</code></pre> </li> <li> <code>multiplier</code>           \u2013            <pre><code>       Multiplier.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>float or tensor</code> )          \u2013            <p>Ouput data.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def gaussian(x, multiplier = 1.):\n    \"\"\"\n    A Gaussian non-linear activation.\n    For more details: Ramasinghe, Sameera, and Simon Lucey. \"Beyond periodicity: Towards a unifying framework for activations in coordinate-mlps.\" In European Conference on Computer Vision, pp. 142-158. Cham: Springer Nature Switzerland, 2022.\n\n    Parameters\n    ----------\n    x            : float or torch.tensor\n                   Input data.\n    multiplier   : float or torch.tensor\n                   Multiplier.\n\n    Returns\n    -------\n    result       : float or torch.tensor\n                   Ouput data.\n    \"\"\"\n    result = torch.exp(- (multiplier * x) ** 2)\n    return result\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.models.swish","title":"<code>swish(x)</code>","text":"<p>A swish non-linear activation. For more details: https://en.wikipedia.org/wiki/Swish_function</p> <p>Parameters:</p> <ul> <li> <code>x</code>           \u2013            <pre><code>         Input.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>out</code> (              <code>float or tensor</code> )          \u2013            <p>Output.</p> </li> </ul> Source code in <code>odak/learn/models/components.py</code> <pre><code>def swish(x):\n    \"\"\"\n    A swish non-linear activation.\n    For more details: https://en.wikipedia.org/wiki/Swish_function\n\n    Parameters\n    -----------\n    x              : float or torch.tensor\n                     Input.\n\n    Returns\n    -------\n    out            : float or torch.tensor\n                     Output.\n    \"\"\"\n    out = x * torch.sigmoid(x)\n    return out\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer","title":"<code>multi_color_hologram_optimizer</code>","text":"<p>A class for optimizing single or multi color holograms. For more details, see Kavakl\u0131 et al., SIGGRAPH ASIA 2023, Multi-color Holograms Improve Brightness in HOlographic Displays.</p> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>class multi_color_hologram_optimizer():\n    \"\"\"\n    A class for optimizing single or multi color holograms.\n    For more details, see Kavakl\u0131 et al., SIGGRAPH ASIA 2023, Multi-color Holograms Improve Brightness in HOlographic Displays.\n    \"\"\"\n    def __init__(self,\n                 wavelengths,\n                 resolution,\n                 targets,\n                 propagator,\n                 number_of_frames = 3,\n                 number_of_depth_layers = 1,\n                 learning_rate = 2e-2,\n                 learning_rate_floor = 5e-3,\n                 double_phase = True,\n                 scale_factor = 1,\n                 method = 'multi-color',\n                 channel_power_filename = '',\n                 device = None,\n                 loss_function = None,\n                 peak_amplitude = 1.0,\n                 optimize_peak_amplitude = False,\n                 img_loss_thres = 2e-3,\n                 reduction = 'sum'\n                ):\n        self.device = device\n        if isinstance(self.device, type(None)):\n            self.device = torch.device(\"cpu\")\n        torch.cuda.empty_cache()\n        torch.random.seed()\n        self.wavelengths = wavelengths\n        self.resolution = resolution\n        self.targets = targets\n        if propagator.propagation_type != 'Impulse Response Fresnel':\n            scale_factor = 1\n        self.scale_factor = scale_factor\n        self.propagator = propagator\n        self.learning_rate = learning_rate\n        self.learning_rate_floor = learning_rate_floor\n        self.number_of_channels = len(self.wavelengths)\n        self.number_of_frames = number_of_frames\n        self.number_of_depth_layers = number_of_depth_layers\n        self.double_phase = double_phase\n        self.channel_power_filename = channel_power_filename\n        self.method = method\n        if self.method != 'conventional' and self.method != 'multi-color':\n           logging.warning('Unknown optimization method. Options are conventional or multi-color.')\n           import sys\n           sys.exit()\n        self.peak_amplitude = peak_amplitude\n        self.optimize_peak_amplitude = optimize_peak_amplitude\n        if self.optimize_peak_amplitude:\n            self.init_peak_amplitude_scale()\n        self.img_loss_thres = img_loss_thres\n        self.kernels = []\n        self.init_phase()\n        self.init_channel_power()\n        self.init_loss_function(loss_function, reduction = reduction)\n        self.init_amplitude()\n        self.init_phase_scale()\n\n\n    def init_peak_amplitude_scale(self):\n        \"\"\"\n        Internal function to set the phase scale.\n        \"\"\"\n        self.peak_amplitude = torch.tensor(\n                                           self.peak_amplitude,\n                                           requires_grad = True,\n                                           device=self.device\n                                          )\n\n\n    def init_phase_scale(self):\n        \"\"\"\n        Internal function to set the phase scale.\n        \"\"\"\n        if self.method == 'conventional':\n            self.phase_scale = torch.tensor(\n                                            [\n                                             1.,\n                                             1.,\n                                             1.\n                                            ],\n                                            requires_grad = False,\n                                            device = self.device\n                                           )\n        if self.method == 'multi-color':\n            self.phase_scale = torch.tensor(\n                                            [\n                                             1.,\n                                             1.,\n                                             1.\n                                            ],\n                                            requires_grad = False,\n                                            device = self.device\n                                           )\n\n\n    def init_amplitude(self):\n        \"\"\"\n        Internal function to set the amplitude of the illumination source.\n        \"\"\"\n        self.amplitude = torch.zeros(\n                                     self.resolution[0] * self.scale_factor,\n                                     self.resolution[1] * self.scale_factor,\n                                     requires_grad = False,\n                                     device = self.device\n                                    )\n        self.amplitude[::self.scale_factor, ::self.scale_factor] = 1.\n\n\n    def init_phase(self):\n        \"\"\"\n        Internal function to set the starting phase of the phase-only hologram.\n        \"\"\"\n        self.phase = torch.zeros(\n                                 self.number_of_frames,\n                                 self.resolution[0],\n                                 self.resolution[1],\n                                 device = self.device,\n                                 requires_grad = True\n                                )\n        self.offset = torch.rand_like(self.phase, requires_grad = True, device = self.device)\n\n\n    def init_channel_power(self):\n        \"\"\"\n        Internal function to set the starting phase of the phase-only hologram.\n        \"\"\"\n        if self.method == 'conventional':\n            logging.warning('Scheme: Conventional')\n            self.channel_power = torch.eye(\n                                           self.number_of_frames,\n                                           self.number_of_channels,\n                                           device = self.device,\n                                           requires_grad = False\n                                          )\n\n        elif self.method == 'multi-color':\n            logging.warning('Scheme: Multi-color')\n            self.channel_power = torch.ones(\n                                            self.number_of_frames,\n                                            self.number_of_channels,\n                                            device = self.device,\n                                            requires_grad = True\n                                           )\n        if self.channel_power_filename != '':\n            self.channel_power = torch_load(self.channel_power_filename).to(self.device)\n            self.channel_power.requires_grad = False\n            self.channel_power[self.channel_power &lt; 0.] = 0.\n            self.channel_power[self.channel_power &gt; 1.] = 1.\n            if self.method == 'multi-color':\n                self.channel_power.requires_grad = True\n            if self.method == 'conventional':\n                self.channel_power = torch.abs(torch.cos(self.channel_power))\n            logging.warning('Channel powers:')\n            logging.warning(self.channel_power)\n            logging.warning('Channel powers loaded from {}.'.format(self.channel_power_filename))\n        self.propagator.set_laser_powers(self.channel_power)\n\n\n\n    def init_optimizer(self):\n        \"\"\"\n        Internal function to set the optimizer.\n        \"\"\"\n        optimization_variables = [self.phase, self.offset]\n        if self.optimize_peak_amplitude:\n            optimization_variables.append(self.peak_amplitude)\n        if self.method == 'multi-color':\n            optimization_variables.append(self.propagator.channel_power)\n        self.optimizer = torch.optim.Adam(optimization_variables, lr=self.learning_rate)\n\n\n    def init_loss_function(self, loss_function, reduction = 'sum'):\n        \"\"\"\n        Internal function to set the loss function.\n        \"\"\"\n        self.l2_loss = torch.nn.MSELoss(reduction = reduction)\n        self.loss_type = 'custom'\n        self.loss_function = loss_function\n        if isinstance(self.loss_function, type(None)):\n            self.loss_type = 'conventional'\n            self.loss_function = torch.nn.MSELoss(reduction = reduction)\n\n\n\n    def evaluate(self, input_image, target_image, plane_id = 0, noise_ratio = 1e-3, inject_noise = False):\n        \"\"\"\n        Internal function to evaluate the loss.\n        \"\"\"\n        if self.loss_type == 'conventional':\n            loss = self.loss_function(input_image, target_image)\n        elif self.loss_type == 'custom':\n            loss = 0\n            for i in range(len(self.wavelengths)):\n                loss += self.loss_function(\n                                           input_image[i],\n                                           target_image[i],\n                                           plane_id = plane_id,\n                                           noise_ratio = noise_ratio,\n                                           inject_noise = inject_noise\n                                          )\n        return loss\n\n\n    def double_phase_constrain(self, phase, phase_offset):\n        \"\"\"\n        Internal function to constrain a given phase similarly to double phase encoding.\n\n        Parameters\n        ----------\n        phase                      : torch.tensor\n                                     Input phase values to be constrained.\n        phase_offset               : torch.tensor\n                                     Input phase offset value.\n\n        Returns\n        -------\n        phase_only                 : torch.tensor\n                                     Constrained output phase.\n        \"\"\"\n        phase_zero_mean = phase - torch.mean(phase)\n        phase_low = torch.nan_to_num(phase_zero_mean - phase_offset, nan = 2 * torch.pi)\n        phase_high = torch.nan_to_num(phase_zero_mean + phase_offset, nan = 2 * torch.pi)\n        loss = multi_scale_total_variation_loss(phase_low, levels = 6)\n        loss += multi_scale_total_variation_loss(phase_high, levels = 6)\n        loss += torch.std(phase_low)\n        loss += torch.std(phase_high)\n        phase_only = torch.zeros_like(phase)\n        phase_only[0::2, 0::2] = phase_low[0::2, 0::2]\n        phase_only[0::2, 1::2] = phase_high[0::2, 1::2]\n        phase_only[1::2, 0::2] = phase_high[1::2, 0::2]\n        phase_only[1::2, 1::2] = phase_low[1::2, 1::2]\n        return phase_only, loss\n\n\n    def direct_phase_constrain(self, phase, phase_offset):\n        \"\"\"\n        Internal function to constrain a given phase.\n\n        Parameters\n        ----------\n        phase                      : torch.tensor\n                                     Input phase values to be constrained.\n        phase_offset               : torch.tensor\n                                     Input phase offset value.\n\n        Returns\n        -------\n        phase_only                 : torch.tensor\n                                     Constrained output phase.\n        \"\"\"\n        phase_only = torch.nan_to_num(phase - phase_offset, nan = 2 * torch.pi)\n        loss = multi_scale_total_variation_loss(phase, levels = 6)\n        loss += multi_scale_total_variation_loss(phase_offset, levels = 6)\n        return phase_only, loss\n\n\n    def gradient_descent(self, number_of_iterations=100, weights=[1., 1., 0., 0.], inject_noise = False, noise_ratio  = 1e-3):\n        \"\"\"\n        Function to optimize multiplane phase-only holograms using stochastic gradient descent.\n\n        Parameters\n        ----------\n        number_of_iterations       : float\n                                     Number of iterations.\n        weights                    : list\n                                     Weights used in the loss function.\n        inject_noise               : bool\n                                     When set True, this will inject noise with the given `noise_ratio` to the target images.\n        noise_ratio                : float\n                                     Noise ratio, a multiplier (1e-3 is 0.1 percent).\n\n        Returns\n        -------\n        hologram                   : torch.tensor\n                                     Optimised hologram.\n        \"\"\"\n        hologram_phases = torch.zeros(\n                                      self.number_of_frames,\n                                      self.resolution[0],\n                                      self.resolution[1],\n                                      device = self.device\n                                     )\n        t = tqdm(range(number_of_iterations), leave = False, dynamic_ncols = True)\n        if self.optimize_peak_amplitude:\n            peak_amp_cache = self.peak_amplitude.item()\n        for step in t:\n            for g in self.optimizer.param_groups:\n                g['lr'] -= (self.learning_rate - self.learning_rate_floor) / number_of_iterations\n                if g['lr'] &lt; self.learning_rate_floor:\n                    g['lr'] = self.learning_rate_floor\n                learning_rate = g['lr']\n            total_loss = 0\n            t_depth = tqdm(range(self.targets.shape[0]), leave = False, dynamic_ncols = True)\n            for depth_id in t_depth:\n                self.optimizer.zero_grad()\n                depth_target = self.targets[depth_id]\n                reconstruction_intensities = torch.zeros(\n                                                         self.number_of_frames,\n                                                         self.number_of_channels,\n                                                         self.resolution[0] * self.scale_factor,\n                                                         self.resolution[1] * self.scale_factor,\n                                                         device = self.device\n                                                        )\n                loss_variation_hologram = 0\n                laser_powers = self.propagator.get_laser_powers()\n                for frame_id in range(self.number_of_frames):\n                    if self.double_phase:\n                        phase, loss_phase = self.double_phase_constrain(\n                                                                        self.phase[frame_id],\n                                                                        self.offset[frame_id]\n                                                                       )\n                    else:\n                        phase, loss_phase = self.direct_phase_constrain(\n                                                                        self.phase[frame_id],\n                                                                        self.offset[frame_id]\n                                                                       )\n                    loss_variation_hologram += loss_phase\n                    for channel_id in range(self.number_of_channels):\n                        phase_scaled = torch.zeros_like(self.amplitude)\n                        phase_scaled[::self.scale_factor, ::self.scale_factor] = phase\n                        laser_power = laser_powers[frame_id][channel_id]\n                        hologram = generate_complex_field(\n                                                          laser_power * self.amplitude,\n                                                          phase_scaled * self.phase_scale[channel_id]\n                                                         )\n                        reconstruction_field = self.propagator(hologram, channel_id, depth_id)\n                        intensity = calculate_amplitude(reconstruction_field) ** 2\n                        reconstruction_intensities[frame_id, channel_id] += intensity\n                    hologram_phases[frame_id] = phase.detach().clone()\n                loss_laser = self.l2_loss(\n                                          torch.amax(depth_target, dim = (1, 2)) * self.peak_amplitude,\n                                          torch.sum(laser_powers, dim = 0)\n                                         )\n                loss_laser += self.l2_loss(\n                                           torch.tensor([self.number_of_frames * self.peak_amplitude]).to(self.device),\n                                           torch.sum(laser_powers).view(1,)\n                                          )\n                loss_laser += torch.cos(torch.min(torch.sum(laser_powers, dim = 1)))\n                reconstruction_intensity = torch.sum(reconstruction_intensities, dim=0)\n                loss_image = self.evaluate(\n                                           reconstruction_intensity,\n                                           depth_target * self.peak_amplitude,\n                                           noise_ratio = noise_ratio,\n                                           inject_noise = inject_noise,\n                                           plane_id = depth_id\n                                          )\n                loss = weights[0] * loss_image\n                loss += weights[1] * loss_laser\n                loss += weights[2] * loss_variation_hologram\n                include_pa_loss_flag = self.optimize_peak_amplitude and loss_image &lt; self.img_loss_thres\n                if include_pa_loss_flag:\n                    loss -= self.peak_amplitude * 1.\n                if self.method == 'conventional':\n                    loss.backward()\n                else:\n                    loss.backward(retain_graph = True)\n                self.optimizer.step()\n                if include_pa_loss_flag:\n                    peak_amp_cache = self.peak_amplitude.item()\n                else:\n                    with torch.no_grad():\n                        if self.optimize_peak_amplitude:\n                            self.peak_amplitude.view([1])[0] = peak_amp_cache\n                total_loss += loss.detach().item()\n                loss_image = loss_image.detach()\n                del loss_laser\n                del loss_variation_hologram\n                del loss\n            description = \"Loss:{:.3f} Loss Image:{:.3f} Peak Amp:{:.1f} Learning rate:{:.4f}\".format(total_loss, loss_image.item(), self.peak_amplitude, learning_rate)\n            t.set_description(description)\n            del total_loss\n            del loss_image\n            del reconstruction_field\n            del reconstruction_intensities\n            del intensity\n            del phase\n            del hologram\n        logging.warning(description)\n        return hologram_phases.detach()\n\n\n    def optimize(self, number_of_iterations=100, weights=[1., 1., 1.], bits = 8, inject_noise = False, noise_ratio = 1e-3):\n        \"\"\"\n        Function to optimize multiplane phase-only holograms.\n\n        Parameters\n        ----------\n        number_of_iterations       : int\n                                     Number of iterations.\n        weights                    : list\n                                     Loss weights.\n        bits                       : int\n                                     Quantizes the hologram using the given bits and reconstructs.\n        inject_noise               : bool\n                                     When set True, this will inject noise with the given `noise_ratio` to the target images.\n        noise_ratio                : float\n                                     Noise ratio, a multiplier (1e-3 is 0.1 percent).\n\n\n        Returns\n        -------\n        hologram_phases            : torch.tensor\n                                     Phases of the optimized phase-only hologram.\n        reconstruction_intensities : torch.tensor\n                                     Intensities of the images reconstructed at each plane with the optimized phase-only hologram.\n        \"\"\"\n        self.init_optimizer()\n        hologram_phases = self.gradient_descent(\n                                                number_of_iterations=number_of_iterations,\n                                                noise_ratio = noise_ratio,\n                                                inject_noise = inject_noise,\n                                                weights=weights\n                                               )\n        hologram_phases = quantize(hologram_phases % (2 * torch.pi), bits = bits, limits = [0., 2 * torch.pi]) / 2 ** bits * 2 * torch.pi\n        torch.no_grad()\n        reconstruction_intensities = self.propagator.reconstruct(hologram_phases)\n        laser_powers = self.propagator.get_laser_powers()\n        channel_powers = self.propagator.channel_power\n        logging.warning(\"Final peak amplitude: {}\".format(self.peak_amplitude))\n        logging.warning('Laser powers: {}'.format(laser_powers))\n        return hologram_phases, reconstruction_intensities, laser_powers, channel_powers, float(self.peak_amplitude)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.direct_phase_constrain","title":"<code>direct_phase_constrain(phase, phase_offset)</code>","text":"<p>Internal function to constrain a given phase.</p> <p>Parameters:</p> <ul> <li> <code>phase</code>           \u2013            <pre><code>                     Input phase values to be constrained.\n</code></pre> </li> <li> <code>phase_offset</code>           \u2013            <pre><code>                     Input phase offset value.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>phase_only</code> (              <code>tensor</code> )          \u2013            <p>Constrained output phase.</p> </li> </ul> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def direct_phase_constrain(self, phase, phase_offset):\n    \"\"\"\n    Internal function to constrain a given phase.\n\n    Parameters\n    ----------\n    phase                      : torch.tensor\n                                 Input phase values to be constrained.\n    phase_offset               : torch.tensor\n                                 Input phase offset value.\n\n    Returns\n    -------\n    phase_only                 : torch.tensor\n                                 Constrained output phase.\n    \"\"\"\n    phase_only = torch.nan_to_num(phase - phase_offset, nan = 2 * torch.pi)\n    loss = multi_scale_total_variation_loss(phase, levels = 6)\n    loss += multi_scale_total_variation_loss(phase_offset, levels = 6)\n    return phase_only, loss\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.double_phase_constrain","title":"<code>double_phase_constrain(phase, phase_offset)</code>","text":"<p>Internal function to constrain a given phase similarly to double phase encoding.</p> <p>Parameters:</p> <ul> <li> <code>phase</code>           \u2013            <pre><code>                     Input phase values to be constrained.\n</code></pre> </li> <li> <code>phase_offset</code>           \u2013            <pre><code>                     Input phase offset value.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>phase_only</code> (              <code>tensor</code> )          \u2013            <p>Constrained output phase.</p> </li> </ul> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def double_phase_constrain(self, phase, phase_offset):\n    \"\"\"\n    Internal function to constrain a given phase similarly to double phase encoding.\n\n    Parameters\n    ----------\n    phase                      : torch.tensor\n                                 Input phase values to be constrained.\n    phase_offset               : torch.tensor\n                                 Input phase offset value.\n\n    Returns\n    -------\n    phase_only                 : torch.tensor\n                                 Constrained output phase.\n    \"\"\"\n    phase_zero_mean = phase - torch.mean(phase)\n    phase_low = torch.nan_to_num(phase_zero_mean - phase_offset, nan = 2 * torch.pi)\n    phase_high = torch.nan_to_num(phase_zero_mean + phase_offset, nan = 2 * torch.pi)\n    loss = multi_scale_total_variation_loss(phase_low, levels = 6)\n    loss += multi_scale_total_variation_loss(phase_high, levels = 6)\n    loss += torch.std(phase_low)\n    loss += torch.std(phase_high)\n    phase_only = torch.zeros_like(phase)\n    phase_only[0::2, 0::2] = phase_low[0::2, 0::2]\n    phase_only[0::2, 1::2] = phase_high[0::2, 1::2]\n    phase_only[1::2, 0::2] = phase_high[1::2, 0::2]\n    phase_only[1::2, 1::2] = phase_low[1::2, 1::2]\n    return phase_only, loss\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.evaluate","title":"<code>evaluate(input_image, target_image, plane_id=0, noise_ratio=0.001, inject_noise=False)</code>","text":"<p>Internal function to evaluate the loss.</p> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def evaluate(self, input_image, target_image, plane_id = 0, noise_ratio = 1e-3, inject_noise = False):\n    \"\"\"\n    Internal function to evaluate the loss.\n    \"\"\"\n    if self.loss_type == 'conventional':\n        loss = self.loss_function(input_image, target_image)\n    elif self.loss_type == 'custom':\n        loss = 0\n        for i in range(len(self.wavelengths)):\n            loss += self.loss_function(\n                                       input_image[i],\n                                       target_image[i],\n                                       plane_id = plane_id,\n                                       noise_ratio = noise_ratio,\n                                       inject_noise = inject_noise\n                                      )\n    return loss\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.gradient_descent","title":"<code>gradient_descent(number_of_iterations=100, weights=[1.0, 1.0, 0.0, 0.0], inject_noise=False, noise_ratio=0.001)</code>","text":"<p>Function to optimize multiplane phase-only holograms using stochastic gradient descent.</p> <p>Parameters:</p> <ul> <li> <code>number_of_iterations</code>           \u2013            <pre><code>                     Number of iterations.\n</code></pre> </li> <li> <code>weights</code>           \u2013            <pre><code>                     Weights used in the loss function.\n</code></pre> </li> <li> <code>inject_noise</code>           \u2013            <pre><code>                     When set True, this will inject noise with the given `noise_ratio` to the target images.\n</code></pre> </li> <li> <code>noise_ratio</code>           \u2013            <pre><code>                     Noise ratio, a multiplier (1e-3 is 0.1 percent).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>hologram</code> (              <code>tensor</code> )          \u2013            <p>Optimised hologram.</p> </li> </ul> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def gradient_descent(self, number_of_iterations=100, weights=[1., 1., 0., 0.], inject_noise = False, noise_ratio  = 1e-3):\n    \"\"\"\n    Function to optimize multiplane phase-only holograms using stochastic gradient descent.\n\n    Parameters\n    ----------\n    number_of_iterations       : float\n                                 Number of iterations.\n    weights                    : list\n                                 Weights used in the loss function.\n    inject_noise               : bool\n                                 When set True, this will inject noise with the given `noise_ratio` to the target images.\n    noise_ratio                : float\n                                 Noise ratio, a multiplier (1e-3 is 0.1 percent).\n\n    Returns\n    -------\n    hologram                   : torch.tensor\n                                 Optimised hologram.\n    \"\"\"\n    hologram_phases = torch.zeros(\n                                  self.number_of_frames,\n                                  self.resolution[0],\n                                  self.resolution[1],\n                                  device = self.device\n                                 )\n    t = tqdm(range(number_of_iterations), leave = False, dynamic_ncols = True)\n    if self.optimize_peak_amplitude:\n        peak_amp_cache = self.peak_amplitude.item()\n    for step in t:\n        for g in self.optimizer.param_groups:\n            g['lr'] -= (self.learning_rate - self.learning_rate_floor) / number_of_iterations\n            if g['lr'] &lt; self.learning_rate_floor:\n                g['lr'] = self.learning_rate_floor\n            learning_rate = g['lr']\n        total_loss = 0\n        t_depth = tqdm(range(self.targets.shape[0]), leave = False, dynamic_ncols = True)\n        for depth_id in t_depth:\n            self.optimizer.zero_grad()\n            depth_target = self.targets[depth_id]\n            reconstruction_intensities = torch.zeros(\n                                                     self.number_of_frames,\n                                                     self.number_of_channels,\n                                                     self.resolution[0] * self.scale_factor,\n                                                     self.resolution[1] * self.scale_factor,\n                                                     device = self.device\n                                                    )\n            loss_variation_hologram = 0\n            laser_powers = self.propagator.get_laser_powers()\n            for frame_id in range(self.number_of_frames):\n                if self.double_phase:\n                    phase, loss_phase = self.double_phase_constrain(\n                                                                    self.phase[frame_id],\n                                                                    self.offset[frame_id]\n                                                                   )\n                else:\n                    phase, loss_phase = self.direct_phase_constrain(\n                                                                    self.phase[frame_id],\n                                                                    self.offset[frame_id]\n                                                                   )\n                loss_variation_hologram += loss_phase\n                for channel_id in range(self.number_of_channels):\n                    phase_scaled = torch.zeros_like(self.amplitude)\n                    phase_scaled[::self.scale_factor, ::self.scale_factor] = phase\n                    laser_power = laser_powers[frame_id][channel_id]\n                    hologram = generate_complex_field(\n                                                      laser_power * self.amplitude,\n                                                      phase_scaled * self.phase_scale[channel_id]\n                                                     )\n                    reconstruction_field = self.propagator(hologram, channel_id, depth_id)\n                    intensity = calculate_amplitude(reconstruction_field) ** 2\n                    reconstruction_intensities[frame_id, channel_id] += intensity\n                hologram_phases[frame_id] = phase.detach().clone()\n            loss_laser = self.l2_loss(\n                                      torch.amax(depth_target, dim = (1, 2)) * self.peak_amplitude,\n                                      torch.sum(laser_powers, dim = 0)\n                                     )\n            loss_laser += self.l2_loss(\n                                       torch.tensor([self.number_of_frames * self.peak_amplitude]).to(self.device),\n                                       torch.sum(laser_powers).view(1,)\n                                      )\n            loss_laser += torch.cos(torch.min(torch.sum(laser_powers, dim = 1)))\n            reconstruction_intensity = torch.sum(reconstruction_intensities, dim=0)\n            loss_image = self.evaluate(\n                                       reconstruction_intensity,\n                                       depth_target * self.peak_amplitude,\n                                       noise_ratio = noise_ratio,\n                                       inject_noise = inject_noise,\n                                       plane_id = depth_id\n                                      )\n            loss = weights[0] * loss_image\n            loss += weights[1] * loss_laser\n            loss += weights[2] * loss_variation_hologram\n            include_pa_loss_flag = self.optimize_peak_amplitude and loss_image &lt; self.img_loss_thres\n            if include_pa_loss_flag:\n                loss -= self.peak_amplitude * 1.\n            if self.method == 'conventional':\n                loss.backward()\n            else:\n                loss.backward(retain_graph = True)\n            self.optimizer.step()\n            if include_pa_loss_flag:\n                peak_amp_cache = self.peak_amplitude.item()\n            else:\n                with torch.no_grad():\n                    if self.optimize_peak_amplitude:\n                        self.peak_amplitude.view([1])[0] = peak_amp_cache\n            total_loss += loss.detach().item()\n            loss_image = loss_image.detach()\n            del loss_laser\n            del loss_variation_hologram\n            del loss\n        description = \"Loss:{:.3f} Loss Image:{:.3f} Peak Amp:{:.1f} Learning rate:{:.4f}\".format(total_loss, loss_image.item(), self.peak_amplitude, learning_rate)\n        t.set_description(description)\n        del total_loss\n        del loss_image\n        del reconstruction_field\n        del reconstruction_intensities\n        del intensity\n        del phase\n        del hologram\n    logging.warning(description)\n    return hologram_phases.detach()\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.init_amplitude","title":"<code>init_amplitude()</code>","text":"<p>Internal function to set the amplitude of the illumination source.</p> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def init_amplitude(self):\n    \"\"\"\n    Internal function to set the amplitude of the illumination source.\n    \"\"\"\n    self.amplitude = torch.zeros(\n                                 self.resolution[0] * self.scale_factor,\n                                 self.resolution[1] * self.scale_factor,\n                                 requires_grad = False,\n                                 device = self.device\n                                )\n    self.amplitude[::self.scale_factor, ::self.scale_factor] = 1.\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.init_channel_power","title":"<code>init_channel_power()</code>","text":"<p>Internal function to set the starting phase of the phase-only hologram.</p> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def init_channel_power(self):\n    \"\"\"\n    Internal function to set the starting phase of the phase-only hologram.\n    \"\"\"\n    if self.method == 'conventional':\n        logging.warning('Scheme: Conventional')\n        self.channel_power = torch.eye(\n                                       self.number_of_frames,\n                                       self.number_of_channels,\n                                       device = self.device,\n                                       requires_grad = False\n                                      )\n\n    elif self.method == 'multi-color':\n        logging.warning('Scheme: Multi-color')\n        self.channel_power = torch.ones(\n                                        self.number_of_frames,\n                                        self.number_of_channels,\n                                        device = self.device,\n                                        requires_grad = True\n                                       )\n    if self.channel_power_filename != '':\n        self.channel_power = torch_load(self.channel_power_filename).to(self.device)\n        self.channel_power.requires_grad = False\n        self.channel_power[self.channel_power &lt; 0.] = 0.\n        self.channel_power[self.channel_power &gt; 1.] = 1.\n        if self.method == 'multi-color':\n            self.channel_power.requires_grad = True\n        if self.method == 'conventional':\n            self.channel_power = torch.abs(torch.cos(self.channel_power))\n        logging.warning('Channel powers:')\n        logging.warning(self.channel_power)\n        logging.warning('Channel powers loaded from {}.'.format(self.channel_power_filename))\n    self.propagator.set_laser_powers(self.channel_power)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.init_loss_function","title":"<code>init_loss_function(loss_function, reduction='sum')</code>","text":"<p>Internal function to set the loss function.</p> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def init_loss_function(self, loss_function, reduction = 'sum'):\n    \"\"\"\n    Internal function to set the loss function.\n    \"\"\"\n    self.l2_loss = torch.nn.MSELoss(reduction = reduction)\n    self.loss_type = 'custom'\n    self.loss_function = loss_function\n    if isinstance(self.loss_function, type(None)):\n        self.loss_type = 'conventional'\n        self.loss_function = torch.nn.MSELoss(reduction = reduction)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.init_optimizer","title":"<code>init_optimizer()</code>","text":"<p>Internal function to set the optimizer.</p> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def init_optimizer(self):\n    \"\"\"\n    Internal function to set the optimizer.\n    \"\"\"\n    optimization_variables = [self.phase, self.offset]\n    if self.optimize_peak_amplitude:\n        optimization_variables.append(self.peak_amplitude)\n    if self.method == 'multi-color':\n        optimization_variables.append(self.propagator.channel_power)\n    self.optimizer = torch.optim.Adam(optimization_variables, lr=self.learning_rate)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.init_peak_amplitude_scale","title":"<code>init_peak_amplitude_scale()</code>","text":"<p>Internal function to set the phase scale.</p> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def init_peak_amplitude_scale(self):\n    \"\"\"\n    Internal function to set the phase scale.\n    \"\"\"\n    self.peak_amplitude = torch.tensor(\n                                       self.peak_amplitude,\n                                       requires_grad = True,\n                                       device=self.device\n                                      )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.init_phase","title":"<code>init_phase()</code>","text":"<p>Internal function to set the starting phase of the phase-only hologram.</p> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def init_phase(self):\n    \"\"\"\n    Internal function to set the starting phase of the phase-only hologram.\n    \"\"\"\n    self.phase = torch.zeros(\n                             self.number_of_frames,\n                             self.resolution[0],\n                             self.resolution[1],\n                             device = self.device,\n                             requires_grad = True\n                            )\n    self.offset = torch.rand_like(self.phase, requires_grad = True, device = self.device)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.init_phase_scale","title":"<code>init_phase_scale()</code>","text":"<p>Internal function to set the phase scale.</p> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def init_phase_scale(self):\n    \"\"\"\n    Internal function to set the phase scale.\n    \"\"\"\n    if self.method == 'conventional':\n        self.phase_scale = torch.tensor(\n                                        [\n                                         1.,\n                                         1.,\n                                         1.\n                                        ],\n                                        requires_grad = False,\n                                        device = self.device\n                                       )\n    if self.method == 'multi-color':\n        self.phase_scale = torch.tensor(\n                                        [\n                                         1.,\n                                         1.,\n                                         1.\n                                        ],\n                                        requires_grad = False,\n                                        device = self.device\n                                       )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.optimizers.multi_color_hologram_optimizer.optimize","title":"<code>optimize(number_of_iterations=100, weights=[1.0, 1.0, 1.0], bits=8, inject_noise=False, noise_ratio=0.001)</code>","text":"<p>Function to optimize multiplane phase-only holograms.</p> <p>Parameters:</p> <ul> <li> <code>number_of_iterations</code>           \u2013            <pre><code>                     Number of iterations.\n</code></pre> </li> <li> <code>weights</code>           \u2013            <pre><code>                     Loss weights.\n</code></pre> </li> <li> <code>bits</code>           \u2013            <pre><code>                     Quantizes the hologram using the given bits and reconstructs.\n</code></pre> </li> <li> <code>inject_noise</code>           \u2013            <pre><code>                     When set True, this will inject noise with the given `noise_ratio` to the target images.\n</code></pre> </li> <li> <code>noise_ratio</code>           \u2013            <pre><code>                     Noise ratio, a multiplier (1e-3 is 0.1 percent).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>hologram_phases</code> (              <code>tensor</code> )          \u2013            <p>Phases of the optimized phase-only hologram.</p> </li> <li> <code>reconstruction_intensities</code> (              <code>tensor</code> )          \u2013            <p>Intensities of the images reconstructed at each plane with the optimized phase-only hologram.</p> </li> </ul> Source code in <code>odak/learn/wave/optimizers.py</code> <pre><code>def optimize(self, number_of_iterations=100, weights=[1., 1., 1.], bits = 8, inject_noise = False, noise_ratio = 1e-3):\n    \"\"\"\n    Function to optimize multiplane phase-only holograms.\n\n    Parameters\n    ----------\n    number_of_iterations       : int\n                                 Number of iterations.\n    weights                    : list\n                                 Loss weights.\n    bits                       : int\n                                 Quantizes the hologram using the given bits and reconstructs.\n    inject_noise               : bool\n                                 When set True, this will inject noise with the given `noise_ratio` to the target images.\n    noise_ratio                : float\n                                 Noise ratio, a multiplier (1e-3 is 0.1 percent).\n\n\n    Returns\n    -------\n    hologram_phases            : torch.tensor\n                                 Phases of the optimized phase-only hologram.\n    reconstruction_intensities : torch.tensor\n                                 Intensities of the images reconstructed at each plane with the optimized phase-only hologram.\n    \"\"\"\n    self.init_optimizer()\n    hologram_phases = self.gradient_descent(\n                                            number_of_iterations=number_of_iterations,\n                                            noise_ratio = noise_ratio,\n                                            inject_noise = inject_noise,\n                                            weights=weights\n                                           )\n    hologram_phases = quantize(hologram_phases % (2 * torch.pi), bits = bits, limits = [0., 2 * torch.pi]) / 2 ** bits * 2 * torch.pi\n    torch.no_grad()\n    reconstruction_intensities = self.propagator.reconstruct(hologram_phases)\n    laser_powers = self.propagator.get_laser_powers()\n    channel_powers = self.propagator.channel_power\n    logging.warning(\"Final peak amplitude: {}\".format(self.peak_amplitude))\n    logging.warning('Laser powers: {}'.format(laser_powers))\n    return hologram_phases, reconstruction_intensities, laser_powers, channel_powers, float(self.peak_amplitude)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator","title":"<code>propagator</code>","text":"<p>A light propagation model that propagates light to desired image plane with two separate propagations.  We use this class in our various works including <code>Kavakl\u0131 et al., Realistic Defocus Blur for Multiplane Computer-Generated Holography</code>.</p> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>class propagator():\n    \"\"\"\n    A light propagation model that propagates light to desired image plane with two separate propagations. \n    We use this class in our various works including `Kavakl\u0131 et al., Realistic Defocus Blur for Multiplane Computer-Generated Holography`.\n    \"\"\"\n    def __init__(\n                 self,\n                 resolution = [1920, 1080],\n                 wavelengths = [515e-9,],\n                 pixel_pitch = 8e-6,\n                 resolution_factor = 1,\n                 number_of_frames = 1,\n                 number_of_depth_layers = 1,\n                 volume_depth = 1e-2,\n                 image_location_offset = 5e-3,\n                 propagation_type = 'Bandlimited Angular Spectrum',\n                 propagator_type = 'back and forth',\n                 back_and_forth_distance = 0.3,\n                 laser_channel_power = None,\n                 aperture = None,\n                 aperture_size = None,\n                 distances = None,\n                 aperture_samples = [20, 20, 5, 5],\n                 method = 'conventional',\n                 device = torch.device('cpu')\n                ):\n        \"\"\"\n        Parameters\n        ----------\n        resolution              : list\n                                  Resolution.\n        wavelengths             : float\n                                  Wavelength of light in meters.\n        pixel_pitch             : float\n                                  Pixel pitch in meters.\n        resolution_factor       : int\n                                  Resolution factor for scaled simulations.\n        number_of_frames        : int\n                                  Number of hologram frames.\n                                  Typically, there are three frames, each one for a single color primary.\n        number_of_depth_layers  : int\n                                  Equ-distance number of depth layers within the desired volume. If `distances` parameter is passed, this value will be automatically set to the length of the `distances` verson provided.\n        volume_depth            : float\n                                  Width of the volume along the propagation direction.\n        image_location_offset   : float\n                                  Center of the volume along the propagation direction.\n        propagation_type        : str\n                                  Propagation type. \n                                  See ropagate_beam() and odak.learn.wave.get_propagation_kernel() for more.\n        propagator_type         : str\n                                  Propagator type.\n                                  The options are `back and forth` and `forward` propagators.\n        back_and_forth_distance : float\n                                  Zero mode distance for `back and forth` propagator type.\n        laser_channel_power     : torch.tensor\n                                  Laser channel powers for given number of frames and number of wavelengths.\n        aperture                : torch.tensor\n                                  Aperture at the Fourier plane.\n        aperture_size           : float\n                                  Aperture width for a circular aperture.\n        aperture_samples        : list\n                                  When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for hologram plane pixel and the last two is for image plane pixel.\n        distances               : torch.tensor\n                                  Propagation distances in meters.\n        method                  : str\n                                  Hologram type conventional or multi-color.\n        device                  : torch.device\n                                  Device to be used for computation. For more see torch.device().\n        \"\"\"\n        self.device = device\n        self.pixel_pitch = pixel_pitch\n        self.wavelengths = wavelengths\n        self.resolution = resolution\n        self.propagation_type = propagation_type\n        if self.propagation_type != 'Impulse Response Fresnel':\n            resolution_factor = 1\n        self.resolution_factor = resolution_factor\n        self.number_of_frames = number_of_frames\n        self.number_of_depth_layers = number_of_depth_layers\n        self.number_of_channels = len(self.wavelengths)\n        self.volume_depth = volume_depth\n        self.image_location_offset = image_location_offset\n        self.propagator_type = propagator_type\n        self.aperture_samples = aperture_samples\n        self.zero_mode_distance = torch.tensor(back_and_forth_distance, device = device)\n        self.method = method\n        self.aperture = aperture\n        self.init_distances(distances)\n        self.init_kernels()\n        self.init_channel_power(laser_channel_power)\n        self.init_phase_scale()\n        self.set_aperture(aperture, aperture_size)\n\n\n    def init_distances(self, distances):\n        \"\"\"\n        Internal function to initialize distances.\n\n        Parameters\n        ----------\n        distances               : torch.tensor\n                                  Propagation distances.\n        \"\"\"\n        if isinstance(distances, type(None)):\n            self.distances = torch.linspace(-self.volume_depth / 2., self.volume_depth / 2., self.number_of_depth_layers) + self.image_location_offset\n        else:\n            self.distances = torch.as_tensor(distances)\n            self.number_of_depth_layers = self.distances.shape[0]\n        logging.warning('Distances: {}'.format(self.distances))\n\n\n    def init_kernels(self):\n        \"\"\"\n        Internal function to initialize kernels.\n        \"\"\"\n        self.generated_kernels = torch.zeros(\n                                             self.number_of_depth_layers,\n                                             self.number_of_channels,\n                                             device = self.device\n                                            )\n        self.kernels = torch.zeros(\n                                   self.number_of_depth_layers,\n                                   self.number_of_channels,\n                                   self.resolution[0] * self.resolution_factor * 2,\n                                   self.resolution[1] * self.resolution_factor * 2,\n                                   dtype = torch.complex64,\n                                   device = self.device\n                                  )\n\n\n    def init_channel_power(self, channel_power):\n        \"\"\"\n        Internal function to set the starting phase of the phase-only hologram.\n        \"\"\"\n        self.channel_power = channel_power\n        if isinstance(self.channel_power, type(None)):\n            self.channel_power = torch.eye(\n                                           self.number_of_frames,\n                                           self.number_of_channels,\n                                           device = self.device,\n                                           requires_grad = False\n                                          )\n\n\n    def init_phase_scale(self):\n        \"\"\"\n        Internal function to set the phase scale.\n        In some cases, you may want to modify this init to ratio phases for different color primaries as an SLM is configured for a specific central wavelength.\n        \"\"\"\n        self.phase_scale = torch.tensor(\n                                        [\n                                         1.,\n                                         1.,\n                                         1.\n                                        ],\n                                        requires_grad = False,\n                                        device = self.device\n                                       )\n\n\n    def set_aperture(self, aperture = None, aperture_size = None):\n        \"\"\"\n        Set aperture in the Fourier plane.\n\n\n        Parameters\n        ----------\n        aperture        : torch.tensor\n                          Aperture at the original resolution of a hologram.\n                          If aperture is provided as None, it will assign a circular aperture at the size of the short edge (width or height).\n        aperture_size   : int\n                          If no aperture is provided, this will determine the size of the circular aperture.\n        \"\"\"\n        if isinstance(aperture, type(None)):\n            if isinstance(aperture_size, type(None)):\n                aperture_size = torch.max(\n                                          torch.tensor([\n                                                        self.resolution[0] * self.resolution_factor, \n                                                        self.resolution[1] * self.resolution_factor\n                                                       ])\n                                         )\n            self.aperture = circular_binary_mask(\n                                                 self.resolution[0] * self.resolution_factor * 2,\n                                                 self.resolution[1] * self.resolution_factor * 2,\n                                                 aperture_size,\n                                                ).to(self.device) * 1.\n        else:\n            self.aperture = zero_pad(aperture).to(self.device) * 1.\n\n\n    def get_laser_powers(self):\n        \"\"\"\n        Internal function to get the laser powers.\n\n        Returns\n        -------\n        laser_power      : torch.tensor\n                           Laser powers.\n        \"\"\"\n        if self.method == 'conventional':\n            laser_power = self.channel_power\n        if self.method == 'multi-color':\n            laser_power = torch.abs(torch.cos(self.channel_power))\n        return laser_power\n\n\n    def set_laser_powers(self, laser_power):\n        \"\"\"\n        Internal function to set the laser powers.\n\n        Parameters\n        -------\n        laser_power      : torch.tensor\n                           Laser powers.\n        \"\"\"\n        self.channel_power = laser_power\n\n\n\n    def get_kernels(self):\n        \"\"\"\n        Function to return the kernels used in the light transport.\n\n        Returns\n        -------\n        kernels           : torch.tensor\n                            Kernel amplitudes.\n        \"\"\"\n        h = torch.fft.ifftshift(torch.fft.ifft2(torch.fft.ifftshift(self.kernels)))\n        kernels_amplitude = calculate_amplitude(h)\n        kernels_phase = calculate_phase(h)\n        return kernels_amplitude, kernels_phase\n\n\n    def __call__(self, input_field, channel_id, depth_id):\n        \"\"\"\n        Function that represents the forward model in hologram optimization.\n\n        Parameters\n        ----------\n        input_field         : torch.tensor\n                              Input complex input field.\n        channel_id          : int\n                              Identifying the color primary to be used.\n        depth_id            : int\n                              Identifying the depth layer to be used.\n\n        Returns\n        -------\n        output_field        : torch.tensor\n                              Propagated output complex field.\n        \"\"\"\n        distance = self.distances[depth_id]\n        if not self.generated_kernels[depth_id, channel_id]:\n            if self.propagator_type == 'forward':\n                H = get_propagation_kernel(\n                                           nu = self.resolution[0] * 2,\n                                           nv = self.resolution[1] * 2,\n                                           dx = self.pixel_pitch,\n                                           wavelength = self.wavelengths[channel_id],\n                                           distance = distance,\n                                           device = self.device,\n                                           propagation_type = self.propagation_type,\n                                           samples = self.aperture_samples,\n                                           scale = self.resolution_factor\n                                          )\n            elif self.propagator_type == 'back and forth':\n                H_forward = get_propagation_kernel(\n                                                   nu = self.resolution[0] * 2,\n                                                   nv = self.resolution[1] * 2,\n                                                   dx = self.pixel_pitch,\n                                                   wavelength = self.wavelengths[channel_id],\n                                                   distance = self.zero_mode_distance,\n                                                   device = self.device,\n                                                   propagation_type = self.propagation_type,\n                                                   samples = self.aperture_samples,\n                                                   scale = self.resolution_factor\n                                                  )\n                distance_back = -(self.zero_mode_distance + self.image_location_offset - distance)\n                H_back = get_propagation_kernel(\n                                                nu = self.resolution[0] * 2,\n                                                nv = self.resolution[1] * 2,\n                                                dx = self.pixel_pitch,\n                                                wavelength = self.wavelengths[channel_id],\n                                                distance = distance_back,\n                                                device = self.device,\n                                                propagation_type = self.propagation_type,\n                                                samples = self.aperture_samples,\n                                                scale = self.resolution_factor\n                                               )\n                H = H_forward * H_back\n            self.kernels[depth_id, channel_id] = H\n            self.generated_kernels[depth_id, channel_id] = True\n        else:\n            H = self.kernels[depth_id, channel_id].detach().clone()\n        field_scale = input_field\n        field_scale_padded = zero_pad(field_scale)\n        output_field_padded = custom(field_scale_padded, H, aperture = self.aperture)\n        output_field = crop_center(output_field_padded)\n        return output_field\n\n\n    def reconstruct(self, hologram_phases, amplitude = None, no_grad = True, get_complex = False):\n        \"\"\"\n        Internal function to reconstruct a given hologram.\n\n\n        Parameters\n        ----------\n        hologram_phases            : torch.tensor\n                                     Hologram phases [ch x m x n].\n        amplitude                  : torch.tensor\n                                     Amplitude profiles for each color primary [ch x m x n]\n        no_grad                    : bool\n                                     If set True, uses torch.no_grad in reconstruction.\n        get_complex                : bool\n                                     If set True, reconstructor returns the complex field but not the intensities.\n\n        Returns\n        -------\n        reconstructions            : torch.tensor\n                                     Reconstructed frames.\n        \"\"\"\n        if no_grad:\n            torch.no_grad()\n        if len(hologram_phases.shape) &gt; 3:\n            hologram_phases = hologram_phases.squeeze(0)\n        if get_complex == True:\n            reconstruction_type = torch.complex64\n        else:\n            reconstruction_type = torch.float32\n        if hologram_phases.shape[0] != self.number_of_frames:\n            logging.warning('Provided hologram frame count is {} but the configured number of frames is {}.'.format(hologram_phases.shape[0], self.number_of_frames))\n        reconstructions = torch.zeros(\n                                      self.number_of_frames,\n                                      self.number_of_depth_layers,\n                                      self.number_of_channels,\n                                      self.resolution[0] * self.resolution_factor,\n                                      self.resolution[1] * self.resolution_factor,\n                                      dtype = reconstruction_type,\n                                      device = self.device\n                                     )\n        if isinstance(amplitude, type(None)):\n            amplitude = torch.zeros(\n                                    self.number_of_channels,\n                                    self.resolution[0] * self.resolution_factor,\n                                    self.resolution[1] * self.resolution_factor,\n                                    device = self.device\n                                   )\n            amplitude[:, ::self.resolution_factor, ::self.resolution_factor] = 1.\n        if self.resolution_factor != 1:\n            hologram_phases_scaled = torch.zeros_like(amplitude)\n            hologram_phases_scaled[\n                                   :,\n                                   ::self.resolution_factor,\n                                   ::self.resolution_factor\n                                  ] = hologram_phases\n        else:\n            hologram_phases_scaled = hologram_phases\n        for frame_id in range(self.number_of_frames):\n            for depth_id in range(self.number_of_depth_layers):\n                for channel_id in range(self.number_of_channels):\n                    laser_power = self.get_laser_powers()[frame_id][channel_id]\n                    phase = hologram_phases_scaled[frame_id]\n                    hologram = generate_complex_field(\n                                                      laser_power * amplitude[channel_id],\n                                                      phase * self.phase_scale[channel_id]\n                                                     )\n                    reconstruction_field = self.__call__(hologram, channel_id, depth_id)\n                    if get_complex == True:\n                        result = reconstruction_field\n                    else:\n                        result = calculate_amplitude(reconstruction_field) ** 2\n\n                    if no_grad: \n                        result = result.detach().clone()\n\n                    reconstructions[\n                                    frame_id,\n                                    depth_id,\n                                    channel_id\n                                   ] = result\n\n        return reconstructions\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator.__call__","title":"<code>__call__(input_field, channel_id, depth_id)</code>","text":"<p>Function that represents the forward model in hologram optimization.</p> <p>Parameters:</p> <ul> <li> <code>input_field</code>           \u2013            <pre><code>              Input complex input field.\n</code></pre> </li> <li> <code>channel_id</code>           \u2013            <pre><code>              Identifying the color primary to be used.\n</code></pre> </li> <li> <code>depth_id</code>           \u2013            <pre><code>              Identifying the depth layer to be used.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output_field</code> (              <code>tensor</code> )          \u2013            <p>Propagated output complex field.</p> </li> </ul> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def __call__(self, input_field, channel_id, depth_id):\n    \"\"\"\n    Function that represents the forward model in hologram optimization.\n\n    Parameters\n    ----------\n    input_field         : torch.tensor\n                          Input complex input field.\n    channel_id          : int\n                          Identifying the color primary to be used.\n    depth_id            : int\n                          Identifying the depth layer to be used.\n\n    Returns\n    -------\n    output_field        : torch.tensor\n                          Propagated output complex field.\n    \"\"\"\n    distance = self.distances[depth_id]\n    if not self.generated_kernels[depth_id, channel_id]:\n        if self.propagator_type == 'forward':\n            H = get_propagation_kernel(\n                                       nu = self.resolution[0] * 2,\n                                       nv = self.resolution[1] * 2,\n                                       dx = self.pixel_pitch,\n                                       wavelength = self.wavelengths[channel_id],\n                                       distance = distance,\n                                       device = self.device,\n                                       propagation_type = self.propagation_type,\n                                       samples = self.aperture_samples,\n                                       scale = self.resolution_factor\n                                      )\n        elif self.propagator_type == 'back and forth':\n            H_forward = get_propagation_kernel(\n                                               nu = self.resolution[0] * 2,\n                                               nv = self.resolution[1] * 2,\n                                               dx = self.pixel_pitch,\n                                               wavelength = self.wavelengths[channel_id],\n                                               distance = self.zero_mode_distance,\n                                               device = self.device,\n                                               propagation_type = self.propagation_type,\n                                               samples = self.aperture_samples,\n                                               scale = self.resolution_factor\n                                              )\n            distance_back = -(self.zero_mode_distance + self.image_location_offset - distance)\n            H_back = get_propagation_kernel(\n                                            nu = self.resolution[0] * 2,\n                                            nv = self.resolution[1] * 2,\n                                            dx = self.pixel_pitch,\n                                            wavelength = self.wavelengths[channel_id],\n                                            distance = distance_back,\n                                            device = self.device,\n                                            propagation_type = self.propagation_type,\n                                            samples = self.aperture_samples,\n                                            scale = self.resolution_factor\n                                           )\n            H = H_forward * H_back\n        self.kernels[depth_id, channel_id] = H\n        self.generated_kernels[depth_id, channel_id] = True\n    else:\n        H = self.kernels[depth_id, channel_id].detach().clone()\n    field_scale = input_field\n    field_scale_padded = zero_pad(field_scale)\n    output_field_padded = custom(field_scale_padded, H, aperture = self.aperture)\n    output_field = crop_center(output_field_padded)\n    return output_field\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator.__init__","title":"<code>__init__(resolution=[1920, 1080], wavelengths=[5.15e-07], pixel_pitch=8e-06, resolution_factor=1, number_of_frames=1, number_of_depth_layers=1, volume_depth=0.01, image_location_offset=0.005, propagation_type='Bandlimited Angular Spectrum', propagator_type='back and forth', back_and_forth_distance=0.3, laser_channel_power=None, aperture=None, aperture_size=None, distances=None, aperture_samples=[20, 20, 5, 5], method='conventional', device=torch.device('cpu'))</code>","text":"<p>Parameters:</p> <ul> <li> <code>resolution</code>           \u2013            <pre><code>                  Resolution.\n</code></pre> </li> <li> <code>wavelengths</code>           \u2013            <pre><code>                  Wavelength of light in meters.\n</code></pre> </li> <li> <code>pixel_pitch</code>           \u2013            <pre><code>                  Pixel pitch in meters.\n</code></pre> </li> <li> <code>resolution_factor</code>           \u2013            <pre><code>                  Resolution factor for scaled simulations.\n</code></pre> </li> <li> <code>number_of_frames</code>           \u2013            <pre><code>                  Number of hologram frames.\n                  Typically, there are three frames, each one for a single color primary.\n</code></pre> </li> <li> <code>number_of_depth_layers</code>           \u2013            <pre><code>                  Equ-distance number of depth layers within the desired volume. If `distances` parameter is passed, this value will be automatically set to the length of the `distances` verson provided.\n</code></pre> </li> <li> <code>volume_depth</code>           \u2013            <pre><code>                  Width of the volume along the propagation direction.\n</code></pre> </li> <li> <code>image_location_offset</code>           \u2013            <pre><code>                  Center of the volume along the propagation direction.\n</code></pre> </li> <li> <code>propagation_type</code>           \u2013            <pre><code>                  Propagation type. \n                  See ropagate_beam() and odak.learn.wave.get_propagation_kernel() for more.\n</code></pre> </li> <li> <code>propagator_type</code>           \u2013            <pre><code>                  Propagator type.\n                  The options are `back and forth` and `forward` propagators.\n</code></pre> </li> <li> <code>back_and_forth_distance</code>               (<code>float</code>, default:                   <code>0.3</code> )           \u2013            <pre><code>                  Zero mode distance for `back and forth` propagator type.\n</code></pre> </li> <li> <code>laser_channel_power</code>           \u2013            <pre><code>                  Laser channel powers for given number of frames and number of wavelengths.\n</code></pre> </li> <li> <code>aperture</code>           \u2013            <pre><code>                  Aperture at the Fourier plane.\n</code></pre> </li> <li> <code>aperture_size</code>           \u2013            <pre><code>                  Aperture width for a circular aperture.\n</code></pre> </li> <li> <code>aperture_samples</code>           \u2013            <pre><code>                  When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for hologram plane pixel and the last two is for image plane pixel.\n</code></pre> </li> <li> <code>distances</code>           \u2013            <pre><code>                  Propagation distances in meters.\n</code></pre> </li> <li> <code>method</code>           \u2013            <pre><code>                  Hologram type conventional or multi-color.\n</code></pre> </li> <li> <code>device</code>           \u2013            <pre><code>                  Device to be used for computation. For more see torch.device().\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def __init__(\n             self,\n             resolution = [1920, 1080],\n             wavelengths = [515e-9,],\n             pixel_pitch = 8e-6,\n             resolution_factor = 1,\n             number_of_frames = 1,\n             number_of_depth_layers = 1,\n             volume_depth = 1e-2,\n             image_location_offset = 5e-3,\n             propagation_type = 'Bandlimited Angular Spectrum',\n             propagator_type = 'back and forth',\n             back_and_forth_distance = 0.3,\n             laser_channel_power = None,\n             aperture = None,\n             aperture_size = None,\n             distances = None,\n             aperture_samples = [20, 20, 5, 5],\n             method = 'conventional',\n             device = torch.device('cpu')\n            ):\n    \"\"\"\n    Parameters\n    ----------\n    resolution              : list\n                              Resolution.\n    wavelengths             : float\n                              Wavelength of light in meters.\n    pixel_pitch             : float\n                              Pixel pitch in meters.\n    resolution_factor       : int\n                              Resolution factor for scaled simulations.\n    number_of_frames        : int\n                              Number of hologram frames.\n                              Typically, there are three frames, each one for a single color primary.\n    number_of_depth_layers  : int\n                              Equ-distance number of depth layers within the desired volume. If `distances` parameter is passed, this value will be automatically set to the length of the `distances` verson provided.\n    volume_depth            : float\n                              Width of the volume along the propagation direction.\n    image_location_offset   : float\n                              Center of the volume along the propagation direction.\n    propagation_type        : str\n                              Propagation type. \n                              See ropagate_beam() and odak.learn.wave.get_propagation_kernel() for more.\n    propagator_type         : str\n                              Propagator type.\n                              The options are `back and forth` and `forward` propagators.\n    back_and_forth_distance : float\n                              Zero mode distance for `back and forth` propagator type.\n    laser_channel_power     : torch.tensor\n                              Laser channel powers for given number of frames and number of wavelengths.\n    aperture                : torch.tensor\n                              Aperture at the Fourier plane.\n    aperture_size           : float\n                              Aperture width for a circular aperture.\n    aperture_samples        : list\n                              When using `Impulse Response Fresnel` propagation, these sample counts along X and Y will be used to represent a rectangular aperture. First two is for hologram plane pixel and the last two is for image plane pixel.\n    distances               : torch.tensor\n                              Propagation distances in meters.\n    method                  : str\n                              Hologram type conventional or multi-color.\n    device                  : torch.device\n                              Device to be used for computation. For more see torch.device().\n    \"\"\"\n    self.device = device\n    self.pixel_pitch = pixel_pitch\n    self.wavelengths = wavelengths\n    self.resolution = resolution\n    self.propagation_type = propagation_type\n    if self.propagation_type != 'Impulse Response Fresnel':\n        resolution_factor = 1\n    self.resolution_factor = resolution_factor\n    self.number_of_frames = number_of_frames\n    self.number_of_depth_layers = number_of_depth_layers\n    self.number_of_channels = len(self.wavelengths)\n    self.volume_depth = volume_depth\n    self.image_location_offset = image_location_offset\n    self.propagator_type = propagator_type\n    self.aperture_samples = aperture_samples\n    self.zero_mode_distance = torch.tensor(back_and_forth_distance, device = device)\n    self.method = method\n    self.aperture = aperture\n    self.init_distances(distances)\n    self.init_kernels()\n    self.init_channel_power(laser_channel_power)\n    self.init_phase_scale()\n    self.set_aperture(aperture, aperture_size)\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator.get_kernels","title":"<code>get_kernels()</code>","text":"<p>Function to return the kernels used in the light transport.</p> <p>Returns:</p> <ul> <li> <code>kernels</code> (              <code>tensor</code> )          \u2013            <p>Kernel amplitudes.</p> </li> </ul> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def get_kernels(self):\n    \"\"\"\n    Function to return the kernels used in the light transport.\n\n    Returns\n    -------\n    kernels           : torch.tensor\n                        Kernel amplitudes.\n    \"\"\"\n    h = torch.fft.ifftshift(torch.fft.ifft2(torch.fft.ifftshift(self.kernels)))\n    kernels_amplitude = calculate_amplitude(h)\n    kernels_phase = calculate_phase(h)\n    return kernels_amplitude, kernels_phase\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator.get_laser_powers","title":"<code>get_laser_powers()</code>","text":"<p>Internal function to get the laser powers.</p> <p>Returns:</p> <ul> <li> <code>laser_power</code> (              <code>tensor</code> )          \u2013            <p>Laser powers.</p> </li> </ul> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def get_laser_powers(self):\n    \"\"\"\n    Internal function to get the laser powers.\n\n    Returns\n    -------\n    laser_power      : torch.tensor\n                       Laser powers.\n    \"\"\"\n    if self.method == 'conventional':\n        laser_power = self.channel_power\n    if self.method == 'multi-color':\n        laser_power = torch.abs(torch.cos(self.channel_power))\n    return laser_power\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator.init_channel_power","title":"<code>init_channel_power(channel_power)</code>","text":"<p>Internal function to set the starting phase of the phase-only hologram.</p> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def init_channel_power(self, channel_power):\n    \"\"\"\n    Internal function to set the starting phase of the phase-only hologram.\n    \"\"\"\n    self.channel_power = channel_power\n    if isinstance(self.channel_power, type(None)):\n        self.channel_power = torch.eye(\n                                       self.number_of_frames,\n                                       self.number_of_channels,\n                                       device = self.device,\n                                       requires_grad = False\n                                      )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator.init_distances","title":"<code>init_distances(distances)</code>","text":"<p>Internal function to initialize distances.</p> <p>Parameters:</p> <ul> <li> <code>distances</code>           \u2013            <pre><code>                  Propagation distances.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def init_distances(self, distances):\n    \"\"\"\n    Internal function to initialize distances.\n\n    Parameters\n    ----------\n    distances               : torch.tensor\n                              Propagation distances.\n    \"\"\"\n    if isinstance(distances, type(None)):\n        self.distances = torch.linspace(-self.volume_depth / 2., self.volume_depth / 2., self.number_of_depth_layers) + self.image_location_offset\n    else:\n        self.distances = torch.as_tensor(distances)\n        self.number_of_depth_layers = self.distances.shape[0]\n    logging.warning('Distances: {}'.format(self.distances))\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator.init_kernels","title":"<code>init_kernels()</code>","text":"<p>Internal function to initialize kernels.</p> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def init_kernels(self):\n    \"\"\"\n    Internal function to initialize kernels.\n    \"\"\"\n    self.generated_kernels = torch.zeros(\n                                         self.number_of_depth_layers,\n                                         self.number_of_channels,\n                                         device = self.device\n                                        )\n    self.kernels = torch.zeros(\n                               self.number_of_depth_layers,\n                               self.number_of_channels,\n                               self.resolution[0] * self.resolution_factor * 2,\n                               self.resolution[1] * self.resolution_factor * 2,\n                               dtype = torch.complex64,\n                               device = self.device\n                              )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator.init_phase_scale","title":"<code>init_phase_scale()</code>","text":"<p>Internal function to set the phase scale. In some cases, you may want to modify this init to ratio phases for different color primaries as an SLM is configured for a specific central wavelength.</p> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def init_phase_scale(self):\n    \"\"\"\n    Internal function to set the phase scale.\n    In some cases, you may want to modify this init to ratio phases for different color primaries as an SLM is configured for a specific central wavelength.\n    \"\"\"\n    self.phase_scale = torch.tensor(\n                                    [\n                                     1.,\n                                     1.,\n                                     1.\n                                    ],\n                                    requires_grad = False,\n                                    device = self.device\n                                   )\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator.reconstruct","title":"<code>reconstruct(hologram_phases, amplitude=None, no_grad=True, get_complex=False)</code>","text":"<p>Internal function to reconstruct a given hologram.</p> <p>Parameters:</p> <ul> <li> <code>hologram_phases</code>           \u2013            <pre><code>                     Hologram phases [ch x m x n].\n</code></pre> </li> <li> <code>amplitude</code>           \u2013            <pre><code>                     Amplitude profiles for each color primary [ch x m x n]\n</code></pre> </li> <li> <code>no_grad</code>           \u2013            <pre><code>                     If set True, uses torch.no_grad in reconstruction.\n</code></pre> </li> <li> <code>get_complex</code>           \u2013            <pre><code>                     If set True, reconstructor returns the complex field but not the intensities.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>reconstructions</code> (              <code>tensor</code> )          \u2013            <p>Reconstructed frames.</p> </li> </ul> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def reconstruct(self, hologram_phases, amplitude = None, no_grad = True, get_complex = False):\n    \"\"\"\n    Internal function to reconstruct a given hologram.\n\n\n    Parameters\n    ----------\n    hologram_phases            : torch.tensor\n                                 Hologram phases [ch x m x n].\n    amplitude                  : torch.tensor\n                                 Amplitude profiles for each color primary [ch x m x n]\n    no_grad                    : bool\n                                 If set True, uses torch.no_grad in reconstruction.\n    get_complex                : bool\n                                 If set True, reconstructor returns the complex field but not the intensities.\n\n    Returns\n    -------\n    reconstructions            : torch.tensor\n                                 Reconstructed frames.\n    \"\"\"\n    if no_grad:\n        torch.no_grad()\n    if len(hologram_phases.shape) &gt; 3:\n        hologram_phases = hologram_phases.squeeze(0)\n    if get_complex == True:\n        reconstruction_type = torch.complex64\n    else:\n        reconstruction_type = torch.float32\n    if hologram_phases.shape[0] != self.number_of_frames:\n        logging.warning('Provided hologram frame count is {} but the configured number of frames is {}.'.format(hologram_phases.shape[0], self.number_of_frames))\n    reconstructions = torch.zeros(\n                                  self.number_of_frames,\n                                  self.number_of_depth_layers,\n                                  self.number_of_channels,\n                                  self.resolution[0] * self.resolution_factor,\n                                  self.resolution[1] * self.resolution_factor,\n                                  dtype = reconstruction_type,\n                                  device = self.device\n                                 )\n    if isinstance(amplitude, type(None)):\n        amplitude = torch.zeros(\n                                self.number_of_channels,\n                                self.resolution[0] * self.resolution_factor,\n                                self.resolution[1] * self.resolution_factor,\n                                device = self.device\n                               )\n        amplitude[:, ::self.resolution_factor, ::self.resolution_factor] = 1.\n    if self.resolution_factor != 1:\n        hologram_phases_scaled = torch.zeros_like(amplitude)\n        hologram_phases_scaled[\n                               :,\n                               ::self.resolution_factor,\n                               ::self.resolution_factor\n                              ] = hologram_phases\n    else:\n        hologram_phases_scaled = hologram_phases\n    for frame_id in range(self.number_of_frames):\n        for depth_id in range(self.number_of_depth_layers):\n            for channel_id in range(self.number_of_channels):\n                laser_power = self.get_laser_powers()[frame_id][channel_id]\n                phase = hologram_phases_scaled[frame_id]\n                hologram = generate_complex_field(\n                                                  laser_power * amplitude[channel_id],\n                                                  phase * self.phase_scale[channel_id]\n                                                 )\n                reconstruction_field = self.__call__(hologram, channel_id, depth_id)\n                if get_complex == True:\n                    result = reconstruction_field\n                else:\n                    result = calculate_amplitude(reconstruction_field) ** 2\n\n                if no_grad: \n                    result = result.detach().clone()\n\n                reconstructions[\n                                frame_id,\n                                depth_id,\n                                channel_id\n                               ] = result\n\n    return reconstructions\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator.set_aperture","title":"<code>set_aperture(aperture=None, aperture_size=None)</code>","text":"<p>Set aperture in the Fourier plane.</p> <p>Parameters:</p> <ul> <li> <code>aperture</code>           \u2013            <pre><code>          Aperture at the original resolution of a hologram.\n          If aperture is provided as None, it will assign a circular aperture at the size of the short edge (width or height).\n</code></pre> </li> <li> <code>aperture_size</code>           \u2013            <pre><code>          If no aperture is provided, this will determine the size of the circular aperture.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def set_aperture(self, aperture = None, aperture_size = None):\n    \"\"\"\n    Set aperture in the Fourier plane.\n\n\n    Parameters\n    ----------\n    aperture        : torch.tensor\n                      Aperture at the original resolution of a hologram.\n                      If aperture is provided as None, it will assign a circular aperture at the size of the short edge (width or height).\n    aperture_size   : int\n                      If no aperture is provided, this will determine the size of the circular aperture.\n    \"\"\"\n    if isinstance(aperture, type(None)):\n        if isinstance(aperture_size, type(None)):\n            aperture_size = torch.max(\n                                      torch.tensor([\n                                                    self.resolution[0] * self.resolution_factor, \n                                                    self.resolution[1] * self.resolution_factor\n                                                   ])\n                                     )\n        self.aperture = circular_binary_mask(\n                                             self.resolution[0] * self.resolution_factor * 2,\n                                             self.resolution[1] * self.resolution_factor * 2,\n                                             aperture_size,\n                                            ).to(self.device) * 1.\n    else:\n        self.aperture = zero_pad(aperture).to(self.device) * 1.\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.propagators.propagator.set_laser_powers","title":"<code>set_laser_powers(laser_power)</code>","text":"<p>Internal function to set the laser powers.</p> <p>Parameters:</p> <ul> <li> <code>laser_power</code>           \u2013            <pre><code>           Laser powers.\n</code></pre> </li> </ul> Source code in <code>odak/learn/wave/propagators.py</code> <pre><code>def set_laser_powers(self, laser_power):\n    \"\"\"\n    Internal function to set the laser powers.\n\n    Parameters\n    -------\n    laser_power      : torch.tensor\n                       Laser powers.\n    \"\"\"\n    self.channel_power = laser_power\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.util.calculate_amplitude","title":"<code>calculate_amplitude(field)</code>","text":"<p>Definition to calculate amplitude of a single or multiple given electric field(s).</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>       Electric fields or an electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>amplitude</code> (              <code>float</code> )          \u2013            <p>Amplitude or amplitudes of electric field(s).</p> </li> </ul> Source code in <code>odak/learn/wave/util.py</code> <pre><code>def calculate_amplitude(field):\n    \"\"\" \n    Definition to calculate amplitude of a single or multiple given electric field(s).\n\n    Parameters\n    ----------\n    field        : torch.cfloat\n                   Electric fields or an electric field.\n\n    Returns\n    -------\n    amplitude    : torch.float\n                   Amplitude or amplitudes of electric field(s).\n    \"\"\"\n    amplitude = torch.abs(field)\n    return amplitude\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.util.calculate_phase","title":"<code>calculate_phase(field, deg=False)</code>","text":"<p>Definition to calculate phase of a single or multiple given electric field(s).</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>       Electric fields or an electric field.\n</code></pre> </li> <li> <code>deg</code>           \u2013            <pre><code>       If set True, the angles will be returned in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>phase</code> (              <code>float</code> )          \u2013            <p>Phase or phases of electric field(s) in radians.</p> </li> </ul> Source code in <code>odak/learn/wave/util.py</code> <pre><code>def calculate_phase(field, deg = False):\n    \"\"\" \n    Definition to calculate phase of a single or multiple given electric field(s).\n\n    Parameters\n    ----------\n    field        : torch.cfloat\n                   Electric fields or an electric field.\n    deg          : bool\n                   If set True, the angles will be returned in degrees.\n\n    Returns\n    -------\n    phase        : torch.float\n                   Phase or phases of electric field(s) in radians.\n    \"\"\"\n    phase = field.imag.atan2(field.real)\n    if deg:\n        phase *= 180. / torch.pi\n    return phase\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.util.generate_complex_field","title":"<code>generate_complex_field(amplitude, phase)</code>","text":"<p>Definition to generate a complex field with a given amplitude and phase.</p> <p>Parameters:</p> <ul> <li> <code>amplitude</code>           \u2013            <pre><code>            Amplitude of the field.\n            The expected size is [m x n] or [1 x m x n].\n</code></pre> </li> <li> <code>phase</code>           \u2013            <pre><code>            Phase of the field.\n            The expected size is [m x n] or [1 x m x n].\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field</code> (              <code>ndarray</code> )          \u2013            <p>Complex field. Depending on the input, the expected size is [m x n] or [1 x m x n].</p> </li> </ul> Source code in <code>odak/learn/wave/util.py</code> <pre><code>def generate_complex_field(amplitude, phase):\n    \"\"\"\n    Definition to generate a complex field with a given amplitude and phase.\n\n    Parameters\n    ----------\n    amplitude         : torch.tensor\n                        Amplitude of the field.\n                        The expected size is [m x n] or [1 x m x n].\n    phase             : torch.tensor\n                        Phase of the field.\n                        The expected size is [m x n] or [1 x m x n].\n\n    Returns\n    -------\n    field             : ndarray\n                        Complex field.\n                        Depending on the input, the expected size is [m x n] or [1 x m x n].\n    \"\"\"\n    field = amplitude * torch.cos(phase) + 1j * amplitude * torch.sin(phase)\n    return field\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.util.set_amplitude","title":"<code>set_amplitude(field, amplitude)</code>","text":"<p>Definition to keep phase as is and change the amplitude of a given field.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>       Complex field.\n</code></pre> </li> <li> <code>amplitude</code>           \u2013            <pre><code>       Amplitudes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>cfloat</code> )          \u2013            <p>Complex field.</p> </li> </ul> Source code in <code>odak/learn/wave/util.py</code> <pre><code>def set_amplitude(field, amplitude):\n    \"\"\"\n    Definition to keep phase as is and change the amplitude of a given field.\n\n    Parameters\n    ----------\n    field        : torch.cfloat\n                   Complex field.\n    amplitude    : torch.cfloat or torch.float\n                   Amplitudes.\n\n    Returns\n    -------\n    new_field    : torch.cfloat\n                   Complex field.\n    \"\"\"\n    amplitude = calculate_amplitude(amplitude)\n    phase = calculate_phase(field)\n    new_field = amplitude * torch.cos(phase) + 1j * amplitude * torch.sin(phase)\n    return new_field\n</code></pre>"},{"location":"odak/learn_wave/#odak.learn.wave.util.wavenumber","title":"<code>wavenumber(wavelength)</code>","text":"<p>Definition for calculating the wavenumber of a plane wave.</p> <p>Parameters:</p> <ul> <li> <code>wavelength</code>           \u2013            <pre><code>       Wavelength of a wave in mm.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>k</code> (              <code>float</code> )          \u2013            <p>Wave number for a given wavelength.</p> </li> </ul> Source code in <code>odak/learn/wave/util.py</code> <pre><code>def wavenumber(wavelength):\n    \"\"\"\n    Definition for calculating the wavenumber of a plane wave.\n\n    Parameters\n    ----------\n    wavelength   : float\n                   Wavelength of a wave in mm.\n\n    Returns\n    -------\n    k            : float\n                   Wave number for a given wavelength.\n    \"\"\"\n    k = 2 * torch.pi / wavelength\n    return k\n</code></pre>"},{"location":"odak/raytracing/","title":"odak.raytracing","text":"<p><code>odak.raytracing</code></p> <p>Provides necessary definitions for geometric optics. See \"General Ray tracing procedure\" from G.H. Spencerand M.V.R.K Murty for the theoratical explanation.</p>"},{"location":"odak/raytracing/#odak.raytracing.bring_plane_to_origin","title":"<code>bring_plane_to_origin(point, plane, shape=[10.0, 10.0], center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0], mode='XYZ')</code>","text":"<p>Definition to bring points back to reference origin with respect to a plane.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>             Point(s) to be tested.\n</code></pre> </li> <li> <code>shape</code>           \u2013            <pre><code>             Dimensions of the rectangle along X and Y axes.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>             Center of the rectangle.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>             Rotation angle of the rectangle.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>             Rotation mode of the rectangle, for more see odak.tools.rotate_point and odak.tools.rotate_points.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_points</code> (              <code>ndarray</code> )          \u2013            <p>Point(s) that are brought back to reference origin with respect to given plane.</p> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def bring_plane_to_origin(point, plane, shape=[10., 10.], center=[0., 0., 0.], angles=[0., 0., 0.], mode='XYZ'):\n    \"\"\"\n    Definition to bring points back to reference origin with respect to a plane.\n\n    Parameters\n    ----------\n    point              : ndarray\n                         Point(s) to be tested.\n    shape              : list\n                         Dimensions of the rectangle along X and Y axes.\n    center             : list\n                         Center of the rectangle.\n    angles             : list\n                         Rotation angle of the rectangle.\n    mode               : str\n                         Rotation mode of the rectangle, for more see odak.tools.rotate_point and odak.tools.rotate_points.\n\n    Returns\n    ----------\n    transformed_points : ndarray\n                         Point(s) that are brought back to reference origin with respect to given plane.\n    \"\"\"\n    if point.shape[0] == 3:\n        point = point.reshape((1, 3))\n    reverse_mode = mode[::-1]\n    angles = [-angles[0], -angles[1], -angles[2]]\n    center = np.asarray(center).reshape((1, 3))\n    transformed_points = point-center\n    transformed_points = rotate_points(\n        transformed_points,\n        angles=angles,\n        mode=reverse_mode,\n    )\n    if transformed_points.shape[0] == 1:\n        transformed_points = transformed_points.reshape((3,))\n    return transformed_points\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.calculate_intersection_of_two_rays","title":"<code>calculate_intersection_of_two_rays(ray0, ray1)</code>","text":"<p>Definition to calculate the intersection of two rays.</p> <p>Parameters:</p> <ul> <li> <code>ray0</code>           \u2013            <pre><code>     A ray.\n</code></pre> </li> <li> <code>ray1</code>           \u2013            <pre><code>     A ray.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>point</code> (              <code>ndarray</code> )          \u2013            <p>Point in X,Y,Z.</p> </li> <li> <code>distances</code> (              <code>ndarray</code> )          \u2013            <p>Distances.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def calculate_intersection_of_two_rays(ray0, ray1):\n    \"\"\"\n    Definition to calculate the intersection of two rays.\n\n    Parameters\n    ----------\n    ray0       : ndarray\n                 A ray.\n    ray1       : ndarray\n                 A ray.\n\n    Returns\n    ----------\n    point      : ndarray\n                 Point in X,Y,Z.\n    distances  : ndarray\n                 Distances.\n    \"\"\"\n    A = np.array([\n        [float(ray0[1][0]), float(ray1[1][0])],\n        [float(ray0[1][1]), float(ray1[1][1])],\n        [float(ray0[1][2]), float(ray1[1][2])]\n    ])\n    B = np.array([\n        ray0[0][0]-ray1[0][0],\n        ray0[0][1]-ray1[0][1],\n        ray0[0][2]-ray1[0][2]\n    ])\n    distances = np.linalg.lstsq(A, B, rcond=None)[0]\n    if np.allclose(np.dot(A, distances), B) == False:\n        distances = np.array([0, 0])\n    distances = distances[np.argsort(-distances)]\n    point = propagate_a_ray(ray0, distances[0])[0]\n    return point, distances\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.center_of_triangle","title":"<code>center_of_triangle(triangle)</code>","text":"<p>Definition to calculate center of a triangle.</p> <p>Parameters:</p> <ul> <li> <code>triangle</code>           \u2013            <pre><code>        An array that contains three points defining a triangle (Mx3). It can also parallel process many triangles (NxMx3).\n</code></pre> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def center_of_triangle(triangle):\n    \"\"\"\n    Definition to calculate center of a triangle.\n\n    Parameters\n    ----------\n    triangle      : ndarray\n                    An array that contains three points defining a triangle (Mx3). It can also parallel process many triangles (NxMx3).\n    \"\"\"\n    if len(triangle.shape) == 2:\n        triangle = triangle.reshape((1, 3, 3))\n    center = np.mean(triangle, axis=1)\n    return center\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.closest_point_to_a_ray","title":"<code>closest_point_to_a_ray(point, ray)</code>","text":"<p>Definition to calculate the point on a ray that is closest to given point.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>        Given point in X,Y,Z.\n</code></pre> </li> <li> <code>ray</code>           \u2013            <pre><code>        Given ray.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>closest_point</code> (              <code>ndarray</code> )          \u2013            <p>Calculated closest point.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def closest_point_to_a_ray(point, ray):\n    \"\"\"\n    Definition to calculate the point on a ray that is closest to given point.\n\n    Parameters\n    ----------\n    point         : list\n                    Given point in X,Y,Z.\n    ray           : ndarray\n                    Given ray.\n\n    Returns\n    ---------\n    closest_point : ndarray\n                    Calculated closest point.\n    \"\"\"\n    from odak.raytracing import propagate_a_ray\n    if len(ray.shape) == 2:\n        ray = ray.reshape((1, 2, 3))\n    p0 = ray[:, 0]\n    p1 = propagate_a_ray(ray, 1.)\n    if len(p1.shape) == 2:\n        p1 = p1.reshape((1, 2, 3))\n    p1 = p1[:, 0]\n    p1 = p1.reshape(3)\n    p0 = p0.reshape(3)\n    point = point.reshape(3)\n    closest_distance = -np.dot((p0-point), (p1-p0))/np.sum((p1-p0)**2)\n    closest_point = propagate_a_ray(ray, closest_distance)[0]\n    return closest_point\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.create_ray","title":"<code>create_ray(x0y0z0, abg)</code>","text":"<p>Definition to create a ray.</p> <p>Parameters:</p> <ul> <li> <code>x0y0z0</code>           \u2013            <pre><code>       List that contains X,Y and Z start locations of a ray.\n</code></pre> </li> <li> <code>abg</code>           \u2013            <pre><code>       List that contaings angles in degrees with respect to the X,Y and Z axes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>ndarray</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def create_ray(x0y0z0, abg):\n    \"\"\"\n    Definition to create a ray.\n\n    Parameters\n    ----------\n    x0y0z0       : list\n                   List that contains X,Y and Z start locations of a ray.\n    abg          : list\n                   List that contaings angles in degrees with respect to the X,Y and Z axes.\n\n    Returns\n    ----------\n    ray          : ndarray\n                   Array that contains starting points and cosines of a created ray.\n    \"\"\"\n    # Due to Python 2 -&gt; Python 3.\n    x0, y0, z0 = x0y0z0\n    alpha, beta, gamma = abg\n    # Create a vector with the given points and angles in each direction\n    point = np.array([x0, y0, z0], dtype=np.float64)\n    alpha = np.cos(np.radians(alpha))\n    beta = np.cos(np.radians(beta))\n    gamma = np.cos(np.radians(gamma))\n    # Cosines vector.\n    cosines = np.array([alpha, beta, gamma], dtype=np.float64)\n    ray = np.array([point, cosines], dtype=np.float64)\n    return ray\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.create_ray_from_angles","title":"<code>create_ray_from_angles(point, angles, mode='XYZ')</code>","text":"<p>Definition to create a ray from a point and angles.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>     Point in X,Y and Z.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>     Angles with X,Y,Z axes in degrees. All zeros point Z axis.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>     Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ    ,ZXY and ZYX modes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>ndarray</code> )          \u2013            <p>Created ray.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def create_ray_from_angles(point, angles, mode='XYZ'):\n    \"\"\"\n    Definition to create a ray from a point and angles.\n\n    Parameters\n    ----------\n    point      : ndarray\n                 Point in X,Y and Z.\n    angles     : ndarray\n                 Angles with X,Y,Z axes in degrees. All zeros point Z axis.\n    mode       : str\n                 Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ    ,ZXY and ZYX modes.\n\n    Returns\n    ----------\n    ray        : ndarray\n                 Created ray.\n    \"\"\"\n    if len(point.shape) == 1:\n        point = point.reshape((1, 3))\n    new_point = np.zeros(point.shape)\n    new_point[:, 2] += 5.\n    new_point = rotate_points(new_point, angles, mode=mode, offset=point[:, 0])\n    ray = create_ray_from_two_points(point, new_point)\n    if ray.shape[0] == 1:\n        ray = ray.reshape((2, 3))\n    return ray\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.create_ray_from_two_points","title":"<code>create_ray_from_two_points(x0y0z0, x1y1z1)</code>","text":"<p>Definition to create a ray from two given points. Note that both inputs must match in shape.</p> <p>Parameters:</p> <ul> <li> <code>x0y0z0</code>           \u2013            <pre><code>       List that contains X,Y and Z start locations of a ray (3). It can also be a list of points as well (mx3). This is the starting point.\n</code></pre> </li> <li> <code>x1y1z1</code>           \u2013            <pre><code>       List that contains X,Y and Z ending locations of a ray (3). It can also be a list of points as well (mx3). This is the end point.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>ndarray</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def create_ray_from_two_points(x0y0z0, x1y1z1):\n    \"\"\"\n    Definition to create a ray from two given points. Note that both inputs must match in shape.\n\n    Parameters\n    ----------\n    x0y0z0       : list\n                   List that contains X,Y and Z start locations of a ray (3). It can also be a list of points as well (mx3). This is the starting point.\n    x1y1z1       : list\n                   List that contains X,Y and Z ending locations of a ray (3). It can also be a list of points as well (mx3). This is the end point.\n\n    Returns\n    ----------\n    ray          : ndarray\n                   Array that contains starting points and cosines of a created ray.\n    \"\"\"\n    x0y0z0 = np.asarray(x0y0z0, dtype=np.float64)\n    x1y1z1 = np.asarray(x1y1z1, dtype=np.float64)\n    if len(x0y0z0.shape) == 1:\n        x0y0z0 = x0y0z0.reshape((1, 3))\n    if len(x1y1z1.shape) == 1:\n        x1y1z1 = x1y1z1.reshape((1, 3))\n    xdiff = x1y1z1[:, 0] - x0y0z0[:, 0]\n    ydiff = x1y1z1[:, 1] - x0y0z0[:, 1]\n    zdiff = x1y1z1[:, 2] - x0y0z0[:, 2]\n    s = np.sqrt(xdiff ** 2 + ydiff ** 2 + zdiff ** 2)\n    s[s == 0] = np.nan\n    cosines = np.zeros((xdiff.shape[0], 3))\n    cosines[:, 0] = xdiff/s\n    cosines[:, 1] = ydiff/s\n    cosines[:, 2] = zdiff/s\n    ray = np.zeros((xdiff.shape[0], 2, 3), dtype=np.float64)\n    ray[:, 0] = x0y0z0\n    ray[:, 1] = cosines\n    if ray.shape[0] == 1:\n        ray = ray.reshape((2, 3))\n    return ray\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.cylinder_function","title":"<code>cylinder_function(point, cylinder)</code>","text":"<p>Definition of a cylinder function. Evaluate a point against a cylinder function. Inspired from https://mathworld.wolfram.com/Point-LineDistance3-Dimensional.html</p> <p>Parameters:</p> <ul> <li> <code>cylinder</code>           \u2013            <pre><code>     Cylinder parameters, XYZ center and radius.\n</code></pre> </li> <li> <code>point</code>           \u2013            <pre><code>     Point in XYZ.\n</code></pre> </li> </ul> Return <p>result     : float              Result of the evaluation. Zero if point is on sphere.</p> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def cylinder_function(point, cylinder):\n    \"\"\"\n    Definition of a cylinder function. Evaluate a point against a cylinder function. Inspired from https://mathworld.wolfram.com/Point-LineDistance3-Dimensional.html\n\n    Parameters\n    ----------\n    cylinder   : ndarray\n                 Cylinder parameters, XYZ center and radius.\n    point      : ndarray\n                 Point in XYZ.\n\n    Return\n    ----------\n    result     : float\n                 Result of the evaluation. Zero if point is on sphere.\n    \"\"\"\n    point = np.asarray(point)\n    if len(point.shape) == 1:\n        point = point.reshape((1, 3))\n    distance = point_to_ray_distance(\n        point,\n        np.array([cylinder[0], cylinder[1], cylinder[2]], dtype=np.float64),\n        np.array([cylinder[4], cylinder[5], cylinder[6]], dtype=np.float64)\n    )\n    r = cylinder[3]\n    result = distance - r ** 2\n    return result\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.define_circle","title":"<code>define_circle(center, radius, angles)</code>","text":"<p>Definition to describe a circle in a single variable packed form.</p> <p>Parameters:</p> <ul> <li> <code>center</code>           \u2013            <pre><code>  Center of a circle to be defined.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>  Radius of a circle to be defined.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>  Angular tilt of a circle.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>circle</code> (              <code>list</code> )          \u2013            <p>Single variable packed form.</p> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def define_circle(center, radius, angles):\n    \"\"\"\n    Definition to describe a circle in a single variable packed form.\n\n    Parameters\n    ----------\n    center  : float\n              Center of a circle to be defined.\n    radius  : float\n              Radius of a circle to be defined.\n    angles  : float\n              Angular tilt of a circle.\n\n    Returns\n    ----------\n    circle  : list\n              Single variable packed form.\n    \"\"\"\n    points = define_plane(center, angles=angles)\n    circle = [\n        points,\n        center,\n        radius\n    ]\n    return circle\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.define_cylinder","title":"<code>define_cylinder(center, radius, rotation=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to define a cylinder</p> <p>Parameters:</p> <ul> <li> <code>center</code>           \u2013            <pre><code>     Center of a cylinder in X,Y,Z.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>     Radius of a cylinder along X axis.\n</code></pre> </li> <li> <code>rotation</code>           \u2013            <pre><code>     Direction angles in degrees for the orientation of a cylinder.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>cylinder</code> (              <code>ndarray</code> )          \u2013            <p>Single variable packed form.</p> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def define_cylinder(center, radius, rotation=[0., 0., 0.]):\n    \"\"\"\n    Definition to define a cylinder\n\n    Parameters\n    ----------\n    center     : ndarray\n                 Center of a cylinder in X,Y,Z.\n    radius     : float\n                 Radius of a cylinder along X axis.\n    rotation   : list\n                 Direction angles in degrees for the orientation of a cylinder.\n\n    Returns\n    ----------\n    cylinder   : ndarray\n                 Single variable packed form.\n    \"\"\"\n    cylinder_ray = create_ray_from_angles(\n        np.asarray(center), np.asarray(rotation))\n    cylinder = np.array(\n        [\n            center[0],\n            center[1],\n            center[2],\n            radius,\n            center[0]+cylinder_ray[1, 0],\n            center[1]+cylinder_ray[1, 1],\n            center[2]+cylinder_ray[1, 2]\n        ],\n        dtype=np.float64\n    )\n    return cylinder\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.define_plane","title":"<code>define_plane(point, angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate a rotation matrix along X axis.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>       A point that is at the center of a plane.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>plane</code> (              <code>ndarray</code> )          \u2013            <p>Points defining plane.</p> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def define_plane(point, angles=[0., 0., 0.]):\n    \"\"\" \n    Definition to generate a rotation matrix along X axis.\n\n    Parameters\n    ----------\n    point        : ndarray\n                   A point that is at the center of a plane.\n    angles       : list\n                   Rotation angles in degrees.\n\n    Returns\n    ----------\n    plane        : ndarray\n                   Points defining plane.\n    \"\"\"\n    plane = np.array([\n        [10., 10., 0.],\n        [0., 10., 0.],\n        [0.,  0., 0.]\n    ], dtype=np.float64)\n    point = np.asarray(point)\n    for i in range(0, plane.shape[0]):\n        plane[i], _, _, _ = rotate_point(plane[i], angles=angles)\n        plane[i] = plane[i]+point\n    return plane\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.define_sphere","title":"<code>define_sphere(center, radius)</code>","text":"<p>Definition to define a sphere.</p> <p>Parameters:</p> <ul> <li> <code>center</code>           \u2013            <pre><code>     Center of a sphere in X,Y,Z.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>     Radius of a sphere.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>sphere</code> (              <code>ndarray</code> )          \u2013            <p>Single variable packed form.</p> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def define_sphere(center, radius):\n    \"\"\"\n    Definition to define a sphere.\n\n    Parameters\n    ----------\n    center     : ndarray\n                 Center of a sphere in X,Y,Z.\n    radius     : float\n                 Radius of a sphere.\n\n    Returns\n    ----------\n    sphere     : ndarray\n                 Single variable packed form.\n    \"\"\"\n    sphere = np.array(\n        [center[0], center[1], center[2], radius], dtype=np.float64)\n    return sphere\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.distance_between_two_points","title":"<code>distance_between_two_points(point1, point2)</code>","text":"<p>Definition to calculate distance between two given points.</p> <p>Parameters:</p> <ul> <li> <code>point1</code>           \u2013            <pre><code>      First point in X,Y,Z.\n</code></pre> </li> <li> <code>point2</code>           \u2013            <pre><code>      Second point in X,Y,Z.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between given two points.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def distance_between_two_points(point1, point2):\n    \"\"\"\n    Definition to calculate distance between two given points.\n\n    Parameters\n    ----------\n    point1      : list\n                  First point in X,Y,Z.\n    point2      : list\n                  Second point in X,Y,Z.\n\n    Returns\n    ----------\n    distance    : float\n                  Distance in between given two points.\n    \"\"\"\n    point1 = np.asarray(point1)\n    point2 = np.asarray(point2)\n    if len(point1.shape) == 1 and len(point2.shape) == 1:\n        distance = np.sqrt(np.sum((point1-point2)**2))\n    elif len(point1.shape) == 2 or len(point2.shape) == 2:\n        distance = np.sqrt(np.sum((point1-point2)**2, axis=1))\n    return distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.find_nearest_points","title":"<code>find_nearest_points(ray0, ray1)</code>","text":"<p>Find the nearest points on given rays with respect to the other ray.</p> <p>Parameters:</p> <ul> <li> <code>ray0</code>           \u2013            <pre><code>     A ray.\n</code></pre> </li> <li> <code>ray1</code>           \u2013            <pre><code>     A ray.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>c0</code> (              <code>ndarray</code> )          \u2013            <p>Closest point on ray0.</p> </li> <li> <code>c1</code> (              <code>ndarray</code> )          \u2013            <p>Closest point on ray1.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def find_nearest_points(ray0, ray1):\n    \"\"\"\n    Find the nearest points on given rays with respect to the other ray.\n\n    Parameters\n    ----------\n    ray0       : ndarray\n                 A ray.\n    ray1       : ndarray\n                 A ray.\n\n    Returns\n    ----------\n    c0         : ndarray\n                 Closest point on ray0.\n    c1         : ndarray\n                 Closest point on ray1.\n    \"\"\"\n    p0 = ray0[0].reshape(3,)\n    d0 = ray0[1].reshape(3,)\n    p1 = ray1[0].reshape(3,)\n    d1 = ray1[1].reshape(3,)\n    n = np.cross(d0, d1)\n    if np.all(n) == 0:\n        point, distances = calculate_intersection_of_two_rays(ray0, ray1)\n        c0 = c1 = point\n    else:\n        n0 = np.cross(d0, n)\n        n1 = np.cross(d1, n)\n        c0 = p0+(np.dot((p1-p0), n1)/np.dot(d0, n1))*d0\n        c1 = p1+(np.dot((p0-p1), n0)/np.dot(d1, n0))*d1\n    return c0, c1\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.get_cylinder_normal","title":"<code>get_cylinder_normal(point, cylinder)</code>","text":"<p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>        Point on a cylinder defined in X,Y,Z.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal_vector</code> (              <code>ndarray</code> )          \u2013            <p>Normal vector.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def get_cylinder_normal(point, cylinder):\n    \"\"\"\n    Parameters\n    ----------\n    point         : ndarray\n                    Point on a cylinder defined in X,Y,Z.\n\n    Returns\n    ----------\n    normal_vector : ndarray\n                    Normal vector.\n    \"\"\"\n    cylinder_ray = create_ray_from_two_points(cylinder[0:3], cylinder[4:7])\n    closest_point = closest_point_to_a_ray(\n        point,\n        cylinder_ray\n    )\n    normal_vector = create_ray_from_two_points(closest_point, point)\n    return normal_vector\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.get_sphere_normal","title":"<code>get_sphere_normal(point, sphere)</code>","text":"<p>Definition to get a normal of a point on a given sphere.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>        Point on sphere in X,Y,Z.\n</code></pre> </li> <li> <code>sphere</code>           \u2013            <pre><code>        Center defined in X,Y,Z and radius.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal_vector</code> (              <code>ndarray</code> )          \u2013            <p>Normal vector.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def get_sphere_normal(point, sphere):\n    \"\"\"\n    Definition to get a normal of a point on a given sphere.\n\n    Parameters\n    ----------\n    point         : ndarray\n                    Point on sphere in X,Y,Z.\n    sphere        : ndarray\n                    Center defined in X,Y,Z and radius.\n\n    Returns\n    ----------\n    normal_vector : ndarray\n                    Normal vector.\n    \"\"\"\n    if len(point.shape) == 1:\n        point = point.reshape((1, 3))\n    normal_vector = create_ray_from_two_points(point, sphere[0:3])\n    return normal_vector\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.get_triangle_normal","title":"<code>get_triangle_normal(triangle, triangle_center=None)</code>","text":"<p>Definition to calculate surface normal of a triangle.</p> <p>Parameters:</p> <ul> <li> <code>triangle</code>           \u2013            <pre><code>          Set of points in X,Y and Z to define a planar surface (3,3). It can also be list of triangles (mx3x3).\n</code></pre> </li> <li> <code>triangle_center</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <pre><code>          Center point of the given triangle. See odak.raytracing.center_of_triangle for more. In many scenarios you can accelerate things by precomputing triangle centers.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def get_triangle_normal(triangle, triangle_center=None):\n    \"\"\"\n    Definition to calculate surface normal of a triangle.\n\n    Parameters\n    ----------\n    triangle        : ndarray\n                      Set of points in X,Y and Z to define a planar surface (3,3). It can also be list of triangles (mx3x3).\n    triangle_center : ndarray\n                      Center point of the given triangle. See odak.raytracing.center_of_triangle for more. In many scenarios you can accelerate things by precomputing triangle centers.\n\n    Returns\n    ----------\n    normal          : ndarray\n                      Surface normal at the point of intersection.\n    \"\"\"\n    triangle = np.asarray(triangle)\n    if len(triangle.shape) == 2:\n        triangle = triangle.reshape((1, 3, 3))\n    normal = np.zeros((triangle.shape[0], 2, 3))\n    direction = np.cross(\n        triangle[:, 0]-triangle[:, 1], triangle[:, 2]-triangle[:, 1])\n    if type(triangle_center) == type(None):\n        normal[:, 0] = center_of_triangle(triangle)\n    else:\n        normal[:, 0] = triangle_center\n    normal[:, 1] = direction/np.sum(direction, axis=1)[0]\n    if normal.shape[0] == 1:\n        normal = normal.reshape((2, 3))\n    return normal\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.intersect_parametric","title":"<code>intersect_parametric(ray, parametric_surface, surface_function, surface_normal_function, target_error=1e-08, iter_no_limit=100000)</code>","text":"<p>Definition to intersect a ray with a parametric surface.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>                  Ray.\n</code></pre> </li> <li> <code>parametric_surface</code>           \u2013            <pre><code>                  Parameters of the surfaces.\n</code></pre> </li> <li> <code>surface_function</code>           \u2013            <pre><code>                  Function to evaluate a point against a surface.\n</code></pre> </li> <li> <code>surface_normal_function</code>               (<code>function</code>)           \u2013            <pre><code>                  Function to calculate surface normal for a given point on a surface.\n</code></pre> </li> <li> <code>target_error</code>           \u2013            <pre><code>                  Target error that defines the precision.\n</code></pre> </li> <li> <code>iter_no_limit</code>           \u2013            <pre><code>                  Maximum number of iterations.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Propagation distance.</p> </li> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>Ray that defines a surface normal for the intersection.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_parametric(ray, parametric_surface, surface_function, surface_normal_function, target_error=0.00000001, iter_no_limit=100000):\n    \"\"\"\n    Definition to intersect a ray with a parametric surface.\n\n    Parameters\n    ----------\n    ray                     : ndarray\n                              Ray.\n    parametric_surface      : ndarray\n                              Parameters of the surfaces.\n    surface_function        : function\n                              Function to evaluate a point against a surface.\n    surface_normal_function : function\n                              Function to calculate surface normal for a given point on a surface.\n    target_error            : float\n                              Target error that defines the precision.  \n    iter_no_limit           : int\n                              Maximum number of iterations.\n\n    Returns\n    ----------\n    distance                : float\n                              Propagation distance.\n    normal                  : ndarray\n                              Ray that defines a surface normal for the intersection.\n    \"\"\"\n    if len(ray.shape) == 2:\n        ray = ray.reshape((1, 2, 3))\n    error = [150, 100]\n    distance = [0, 0.1]\n    iter_no = 0\n    while np.abs(np.max(np.asarray(error[1]))) &gt; target_error:\n        error[1], point = intersection_kernel_for_parametric_surfaces(\n            distance[1],\n            ray,\n            parametric_surface,\n            surface_function\n        )\n        distance, error = propagate_parametric_intersection_error(\n            distance,\n            error\n        )\n        iter_no += 1\n        if iter_no &gt; iter_no_limit:\n            return False, False\n        if np.isnan(np.sum(point)):\n            return False, False\n    normal = surface_normal_function(\n        point,\n        parametric_surface\n    )\n    return distance[1], normal\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.intersect_w_circle","title":"<code>intersect_w_circle(ray, circle)</code>","text":"<p>Definition to find intersection point of a ray with a circle. Returns False for each variable if the ray doesn't intersect with a given circle. Returns distance as zero if there isn't an intersection.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>circle</code>           \u2013            <pre><code>       A list that contains (0) Set of points in X,Y and Z to define plane of a circle, (1) circle center, and (2) circle radius.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between a starting point of a ray and the intersection point with a given triangle.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_w_circle(ray, circle):\n    \"\"\"\n    Definition to find intersection point of a ray with a circle. Returns False for each variable if the ray doesn't intersect with a given circle. Returns distance as zero if there isn't an intersection.\n\n    Parameters\n    ----------\n    ray          : ndarray\n                   A vector/ray.\n    circle       : list\n                   A list that contains (0) Set of points in X,Y and Z to define plane of a circle, (1) circle center, and (2) circle radius.\n\n    Returns\n    ----------\n    normal       : ndarray\n                   Surface normal at the point of intersection.\n    distance     : float\n                   Distance in between a starting point of a ray and the intersection point with a given triangle.\n    \"\"\"\n    normal, distance = intersect_w_surface(ray, circle[0])\n    if len(normal.shape) == 2:\n        normal = normal.reshape((1, 2, 3))\n    distance_to_center = distance_between_two_points(normal[:, 0], circle[1])\n    distance[np.nonzero(distance_to_center &gt; circle[2])] = 0\n    if len(ray.shape) == 2:\n        normal = normal.reshape((2, 3))\n    return normal, distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.intersect_w_cylinder","title":"<code>intersect_w_cylinder(ray, cylinder)</code>","text":"<p>Definition to intersect a ray with a cylinder.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>     A ray definition.\n</code></pre> </li> <li> <code>cylinder</code>           \u2013            <pre><code>     A cylinder defined with a center in XYZ and radius of curvature.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>A ray defining surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Total optical propagation distance.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_w_cylinder(ray, cylinder):\n    \"\"\"\n    Definition to intersect a ray with a cylinder.\n\n    Parameters\n    ----------\n    ray        : ndarray\n                 A ray definition.\n    cylinder   : ndarray\n                 A cylinder defined with a center in XYZ and radius of curvature.\n\n    Returns\n    ----------\n    normal     : ndarray\n                 A ray defining surface normal at the point of intersection.\n    distance   : float\n                 Total optical propagation distance.\n    \"\"\"\n    distance, normal = intersect_parametric(\n        ray,\n        cylinder,\n        cylinder_function,\n        get_cylinder_normal\n    )\n    return normal, distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.intersect_w_sphere","title":"<code>intersect_w_sphere(ray, sphere)</code>","text":"<p>Definition to intersect a ray with a sphere.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>     A ray definition.\n</code></pre> </li> <li> <code>sphere</code>           \u2013            <pre><code>     A sphere defined with a center in XYZ and radius of curvature.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>A ray defining surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Total optical propagation distance.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_w_sphere(ray, sphere):\n    \"\"\"\n    Definition to intersect a ray with a sphere.\n\n    Parameters\n    ----------\n    ray        : ndarray\n                 A ray definition.\n    sphere     : ndarray\n                 A sphere defined with a center in XYZ and radius of curvature.\n\n    Returns\n    ----------\n    normal     : ndarray\n                 A ray defining surface normal at the point of intersection.\n    distance   : float\n                 Total optical propagation distance.\n    \"\"\"\n    distance, normal = intersect_parametric(\n        ray,\n        sphere,\n        sphere_function,\n        get_sphere_normal\n    )\n    return normal, distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.intersect_w_surface","title":"<code>intersect_w_surface(ray, points)</code>","text":"<p>Definition to find intersection point inbetween a surface and a ray. For more see: http://geomalgorithms.com/a06-_intersect-2.html</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>points</code>           \u2013            <pre><code>       Set of points in X,Y and Z to define a planar surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between starting point of a ray with it's intersection with a planar surface.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_w_surface(ray, points):\n    \"\"\"\n    Definition to find intersection point inbetween a surface and a ray. For more see: http://geomalgorithms.com/a06-_intersect-2.html\n\n    Parameters\n    ----------\n    ray          : ndarray\n                   A vector/ray.\n    points       : ndarray\n                   Set of points in X,Y and Z to define a planar surface.\n\n    Returns\n    ----------\n    normal       : ndarray\n                   Surface normal at the point of intersection.\n    distance     : float\n                   Distance in between starting point of a ray with it's intersection with a planar surface.\n    \"\"\"\n    points = np.asarray(points)\n    normal = get_triangle_normal(points)\n    if len(ray.shape) == 2:\n        ray = ray.reshape((1, 2, 3))\n    if len(points) == 2:\n        points = points.reshape((1, 3, 3))\n    if len(normal.shape) == 2:\n        normal = normal.reshape((1, 2, 3))\n    f = normal[:, 0]-ray[:, 0]\n    distance = np.dot(normal[:, 1], f.T)/np.dot(normal[:, 1], ray[:, 1].T)\n    n = np.int64(np.amax(np.array([ray.shape[0], normal.shape[0]])))\n    normal = np.zeros((n, 2, 3))\n    normal[:, 0] = ray[:, 0]+distance.T*ray[:, 1]\n    distance = np.abs(distance)\n    if normal.shape[0] == 1:\n        normal = normal.reshape((2, 3))\n        distance = distance.reshape((1))\n    if distance.shape[0] == 1 and len(distance.shape) &gt; 1:\n        distance = distance.reshape((distance.shape[1]))\n    return normal, distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.intersect_w_triangle","title":"<code>intersect_w_triangle(ray, triangle)</code>","text":"<p>Definition to find intersection point of a ray with a triangle. Returns False for each variable if the ray doesn't intersect with a given triangle.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray (2 x 3). It can also be a list of rays (n x 2 x 3).\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>       Set of points in X,Y and Z to define a planar surface. It can also be a list of triangles (m x 3 x 3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between a starting point of a ray and the intersection point with a given triangle.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_w_triangle(ray, triangle):\n    \"\"\"\n    Definition to find intersection point of a ray with a triangle. Returns False for each variable if the ray doesn't intersect with a given triangle.\n\n    Parameters\n    ----------\n    ray          : torch.tensor\n                   A vector/ray (2 x 3). It can also be a list of rays (n x 2 x 3).\n    triangle     : torch.tensor\n                   Set of points in X,Y and Z to define a planar surface. It can also be a list of triangles (m x 3 x 3).\n\n    Returns\n    ----------\n    normal       : ndarray\n                   Surface normal at the point of intersection.\n    distance     : float\n                   Distance in between a starting point of a ray and the intersection point with a given triangle.\n    \"\"\"\n    normal, distance = intersect_w_surface(ray, triangle)\n    if is_it_on_triangle(normal[0], triangle[0], triangle[1], triangle[2]) == False:\n        return 0, 0\n    return normal, distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.intersection_kernel_for_parametric_surfaces","title":"<code>intersection_kernel_for_parametric_surfaces(distance, ray, parametric_surface, surface_function)</code>","text":"<p>Definition for the intersection kernel when dealing with parametric surfaces.</p> <p>Parameters:</p> <ul> <li> <code>distance</code>           \u2013            <pre><code>             Distance.\n</code></pre> </li> <li> <code>ray</code>           \u2013            <pre><code>             Ray.\n</code></pre> </li> <li> <code>parametric_surface</code>               (<code>ndarray</code>)           \u2013            <pre><code>             Array that defines a parametric surface.\n</code></pre> </li> <li> <code>surface_function</code>           \u2013            <pre><code>             Function to evaluate a point against a parametric surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>point</code> (              <code>ndarray</code> )          \u2013            <p>Location in X,Y,Z after propagation.</p> </li> <li> <code>error</code> (              <code>float</code> )          \u2013            <p>Error.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersection_kernel_for_parametric_surfaces(distance, ray, parametric_surface, surface_function):\n    \"\"\"\n    Definition for the intersection kernel when dealing with parametric surfaces.\n\n    Parameters\n    ----------\n    distance           : float\n                         Distance.\n    ray                : ndarray\n                         Ray.\n    parametric_surface : ndarray\n                         Array that defines a parametric surface.\n    surface_function   : ndarray\n                         Function to evaluate a point against a parametric surface.\n\n    Returns\n    ----------\n    point              : ndarray\n                         Location in X,Y,Z after propagation.\n    error              : float\n                         Error.\n    \"\"\"\n    new_ray = propagate_a_ray(ray, distance)\n    if len(new_ray) == 2:\n        new_ray = new_ray.reshape((1, 2, 3))\n    point = new_ray[:, 0]\n    error = surface_function(point, parametric_surface)\n    return error, point\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.is_it_on_triangle","title":"<code>is_it_on_triangle(pointtocheck, point0, point1, point2)</code>","text":"<p>Definition to check if a given point is inside a triangle. If the given point is inside a defined triangle, this definition returns True.</p> <p>Parameters:</p> <ul> <li> <code>pointtocheck</code>           \u2013            <pre><code>        Point to check.\n</code></pre> </li> <li> <code>point0</code>           \u2013            <pre><code>        First point of a triangle.\n</code></pre> </li> <li> <code>point1</code>           \u2013            <pre><code>        Second point of a triangle.\n</code></pre> </li> <li> <code>point2</code>           \u2013            <pre><code>        Third point of a triangle.\n</code></pre> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def is_it_on_triangle(pointtocheck, point0, point1, point2):\n    \"\"\"\n    Definition to check if a given point is inside a triangle. If the given point is inside a defined triangle, this definition returns True.\n\n    Parameters\n    ----------\n    pointtocheck  : list\n                    Point to check.\n    point0        : list\n                    First point of a triangle.\n    point1        : list\n                    Second point of a triangle.\n    point2        : list\n                    Third point of a triangle.\n    \"\"\"\n    # point0, point1 and point2 are the corners of the triangle.\n    pointtocheck = np.asarray(pointtocheck).reshape(3)\n    point0 = np.asarray(point0)\n    point1 = np.asarray(point1)\n    point2 = np.asarray(point2)\n    side0 = same_side(pointtocheck, point0, point1, point2)\n    side1 = same_side(pointtocheck, point1, point0, point2)\n    side2 = same_side(pointtocheck, point2, point0, point1)\n    if side0 == True and side1 == True and side2 == True:\n        return True\n    return False\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.point_to_ray_distance","title":"<code>point_to_ray_distance(point, ray_point_0, ray_point_1)</code>","text":"<p>Definition to find point's closest distance to a line represented with two points.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>      Point to be tested.\n</code></pre> </li> <li> <code>ray_point_0</code>               (<code>ndarray</code>)           \u2013            <pre><code>      First point to represent a line.\n</code></pre> </li> <li> <code>ray_point_1</code>               (<code>ndarray</code>)           \u2013            <pre><code>      Second point to represent a line.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Calculated distance.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def point_to_ray_distance(point, ray_point_0, ray_point_1):\n    \"\"\"\n    Definition to find point's closest distance to a line represented with two points.\n\n    Parameters\n    ----------\n    point       : ndarray\n                  Point to be tested.\n    ray_point_0 : ndarray\n                  First point to represent a line.\n    ray_point_1 : ndarray\n                  Second point to represent a line.\n\n    Returns\n    ----------\n    distance    : float\n                  Calculated distance.\n    \"\"\"\n    distance = np.sum(np.cross((point-ray_point_0), (point-ray_point_1))\n                      ** 2)/np.sum((ray_point_1-ray_point_0)**2)\n    return distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.propagate_a_ray","title":"<code>propagate_a_ray(ray, distance)</code>","text":"<p>Definition to propagate a ray at a certain given distance.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>     A ray.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>     Distance.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_ray</code> (              <code>ndarray</code> )          \u2013            <p>Propagated ray.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def propagate_a_ray(ray, distance):\n    \"\"\"\n    Definition to propagate a ray at a certain given distance.\n\n    Parameters\n    ----------\n    ray        : ndarray\n                 A ray.\n    distance   : float\n                 Distance.\n\n    Returns\n    ----------\n    new_ray    : ndarray\n                 Propagated ray.\n    \"\"\"\n    if len(ray.shape) == 2:\n        ray = ray.reshape((1, 2, 3))\n    new_ray = np.copy(ray)\n    new_ray[:, 0, 0] = distance*new_ray[:, 1, 0] + new_ray[:, 0, 0]\n    new_ray[:, 0, 1] = distance*new_ray[:, 1, 1] + new_ray[:, 0, 1]\n    new_ray[:, 0, 2] = distance*new_ray[:, 1, 2] + new_ray[:, 0, 2]\n    if new_ray.shape[0] == 1:\n        new_ray = new_ray.reshape((2, 3))\n    return new_ray\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.propagate_parametric_intersection_error","title":"<code>propagate_parametric_intersection_error(distance, error)</code>","text":"<p>Definition to propagate the error in parametric intersection to find the next distance to try.</p> <p>Parameters:</p> <ul> <li> <code>distance</code>           \u2013            <pre><code>       List that contains the new and the old distance.\n</code></pre> </li> <li> <code>error</code>           \u2013            <pre><code>       List that contains the new and the old error.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>list</code> )          \u2013            <p>New distance.</p> </li> <li> <code>error</code> (              <code>list</code> )          \u2013            <p>New error.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def propagate_parametric_intersection_error(distance, error):\n    \"\"\"\n    Definition to propagate the error in parametric intersection to find the next distance to try.\n\n    Parameters\n    ----------\n    distance     : list\n                   List that contains the new and the old distance.\n    error        : list\n                   List that contains the new and the old error.\n\n    Returns\n    ----------\n    distance     : list\n                   New distance.\n    error        : list\n                   New error.\n    \"\"\"\n    new_distance = distance[1]-error[1] * \\\n        (distance[1]-distance[0])/(error[1]-error[0])\n    distance[0] = distance[1]\n    distance[1] = np.abs(new_distance)\n    error[0] = error[1]\n    return distance, error\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.reflect","title":"<code>reflect(input_ray, normal)</code>","text":"<p>Definition to reflect an incoming ray from a surface defined by a surface normal. Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.</p> <p>Parameters:</p> <ul> <li> <code>input_ray</code>           \u2013            <pre><code>       A vector/ray (2x3). It can also be a list of rays (nx2x3).\n</code></pre> </li> <li> <code>normal</code>           \u2013            <pre><code>       A surface normal (2x3). It also be a list of normals (nx2x3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output_ray</code> (              <code>ndarray</code> )          \u2013            <p>Array that contains starting points and cosines of a reflected ray.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def reflect(input_ray, normal):\n    \"\"\" \n    Definition to reflect an incoming ray from a surface defined by a surface normal. Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.\n\n    Parameters\n    ----------\n    input_ray    : ndarray\n                   A vector/ray (2x3). It can also be a list of rays (nx2x3).\n    normal       : ndarray\n                   A surface normal (2x3). It also be a list of normals (nx2x3).\n\n    Returns\n    ----------\n    output_ray   : ndarray\n                   Array that contains starting points and cosines of a reflected ray.\n    \"\"\"\n    input_ray = np.asarray(input_ray)\n    normal = np.asarray(normal)\n    if len(input_ray.shape) == 2:\n        input_ray = input_ray.reshape((1, 2, 3))\n    if len(normal.shape) == 2:\n        normal = normal.reshape((1, 2, 3))\n    mu = 1\n    div = normal[:, 1, 0]**2 + normal[:, 1, 1]**2 + normal[:, 1, 2]**2\n    a = mu * (input_ray[:, 1, 0]*normal[:, 1, 0]\n              + input_ray[:, 1, 1]*normal[:, 1, 1]\n              + input_ray[:, 1, 2]*normal[:, 1, 2]) / div\n    n = np.int64(np.amax(np.array([normal.shape[0], input_ray.shape[0]])))\n    output_ray = np.zeros((n, 2, 3))\n    output_ray[:, 0] = normal[:, 0]\n    output_ray[:, 1] = input_ray[:, 1]-2*a*normal[:, 1]\n    if output_ray.shape[0] == 1:\n        output_ray = output_ray.reshape((2, 3))\n    return output_ray\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.rotate_point","title":"<code>rotate_point(point, angles=[0, 0, 0], mode='XYZ', origin=[0, 0, 0], offset=[0, 0, 0])</code>","text":"<p>Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>       A point.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>       Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n</code></pre> </li> <li> <code>origin</code>           \u2013            <pre><code>       Reference point for a rotation.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>       Shift with the given offset.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>Result of the rotation</p> </li> <li> <code>rotx</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along X axis.</p> </li> <li> <code>roty</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along Y axis.</p> </li> <li> <code>rotz</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along Z axis.</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotate_point(point, angles = [0, 0, 0], mode = 'XYZ', origin = [0, 0, 0], offset = [0, 0, 0]):\n    \"\"\"\n    Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.\n\n    Parameters\n    ----------\n    point        : ndarray\n                   A point.\n    angles       : list\n                   Rotation angles in degrees. \n    mode         : str\n                   Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n    origin       : list\n                   Reference point for a rotation.\n    offset       : list\n                   Shift with the given offset.\n\n    Returns\n    ----------\n    result       : ndarray\n                   Result of the rotation\n    rotx         : ndarray\n                   Rotation matrix along X axis.\n    roty         : ndarray\n                   Rotation matrix along Y axis.\n    rotz         : ndarray\n                   Rotation matrix along Z axis.\n    \"\"\"\n    point = np.asarray(point)\n    point -= np.asarray(origin)\n    rotx = rotmatx(angles[0])\n    roty = rotmaty(angles[1])\n    rotz = rotmatz(angles[2])\n    if mode == 'XYZ':\n        result = np.dot(rotz, np.dot(roty, np.dot(rotx, point)))\n    elif mode == 'XZY':\n        result = np.dot(roty, np.dot(rotz, np.dot(rotx, point)))\n    elif mode == 'YXZ':\n        result = np.dot(rotz, np.dot(rotx, np.dot(roty, point)))\n    elif mode == 'ZXY':\n        result = np.dot(roty, np.dot(rotx, np.dot(rotz, point)))\n    elif mode == 'ZYX':\n        result = np.dot(rotx, np.dot(roty, np.dot(rotz, point)))\n    result += np.asarray(origin)\n    result += np.asarray(offset)\n    return result, rotx, roty, rotz\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.rotate_points","title":"<code>rotate_points(points, angles=[0, 0, 0], mode='XYZ', origin=[0, 0, 0], offset=[0, 0, 0])</code>","text":"<p>Definition to rotate points.</p> <p>Parameters:</p> <ul> <li> <code>points</code>           \u2013            <pre><code>       Points.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>       Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n</code></pre> </li> <li> <code>origin</code>           \u2013            <pre><code>       Reference point for a rotation.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>       Shift with the given offset.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>Result of the rotation</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotate_points(points, angles = [0, 0, 0], mode = 'XYZ', origin = [0, 0, 0], offset = [0, 0, 0]):\n    \"\"\"\n    Definition to rotate points.\n\n    Parameters\n    ----------\n    points       : ndarray\n                   Points.\n    angles       : list\n                   Rotation angles in degrees. \n    mode         : str\n                   Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n    origin       : list\n                   Reference point for a rotation.\n    offset       : list\n                   Shift with the given offset.\n\n    Returns\n    ----------\n    result       : ndarray\n                   Result of the rotation   \n    \"\"\"\n    points = np.asarray(points)\n    if angles[0] == 0 and angles[1] == 0 and angles[2] == 0:\n        result = np.array(offset) + points\n        return result\n    points -= np.array(origin)\n    rotx = rotmatx(angles[0])\n    roty = rotmaty(angles[1])\n    rotz = rotmatz(angles[2])\n    if mode == 'XYZ':\n        result = np.dot(rotz, np.dot(roty, np.dot(rotx, points.T))).T\n    elif mode == 'XZY':\n        result = np.dot(roty, np.dot(rotz, np.dot(rotx, points.T))).T\n    elif mode == 'YXZ':\n        result = np.dot(rotz, np.dot(rotx, np.dot(roty, points.T))).T\n    elif mode == 'ZXY':\n        result = np.dot(roty, np.dot(rotx, np.dot(rotz, points.T))).T\n    elif mode == 'ZYX':\n        result = np.dot(rotx, np.dot(roty, np.dot(rotz, points.T))).T\n    result += np.array(origin)\n    result += np.array(offset)\n    return result\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.same_side","title":"<code>same_side(p1, p2, a, b)</code>","text":"<p>Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.</p> <p>Parameters:</p> <ul> <li> <code>p1</code>           \u2013            <pre><code>      Point(s) to check.\n</code></pre> </li> <li> <code>p2</code>           \u2013            <pre><code>      This is the point check against.\n</code></pre> </li> <li> <code>a</code>           \u2013            <pre><code>      First point that forms the line.\n</code></pre> </li> <li> <code>b</code>           \u2013            <pre><code>      Second point that forms the line.\n</code></pre> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def same_side(p1, p2, a, b):\n    \"\"\"\n    Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.\n\n    Parameters\n    ----------\n    p1          : list\n                  Point(s) to check.\n    p2          : list\n                  This is the point check against.\n    a           : list\n                  First point that forms the line.\n    b           : list\n                  Second point that forms the line.\n    \"\"\"\n    ba = np.subtract(b, a)\n    p1a = np.subtract(p1, a)\n    p2a = np.subtract(p2, a)\n    cp1 = np.cross(ba, p1a)\n    cp2 = np.cross(ba, p2a)\n    test = np.dot(cp1, cp2)\n    if len(p1.shape) &gt; 1:\n        return test &gt;= 0\n    if test &gt;= 0:\n        return True\n    return False\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.sphere_function","title":"<code>sphere_function(point, sphere)</code>","text":"<p>Definition of a sphere function. Evaluate a point against a sphere function.</p> <p>Parameters:</p> <ul> <li> <code>sphere</code>           \u2013            <pre><code>     Sphere parameters, XYZ center and radius.\n</code></pre> </li> <li> <code>point</code>           \u2013            <pre><code>     Point in XYZ.\n</code></pre> </li> </ul> Return <p>result     : float              Result of the evaluation. Zero if point is on sphere.</p> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def sphere_function(point, sphere):\n    \"\"\"\n    Definition of a sphere function. Evaluate a point against a sphere function.\n\n    Parameters\n    ----------\n    sphere     : ndarray\n                 Sphere parameters, XYZ center and radius.\n    point      : ndarray\n                 Point in XYZ.\n\n    Return\n    ----------\n    result     : float\n                 Result of the evaluation. Zero if point is on sphere.\n    \"\"\"\n    point = np.asarray(point)\n    if len(point.shape) == 1:\n        point = point.reshape((1, 3))\n    result = (point[:, 0]-sphere[0])**2 + (point[:, 1]-sphere[1]\n                                           )**2 + (point[:, 2]-sphere[2])**2 - sphere[3]**2\n    return result\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.get_cylinder_normal","title":"<code>get_cylinder_normal(point, cylinder)</code>","text":"<p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>        Point on a cylinder defined in X,Y,Z.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal_vector</code> (              <code>ndarray</code> )          \u2013            <p>Normal vector.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def get_cylinder_normal(point, cylinder):\n    \"\"\"\n    Parameters\n    ----------\n    point         : ndarray\n                    Point on a cylinder defined in X,Y,Z.\n\n    Returns\n    ----------\n    normal_vector : ndarray\n                    Normal vector.\n    \"\"\"\n    cylinder_ray = create_ray_from_two_points(cylinder[0:3], cylinder[4:7])\n    closest_point = closest_point_to_a_ray(\n        point,\n        cylinder_ray\n    )\n    normal_vector = create_ray_from_two_points(closest_point, point)\n    return normal_vector\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.get_sphere_normal","title":"<code>get_sphere_normal(point, sphere)</code>","text":"<p>Definition to get a normal of a point on a given sphere.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>        Point on sphere in X,Y,Z.\n</code></pre> </li> <li> <code>sphere</code>           \u2013            <pre><code>        Center defined in X,Y,Z and radius.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal_vector</code> (              <code>ndarray</code> )          \u2013            <p>Normal vector.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def get_sphere_normal(point, sphere):\n    \"\"\"\n    Definition to get a normal of a point on a given sphere.\n\n    Parameters\n    ----------\n    point         : ndarray\n                    Point on sphere in X,Y,Z.\n    sphere        : ndarray\n                    Center defined in X,Y,Z and radius.\n\n    Returns\n    ----------\n    normal_vector : ndarray\n                    Normal vector.\n    \"\"\"\n    if len(point.shape) == 1:\n        point = point.reshape((1, 3))\n    normal_vector = create_ray_from_two_points(point, sphere[0:3])\n    return normal_vector\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.get_triangle_normal","title":"<code>get_triangle_normal(triangle, triangle_center=None)</code>","text":"<p>Definition to calculate surface normal of a triangle.</p> <p>Parameters:</p> <ul> <li> <code>triangle</code>           \u2013            <pre><code>          Set of points in X,Y and Z to define a planar surface (3,3). It can also be list of triangles (mx3x3).\n</code></pre> </li> <li> <code>triangle_center</code>               (<code>ndarray</code>, default:                   <code>None</code> )           \u2013            <pre><code>          Center point of the given triangle. See odak.raytracing.center_of_triangle for more. In many scenarios you can accelerate things by precomputing triangle centers.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def get_triangle_normal(triangle, triangle_center=None):\n    \"\"\"\n    Definition to calculate surface normal of a triangle.\n\n    Parameters\n    ----------\n    triangle        : ndarray\n                      Set of points in X,Y and Z to define a planar surface (3,3). It can also be list of triangles (mx3x3).\n    triangle_center : ndarray\n                      Center point of the given triangle. See odak.raytracing.center_of_triangle for more. In many scenarios you can accelerate things by precomputing triangle centers.\n\n    Returns\n    ----------\n    normal          : ndarray\n                      Surface normal at the point of intersection.\n    \"\"\"\n    triangle = np.asarray(triangle)\n    if len(triangle.shape) == 2:\n        triangle = triangle.reshape((1, 3, 3))\n    normal = np.zeros((triangle.shape[0], 2, 3))\n    direction = np.cross(\n        triangle[:, 0]-triangle[:, 1], triangle[:, 2]-triangle[:, 1])\n    if type(triangle_center) == type(None):\n        normal[:, 0] = center_of_triangle(triangle)\n    else:\n        normal[:, 0] = triangle_center\n    normal[:, 1] = direction/np.sum(direction, axis=1)[0]\n    if normal.shape[0] == 1:\n        normal = normal.reshape((2, 3))\n    return normal\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.intersect_parametric","title":"<code>intersect_parametric(ray, parametric_surface, surface_function, surface_normal_function, target_error=1e-08, iter_no_limit=100000)</code>","text":"<p>Definition to intersect a ray with a parametric surface.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>                  Ray.\n</code></pre> </li> <li> <code>parametric_surface</code>           \u2013            <pre><code>                  Parameters of the surfaces.\n</code></pre> </li> <li> <code>surface_function</code>           \u2013            <pre><code>                  Function to evaluate a point against a surface.\n</code></pre> </li> <li> <code>surface_normal_function</code>               (<code>function</code>)           \u2013            <pre><code>                  Function to calculate surface normal for a given point on a surface.\n</code></pre> </li> <li> <code>target_error</code>           \u2013            <pre><code>                  Target error that defines the precision.\n</code></pre> </li> <li> <code>iter_no_limit</code>           \u2013            <pre><code>                  Maximum number of iterations.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Propagation distance.</p> </li> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>Ray that defines a surface normal for the intersection.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_parametric(ray, parametric_surface, surface_function, surface_normal_function, target_error=0.00000001, iter_no_limit=100000):\n    \"\"\"\n    Definition to intersect a ray with a parametric surface.\n\n    Parameters\n    ----------\n    ray                     : ndarray\n                              Ray.\n    parametric_surface      : ndarray\n                              Parameters of the surfaces.\n    surface_function        : function\n                              Function to evaluate a point against a surface.\n    surface_normal_function : function\n                              Function to calculate surface normal for a given point on a surface.\n    target_error            : float\n                              Target error that defines the precision.  \n    iter_no_limit           : int\n                              Maximum number of iterations.\n\n    Returns\n    ----------\n    distance                : float\n                              Propagation distance.\n    normal                  : ndarray\n                              Ray that defines a surface normal for the intersection.\n    \"\"\"\n    if len(ray.shape) == 2:\n        ray = ray.reshape((1, 2, 3))\n    error = [150, 100]\n    distance = [0, 0.1]\n    iter_no = 0\n    while np.abs(np.max(np.asarray(error[1]))) &gt; target_error:\n        error[1], point = intersection_kernel_for_parametric_surfaces(\n            distance[1],\n            ray,\n            parametric_surface,\n            surface_function\n        )\n        distance, error = propagate_parametric_intersection_error(\n            distance,\n            error\n        )\n        iter_no += 1\n        if iter_no &gt; iter_no_limit:\n            return False, False\n        if np.isnan(np.sum(point)):\n            return False, False\n    normal = surface_normal_function(\n        point,\n        parametric_surface\n    )\n    return distance[1], normal\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.intersect_w_circle","title":"<code>intersect_w_circle(ray, circle)</code>","text":"<p>Definition to find intersection point of a ray with a circle. Returns False for each variable if the ray doesn't intersect with a given circle. Returns distance as zero if there isn't an intersection.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>circle</code>           \u2013            <pre><code>       A list that contains (0) Set of points in X,Y and Z to define plane of a circle, (1) circle center, and (2) circle radius.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between a starting point of a ray and the intersection point with a given triangle.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_w_circle(ray, circle):\n    \"\"\"\n    Definition to find intersection point of a ray with a circle. Returns False for each variable if the ray doesn't intersect with a given circle. Returns distance as zero if there isn't an intersection.\n\n    Parameters\n    ----------\n    ray          : ndarray\n                   A vector/ray.\n    circle       : list\n                   A list that contains (0) Set of points in X,Y and Z to define plane of a circle, (1) circle center, and (2) circle radius.\n\n    Returns\n    ----------\n    normal       : ndarray\n                   Surface normal at the point of intersection.\n    distance     : float\n                   Distance in between a starting point of a ray and the intersection point with a given triangle.\n    \"\"\"\n    normal, distance = intersect_w_surface(ray, circle[0])\n    if len(normal.shape) == 2:\n        normal = normal.reshape((1, 2, 3))\n    distance_to_center = distance_between_two_points(normal[:, 0], circle[1])\n    distance[np.nonzero(distance_to_center &gt; circle[2])] = 0\n    if len(ray.shape) == 2:\n        normal = normal.reshape((2, 3))\n    return normal, distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.intersect_w_cylinder","title":"<code>intersect_w_cylinder(ray, cylinder)</code>","text":"<p>Definition to intersect a ray with a cylinder.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>     A ray definition.\n</code></pre> </li> <li> <code>cylinder</code>           \u2013            <pre><code>     A cylinder defined with a center in XYZ and radius of curvature.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>A ray defining surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Total optical propagation distance.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_w_cylinder(ray, cylinder):\n    \"\"\"\n    Definition to intersect a ray with a cylinder.\n\n    Parameters\n    ----------\n    ray        : ndarray\n                 A ray definition.\n    cylinder   : ndarray\n                 A cylinder defined with a center in XYZ and radius of curvature.\n\n    Returns\n    ----------\n    normal     : ndarray\n                 A ray defining surface normal at the point of intersection.\n    distance   : float\n                 Total optical propagation distance.\n    \"\"\"\n    distance, normal = intersect_parametric(\n        ray,\n        cylinder,\n        cylinder_function,\n        get_cylinder_normal\n    )\n    return normal, distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.intersect_w_sphere","title":"<code>intersect_w_sphere(ray, sphere)</code>","text":"<p>Definition to intersect a ray with a sphere.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>     A ray definition.\n</code></pre> </li> <li> <code>sphere</code>           \u2013            <pre><code>     A sphere defined with a center in XYZ and radius of curvature.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>A ray defining surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Total optical propagation distance.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_w_sphere(ray, sphere):\n    \"\"\"\n    Definition to intersect a ray with a sphere.\n\n    Parameters\n    ----------\n    ray        : ndarray\n                 A ray definition.\n    sphere     : ndarray\n                 A sphere defined with a center in XYZ and radius of curvature.\n\n    Returns\n    ----------\n    normal     : ndarray\n                 A ray defining surface normal at the point of intersection.\n    distance   : float\n                 Total optical propagation distance.\n    \"\"\"\n    distance, normal = intersect_parametric(\n        ray,\n        sphere,\n        sphere_function,\n        get_sphere_normal\n    )\n    return normal, distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.intersect_w_surface","title":"<code>intersect_w_surface(ray, points)</code>","text":"<p>Definition to find intersection point inbetween a surface and a ray. For more see: http://geomalgorithms.com/a06-_intersect-2.html</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>points</code>           \u2013            <pre><code>       Set of points in X,Y and Z to define a planar surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between starting point of a ray with it's intersection with a planar surface.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_w_surface(ray, points):\n    \"\"\"\n    Definition to find intersection point inbetween a surface and a ray. For more see: http://geomalgorithms.com/a06-_intersect-2.html\n\n    Parameters\n    ----------\n    ray          : ndarray\n                   A vector/ray.\n    points       : ndarray\n                   Set of points in X,Y and Z to define a planar surface.\n\n    Returns\n    ----------\n    normal       : ndarray\n                   Surface normal at the point of intersection.\n    distance     : float\n                   Distance in between starting point of a ray with it's intersection with a planar surface.\n    \"\"\"\n    points = np.asarray(points)\n    normal = get_triangle_normal(points)\n    if len(ray.shape) == 2:\n        ray = ray.reshape((1, 2, 3))\n    if len(points) == 2:\n        points = points.reshape((1, 3, 3))\n    if len(normal.shape) == 2:\n        normal = normal.reshape((1, 2, 3))\n    f = normal[:, 0]-ray[:, 0]\n    distance = np.dot(normal[:, 1], f.T)/np.dot(normal[:, 1], ray[:, 1].T)\n    n = np.int64(np.amax(np.array([ray.shape[0], normal.shape[0]])))\n    normal = np.zeros((n, 2, 3))\n    normal[:, 0] = ray[:, 0]+distance.T*ray[:, 1]\n    distance = np.abs(distance)\n    if normal.shape[0] == 1:\n        normal = normal.reshape((2, 3))\n        distance = distance.reshape((1))\n    if distance.shape[0] == 1 and len(distance.shape) &gt; 1:\n        distance = distance.reshape((distance.shape[1]))\n    return normal, distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.intersect_w_triangle","title":"<code>intersect_w_triangle(ray, triangle)</code>","text":"<p>Definition to find intersection point of a ray with a triangle. Returns False for each variable if the ray doesn't intersect with a given triangle.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>       A vector/ray (2 x 3). It can also be a list of rays (n x 2 x 3).\n</code></pre> </li> <li> <code>triangle</code>           \u2013            <pre><code>       Set of points in X,Y and Z to define a planar surface. It can also be a list of triangles (m x 3 x 3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>normal</code> (              <code>ndarray</code> )          \u2013            <p>Surface normal at the point of intersection.</p> </li> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between a starting point of a ray and the intersection point with a given triangle.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersect_w_triangle(ray, triangle):\n    \"\"\"\n    Definition to find intersection point of a ray with a triangle. Returns False for each variable if the ray doesn't intersect with a given triangle.\n\n    Parameters\n    ----------\n    ray          : torch.tensor\n                   A vector/ray (2 x 3). It can also be a list of rays (n x 2 x 3).\n    triangle     : torch.tensor\n                   Set of points in X,Y and Z to define a planar surface. It can also be a list of triangles (m x 3 x 3).\n\n    Returns\n    ----------\n    normal       : ndarray\n                   Surface normal at the point of intersection.\n    distance     : float\n                   Distance in between a starting point of a ray and the intersection point with a given triangle.\n    \"\"\"\n    normal, distance = intersect_w_surface(ray, triangle)\n    if is_it_on_triangle(normal[0], triangle[0], triangle[1], triangle[2]) == False:\n        return 0, 0\n    return normal, distance\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.intersection_kernel_for_parametric_surfaces","title":"<code>intersection_kernel_for_parametric_surfaces(distance, ray, parametric_surface, surface_function)</code>","text":"<p>Definition for the intersection kernel when dealing with parametric surfaces.</p> <p>Parameters:</p> <ul> <li> <code>distance</code>           \u2013            <pre><code>             Distance.\n</code></pre> </li> <li> <code>ray</code>           \u2013            <pre><code>             Ray.\n</code></pre> </li> <li> <code>parametric_surface</code>               (<code>ndarray</code>)           \u2013            <pre><code>             Array that defines a parametric surface.\n</code></pre> </li> <li> <code>surface_function</code>           \u2013            <pre><code>             Function to evaluate a point against a parametric surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>point</code> (              <code>ndarray</code> )          \u2013            <p>Location in X,Y,Z after propagation.</p> </li> <li> <code>error</code> (              <code>float</code> )          \u2013            <p>Error.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def intersection_kernel_for_parametric_surfaces(distance, ray, parametric_surface, surface_function):\n    \"\"\"\n    Definition for the intersection kernel when dealing with parametric surfaces.\n\n    Parameters\n    ----------\n    distance           : float\n                         Distance.\n    ray                : ndarray\n                         Ray.\n    parametric_surface : ndarray\n                         Array that defines a parametric surface.\n    surface_function   : ndarray\n                         Function to evaluate a point against a parametric surface.\n\n    Returns\n    ----------\n    point              : ndarray\n                         Location in X,Y,Z after propagation.\n    error              : float\n                         Error.\n    \"\"\"\n    new_ray = propagate_a_ray(ray, distance)\n    if len(new_ray) == 2:\n        new_ray = new_ray.reshape((1, 2, 3))\n    point = new_ray[:, 0]\n    error = surface_function(point, parametric_surface)\n    return error, point\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.propagate_parametric_intersection_error","title":"<code>propagate_parametric_intersection_error(distance, error)</code>","text":"<p>Definition to propagate the error in parametric intersection to find the next distance to try.</p> <p>Parameters:</p> <ul> <li> <code>distance</code>           \u2013            <pre><code>       List that contains the new and the old distance.\n</code></pre> </li> <li> <code>error</code>           \u2013            <pre><code>       List that contains the new and the old error.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>list</code> )          \u2013            <p>New distance.</p> </li> <li> <code>error</code> (              <code>list</code> )          \u2013            <p>New error.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def propagate_parametric_intersection_error(distance, error):\n    \"\"\"\n    Definition to propagate the error in parametric intersection to find the next distance to try.\n\n    Parameters\n    ----------\n    distance     : list\n                   List that contains the new and the old distance.\n    error        : list\n                   List that contains the new and the old error.\n\n    Returns\n    ----------\n    distance     : list\n                   New distance.\n    error        : list\n                   New error.\n    \"\"\"\n    new_distance = distance[1]-error[1] * \\\n        (distance[1]-distance[0])/(error[1]-error[0])\n    distance[0] = distance[1]\n    distance[1] = np.abs(new_distance)\n    error[0] = error[1]\n    return distance, error\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.boundary.reflect","title":"<code>reflect(input_ray, normal)</code>","text":"<p>Definition to reflect an incoming ray from a surface defined by a surface normal. Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.</p> <p>Parameters:</p> <ul> <li> <code>input_ray</code>           \u2013            <pre><code>       A vector/ray (2x3). It can also be a list of rays (nx2x3).\n</code></pre> </li> <li> <code>normal</code>           \u2013            <pre><code>       A surface normal (2x3). It also be a list of normals (nx2x3).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>output_ray</code> (              <code>ndarray</code> )          \u2013            <p>Array that contains starting points and cosines of a reflected ray.</p> </li> </ul> Source code in <code>odak/raytracing/boundary.py</code> <pre><code>def reflect(input_ray, normal):\n    \"\"\" \n    Definition to reflect an incoming ray from a surface defined by a surface normal. Used method described in G.H. Spencer and M.V.R.K. Murty, \"General Ray-Tracing Procedure\", 1961.\n\n    Parameters\n    ----------\n    input_ray    : ndarray\n                   A vector/ray (2x3). It can also be a list of rays (nx2x3).\n    normal       : ndarray\n                   A surface normal (2x3). It also be a list of normals (nx2x3).\n\n    Returns\n    ----------\n    output_ray   : ndarray\n                   Array that contains starting points and cosines of a reflected ray.\n    \"\"\"\n    input_ray = np.asarray(input_ray)\n    normal = np.asarray(normal)\n    if len(input_ray.shape) == 2:\n        input_ray = input_ray.reshape((1, 2, 3))\n    if len(normal.shape) == 2:\n        normal = normal.reshape((1, 2, 3))\n    mu = 1\n    div = normal[:, 1, 0]**2 + normal[:, 1, 1]**2 + normal[:, 1, 2]**2\n    a = mu * (input_ray[:, 1, 0]*normal[:, 1, 0]\n              + input_ray[:, 1, 1]*normal[:, 1, 1]\n              + input_ray[:, 1, 2]*normal[:, 1, 2]) / div\n    n = np.int64(np.amax(np.array([normal.shape[0], input_ray.shape[0]])))\n    output_ray = np.zeros((n, 2, 3))\n    output_ray[:, 0] = normal[:, 0]\n    output_ray[:, 1] = input_ray[:, 1]-2*a*normal[:, 1]\n    if output_ray.shape[0] == 1:\n        output_ray = output_ray.reshape((2, 3))\n    return output_ray\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.primitives.bring_plane_to_origin","title":"<code>bring_plane_to_origin(point, plane, shape=[10.0, 10.0], center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0], mode='XYZ')</code>","text":"<p>Definition to bring points back to reference origin with respect to a plane.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>             Point(s) to be tested.\n</code></pre> </li> <li> <code>shape</code>           \u2013            <pre><code>             Dimensions of the rectangle along X and Y axes.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>             Center of the rectangle.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>             Rotation angle of the rectangle.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>             Rotation mode of the rectangle, for more see odak.tools.rotate_point and odak.tools.rotate_points.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>transformed_points</code> (              <code>ndarray</code> )          \u2013            <p>Point(s) that are brought back to reference origin with respect to given plane.</p> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def bring_plane_to_origin(point, plane, shape=[10., 10.], center=[0., 0., 0.], angles=[0., 0., 0.], mode='XYZ'):\n    \"\"\"\n    Definition to bring points back to reference origin with respect to a plane.\n\n    Parameters\n    ----------\n    point              : ndarray\n                         Point(s) to be tested.\n    shape              : list\n                         Dimensions of the rectangle along X and Y axes.\n    center             : list\n                         Center of the rectangle.\n    angles             : list\n                         Rotation angle of the rectangle.\n    mode               : str\n                         Rotation mode of the rectangle, for more see odak.tools.rotate_point and odak.tools.rotate_points.\n\n    Returns\n    ----------\n    transformed_points : ndarray\n                         Point(s) that are brought back to reference origin with respect to given plane.\n    \"\"\"\n    if point.shape[0] == 3:\n        point = point.reshape((1, 3))\n    reverse_mode = mode[::-1]\n    angles = [-angles[0], -angles[1], -angles[2]]\n    center = np.asarray(center).reshape((1, 3))\n    transformed_points = point-center\n    transformed_points = rotate_points(\n        transformed_points,\n        angles=angles,\n        mode=reverse_mode,\n    )\n    if transformed_points.shape[0] == 1:\n        transformed_points = transformed_points.reshape((3,))\n    return transformed_points\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.primitives.center_of_triangle","title":"<code>center_of_triangle(triangle)</code>","text":"<p>Definition to calculate center of a triangle.</p> <p>Parameters:</p> <ul> <li> <code>triangle</code>           \u2013            <pre><code>        An array that contains three points defining a triangle (Mx3). It can also parallel process many triangles (NxMx3).\n</code></pre> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def center_of_triangle(triangle):\n    \"\"\"\n    Definition to calculate center of a triangle.\n\n    Parameters\n    ----------\n    triangle      : ndarray\n                    An array that contains three points defining a triangle (Mx3). It can also parallel process many triangles (NxMx3).\n    \"\"\"\n    if len(triangle.shape) == 2:\n        triangle = triangle.reshape((1, 3, 3))\n    center = np.mean(triangle, axis=1)\n    return center\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.primitives.cylinder_function","title":"<code>cylinder_function(point, cylinder)</code>","text":"<p>Definition of a cylinder function. Evaluate a point against a cylinder function. Inspired from https://mathworld.wolfram.com/Point-LineDistance3-Dimensional.html</p> <p>Parameters:</p> <ul> <li> <code>cylinder</code>           \u2013            <pre><code>     Cylinder parameters, XYZ center and radius.\n</code></pre> </li> <li> <code>point</code>           \u2013            <pre><code>     Point in XYZ.\n</code></pre> </li> </ul> Return <p>result     : float              Result of the evaluation. Zero if point is on sphere.</p> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def cylinder_function(point, cylinder):\n    \"\"\"\n    Definition of a cylinder function. Evaluate a point against a cylinder function. Inspired from https://mathworld.wolfram.com/Point-LineDistance3-Dimensional.html\n\n    Parameters\n    ----------\n    cylinder   : ndarray\n                 Cylinder parameters, XYZ center and radius.\n    point      : ndarray\n                 Point in XYZ.\n\n    Return\n    ----------\n    result     : float\n                 Result of the evaluation. Zero if point is on sphere.\n    \"\"\"\n    point = np.asarray(point)\n    if len(point.shape) == 1:\n        point = point.reshape((1, 3))\n    distance = point_to_ray_distance(\n        point,\n        np.array([cylinder[0], cylinder[1], cylinder[2]], dtype=np.float64),\n        np.array([cylinder[4], cylinder[5], cylinder[6]], dtype=np.float64)\n    )\n    r = cylinder[3]\n    result = distance - r ** 2\n    return result\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.primitives.define_circle","title":"<code>define_circle(center, radius, angles)</code>","text":"<p>Definition to describe a circle in a single variable packed form.</p> <p>Parameters:</p> <ul> <li> <code>center</code>           \u2013            <pre><code>  Center of a circle to be defined.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>  Radius of a circle to be defined.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>  Angular tilt of a circle.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>circle</code> (              <code>list</code> )          \u2013            <p>Single variable packed form.</p> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def define_circle(center, radius, angles):\n    \"\"\"\n    Definition to describe a circle in a single variable packed form.\n\n    Parameters\n    ----------\n    center  : float\n              Center of a circle to be defined.\n    radius  : float\n              Radius of a circle to be defined.\n    angles  : float\n              Angular tilt of a circle.\n\n    Returns\n    ----------\n    circle  : list\n              Single variable packed form.\n    \"\"\"\n    points = define_plane(center, angles=angles)\n    circle = [\n        points,\n        center,\n        radius\n    ]\n    return circle\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.primitives.define_cylinder","title":"<code>define_cylinder(center, radius, rotation=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to define a cylinder</p> <p>Parameters:</p> <ul> <li> <code>center</code>           \u2013            <pre><code>     Center of a cylinder in X,Y,Z.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>     Radius of a cylinder along X axis.\n</code></pre> </li> <li> <code>rotation</code>           \u2013            <pre><code>     Direction angles in degrees for the orientation of a cylinder.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>cylinder</code> (              <code>ndarray</code> )          \u2013            <p>Single variable packed form.</p> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def define_cylinder(center, radius, rotation=[0., 0., 0.]):\n    \"\"\"\n    Definition to define a cylinder\n\n    Parameters\n    ----------\n    center     : ndarray\n                 Center of a cylinder in X,Y,Z.\n    radius     : float\n                 Radius of a cylinder along X axis.\n    rotation   : list\n                 Direction angles in degrees for the orientation of a cylinder.\n\n    Returns\n    ----------\n    cylinder   : ndarray\n                 Single variable packed form.\n    \"\"\"\n    cylinder_ray = create_ray_from_angles(\n        np.asarray(center), np.asarray(rotation))\n    cylinder = np.array(\n        [\n            center[0],\n            center[1],\n            center[2],\n            radius,\n            center[0]+cylinder_ray[1, 0],\n            center[1]+cylinder_ray[1, 1],\n            center[2]+cylinder_ray[1, 2]\n        ],\n        dtype=np.float64\n    )\n    return cylinder\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.primitives.define_plane","title":"<code>define_plane(point, angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate a rotation matrix along X axis.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>       A point that is at the center of a plane.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>plane</code> (              <code>ndarray</code> )          \u2013            <p>Points defining plane.</p> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def define_plane(point, angles=[0., 0., 0.]):\n    \"\"\" \n    Definition to generate a rotation matrix along X axis.\n\n    Parameters\n    ----------\n    point        : ndarray\n                   A point that is at the center of a plane.\n    angles       : list\n                   Rotation angles in degrees.\n\n    Returns\n    ----------\n    plane        : ndarray\n                   Points defining plane.\n    \"\"\"\n    plane = np.array([\n        [10., 10., 0.],\n        [0., 10., 0.],\n        [0.,  0., 0.]\n    ], dtype=np.float64)\n    point = np.asarray(point)\n    for i in range(0, plane.shape[0]):\n        plane[i], _, _, _ = rotate_point(plane[i], angles=angles)\n        plane[i] = plane[i]+point\n    return plane\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.primitives.define_sphere","title":"<code>define_sphere(center, radius)</code>","text":"<p>Definition to define a sphere.</p> <p>Parameters:</p> <ul> <li> <code>center</code>           \u2013            <pre><code>     Center of a sphere in X,Y,Z.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>     Radius of a sphere.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>sphere</code> (              <code>ndarray</code> )          \u2013            <p>Single variable packed form.</p> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def define_sphere(center, radius):\n    \"\"\"\n    Definition to define a sphere.\n\n    Parameters\n    ----------\n    center     : ndarray\n                 Center of a sphere in X,Y,Z.\n    radius     : float\n                 Radius of a sphere.\n\n    Returns\n    ----------\n    sphere     : ndarray\n                 Single variable packed form.\n    \"\"\"\n    sphere = np.array(\n        [center[0], center[1], center[2], radius], dtype=np.float64)\n    return sphere\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.primitives.is_it_on_triangle","title":"<code>is_it_on_triangle(pointtocheck, point0, point1, point2)</code>","text":"<p>Definition to check if a given point is inside a triangle. If the given point is inside a defined triangle, this definition returns True.</p> <p>Parameters:</p> <ul> <li> <code>pointtocheck</code>           \u2013            <pre><code>        Point to check.\n</code></pre> </li> <li> <code>point0</code>           \u2013            <pre><code>        First point of a triangle.\n</code></pre> </li> <li> <code>point1</code>           \u2013            <pre><code>        Second point of a triangle.\n</code></pre> </li> <li> <code>point2</code>           \u2013            <pre><code>        Third point of a triangle.\n</code></pre> </li> </ul> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def is_it_on_triangle(pointtocheck, point0, point1, point2):\n    \"\"\"\n    Definition to check if a given point is inside a triangle. If the given point is inside a defined triangle, this definition returns True.\n\n    Parameters\n    ----------\n    pointtocheck  : list\n                    Point to check.\n    point0        : list\n                    First point of a triangle.\n    point1        : list\n                    Second point of a triangle.\n    point2        : list\n                    Third point of a triangle.\n    \"\"\"\n    # point0, point1 and point2 are the corners of the triangle.\n    pointtocheck = np.asarray(pointtocheck).reshape(3)\n    point0 = np.asarray(point0)\n    point1 = np.asarray(point1)\n    point2 = np.asarray(point2)\n    side0 = same_side(pointtocheck, point0, point1, point2)\n    side1 = same_side(pointtocheck, point1, point0, point2)\n    side2 = same_side(pointtocheck, point2, point0, point1)\n    if side0 == True and side1 == True and side2 == True:\n        return True\n    return False\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.primitives.sphere_function","title":"<code>sphere_function(point, sphere)</code>","text":"<p>Definition of a sphere function. Evaluate a point against a sphere function.</p> <p>Parameters:</p> <ul> <li> <code>sphere</code>           \u2013            <pre><code>     Sphere parameters, XYZ center and radius.\n</code></pre> </li> <li> <code>point</code>           \u2013            <pre><code>     Point in XYZ.\n</code></pre> </li> </ul> Return <p>result     : float              Result of the evaluation. Zero if point is on sphere.</p> Source code in <code>odak/raytracing/primitives.py</code> <pre><code>def sphere_function(point, sphere):\n    \"\"\"\n    Definition of a sphere function. Evaluate a point against a sphere function.\n\n    Parameters\n    ----------\n    sphere     : ndarray\n                 Sphere parameters, XYZ center and radius.\n    point      : ndarray\n                 Point in XYZ.\n\n    Return\n    ----------\n    result     : float\n                 Result of the evaluation. Zero if point is on sphere.\n    \"\"\"\n    point = np.asarray(point)\n    if len(point.shape) == 1:\n        point = point.reshape((1, 3))\n    result = (point[:, 0]-sphere[0])**2 + (point[:, 1]-sphere[1]\n                                           )**2 + (point[:, 2]-sphere[2])**2 - sphere[3]**2\n    return result\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.ray.calculate_intersection_of_two_rays","title":"<code>calculate_intersection_of_two_rays(ray0, ray1)</code>","text":"<p>Definition to calculate the intersection of two rays.</p> <p>Parameters:</p> <ul> <li> <code>ray0</code>           \u2013            <pre><code>     A ray.\n</code></pre> </li> <li> <code>ray1</code>           \u2013            <pre><code>     A ray.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>point</code> (              <code>ndarray</code> )          \u2013            <p>Point in X,Y,Z.</p> </li> <li> <code>distances</code> (              <code>ndarray</code> )          \u2013            <p>Distances.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def calculate_intersection_of_two_rays(ray0, ray1):\n    \"\"\"\n    Definition to calculate the intersection of two rays.\n\n    Parameters\n    ----------\n    ray0       : ndarray\n                 A ray.\n    ray1       : ndarray\n                 A ray.\n\n    Returns\n    ----------\n    point      : ndarray\n                 Point in X,Y,Z.\n    distances  : ndarray\n                 Distances.\n    \"\"\"\n    A = np.array([\n        [float(ray0[1][0]), float(ray1[1][0])],\n        [float(ray0[1][1]), float(ray1[1][1])],\n        [float(ray0[1][2]), float(ray1[1][2])]\n    ])\n    B = np.array([\n        ray0[0][0]-ray1[0][0],\n        ray0[0][1]-ray1[0][1],\n        ray0[0][2]-ray1[0][2]\n    ])\n    distances = np.linalg.lstsq(A, B, rcond=None)[0]\n    if np.allclose(np.dot(A, distances), B) == False:\n        distances = np.array([0, 0])\n    distances = distances[np.argsort(-distances)]\n    point = propagate_a_ray(ray0, distances[0])[0]\n    return point, distances\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.ray.create_ray","title":"<code>create_ray(x0y0z0, abg)</code>","text":"<p>Definition to create a ray.</p> <p>Parameters:</p> <ul> <li> <code>x0y0z0</code>           \u2013            <pre><code>       List that contains X,Y and Z start locations of a ray.\n</code></pre> </li> <li> <code>abg</code>           \u2013            <pre><code>       List that contaings angles in degrees with respect to the X,Y and Z axes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>ndarray</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def create_ray(x0y0z0, abg):\n    \"\"\"\n    Definition to create a ray.\n\n    Parameters\n    ----------\n    x0y0z0       : list\n                   List that contains X,Y and Z start locations of a ray.\n    abg          : list\n                   List that contaings angles in degrees with respect to the X,Y and Z axes.\n\n    Returns\n    ----------\n    ray          : ndarray\n                   Array that contains starting points and cosines of a created ray.\n    \"\"\"\n    # Due to Python 2 -&gt; Python 3.\n    x0, y0, z0 = x0y0z0\n    alpha, beta, gamma = abg\n    # Create a vector with the given points and angles in each direction\n    point = np.array([x0, y0, z0], dtype=np.float64)\n    alpha = np.cos(np.radians(alpha))\n    beta = np.cos(np.radians(beta))\n    gamma = np.cos(np.radians(gamma))\n    # Cosines vector.\n    cosines = np.array([alpha, beta, gamma], dtype=np.float64)\n    ray = np.array([point, cosines], dtype=np.float64)\n    return ray\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.ray.create_ray_from_angles","title":"<code>create_ray_from_angles(point, angles, mode='XYZ')</code>","text":"<p>Definition to create a ray from a point and angles.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>     Point in X,Y and Z.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>     Angles with X,Y,Z axes in degrees. All zeros point Z axis.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>     Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ    ,ZXY and ZYX modes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>ndarray</code> )          \u2013            <p>Created ray.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def create_ray_from_angles(point, angles, mode='XYZ'):\n    \"\"\"\n    Definition to create a ray from a point and angles.\n\n    Parameters\n    ----------\n    point      : ndarray\n                 Point in X,Y and Z.\n    angles     : ndarray\n                 Angles with X,Y,Z axes in degrees. All zeros point Z axis.\n    mode       : str\n                 Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ    ,ZXY and ZYX modes.\n\n    Returns\n    ----------\n    ray        : ndarray\n                 Created ray.\n    \"\"\"\n    if len(point.shape) == 1:\n        point = point.reshape((1, 3))\n    new_point = np.zeros(point.shape)\n    new_point[:, 2] += 5.\n    new_point = rotate_points(new_point, angles, mode=mode, offset=point[:, 0])\n    ray = create_ray_from_two_points(point, new_point)\n    if ray.shape[0] == 1:\n        ray = ray.reshape((2, 3))\n    return ray\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.ray.create_ray_from_two_points","title":"<code>create_ray_from_two_points(x0y0z0, x1y1z1)</code>","text":"<p>Definition to create a ray from two given points. Note that both inputs must match in shape.</p> <p>Parameters:</p> <ul> <li> <code>x0y0z0</code>           \u2013            <pre><code>       List that contains X,Y and Z start locations of a ray (3). It can also be a list of points as well (mx3). This is the starting point.\n</code></pre> </li> <li> <code>x1y1z1</code>           \u2013            <pre><code>       List that contains X,Y and Z ending locations of a ray (3). It can also be a list of points as well (mx3). This is the end point.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>ndarray</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def create_ray_from_two_points(x0y0z0, x1y1z1):\n    \"\"\"\n    Definition to create a ray from two given points. Note that both inputs must match in shape.\n\n    Parameters\n    ----------\n    x0y0z0       : list\n                   List that contains X,Y and Z start locations of a ray (3). It can also be a list of points as well (mx3). This is the starting point.\n    x1y1z1       : list\n                   List that contains X,Y and Z ending locations of a ray (3). It can also be a list of points as well (mx3). This is the end point.\n\n    Returns\n    ----------\n    ray          : ndarray\n                   Array that contains starting points and cosines of a created ray.\n    \"\"\"\n    x0y0z0 = np.asarray(x0y0z0, dtype=np.float64)\n    x1y1z1 = np.asarray(x1y1z1, dtype=np.float64)\n    if len(x0y0z0.shape) == 1:\n        x0y0z0 = x0y0z0.reshape((1, 3))\n    if len(x1y1z1.shape) == 1:\n        x1y1z1 = x1y1z1.reshape((1, 3))\n    xdiff = x1y1z1[:, 0] - x0y0z0[:, 0]\n    ydiff = x1y1z1[:, 1] - x0y0z0[:, 1]\n    zdiff = x1y1z1[:, 2] - x0y0z0[:, 2]\n    s = np.sqrt(xdiff ** 2 + ydiff ** 2 + zdiff ** 2)\n    s[s == 0] = np.nan\n    cosines = np.zeros((xdiff.shape[0], 3))\n    cosines[:, 0] = xdiff/s\n    cosines[:, 1] = ydiff/s\n    cosines[:, 2] = zdiff/s\n    ray = np.zeros((xdiff.shape[0], 2, 3), dtype=np.float64)\n    ray[:, 0] = x0y0z0\n    ray[:, 1] = cosines\n    if ray.shape[0] == 1:\n        ray = ray.reshape((2, 3))\n    return ray\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.ray.find_nearest_points","title":"<code>find_nearest_points(ray0, ray1)</code>","text":"<p>Find the nearest points on given rays with respect to the other ray.</p> <p>Parameters:</p> <ul> <li> <code>ray0</code>           \u2013            <pre><code>     A ray.\n</code></pre> </li> <li> <code>ray1</code>           \u2013            <pre><code>     A ray.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>c0</code> (              <code>ndarray</code> )          \u2013            <p>Closest point on ray0.</p> </li> <li> <code>c1</code> (              <code>ndarray</code> )          \u2013            <p>Closest point on ray1.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def find_nearest_points(ray0, ray1):\n    \"\"\"\n    Find the nearest points on given rays with respect to the other ray.\n\n    Parameters\n    ----------\n    ray0       : ndarray\n                 A ray.\n    ray1       : ndarray\n                 A ray.\n\n    Returns\n    ----------\n    c0         : ndarray\n                 Closest point on ray0.\n    c1         : ndarray\n                 Closest point on ray1.\n    \"\"\"\n    p0 = ray0[0].reshape(3,)\n    d0 = ray0[1].reshape(3,)\n    p1 = ray1[0].reshape(3,)\n    d1 = ray1[1].reshape(3,)\n    n = np.cross(d0, d1)\n    if np.all(n) == 0:\n        point, distances = calculate_intersection_of_two_rays(ray0, ray1)\n        c0 = c1 = point\n    else:\n        n0 = np.cross(d0, n)\n        n1 = np.cross(d1, n)\n        c0 = p0+(np.dot((p1-p0), n1)/np.dot(d0, n1))*d0\n        c1 = p1+(np.dot((p0-p1), n0)/np.dot(d1, n0))*d1\n    return c0, c1\n</code></pre>"},{"location":"odak/raytracing/#odak.raytracing.ray.propagate_a_ray","title":"<code>propagate_a_ray(ray, distance)</code>","text":"<p>Definition to propagate a ray at a certain given distance.</p> <p>Parameters:</p> <ul> <li> <code>ray</code>           \u2013            <pre><code>     A ray.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>     Distance.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_ray</code> (              <code>ndarray</code> )          \u2013            <p>Propagated ray.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def propagate_a_ray(ray, distance):\n    \"\"\"\n    Definition to propagate a ray at a certain given distance.\n\n    Parameters\n    ----------\n    ray        : ndarray\n                 A ray.\n    distance   : float\n                 Distance.\n\n    Returns\n    ----------\n    new_ray    : ndarray\n                 Propagated ray.\n    \"\"\"\n    if len(ray.shape) == 2:\n        ray = ray.reshape((1, 2, 3))\n    new_ray = np.copy(ray)\n    new_ray[:, 0, 0] = distance*new_ray[:, 1, 0] + new_ray[:, 0, 0]\n    new_ray[:, 0, 1] = distance*new_ray[:, 1, 1] + new_ray[:, 0, 1]\n    new_ray[:, 0, 2] = distance*new_ray[:, 1, 2] + new_ray[:, 0, 2]\n    if new_ray.shape[0] == 1:\n        new_ray = new_ray.reshape((2, 3))\n    return new_ray\n</code></pre>"},{"location":"odak/tools/","title":"odak.tools","text":"<p><code>odak.tools</code></p> <p>Provides necessary definitions for general tools used across the library.</p>"},{"location":"odak/tools/#odak.tools.batch_of_rays","title":"<code>batch_of_rays(entry, exit)</code>","text":"<p>Definition to generate a batch of rays with given entry point(s) and exit point(s). Note that the mapping is one to one, meaning nth item in your entry points list will exit from nth item in your exit list and generate that particular ray. Note that you can have a combination like nx3 points for entry or exit and 1 point for entry or exit. But if you have multiple points both for entry and exit, the number of points have to be same both for entry and exit.</p> <p>Parameters:</p> <ul> <li> <code>entry</code>           \u2013            <pre><code>     Either a single point with size of 3 or multiple points with the size of nx3.\n</code></pre> </li> <li> <code>exit</code>           \u2013            <pre><code>     Either a single point with size of 3 or multiple points with the size of nx3.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rays</code> (              <code>ndarray</code> )          \u2013            <p>Generated batch of rays.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def batch_of_rays(entry, exit):\n    \"\"\"\n    Definition to generate a batch of rays with given entry point(s) and exit point(s). Note that the mapping is one to one, meaning nth item in your entry points list will exit from nth item in your exit list and generate that particular ray. Note that you can have a combination like nx3 points for entry or exit and 1 point for entry or exit. But if you have multiple points both for entry and exit, the number of points have to be same both for entry and exit.\n\n    Parameters\n    ----------\n    entry      : ndarray\n                 Either a single point with size of 3 or multiple points with the size of nx3.\n    exit       : ndarray\n                 Either a single point with size of 3 or multiple points with the size of nx3.\n\n    Returns\n    ----------\n    rays       : ndarray\n                 Generated batch of rays.\n    \"\"\"\n    norays = np.array([0, 0])\n    if len(entry.shape) == 1:\n        entry = entry.reshape((1, 3))\n    if len(exit.shape) == 1:\n        exit = exit.reshape((1, 3))\n    norays = np.amax(np.asarray([entry.shape[0], exit.shape[0]]))\n    if norays &gt; exit.shape[0]:\n        exit = np.repeat(exit, norays, axis=0)\n    elif norays &gt; entry.shape[0]:\n        entry = np.repeat(entry, norays, axis=0)\n    rays = []\n    norays = int(norays)\n    for i in range(norays):\n        rays.append(\n            create_ray_from_two_points(\n                entry[i],\n                exit[i]\n            )\n        )\n    rays = np.asarray(rays)\n    return rays\n</code></pre>"},{"location":"odak/tools/#odak.tools.blur_gaussian","title":"<code>blur_gaussian(field, kernel_length=[21, 21], nsigma=[3, 3])</code>","text":"<p>A definition to blur a field using a Gaussian kernel.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>        MxN field.\n</code></pre> </li> <li> <code>kernel_length</code>               (<code>list</code>, default:                   <code>[21, 21]</code> )           \u2013            <pre><code>        Length of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>nsigma</code>           \u2013            <pre><code>        Sigma of the Gaussian kernel along X and Y axes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>blurred_field</code> (              <code>ndarray</code> )          \u2013            <p>Blurred field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def blur_gaussian(field, kernel_length=[21, 21], nsigma=[3, 3]):\n    \"\"\"\n    A definition to blur a field using a Gaussian kernel.\n\n    Parameters\n    ----------\n    field         : ndarray\n                    MxN field.\n    kernel_length : list\n                    Length of the Gaussian kernel along X and Y axes.\n    nsigma        : list\n                    Sigma of the Gaussian kernel along X and Y axes.\n\n    Returns\n    ----------\n    blurred_field : ndarray\n                    Blurred field.\n    \"\"\"\n    kernel = generate_2d_gaussian(kernel_length, nsigma)\n    kernel = zero_pad(kernel, field.shape)\n    blurred_field = convolve2d(field, kernel)\n    blurred_field = blurred_field/np.amax(blurred_field)\n    return blurred_field\n</code></pre>"},{"location":"odak/tools/#odak.tools.box_volume_sample","title":"<code>box_volume_sample(no=[10, 10, 10], size=[100.0, 100.0, 100.0], center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate samples in a box volume.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Physical size of the volume.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the volume.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the volume.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def box_volume_sample(no=[10, 10, 10], size=[100., 100., 100.], center=[0., 0., 0.], angles=[0., 0., 0.]):\n    \"\"\"\n    Definition to generate samples in a box volume.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    size        : list\n                  Physical size of the volume.\n    center      : list\n                  Center location of the volume.\n    angles      : list\n                  Tilt of the volume.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.zeros((no[0], no[1], no[2], 3))\n    x, y, z = np.mgrid[0:no[0], 0:no[1], 0:no[2]]\n    step = [\n        size[0]/no[0],\n        size[1]/no[1],\n        size[2]/no[2]\n    ]\n    samples[:, :, :, 0] = x*step[0]+step[0]/2.-size[0]/2.\n    samples[:, :, :, 1] = y*step[1]+step[1]/2.-size[1]/2.\n    samples[:, :, :, 2] = z*step[2]+step[2]/2.-size[2]/2.\n    samples = samples.reshape(\n        (samples.shape[0]*samples.shape[1]*samples.shape[2], samples.shape[3]))\n    samples = rotate_points(samples, angles=angles, offset=center)\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.check_directory","title":"<code>check_directory(directory)</code>","text":"<p>Definition to check if a directory exist. If it doesn't exist, this definition will create one.</p> <p>Parameters:</p> <ul> <li> <code>directory</code>           \u2013            <pre><code>        Full directory path.\n</code></pre> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def check_directory(directory):\n    \"\"\"\n    Definition to check if a directory exist. If it doesn't exist, this definition will create one.\n\n\n    Parameters\n    ----------\n    directory     : str\n                    Full directory path.\n    \"\"\"\n    if not os.path.exists(expanduser(directory)):\n        os.makedirs(expanduser(directory))\n        return False\n    return True\n</code></pre>"},{"location":"odak/tools/#odak.tools.circular_sample","title":"<code>circular_sample(no=[10, 10], radius=10.0, center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate samples inside a circle over a surface.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of the circle.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def circular_sample(no=[10, 10], radius=10., center=[0., 0., 0.], angles=[0., 0., 0.]):\n    \"\"\"\n    Definition to generate samples inside a circle over a surface.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    radius      : float\n                  Radius of the circle.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.zeros((no[0]+1, no[1]+1, 3))\n    r_angles, r = np.mgrid[0:no[0]+1, 0:no[1]+1]\n    r = r/np.amax(r)*radius\n    r_angles = r_angles/np.amax(r_angles)*np.pi*2\n    samples[:, :, 0] = r*np.cos(r_angles)\n    samples[:, :, 1] = r*np.sin(r_angles)\n    samples = samples[1:no[0]+1, 1:no[1]+1, :]\n    samples = samples.reshape(\n        (samples.shape[0]*samples.shape[1], samples.shape[2]))\n    samples = rotate_points(samples, angles=angles, offset=center)\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.circular_uniform_random_sample","title":"<code>circular_uniform_random_sample(no=[10, 50], radius=10.0, center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate sample inside a circle uniformly but randomly.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of the circle.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def circular_uniform_random_sample(no=[10, 50], radius=10., center=[0., 0., 0.], angles=[0., 0., 0.]):\n    \"\"\" \n    Definition to generate sample inside a circle uniformly but randomly.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    radius      : float\n                  Radius of the circle.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.empty((0, 3))\n    rs = np.sqrt(np.random.uniform(0, 1, no[0]))\n    angs = np.random.uniform(0, 2*np.pi, no[1])\n    for i in rs:\n        for angle in angs:\n            r = radius*i\n            point = np.array(\n                [float(r*np.cos(angle)), float(r*np.sin(angle)), 0])\n            samples = np.vstack((samples, point))\n    samples = rotate_points(samples, angles=angles, offset=center)\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.circular_uniform_sample","title":"<code>circular_uniform_sample(no=[10, 50], radius=10.0, center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate sample inside a circle uniformly.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of the circle.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def circular_uniform_sample(no=[10, 50], radius=10., center=[0., 0., 0.], angles=[0., 0., 0.]):\n    \"\"\"\n    Definition to generate sample inside a circle uniformly.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    radius      : float\n                  Radius of the circle.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.empty((0, 3))\n    for i in range(0, no[0]):\n        r = i/no[0]*radius\n        ang_no = no[1]*i/no[0]\n        for j in range(0, int(no[1]*i/no[0])):\n            angle = j/ang_no*2*np.pi\n            point = np.array(\n                [float(r*np.cos(angle)), float(r*np.sin(angle)), 0])\n            samples = np.vstack((samples, point))\n    samples = rotate_points(samples, angles=angles, offset=center)\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.closest_point_to_a_ray","title":"<code>closest_point_to_a_ray(point, ray)</code>","text":"<p>Definition to calculate the point on a ray that is closest to given point.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>        Given point in X,Y,Z.\n</code></pre> </li> <li> <code>ray</code>           \u2013            <pre><code>        Given ray.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>closest_point</code> (              <code>ndarray</code> )          \u2013            <p>Calculated closest point.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def closest_point_to_a_ray(point, ray):\n    \"\"\"\n    Definition to calculate the point on a ray that is closest to given point.\n\n    Parameters\n    ----------\n    point         : list\n                    Given point in X,Y,Z.\n    ray           : ndarray\n                    Given ray.\n\n    Returns\n    ---------\n    closest_point : ndarray\n                    Calculated closest point.\n    \"\"\"\n    from odak.raytracing import propagate_a_ray\n    if len(ray.shape) == 2:\n        ray = ray.reshape((1, 2, 3))\n    p0 = ray[:, 0]\n    p1 = propagate_a_ray(ray, 1.)\n    if len(p1.shape) == 2:\n        p1 = p1.reshape((1, 2, 3))\n    p1 = p1[:, 0]\n    p1 = p1.reshape(3)\n    p0 = p0.reshape(3)\n    point = point.reshape(3)\n    closest_distance = -np.dot((p0-point), (p1-p0))/np.sum((p1-p0)**2)\n    closest_point = propagate_a_ray(ray, closest_distance)[0]\n    return closest_point\n</code></pre>"},{"location":"odak/tools/#odak.tools.convert_bytes","title":"<code>convert_bytes(num)</code>","text":"<p>A definition to convert bytes to semantic scheme (MB,GB or alike). Inspired from https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python#2104083.</p> <p>Parameters:</p> <ul> <li> <code>num</code>           \u2013            <pre><code>     Size in bytes\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>num</code> (              <code>float</code> )          \u2013            <p>Size in new unit.</p> </li> <li> <code>x</code> (              <code>str</code> )          \u2013            <p>New unit bytes, KB, MB, GB or TB.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def convert_bytes(num):\n    \"\"\"\n    A definition to convert bytes to semantic scheme (MB,GB or alike). Inspired from https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python#2104083.\n\n\n    Parameters\n    ----------\n    num        : float\n                 Size in bytes\n\n\n    Returns\n    ----------\n    num        : float\n                 Size in new unit.\n    x          : str\n                 New unit bytes, KB, MB, GB or TB.\n    \"\"\"\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if num &lt; 1024.0:\n            return num, x\n        num /= 1024.0\n    return None, None\n</code></pre>"},{"location":"odak/tools/#odak.tools.convert_to_numpy","title":"<code>convert_to_numpy(a)</code>","text":"<p>A definition to convert Torch to Numpy.</p> <p>Parameters:</p> <ul> <li> <code>a</code>           \u2013            <pre><code>     Input Torch array.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>b</code> (              <code>ndarray</code> )          \u2013            <p>Converted array.</p> </li> </ul> Source code in <code>odak/tools/conversions.py</code> <pre><code>def convert_to_numpy(a):\n    \"\"\"\n    A definition to convert Torch to Numpy.\n\n    Parameters\n    ----------\n    a          : torch.Tensor\n                 Input Torch array.\n\n    Returns\n    ----------\n    b          : numpy.ndarray\n                 Converted array.\n    \"\"\"\n    b = a.to('cpu').detach().numpy()\n    return b\n</code></pre>"},{"location":"odak/tools/#odak.tools.convert_to_torch","title":"<code>convert_to_torch(a, grad=True)</code>","text":"<p>A definition to convert Numpy arrays to Torch.</p> <p>Parameters:</p> <ul> <li> <code>a</code>           \u2013            <pre><code>     Input Numpy array.\n</code></pre> </li> <li> <code>grad</code>           \u2013            <pre><code>     Set if the converted array requires gradient.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>c</code> (              <code>Tensor</code> )          \u2013            <p>Converted array.</p> </li> </ul> Source code in <code>odak/tools/conversions.py</code> <pre><code>def convert_to_torch(a, grad=True):\n    \"\"\"\n    A definition to convert Numpy arrays to Torch.\n\n    Parameters\n    ----------\n    a          : ndarray\n                 Input Numpy array.\n    grad       : bool\n                 Set if the converted array requires gradient.\n\n    Returns\n    ----------\n    c          : torch.Tensor\n                 Converted array.\n    \"\"\"\n    b = np.copy(a)\n    c = torch.from_numpy(b)\n    c.requires_grad_(grad)\n    return c\n</code></pre>"},{"location":"odak/tools/#odak.tools.convolve2d","title":"<code>convolve2d(field, kernel)</code>","text":"<p>Definition to convolve a field with a kernel by multiplying in frequency space.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field with MxN shape.\n</code></pre> </li> <li> <code>kernel</code>           \u2013            <pre><code>      Input kernel with MxN shape.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>ndarray</code> )          \u2013            <p>Convolved field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def convolve2d(field, kernel):\n    \"\"\"\n    Definition to convolve a field with a kernel by multiplying in frequency space.\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field with MxN shape.\n    kernel      : ndarray\n                  Input kernel with MxN shape.\n\n    Returns\n    ----------\n    new_field   : ndarray\n                  Convolved field.\n    \"\"\"\n    fr = np.fft.fft2(field)\n    fr2 = np.fft.fft2(np.flipud(np.fliplr(kernel)))\n    m, n = fr.shape\n    new_field = np.real(np.fft.ifft2(fr*fr2))\n    new_field = np.roll(new_field, int(-m/2+1), axis=0)\n    new_field = np.roll(new_field, int(-n/2+1), axis=1)\n    return new_field\n</code></pre>"},{"location":"odak/tools/#odak.tools.copy_file","title":"<code>copy_file(source, destination, follow_symlinks=True)</code>","text":"<p>Definition to copy a file from one location to another.</p> <p>Parameters:</p> <ul> <li> <code>source</code>           \u2013            <pre><code>          Source filename.\n</code></pre> </li> <li> <code>destination</code>           \u2013            <pre><code>          Destination filename.\n</code></pre> </li> <li> <code>follow_symlinks</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <pre><code>          Set to True to follow the source of symbolic links.\n</code></pre> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def copy_file(source, destination, follow_symlinks = True):\n    \"\"\"\n    Definition to copy a file from one location to another.\n\n\n\n    Parameters\n    ----------\n    source          : str\n                      Source filename.\n    destination     : str\n                      Destination filename.\n    follow_symlinks : bool\n                      Set to True to follow the source of symbolic links.\n    \"\"\"\n    return shutil.copyfile(\n                           expanduser(source),\n                           expanduser(source),\n                           follow_symlinks = follow_symlinks\n                          )\n</code></pre>"},{"location":"odak/tools/#odak.tools.create_empty_list","title":"<code>create_empty_list(dimensions=[1, 1])</code>","text":"<p>A definition to create an empty Pythonic list.</p> <p>Parameters:</p> <ul> <li> <code>dimensions</code>           \u2013            <pre><code>       Dimensions of the list to be created.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_list</code> (              <code>list</code> )          \u2013            <p>New empty list.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def create_empty_list(dimensions = [1, 1]):\n    \"\"\"\n    A definition to create an empty Pythonic list.\n\n    Parameters\n    ----------\n    dimensions   : list\n                   Dimensions of the list to be created.\n\n    Returns\n    -------\n    new_list     : list\n                   New empty list.\n    \"\"\"\n    new_list = 0\n    for n in reversed(dimensions):\n        new_list = [new_list] * n\n    return new_list\n</code></pre>"},{"location":"odak/tools/#odak.tools.create_ray_from_two_points","title":"<code>create_ray_from_two_points(x0y0z0, x1y1z1)</code>","text":"<p>Definition to create a ray from two given points. Note that both inputs must match in shape.</p> <p>Parameters:</p> <ul> <li> <code>x0y0z0</code>           \u2013            <pre><code>       List that contains X,Y and Z start locations of a ray (3). It can also be a list of points as well (mx3). This is the starting point.\n</code></pre> </li> <li> <code>x1y1z1</code>           \u2013            <pre><code>       List that contains X,Y and Z ending locations of a ray (3). It can also be a list of points as well (mx3). This is the end point.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>ndarray</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray.</p> </li> </ul> Source code in <code>odak/raytracing/ray.py</code> <pre><code>def create_ray_from_two_points(x0y0z0, x1y1z1):\n    \"\"\"\n    Definition to create a ray from two given points. Note that both inputs must match in shape.\n\n    Parameters\n    ----------\n    x0y0z0       : list\n                   List that contains X,Y and Z start locations of a ray (3). It can also be a list of points as well (mx3). This is the starting point.\n    x1y1z1       : list\n                   List that contains X,Y and Z ending locations of a ray (3). It can also be a list of points as well (mx3). This is the end point.\n\n    Returns\n    ----------\n    ray          : ndarray\n                   Array that contains starting points and cosines of a created ray.\n    \"\"\"\n    x0y0z0 = np.asarray(x0y0z0, dtype=np.float64)\n    x1y1z1 = np.asarray(x1y1z1, dtype=np.float64)\n    if len(x0y0z0.shape) == 1:\n        x0y0z0 = x0y0z0.reshape((1, 3))\n    if len(x1y1z1.shape) == 1:\n        x1y1z1 = x1y1z1.reshape((1, 3))\n    xdiff = x1y1z1[:, 0] - x0y0z0[:, 0]\n    ydiff = x1y1z1[:, 1] - x0y0z0[:, 1]\n    zdiff = x1y1z1[:, 2] - x0y0z0[:, 2]\n    s = np.sqrt(xdiff ** 2 + ydiff ** 2 + zdiff ** 2)\n    s[s == 0] = np.nan\n    cosines = np.zeros((xdiff.shape[0], 3))\n    cosines[:, 0] = xdiff/s\n    cosines[:, 1] = ydiff/s\n    cosines[:, 2] = zdiff/s\n    ray = np.zeros((xdiff.shape[0], 2, 3), dtype=np.float64)\n    ray[:, 0] = x0y0z0\n    ray[:, 1] = cosines\n    if ray.shape[0] == 1:\n        ray = ray.reshape((2, 3))\n    return ray\n</code></pre>"},{"location":"odak/tools/#odak.tools.crop_center","title":"<code>crop_center(field, size=None)</code>","text":"<p>Definition to crop the center of a field with 2Mx2N size. The outcome is a MxN array.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field 2Mx2N array.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>cropped</code> (              <code>ndarray</code> )          \u2013            <p>Cropped version of the input field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def crop_center(field, size=None):\n    \"\"\"\n    Definition to crop the center of a field with 2Mx2N size. The outcome is a MxN array.\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field 2Mx2N array.\n\n    Returns\n    ----------\n    cropped     : ndarray\n                  Cropped version of the input field.\n    \"\"\"\n    if type(size) == type(None):\n        qx = int(np.ceil(field.shape[0])/4)\n        qy = int(np.ceil(field.shape[1])/4)\n        cropped = np.copy(field[qx:3*qx, qy:3*qy])\n    else:\n        cx = int(np.ceil(field.shape[0]/2))\n        cy = int(np.ceil(field.shape[1]/2))\n        hx = int(np.ceil(size[0]/2))\n        hy = int(np.ceil(size[1]/2))\n        cropped = np.copy(field[cx-hx:cx+hx, cy-hy:cy+hy])\n    return cropped\n</code></pre>"},{"location":"odak/tools/#odak.tools.cross_product","title":"<code>cross_product(vector1, vector2)</code>","text":"<p>Definition to cross product two vectors and return the resultant vector. Used method described under: http://en.wikipedia.org/wiki/Cross_product</p> <p>Parameters:</p> <ul> <li> <code>vector1</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>vector2</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>ndarray</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def cross_product(vector1, vector2):\n    \"\"\"\n    Definition to cross product two vectors and return the resultant vector. Used method described under: http://en.wikipedia.org/wiki/Cross_product\n\n    Parameters\n    ----------\n    vector1      : ndarray\n                   A vector/ray.\n    vector2      : ndarray\n                   A vector/ray.\n\n    Returns\n    ----------\n    ray          : ndarray\n                   Array that contains starting points and cosines of a created ray.\n    \"\"\"\n    angle = np.cross(vector1[1].T, vector2[1].T)\n    angle = np.asarray(angle)\n    ray = np.array([vector1[0], angle], dtype=np.float32)\n    return ray\n</code></pre>"},{"location":"odak/tools/#odak.tools.distance_between_point_clouds","title":"<code>distance_between_point_clouds(points0, points1)</code>","text":"<p>A definition to find distance between every point in one cloud to other points in the other point cloud.</p> <p>Parameters:</p> <ul> <li> <code>points0</code>           \u2013            <pre><code>      Mx3 points.\n</code></pre> </li> <li> <code>points1</code>           \u2013            <pre><code>      Nx3 points.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distances</code> (              <code>ndarray</code> )          \u2013            <p>MxN distances.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def distance_between_point_clouds(points0, points1):\n    \"\"\"\n    A definition to find distance between every point in one cloud to other points in the other point cloud.\n    Parameters\n    ----------\n    points0     : ndarray\n                  Mx3 points.\n    points1     : ndarray\n                  Nx3 points.\n\n    Returns\n    ----------\n    distances   : ndarray\n                  MxN distances.\n    \"\"\"\n    c = points1.reshape((1, points1.shape[0], points1.shape[1]))\n    a = np.repeat(c, points0.shape[0], axis=0)\n    b = points0.reshape((points0.shape[0], 1, points0.shape[1]))\n    b = np.repeat(b, a.shape[1], axis=1)\n    distances = np.sqrt(np.sum((a-b)**2, axis=2))\n    return distances\n</code></pre>"},{"location":"odak/tools/#odak.tools.distance_between_two_points","title":"<code>distance_between_two_points(point1, point2)</code>","text":"<p>Definition to calculate distance between two given points.</p> <p>Parameters:</p> <ul> <li> <code>point1</code>           \u2013            <pre><code>      First point in X,Y,Z.\n</code></pre> </li> <li> <code>point2</code>           \u2013            <pre><code>      Second point in X,Y,Z.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between given two points.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def distance_between_two_points(point1, point2):\n    \"\"\"\n    Definition to calculate distance between two given points.\n\n    Parameters\n    ----------\n    point1      : list\n                  First point in X,Y,Z.\n    point2      : list\n                  Second point in X,Y,Z.\n\n    Returns\n    ----------\n    distance    : float\n                  Distance in between given two points.\n    \"\"\"\n    point1 = np.asarray(point1)\n    point2 = np.asarray(point2)\n    if len(point1.shape) == 1 and len(point2.shape) == 1:\n        distance = np.sqrt(np.sum((point1-point2)**2))\n    elif len(point1.shape) == 2 or len(point2.shape) == 2:\n        distance = np.sqrt(np.sum((point1-point2)**2, axis=1))\n    return distance\n</code></pre>"},{"location":"odak/tools/#odak.tools.expanduser","title":"<code>expanduser(filename)</code>","text":"<p>Definition to decode filename using namespaces and shortcuts.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>        Filename.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_filename</code> (              <code>str</code> )          \u2013            <p>Filename.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def expanduser(filename):\n    \"\"\"\n    Definition to decode filename using namespaces and shortcuts.\n\n\n    Parameters\n    ----------\n    filename      : str\n                    Filename.\n\n\n    Returns\n    -------\n    new_filename  : str\n                    Filename.\n    \"\"\"\n    new_filename = os.path.expanduser(filename)\n    return new_filename\n</code></pre>"},{"location":"odak/tools/#odak.tools.generate_2d_gaussian","title":"<code>generate_2d_gaussian(kernel_length=[21, 21], nsigma=[3, 3])</code>","text":"<p>Generate 2D Gaussian kernel. Inspired from https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy</p> <p>Parameters:</p> <ul> <li> <code>kernel_length</code>               (<code>list</code>, default:                   <code>[21, 21]</code> )           \u2013            <pre><code>        Length of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>nsigma</code>           \u2013            <pre><code>        Sigma of the Gaussian kernel along X and Y axes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>kernel_2d</code> (              <code>ndarray</code> )          \u2013            <p>Generated Gaussian kernel.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def generate_2d_gaussian(kernel_length=[21, 21], nsigma=[3, 3]):\n    \"\"\"\n    Generate 2D Gaussian kernel. Inspired from https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy\n\n    Parameters\n    ----------\n    kernel_length : list\n                    Length of the Gaussian kernel along X and Y axes.\n    nsigma        : list\n                    Sigma of the Gaussian kernel along X and Y axes.\n\n    Returns\n    ----------\n    kernel_2d     : ndarray\n                    Generated Gaussian kernel.\n    \"\"\"\n    x = np.linspace(-nsigma[0], nsigma[0], kernel_length[0]+1)\n    y = np.linspace(-nsigma[1], nsigma[1], kernel_length[1]+1)\n    xx, yy = np.meshgrid(x, y)\n    kernel_2d = np.exp(-0.5*(np.square(xx) /\n                       np.square(nsigma[0]) + np.square(yy)/np.square(nsigma[1])))\n    kernel_2d = kernel_2d/kernel_2d.sum()\n    return kernel_2d\n</code></pre>"},{"location":"odak/tools/#odak.tools.generate_bandlimits","title":"<code>generate_bandlimits(size=[512, 512], levels=9)</code>","text":"<p>A definition to calculate octaves used in bandlimiting frequencies in the frequency domain.</p> <p>Parameters:</p> <ul> <li> <code>size</code>           \u2013            <pre><code>     Size of each mask in octaves.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>masks</code> (              <code>ndarray</code> )          \u2013            <p>Masks (Octaves).</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def generate_bandlimits(size=[512, 512], levels=9):\n    \"\"\"\n    A definition to calculate octaves used in bandlimiting frequencies in the frequency domain.\n\n    Parameters\n    ----------\n    size       : list\n                 Size of each mask in octaves.\n\n    Returns\n    ----------\n    masks      : ndarray\n                 Masks (Octaves).\n    \"\"\"\n    masks = np.zeros((levels, size[0], size[1]))\n    cx = int(size[0]/2)\n    cy = int(size[1]/2)\n    for i in range(0, masks.shape[0]):\n        deltax = int((size[0])/(2**(i+1)))\n        deltay = int((size[1])/(2**(i+1)))\n        masks[\n            i,\n            cx-deltax:cx+deltax,\n            cy-deltay:cy+deltay\n        ] = 1.\n        masks[\n            i,\n            int(cx-deltax/2.):int(cx+deltax/2.),\n            int(cy-deltay/2.):int(cy+deltay/2.)\n        ] = 0.\n    masks = np.asarray(masks)\n    return masks\n</code></pre>"},{"location":"odak/tools/#odak.tools.get_base_filename","title":"<code>get_base_filename(filename)</code>","text":"<p>Definition to retrieve the base filename and extension type.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>         Input filename.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>basename</code> (              <code>str</code> )          \u2013            <p>Basename extracted from the filename.</p> </li> <li> <code>extension</code> (              <code>str</code> )          \u2013            <p>Extension extracted from the filename.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def get_base_filename(filename):\n    \"\"\"\n    Definition to retrieve the base filename and extension type.\n\n\n    Parameters\n    ----------\n    filename       : str\n                     Input filename.\n\n\n    Returns\n    -------\n    basename       : str\n                     Basename extracted from the filename.\n    extension      : str\n                     Extension extracted from the filename.\n    \"\"\"\n    cache = os.path.basename(filename)\n    basename = os.path.splitext(cache)[0]\n    extension = os.path.splitext(cache)[1]\n    return basename, extension\n</code></pre>"},{"location":"odak/tools/#odak.tools.grid_sample","title":"<code>grid_sample(no=[10, 10], size=[100.0, 100.0], center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate samples over a surface.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Physical size of the surface.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def grid_sample(no=[10, 10], size=[100., 100.], center=[0., 0., 0.], angles=[0., 0., 0.]):\n    \"\"\"\n    Definition to generate samples over a surface.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    size        : list\n                  Physical size of the surface.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.zeros((no[0], no[1], 3))\n    step = [\n        size[0]/(no[0]-1),\n        size[1]/(no[1]-1)\n    ]\n    x, y = np.mgrid[0:no[0], 0:no[1]]\n    samples[:, :, 0] = x*step[0]-size[0]/2.\n    samples[:, :, 1] = y*step[1]-size[1]/2.\n    samples = samples.reshape(\n        (samples.shape[0]*samples.shape[1], samples.shape[2]))\n    samples = rotate_points(samples, angles=angles, offset=center)\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.list_files","title":"<code>list_files(path, key='*.*', recursive=True)</code>","text":"<p>Definition to list files in a given path with a given key.</p> <p>Parameters:</p> <ul> <li> <code>path</code>           \u2013            <pre><code>      Path to a folder.\n</code></pre> </li> <li> <code>key</code>           \u2013            <pre><code>      Key used for scanning a path.\n</code></pre> </li> <li> <code>recursive</code>           \u2013            <pre><code>      If set True, scan the path recursively.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>files_list</code> (              <code>ndarray</code> )          \u2013            <p>list of files found in a given path.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def list_files(path, key = '*.*', recursive = True):\n    \"\"\"\n    Definition to list files in a given path with a given key.\n\n\n    Parameters\n    ----------\n    path        : str\n                  Path to a folder.\n    key         : str\n                  Key used for scanning a path.\n    recursive   : bool\n                  If set True, scan the path recursively.\n\n\n    Returns\n    ----------\n    files_list  : ndarray\n                  list of files found in a given path.\n    \"\"\"\n    if recursive == True:\n        search_result = pathlib.Path(expanduser(path)).rglob(key)\n    elif recursive == False:\n        search_result = pathlib.Path(expanduser(path)).glob(key)\n    files_list = []\n    for item in search_result:\n        files_list.append(str(item))\n    files_list = sorted(files_list)\n    return files_list\n</code></pre>"},{"location":"odak/tools/#odak.tools.load_dictionary","title":"<code>load_dictionary(filename)</code>","text":"<p>Definition to load a dictionary (JSON) file.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>        Filename.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>settings</code> (              <code>dict</code> )          \u2013            <p>Dictionary read from the file.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def load_dictionary(filename):\n    \"\"\"\n    Definition to load a dictionary (JSON) file.\n\n\n    Parameters\n    ----------\n    filename      : str\n                    Filename.\n\n\n    Returns\n    ----------\n    settings      : dict\n                    Dictionary read from the file.\n\n    \"\"\"\n    settings = json.load(open(expanduser(filename)))\n    return settings\n</code></pre>"},{"location":"odak/tools/#odak.tools.load_image","title":"<code>load_image(fn, normalizeby=0.0, torch_style=False)</code>","text":"<p>Definition to load an image from a given location as a Numpy array.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>normalizeby</code>           \u2013            <pre><code>       Value to to normalize images with. Default value of zero will lead to no normalization.\n</code></pre> </li> <li> <code>torch_style</code>           \u2013            <pre><code>       If set True, it will load an image mxnx3 as 3xmxn.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image</code> (              <code>ndarray</code> )          \u2013            <p>Image loaded as a Numpy array.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def load_image(fn, normalizeby = 0., torch_style = False):\n    \"\"\" \n    Definition to load an image from a given location as a Numpy array.\n\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    normalizeby  : float\n                   Value to to normalize images with. Default value of zero will lead to no normalization.\n    torch_style  : bool\n                   If set True, it will load an image mxnx3 as 3xmxn.\n\n\n    Returns\n    ----------\n    image        :  ndarray\n                    Image loaded as a Numpy array.\n\n    \"\"\"\n    image = cv2.imread(expanduser(fn), cv2.IMREAD_UNCHANGED)\n    if isinstance(image, type(None)):\n         logging.warning('Image not properly loaded. Check filename or image type.')    \n         sys.exit()\n    if len(image.shape) &gt; 2:\n        new_image = np.copy(image)\n        new_image[:, :, 0] = image[:, :, 2]\n        new_image[:, :, 2] = image[:, :, 0]\n        image = new_image\n    if normalizeby != 0.:\n        image = image * 1. / normalizeby\n    if torch_style == True and len(image.shape) &gt; 2:\n        image = np.moveaxis(image, -1, 0)\n    return image.astype(float)\n</code></pre>"},{"location":"odak/tools/#odak.tools.nufft2","title":"<code>nufft2(field, fx, fy, size=None, sign=1, eps=10 ** -12)</code>","text":"<p>A definition to take 2D Non-Uniform Fast Fourier Transform (NUFFT).</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field.\n</code></pre> </li> <li> <code>fx</code>           \u2013            <pre><code>      Frequencies along x axis.\n</code></pre> </li> <li> <code>fy</code>           \u2013            <pre><code>      Frequencies along y axis.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Size.\n</code></pre> </li> <li> <code>sign</code>           \u2013            <pre><code>      Sign of the exponential used in NUFFT kernel.\n</code></pre> </li> <li> <code>eps</code>           \u2013            <pre><code>      Accuracy of NUFFT.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>Inverse NUFFT of the input field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def nufft2(field, fx, fy, size=None, sign=1, eps=10**(-12)):\n    \"\"\"\n    A definition to take 2D Non-Uniform Fast Fourier Transform (NUFFT).\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field.\n    fx          : ndarray\n                  Frequencies along x axis.\n    fy          : ndarray\n                  Frequencies along y axis.\n    size        : list\n                  Size.\n    sign        : float\n                  Sign of the exponential used in NUFFT kernel.\n    eps         : float\n                  Accuracy of NUFFT.\n\n    Returns\n    ----------\n    result      : ndarray\n                  Inverse NUFFT of the input field.\n    \"\"\"\n    try:\n        import finufft\n    except:\n        print('odak.tools.nufft2 requires finufft to be installed: pip install finufft')\n    image = np.copy(field).astype(np.complex128)\n    result = finufft.nufft2d2(\n        fx.flatten(), fy.flatten(), image, eps=eps, isign=sign)\n    if type(size) == type(None):\n        result = result.reshape(field.shape)\n    else:\n        result = result.reshape(size)\n    return result\n</code></pre>"},{"location":"odak/tools/#odak.tools.nuifft2","title":"<code>nuifft2(field, fx, fy, size=None, sign=1, eps=10 ** -12)</code>","text":"<p>A definition to take 2D Adjoint Non-Uniform Fast Fourier Transform (NUFFT).</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field.\n</code></pre> </li> <li> <code>fx</code>           \u2013            <pre><code>      Frequencies along x axis.\n</code></pre> </li> <li> <code>fy</code>           \u2013            <pre><code>      Frequencies along y axis.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Shape of the NUFFT calculated for an input field.\n</code></pre> </li> <li> <code>sign</code>           \u2013            <pre><code>      Sign of the exponential used in NUFFT kernel.\n</code></pre> </li> <li> <code>eps</code>           \u2013            <pre><code>      Accuracy of NUFFT.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>NUFFT of the input field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def nuifft2(field, fx, fy, size=None, sign=1, eps=10**(-12)):\n    \"\"\"\n    A definition to take 2D Adjoint Non-Uniform Fast Fourier Transform (NUFFT).\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field.\n    fx          : ndarray\n                  Frequencies along x axis.\n    fy          : ndarray\n                  Frequencies along y axis.\n    size        : list or ndarray\n                  Shape of the NUFFT calculated for an input field.\n    sign        : float\n                  Sign of the exponential used in NUFFT kernel.\n    eps         : float\n                  Accuracy of NUFFT.\n\n    Returns\n    ----------\n    result      : ndarray\n                  NUFFT of the input field.\n    \"\"\"\n    try:\n        import finufft\n    except:\n        print('odak.tools.nuifft2 requires finufft to be installed: pip install finufft')\n    image = np.copy(field).astype(np.complex128)\n    if type(size) == type(None):\n        result = finufft.nufft2d1(\n            fx.flatten(),\n            fy.flatten(),\n            image.flatten(),\n            image.shape,\n            eps=eps,\n            isign=sign\n        )\n    else:\n        result = finufft.nufft2d1(\n            fx.flatten(),\n            fy.flatten(),\n            image.flatten(),\n            (size[0], size[1]),\n            eps=eps,\n            isign=sign\n        )\n    result = np.asarray(result)\n    return result\n</code></pre>"},{"location":"odak/tools/#odak.tools.point_to_ray_distance","title":"<code>point_to_ray_distance(point, ray_point_0, ray_point_1)</code>","text":"<p>Definition to find point's closest distance to a line represented with two points.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>      Point to be tested.\n</code></pre> </li> <li> <code>ray_point_0</code>               (<code>ndarray</code>)           \u2013            <pre><code>      First point to represent a line.\n</code></pre> </li> <li> <code>ray_point_1</code>               (<code>ndarray</code>)           \u2013            <pre><code>      Second point to represent a line.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Calculated distance.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def point_to_ray_distance(point, ray_point_0, ray_point_1):\n    \"\"\"\n    Definition to find point's closest distance to a line represented with two points.\n\n    Parameters\n    ----------\n    point       : ndarray\n                  Point to be tested.\n    ray_point_0 : ndarray\n                  First point to represent a line.\n    ray_point_1 : ndarray\n                  Second point to represent a line.\n\n    Returns\n    ----------\n    distance    : float\n                  Calculated distance.\n    \"\"\"\n    distance = np.sum(np.cross((point-ray_point_0), (point-ray_point_1))\n                      ** 2)/np.sum((ray_point_1-ray_point_0)**2)\n    return distance\n</code></pre>"},{"location":"odak/tools/#odak.tools.quantize","title":"<code>quantize(image_field, bits=4)</code>","text":"<p>Definitio to quantize a image field (0-255, 8 bit) to a certain bits level.</p> <p>Parameters:</p> <ul> <li> <code>image_field</code>               (<code>ndarray</code>)           \u2013            <pre><code>      Input image field.\n</code></pre> </li> <li> <code>bits</code>           \u2013            <pre><code>      A value in between 0 to 8. Can not be zero.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>ndarray</code> )          \u2013            <p>Quantized image field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def quantize(image_field, bits=4):\n    \"\"\"\n    Definitio to quantize a image field (0-255, 8 bit) to a certain bits level.\n\n    Parameters\n    ----------\n    image_field : ndarray\n                  Input image field.\n    bits        : int\n                  A value in between 0 to 8. Can not be zero.\n\n    Returns\n    ----------\n    new_field   : ndarray\n                  Quantized image field.\n    \"\"\"\n    divider = 2**(8-bits)\n    new_field = image_field/divider\n    new_field = new_field.astype(np.int64)\n    return new_field\n</code></pre>"},{"location":"odak/tools/#odak.tools.random_sample_point_cloud","title":"<code>random_sample_point_cloud(point_cloud, no, p=None)</code>","text":"<p>Definition to pull a subset of points from a point cloud with a given probability.</p> <p>Parameters:</p> <ul> <li> <code>point_cloud</code>           \u2013            <pre><code>       Point cloud array.\n</code></pre> </li> <li> <code>no</code>           \u2013            <pre><code>       Number of samples.\n</code></pre> </li> <li> <code>p</code>           \u2013            <pre><code>       Probability list in the same size as no.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>subset</code> (              <code>ndarray</code> )          \u2013            <p>Subset of the given point cloud.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def random_sample_point_cloud(point_cloud, no, p=None):\n    \"\"\"\n    Definition to pull a subset of points from a point cloud with a given probability.\n\n    Parameters\n    ----------\n    point_cloud  : ndarray\n                   Point cloud array.\n    no           : list\n                   Number of samples.\n    p            : list\n                   Probability list in the same size as no.\n\n    Returns\n    ----------\n    subset       : ndarray\n                   Subset of the given point cloud.\n    \"\"\"\n    choice = np.random.choice(point_cloud.shape[0], no, p)\n    subset = point_cloud[choice, :]\n    return subset\n</code></pre>"},{"location":"odak/tools/#odak.tools.read_PLY","title":"<code>read_PLY(fn, offset=[0, 0, 0], angles=[0.0, 0.0, 0.0], mode='XYZ')</code>","text":"<p>Definition to read a PLY file and extract meshes from a given PLY file. Note that rotation is always with respect to 0,0,0.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename of a PLY file.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>       Offset in X,Y,Z.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>       Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>triangles</code> (              <code>ndarray</code> )          \u2013            <p>Triangles from a given PLY file. Note that the triangles coming out of this function isn't always structured in the right order and with the size of (MxN)x3. You can use numpy's reshape to restructure it to mxnx3 if you know what you are doing.</p> </li> </ul> Source code in <code>odak/tools/asset.py</code> <pre><code>def read_PLY(fn, offset=[0, 0, 0], angles=[0., 0., 0.], mode='XYZ'):\n    \"\"\"\n    Definition to read a PLY file and extract meshes from a given PLY file. Note that rotation is always with respect to 0,0,0.\n\n    Parameters\n    ----------\n    fn           : string\n                   Filename of a PLY file.\n    offset       : ndarray\n                   Offset in X,Y,Z.\n    angles       : list\n                   Rotation angles in degrees.\n    mode         : str\n                   Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes. \n\n    Returns\n    ----------\n    triangles    : ndarray\n                  Triangles from a given PLY file. Note that the triangles coming out of this function isn't always structured in the right order and with the size of (MxN)x3. You can use numpy's reshape to restructure it to mxnx3 if you know what you are doing.\n    \"\"\"\n    if np.__name__ != 'numpy':\n        import numpy as np_ply\n    else:\n        np_ply = np\n    with open(fn, 'rb') as f:\n        plydata = PlyData.read(f)\n    triangle_ids = np_ply.vstack(plydata['face'].data['vertex_indices'])\n    triangles = []\n    for vertex_ids in triangle_ids:\n        triangle = [\n            rotate_point(plydata['vertex'][int(vertex_ids[0])\n                                           ].tolist(), angles=angles, offset=offset)[0],\n            rotate_point(plydata['vertex'][int(vertex_ids[1])\n                                           ].tolist(), angles=angles, offset=offset)[0],\n            rotate_point(plydata['vertex'][int(vertex_ids[2])\n                                           ].tolist(), angles=angles, offset=offset)[0]\n        ]\n        triangle = np_ply.asarray(triangle)\n        triangles.append(triangle)\n    triangles = np_ply.array(triangles)\n    triangles = np.asarray(triangles, dtype=np.float32)\n    return triangles\n</code></pre>"},{"location":"odak/tools/#odak.tools.read_PLY_point_cloud","title":"<code>read_PLY_point_cloud(filename)</code>","text":"<p>Definition to read a PLY file as a point cloud.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>       Filename of a PLY file.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>point_cloud</code> (              <code>ndarray</code> )          \u2013            <p>An array filled with poitns from the PLY file.</p> </li> </ul> Source code in <code>odak/tools/asset.py</code> <pre><code>def read_PLY_point_cloud(filename):\n    \"\"\"\n    Definition to read a PLY file as a point cloud.\n\n    Parameters\n    ----------\n    filename     : str\n                   Filename of a PLY file.\n\n    Returns\n    ----------\n    point_cloud  : ndarray\n                   An array filled with poitns from the PLY file.\n    \"\"\"\n    plydata = PlyData.read(filename)\n    if np.__name__ != 'numpy':\n        import numpy as np_ply\n        point_cloud = np_ply.zeros((plydata['vertex'][:].shape[0], 3))\n        point_cloud[:, 0] = np_ply.asarray(plydata['vertex']['x'][:])\n        point_cloud[:, 1] = np_ply.asarray(plydata['vertex']['y'][:])\n        point_cloud[:, 2] = np_ply.asarray(plydata['vertex']['z'][:])\n        point_cloud = np.asarray(point_cloud)\n    else:\n        point_cloud = np.zeros((plydata['vertex'][:].shape[0], 3))\n        point_cloud[:, 0] = np.asarray(plydata['vertex']['x'][:])\n        point_cloud[:, 1] = np.asarray(plydata['vertex']['y'][:])\n        point_cloud[:, 2] = np.asarray(plydata['vertex']['z'][:])\n    return point_cloud\n</code></pre>"},{"location":"odak/tools/#odak.tools.read_text_file","title":"<code>read_text_file(filename)</code>","text":"<p>Definition to read a given text file and convert it into a Pythonic list.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>          Source filename (i.e. test.txt).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>content</code> (              <code>list</code> )          \u2013            <p>Pythonic string list containing the text from the file provided.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def read_text_file(filename):\n    \"\"\"\n    Definition to read a given text file and convert it into a Pythonic list.\n\n\n    Parameters\n    ----------\n    filename        : str\n                      Source filename (i.e. test.txt).\n\n\n    Returns\n    -------\n    content         : list\n                      Pythonic string list containing the text from the file provided.\n    \"\"\"\n    content = []\n    loaded_file = open(expanduser(filename))\n    while line := loaded_file.readline():\n        content.append(line.rstrip())\n    return content\n</code></pre>"},{"location":"odak/tools/#odak.tools.resize_image","title":"<code>resize_image(img, target_size)</code>","text":"<p>Definition to resize a given image to a target shape.</p> <p>Parameters:</p> <ul> <li> <code>img</code>           \u2013            <pre><code>        MxN image to be resized.\n        Image must be normalized (0-1).\n</code></pre> </li> <li> <code>target_size</code>           \u2013            <pre><code>        Target shape.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>img</code> (              <code>ndarray</code> )          \u2013            <p>Resized image.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def resize_image(img, target_size):\n    \"\"\"\n    Definition to resize a given image to a target shape.\n\n\n    Parameters\n    ----------\n    img           : ndarray\n                    MxN image to be resized.\n                    Image must be normalized (0-1).\n    target_size   : list\n                    Target shape.\n\n\n    Returns\n    ----------\n    img           : ndarray\n                    Resized image.\n\n    \"\"\"\n    img = cv2.resize(img, dsize=(target_size[0], target_size[1]), interpolation=cv2.INTER_AREA)\n    return img\n</code></pre>"},{"location":"odak/tools/#odak.tools.rotate_point","title":"<code>rotate_point(point, angles=[0, 0, 0], mode='XYZ', origin=[0, 0, 0], offset=[0, 0, 0])</code>","text":"<p>Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>       A point.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>       Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n</code></pre> </li> <li> <code>origin</code>           \u2013            <pre><code>       Reference point for a rotation.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>       Shift with the given offset.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>Result of the rotation</p> </li> <li> <code>rotx</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along X axis.</p> </li> <li> <code>roty</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along Y axis.</p> </li> <li> <code>rotz</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along Z axis.</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotate_point(point, angles = [0, 0, 0], mode = 'XYZ', origin = [0, 0, 0], offset = [0, 0, 0]):\n    \"\"\"\n    Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.\n\n    Parameters\n    ----------\n    point        : ndarray\n                   A point.\n    angles       : list\n                   Rotation angles in degrees. \n    mode         : str\n                   Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n    origin       : list\n                   Reference point for a rotation.\n    offset       : list\n                   Shift with the given offset.\n\n    Returns\n    ----------\n    result       : ndarray\n                   Result of the rotation\n    rotx         : ndarray\n                   Rotation matrix along X axis.\n    roty         : ndarray\n                   Rotation matrix along Y axis.\n    rotz         : ndarray\n                   Rotation matrix along Z axis.\n    \"\"\"\n    point = np.asarray(point)\n    point -= np.asarray(origin)\n    rotx = rotmatx(angles[0])\n    roty = rotmaty(angles[1])\n    rotz = rotmatz(angles[2])\n    if mode == 'XYZ':\n        result = np.dot(rotz, np.dot(roty, np.dot(rotx, point)))\n    elif mode == 'XZY':\n        result = np.dot(roty, np.dot(rotz, np.dot(rotx, point)))\n    elif mode == 'YXZ':\n        result = np.dot(rotz, np.dot(rotx, np.dot(roty, point)))\n    elif mode == 'ZXY':\n        result = np.dot(roty, np.dot(rotx, np.dot(rotz, point)))\n    elif mode == 'ZYX':\n        result = np.dot(rotx, np.dot(roty, np.dot(rotz, point)))\n    result += np.asarray(origin)\n    result += np.asarray(offset)\n    return result, rotx, roty, rotz\n</code></pre>"},{"location":"odak/tools/#odak.tools.rotate_points","title":"<code>rotate_points(points, angles=[0, 0, 0], mode='XYZ', origin=[0, 0, 0], offset=[0, 0, 0])</code>","text":"<p>Definition to rotate points.</p> <p>Parameters:</p> <ul> <li> <code>points</code>           \u2013            <pre><code>       Points.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>       Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n</code></pre> </li> <li> <code>origin</code>           \u2013            <pre><code>       Reference point for a rotation.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>       Shift with the given offset.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>Result of the rotation</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotate_points(points, angles = [0, 0, 0], mode = 'XYZ', origin = [0, 0, 0], offset = [0, 0, 0]):\n    \"\"\"\n    Definition to rotate points.\n\n    Parameters\n    ----------\n    points       : ndarray\n                   Points.\n    angles       : list\n                   Rotation angles in degrees. \n    mode         : str\n                   Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n    origin       : list\n                   Reference point for a rotation.\n    offset       : list\n                   Shift with the given offset.\n\n    Returns\n    ----------\n    result       : ndarray\n                   Result of the rotation   \n    \"\"\"\n    points = np.asarray(points)\n    if angles[0] == 0 and angles[1] == 0 and angles[2] == 0:\n        result = np.array(offset) + points\n        return result\n    points -= np.array(origin)\n    rotx = rotmatx(angles[0])\n    roty = rotmaty(angles[1])\n    rotz = rotmatz(angles[2])\n    if mode == 'XYZ':\n        result = np.dot(rotz, np.dot(roty, np.dot(rotx, points.T))).T\n    elif mode == 'XZY':\n        result = np.dot(roty, np.dot(rotz, np.dot(rotx, points.T))).T\n    elif mode == 'YXZ':\n        result = np.dot(rotz, np.dot(rotx, np.dot(roty, points.T))).T\n    elif mode == 'ZXY':\n        result = np.dot(roty, np.dot(rotx, np.dot(rotz, points.T))).T\n    elif mode == 'ZYX':\n        result = np.dot(rotx, np.dot(roty, np.dot(rotz, points.T))).T\n    result += np.array(origin)\n    result += np.array(offset)\n    return result\n</code></pre>"},{"location":"odak/tools/#odak.tools.rotmatx","title":"<code>rotmatx(angle)</code>","text":"<p>Definition to generate a rotation matrix along X axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rotx</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along X axis.</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotmatx(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along X axis.\n\n    Parameters\n    ----------\n    angle        : list\n                   Rotation angles in degrees.\n\n    Returns\n    -------\n    rotx         : ndarray\n                   Rotation matrix along X axis.\n    \"\"\"\n    angle = np.float64(angle)\n    angle = np.radians(angle)\n    rotx = np.array([\n        [1.,               0.,               0.],\n        [0.,  math.cos(angle), -math.sin(angle)],\n        [0.,  math.sin(angle),  math.cos(angle)]\n    ], dtype=np.float64)\n    return rotx\n</code></pre>"},{"location":"odak/tools/#odak.tools.rotmaty","title":"<code>rotmaty(angle)</code>","text":"<p>Definition to generate a rotation matrix along Y axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>roty</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along Y axis.</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotmaty(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along Y axis.\n\n    Parameters\n    ----------\n    angle        : list\n                   Rotation angles in degrees.\n\n    Returns\n    -------\n    roty         : ndarray\n                   Rotation matrix along Y axis.\n    \"\"\"\n    angle = np.radians(angle)\n    roty = np.array([\n        [math.cos(angle),  0., math.sin(angle)],\n        [0.,               1.,              0.],\n        [-math.sin(angle), 0., math.cos(angle)]\n    ], dtype=np.float64)\n    return roty\n</code></pre>"},{"location":"odak/tools/#odak.tools.rotmatz","title":"<code>rotmatz(angle)</code>","text":"<p>Definition to generate a rotation matrix along Z axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rotz</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along Z axis.</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotmatz(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along Z axis.\n\n    Parameters\n    ----------\n    angle        : list\n                   Rotation angles in degrees.\n\n    Returns\n    -------\n    rotz         : ndarray\n                   Rotation matrix along Z axis.\n    \"\"\"\n    angle = np.radians(angle)\n    rotz = np.array([\n        [math.cos(angle), -math.sin(angle), 0.],\n        [math.sin(angle),  math.cos(angle), 0.],\n        [0.,               0., 1.]\n    ], dtype=np.float64)\n\n    return rotz\n</code></pre>"},{"location":"odak/tools/#odak.tools.same_side","title":"<code>same_side(p1, p2, a, b)</code>","text":"<p>Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.</p> <p>Parameters:</p> <ul> <li> <code>p1</code>           \u2013            <pre><code>      Point(s) to check.\n</code></pre> </li> <li> <code>p2</code>           \u2013            <pre><code>      This is the point check against.\n</code></pre> </li> <li> <code>a</code>           \u2013            <pre><code>      First point that forms the line.\n</code></pre> </li> <li> <code>b</code>           \u2013            <pre><code>      Second point that forms the line.\n</code></pre> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def same_side(p1, p2, a, b):\n    \"\"\"\n    Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.\n\n    Parameters\n    ----------\n    p1          : list\n                  Point(s) to check.\n    p2          : list\n                  This is the point check against.\n    a           : list\n                  First point that forms the line.\n    b           : list\n                  Second point that forms the line.\n    \"\"\"\n    ba = np.subtract(b, a)\n    p1a = np.subtract(p1, a)\n    p2a = np.subtract(p2, a)\n    cp1 = np.cross(ba, p1a)\n    cp2 = np.cross(ba, p2a)\n    test = np.dot(cp1, cp2)\n    if len(p1.shape) &gt; 1:\n        return test &gt;= 0\n    if test &gt;= 0:\n        return True\n    return False\n</code></pre>"},{"location":"odak/tools/#odak.tools.save_dictionary","title":"<code>save_dictionary(settings, filename)</code>","text":"<p>Definition to load a dictionary (JSON) file.</p> <p>Parameters:</p> <ul> <li> <code>settings</code>           \u2013            <pre><code>        Dictionary read from the file.\n</code></pre> </li> <li> <code>filename</code>           \u2013            <pre><code>        Filename.\n</code></pre> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def save_dictionary(settings, filename):\n    \"\"\"\n    Definition to load a dictionary (JSON) file.\n\n\n    Parameters\n    ----------\n    settings      : dict\n                    Dictionary read from the file.\n    filename      : str\n                    Filename.\n    \"\"\"\n    with open(expanduser(filename), 'w', encoding='utf-8') as f:\n        json.dump(settings, f, ensure_ascii=False, indent=4)\n    return settings\n</code></pre>"},{"location":"odak/tools/#odak.tools.save_image","title":"<code>save_image(fn, img, cmin=0, cmax=255, color_depth=8)</code>","text":"<p>Definition to save a Numpy array as an image.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>img</code>           \u2013            <pre><code>       A numpy array with NxMx3 or NxMx1 shapes.\n</code></pre> </li> <li> <code>cmin</code>           \u2013            <pre><code>       Minimum value that will be interpreted as 0 level in the final image.\n</code></pre> </li> <li> <code>cmax</code>           \u2013            <pre><code>       Maximum value that will be interpreted as 255 level in the final image.\n</code></pre> </li> <li> <code>color_depth</code>           \u2013            <pre><code>       Pixel color depth in bits, default is eight bits.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if successful.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def save_image(fn, img, cmin = 0, cmax = 255, color_depth = 8):\n    \"\"\"\n    Definition to save a Numpy array as an image.\n\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    img          : ndarray\n                   A numpy array with NxMx3 or NxMx1 shapes.\n    cmin         : int\n                   Minimum value that will be interpreted as 0 level in the final image.\n    cmax         : int\n                   Maximum value that will be interpreted as 255 level in the final image.\n    color_depth  : int\n                   Pixel color depth in bits, default is eight bits.\n\n\n    Returns\n    ----------\n    bool         :  bool\n                    True if successful.\n\n    \"\"\"\n    input_img = np.copy(img).astype(np.float32)\n    cmin = float(cmin)\n    cmax = float(cmax)\n    input_img[input_img &lt; cmin] = cmin\n    input_img[input_img &gt; cmax] = cmax\n    input_img /= cmax\n    input_img = input_img * 1. * (2 ** color_depth - 1)\n    if color_depth == 8:\n        input_img = input_img.astype(np.uint8)\n    elif color_depth == 16:\n        input_img = input_img.astype(np.uint16)\n    if len(input_img.shape) &gt; 2:\n        if input_img.shape[2] &gt; 1:\n            cache_img = np.copy(input_img)\n            cache_img[:, :, 0] = input_img[:, :, 2]\n            cache_img[:, :, 2] = input_img[:, :, 0]\n            input_img = cache_img\n    cv2.imwrite(expanduser(fn), input_img)\n    return True\n</code></pre>"},{"location":"odak/tools/#odak.tools.shell_command","title":"<code>shell_command(cmd, cwd='.', timeout=None, check=True)</code>","text":"<p>Definition to initiate shell commands.</p> <p>Parameters:</p> <ul> <li> <code>cmd</code>           \u2013            <pre><code>       Command to be executed.\n</code></pre> </li> <li> <code>cwd</code>           \u2013            <pre><code>       Working directory.\n</code></pre> </li> <li> <code>timeout</code>           \u2013            <pre><code>       Timeout if the process isn't complete in the given number of seconds.\n</code></pre> </li> <li> <code>check</code>           \u2013            <pre><code>       Set it to True to return the results and to enable timeout.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>proc</code> (              <code>Popen</code> )          \u2013            <p>Generated process.</p> </li> <li> <code>outs</code> (              <code>str</code> )          \u2013            <p>Outputs of the executed command, returns None when check is set to False.</p> </li> <li> <code>errs</code> (              <code>str</code> )          \u2013            <p>Errors of the executed command, returns None when check is set to False.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def shell_command(cmd, cwd = '.', timeout = None, check = True):\n    \"\"\"\n    Definition to initiate shell commands.\n\n\n    Parameters\n    ----------\n    cmd          : list\n                   Command to be executed. \n    cwd          : str\n                   Working directory.\n    timeout      : int\n                   Timeout if the process isn't complete in the given number of seconds.\n    check        : bool\n                   Set it to True to return the results and to enable timeout.\n\n\n    Returns\n    ----------\n    proc         : subprocess.Popen\n                   Generated process.\n    outs         : str\n                   Outputs of the executed command, returns None when check is set to False.\n    errs         : str\n                   Errors of the executed command, returns None when check is set to False.\n\n    \"\"\"\n    for item_id in range(len(cmd)):\n        cmd[item_id] = expanduser(cmd[item_id])\n    proc = subprocess.Popen(\n                            cmd,\n                            cwd = cwd,\n                            stdout = subprocess.PIPE\n                           )\n    if check == False:\n        return proc, None, None\n    try:\n        outs, errs = proc.communicate(timeout = timeout)\n    except subprocess.TimeoutExpired:\n        proc.kill()\n        outs, errs = proc.communicate()\n    return proc, outs, errs\n</code></pre>"},{"location":"odak/tools/#odak.tools.size_of_a_file","title":"<code>size_of_a_file(file_path)</code>","text":"<p>A definition to get size of a file with a relevant unit.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>           \u2013            <pre><code>     Path of the file.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>a</code> (              <code>float</code> )          \u2013            <p>Size of the file.</p> </li> <li> <code>b</code> (              <code>str</code> )          \u2013            <p>Unit of the size (bytes, KB, MB, GB or TB).</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def size_of_a_file(file_path):\n    \"\"\"\n    A definition to get size of a file with a relevant unit.\n\n\n    Parameters\n    ----------\n    file_path  : float\n                 Path of the file.\n\n\n    Returns\n    ----------\n    a          : float\n                 Size of the file.\n    b          : str\n                 Unit of the size (bytes, KB, MB, GB or TB).\n    \"\"\"\n    if os.path.isfile(file_path):\n        file_info = os.stat(file_path)\n        a, b = convert_bytes(file_info.st_size)\n        return a, b\n    return None, None\n</code></pre>"},{"location":"odak/tools/#odak.tools.sphere_sample","title":"<code>sphere_sample(no=[10, 10], radius=1.0, center=[0.0, 0.0, 0.0], k=[1, 2])</code>","text":"<p>Definition to generate a regular sample set on the surface of a sphere using polar coordinates.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of a sphere.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center of a sphere.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>      Multipliers for gathering samples. If you set k=[1,2] it will draw samples from a perfect sphere.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def sphere_sample(no=[10, 10], radius=1., center=[0., 0., 0.], k=[1, 2]):\n    \"\"\"\n    Definition to generate a regular sample set on the surface of a sphere using polar coordinates.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    radius      : float\n                  Radius of a sphere.\n    center      : list\n                  Center of a sphere.\n    k           : list\n                  Multipliers for gathering samples. If you set k=[1,2] it will draw samples from a perfect sphere.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.zeros((no[0], no[1], 3))\n    psi, teta = np.mgrid[0:no[0], 0:no[1]]\n    psi = k[0]*np.pi/no[0]*psi\n    teta = k[1]*np.pi/no[1]*teta\n    samples[:, :, 0] = center[0]+radius*np.sin(psi)*np.cos(teta)\n    samples[:, :, 1] = center[0]+radius*np.sin(psi)*np.sin(teta)\n    samples[:, :, 2] = center[0]+radius*np.cos(psi)\n    samples = samples.reshape((no[0]*no[1], 3))\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.sphere_sample_uniform","title":"<code>sphere_sample_uniform(no=[10, 10], radius=1.0, center=[0.0, 0.0, 0.0], k=[1, 2])</code>","text":"<p>Definition to generate an uniform sample set on the surface of a sphere using polar coordinates.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of a sphere.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center of a sphere.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>      Multipliers for gathering samples. If you set k=[1,2] it will draw samples from a perfect sphere.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def sphere_sample_uniform(no=[10, 10], radius=1., center=[0., 0., 0.], k=[1, 2]):\n    \"\"\"\n    Definition to generate an uniform sample set on the surface of a sphere using polar coordinates.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    radius      : float\n                  Radius of a sphere.\n    center      : list\n                  Center of a sphere.\n    k           : list\n                  Multipliers for gathering samples. If you set k=[1,2] it will draw samples from a perfect sphere.\n\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n\n    \"\"\"\n    samples = np.zeros((no[0], no[1], 3))\n    row = np.arange(0, no[0])\n    psi, teta = np.mgrid[0:no[0], 0:no[1]]\n    for psi_id in range(0, no[0]):\n        psi[psi_id] = np.roll(row, psi_id, axis=0)\n        teta[psi_id] = np.roll(row, -psi_id, axis=0)\n    psi = k[0]*np.pi/no[0]*psi\n    teta = k[1]*np.pi/no[1]*teta\n    samples[:, :, 0] = center[0]+radius*np.sin(psi)*np.cos(teta)\n    samples[:, :, 1] = center[1]+radius*np.sin(psi)*np.sin(teta)\n    samples[:, :, 2] = center[2]+radius*np.cos(psi)\n    samples = samples.reshape((no[0]*no[1], 3))\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.tilt_towards","title":"<code>tilt_towards(location, lookat)</code>","text":"<p>Definition to tilt surface normal of a plane towards a point.</p> <p>Parameters:</p> <ul> <li> <code>location</code>           \u2013            <pre><code>       Center of the plane to be tilted.\n</code></pre> </li> <li> <code>lookat</code>           \u2013            <pre><code>       Tilt towards this point.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>angles</code> (              <code>list</code> )          \u2013            <p>Rotation angles in degrees.</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def tilt_towards(location, lookat):\n    \"\"\"\n    Definition to tilt surface normal of a plane towards a point.\n\n    Parameters\n    ----------\n    location     : list\n                   Center of the plane to be tilted.\n    lookat       : list\n                   Tilt towards this point.\n\n    Returns\n    ----------\n    angles       : list\n                   Rotation angles in degrees.\n    \"\"\"\n    dx = location[0]-lookat[0]\n    dy = location[1]-lookat[1]\n    dz = location[2]-lookat[2]\n    dist = np.sqrt(dx**2+dy**2+dz**2)\n    phi = np.arctan2(dy, dx)\n    theta = np.arccos(dz/dist)\n    angles = [\n        0,\n        np.degrees(theta).tolist(),\n        np.degrees(phi).tolist()\n    ]\n    return angles\n</code></pre>"},{"location":"odak/tools/#odak.tools.write_PLY","title":"<code>write_PLY(triangles, savefn='output.ply')</code>","text":"<p>Definition to generate a PLY file from given points.</p> <p>Parameters:</p> <ul> <li> <code>triangles</code>           \u2013            <pre><code>      List of triangles with the size of Mx3x3.\n</code></pre> </li> <li> <code>savefn</code>           \u2013            <pre><code>      Filename for a PLY file.\n</code></pre> </li> </ul> Source code in <code>odak/tools/asset.py</code> <pre><code>def write_PLY(triangles, savefn = 'output.ply'):\n    \"\"\"\n    Definition to generate a PLY file from given points.\n\n    Parameters\n    ----------\n    triangles   : ndarray\n                  List of triangles with the size of Mx3x3.\n    savefn      : string\n                  Filename for a PLY file.\n    \"\"\"\n    tris = []\n    pnts = []\n    color = [255, 255, 255]\n    for tri_id in range(triangles.shape[0]):\n        tris.append(\n            (\n                [3*tri_id, 3*tri_id+1, 3*tri_id+2],\n                color[0],\n                color[1],\n                color[2]\n            )\n        )\n        for i in range(0, 3):\n            pnts.append(\n                (\n                    float(triangles[tri_id][i][0]),\n                    float(triangles[tri_id][i][1]),\n                    float(triangles[tri_id][i][2])\n                )\n            )\n    tris = np.asarray(tris, dtype=[\n                          ('vertex_indices', 'i4', (3,)), ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')])\n    pnts = np.asarray(pnts, dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])\n    # Save mesh.\n    el1 = PlyElement.describe(pnts, 'vertex', comments=['Vertex data'])\n    el2 = PlyElement.describe(tris, 'face', comments=['Face data'])\n    PlyData([el1, el2], text=\"True\").write(savefn)\n</code></pre>"},{"location":"odak/tools/#odak.tools.write_PLY_from_points","title":"<code>write_PLY_from_points(points, savefn='output.ply')</code>","text":"<p>Definition to generate a PLY file from given points.</p> <p>Parameters:</p> <ul> <li> <code>points</code>           \u2013            <pre><code>      List of points with the size of MxNx3.\n</code></pre> </li> <li> <code>savefn</code>           \u2013            <pre><code>      Filename for a PLY file.\n</code></pre> </li> </ul> Source code in <code>odak/tools/asset.py</code> <pre><code>def write_PLY_from_points(points, savefn='output.ply'):\n    \"\"\"\n    Definition to generate a PLY file from given points.\n\n    Parameters\n    ----------\n    points      : ndarray\n                  List of points with the size of MxNx3.\n    savefn      : string\n                  Filename for a PLY file.\n\n    \"\"\"\n    if np.__name__ != 'numpy':\n        import numpy as np_ply\n    else:\n        np_ply = np\n    # Generate equation\n    samples = [points.shape[0], points.shape[1]]\n    # Generate vertices.\n    pnts = []\n    tris = []\n    for idx in range(0, samples[0]):\n        for idy in range(0, samples[1]):\n            pnt = (points[idx, idy, 0],\n                   points[idx, idy, 1], points[idx, idy, 2])\n            pnts.append(pnt)\n    color = [255, 255, 255]\n    for idx in range(0, samples[0]-1):\n        for idy in range(0, samples[1]-1):\n            tris.append(([idy+(idx+1)*samples[0], idy+idx*samples[0],\n                        idy+1+idx*samples[0]], color[0], color[1], color[2]))\n            tris.append(([idy+(idx+1)*samples[0], idy+1+idx*samples[0],\n                        idy+1+(idx+1)*samples[0]], color[0], color[1], color[2]))\n    tris = np_ply.asarray(tris, dtype=[(\n        'vertex_indices', 'i4', (3,)), ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')])\n    pnts = np_ply.asarray(pnts, dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])\n    # Save mesh.\n    el1 = PlyElement.describe(pnts, 'vertex', comments=['Vertex data'])\n    el2 = PlyElement.describe(tris, 'face', comments=['Face data'])\n    PlyData([el1, el2], text=\"True\").write(savefn)\n</code></pre>"},{"location":"odak/tools/#odak.tools.write_to_text_file","title":"<code>write_to_text_file(content, filename, write_flag='w')</code>","text":"<p>Defininition to write a Pythonic list to a text file.</p> <p>Parameters:</p> <ul> <li> <code>content</code>           \u2013            <pre><code>          Pythonic string list to be written to a file.\n</code></pre> </li> <li> <code>filename</code>           \u2013            <pre><code>          Destination filename (i.e. test.txt).\n</code></pre> </li> <li> <code>write_flag</code>           \u2013            <pre><code>          Defines the interaction with the file. \n          The default is \"w\" (overwrite any existing content).\n          For more see: https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files\n</code></pre> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def write_to_text_file(content, filename, write_flag = 'w'):\n    \"\"\"\n    Defininition to write a Pythonic list to a text file.\n\n\n    Parameters\n    ----------\n    content         : list\n                      Pythonic string list to be written to a file.\n    filename        : str\n                      Destination filename (i.e. test.txt).\n    write_flag      : str\n                      Defines the interaction with the file. \n                      The default is \"w\" (overwrite any existing content).\n                      For more see: https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files\n    \"\"\"\n    with open(expanduser(filename), write_flag) as f:\n        for line in content:\n            f.write('{}\\n'.format(line))\n    return True\n</code></pre>"},{"location":"odak/tools/#odak.tools.zero_pad","title":"<code>zero_pad(field, size=None, method='center')</code>","text":"<p>Definition to zero pad a MxN array to 2Mx2N array.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>            Input field MxN array.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>            Size to be zeropadded.\n</code></pre> </li> <li> <code>method</code>           \u2013            <pre><code>            Zeropad either by placing the content to center or to the left.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field_zero_padded</code> (              <code>ndarray</code> )          \u2013            <p>Zeropadded version of the input field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def zero_pad(field, size=None, method='center'):\n    \"\"\"\n    Definition to zero pad a MxN array to 2Mx2N array.\n\n    Parameters\n    ----------\n    field             : ndarray\n                        Input field MxN array.\n    size              : list\n                        Size to be zeropadded.\n    method            : str\n                        Zeropad either by placing the content to center or to the left.\n\n    Returns\n    ----------\n    field_zero_padded : ndarray\n                        Zeropadded version of the input field.\n    \"\"\"\n    if type(size) == type(None):\n        hx = int(np.ceil(field.shape[0])/2)\n        hy = int(np.ceil(field.shape[1])/2)\n    else:\n        hx = int(np.ceil((size[0]-field.shape[0])/2))\n        hy = int(np.ceil((size[1]-field.shape[1])/2))\n    if method == 'center':\n        field_zero_padded = np.pad(\n            field, ([hx, hx], [hy, hy]), constant_values=(0, 0))\n    elif method == 'left aligned':\n        field_zero_padded = np.pad(\n            field, ([0, 2*hx], [0, 2*hy]), constant_values=(0, 0))\n    if type(size) != type(None):\n        field_zero_padded = field_zero_padded[0:size[0], 0:size[1]]\n    return field_zero_padded\n</code></pre>"},{"location":"odak/tools/#odak.tools.asset.read_PLY","title":"<code>read_PLY(fn, offset=[0, 0, 0], angles=[0.0, 0.0, 0.0], mode='XYZ')</code>","text":"<p>Definition to read a PLY file and extract meshes from a given PLY file. Note that rotation is always with respect to 0,0,0.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename of a PLY file.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>       Offset in X,Y,Z.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>       Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>triangles</code> (              <code>ndarray</code> )          \u2013            <p>Triangles from a given PLY file. Note that the triangles coming out of this function isn't always structured in the right order and with the size of (MxN)x3. You can use numpy's reshape to restructure it to mxnx3 if you know what you are doing.</p> </li> </ul> Source code in <code>odak/tools/asset.py</code> <pre><code>def read_PLY(fn, offset=[0, 0, 0], angles=[0., 0., 0.], mode='XYZ'):\n    \"\"\"\n    Definition to read a PLY file and extract meshes from a given PLY file. Note that rotation is always with respect to 0,0,0.\n\n    Parameters\n    ----------\n    fn           : string\n                   Filename of a PLY file.\n    offset       : ndarray\n                   Offset in X,Y,Z.\n    angles       : list\n                   Rotation angles in degrees.\n    mode         : str\n                   Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes. \n\n    Returns\n    ----------\n    triangles    : ndarray\n                  Triangles from a given PLY file. Note that the triangles coming out of this function isn't always structured in the right order and with the size of (MxN)x3. You can use numpy's reshape to restructure it to mxnx3 if you know what you are doing.\n    \"\"\"\n    if np.__name__ != 'numpy':\n        import numpy as np_ply\n    else:\n        np_ply = np\n    with open(fn, 'rb') as f:\n        plydata = PlyData.read(f)\n    triangle_ids = np_ply.vstack(plydata['face'].data['vertex_indices'])\n    triangles = []\n    for vertex_ids in triangle_ids:\n        triangle = [\n            rotate_point(plydata['vertex'][int(vertex_ids[0])\n                                           ].tolist(), angles=angles, offset=offset)[0],\n            rotate_point(plydata['vertex'][int(vertex_ids[1])\n                                           ].tolist(), angles=angles, offset=offset)[0],\n            rotate_point(plydata['vertex'][int(vertex_ids[2])\n                                           ].tolist(), angles=angles, offset=offset)[0]\n        ]\n        triangle = np_ply.asarray(triangle)\n        triangles.append(triangle)\n    triangles = np_ply.array(triangles)\n    triangles = np.asarray(triangles, dtype=np.float32)\n    return triangles\n</code></pre>"},{"location":"odak/tools/#odak.tools.asset.read_PLY_point_cloud","title":"<code>read_PLY_point_cloud(filename)</code>","text":"<p>Definition to read a PLY file as a point cloud.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>       Filename of a PLY file.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>point_cloud</code> (              <code>ndarray</code> )          \u2013            <p>An array filled with poitns from the PLY file.</p> </li> </ul> Source code in <code>odak/tools/asset.py</code> <pre><code>def read_PLY_point_cloud(filename):\n    \"\"\"\n    Definition to read a PLY file as a point cloud.\n\n    Parameters\n    ----------\n    filename     : str\n                   Filename of a PLY file.\n\n    Returns\n    ----------\n    point_cloud  : ndarray\n                   An array filled with poitns from the PLY file.\n    \"\"\"\n    plydata = PlyData.read(filename)\n    if np.__name__ != 'numpy':\n        import numpy as np_ply\n        point_cloud = np_ply.zeros((plydata['vertex'][:].shape[0], 3))\n        point_cloud[:, 0] = np_ply.asarray(plydata['vertex']['x'][:])\n        point_cloud[:, 1] = np_ply.asarray(plydata['vertex']['y'][:])\n        point_cloud[:, 2] = np_ply.asarray(plydata['vertex']['z'][:])\n        point_cloud = np.asarray(point_cloud)\n    else:\n        point_cloud = np.zeros((plydata['vertex'][:].shape[0], 3))\n        point_cloud[:, 0] = np.asarray(plydata['vertex']['x'][:])\n        point_cloud[:, 1] = np.asarray(plydata['vertex']['y'][:])\n        point_cloud[:, 2] = np.asarray(plydata['vertex']['z'][:])\n    return point_cloud\n</code></pre>"},{"location":"odak/tools/#odak.tools.asset.write_PLY","title":"<code>write_PLY(triangles, savefn='output.ply')</code>","text":"<p>Definition to generate a PLY file from given points.</p> <p>Parameters:</p> <ul> <li> <code>triangles</code>           \u2013            <pre><code>      List of triangles with the size of Mx3x3.\n</code></pre> </li> <li> <code>savefn</code>           \u2013            <pre><code>      Filename for a PLY file.\n</code></pre> </li> </ul> Source code in <code>odak/tools/asset.py</code> <pre><code>def write_PLY(triangles, savefn = 'output.ply'):\n    \"\"\"\n    Definition to generate a PLY file from given points.\n\n    Parameters\n    ----------\n    triangles   : ndarray\n                  List of triangles with the size of Mx3x3.\n    savefn      : string\n                  Filename for a PLY file.\n    \"\"\"\n    tris = []\n    pnts = []\n    color = [255, 255, 255]\n    for tri_id in range(triangles.shape[0]):\n        tris.append(\n            (\n                [3*tri_id, 3*tri_id+1, 3*tri_id+2],\n                color[0],\n                color[1],\n                color[2]\n            )\n        )\n        for i in range(0, 3):\n            pnts.append(\n                (\n                    float(triangles[tri_id][i][0]),\n                    float(triangles[tri_id][i][1]),\n                    float(triangles[tri_id][i][2])\n                )\n            )\n    tris = np.asarray(tris, dtype=[\n                          ('vertex_indices', 'i4', (3,)), ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')])\n    pnts = np.asarray(pnts, dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])\n    # Save mesh.\n    el1 = PlyElement.describe(pnts, 'vertex', comments=['Vertex data'])\n    el2 = PlyElement.describe(tris, 'face', comments=['Face data'])\n    PlyData([el1, el2], text=\"True\").write(savefn)\n</code></pre>"},{"location":"odak/tools/#odak.tools.asset.write_PLY_from_points","title":"<code>write_PLY_from_points(points, savefn='output.ply')</code>","text":"<p>Definition to generate a PLY file from given points.</p> <p>Parameters:</p> <ul> <li> <code>points</code>           \u2013            <pre><code>      List of points with the size of MxNx3.\n</code></pre> </li> <li> <code>savefn</code>           \u2013            <pre><code>      Filename for a PLY file.\n</code></pre> </li> </ul> Source code in <code>odak/tools/asset.py</code> <pre><code>def write_PLY_from_points(points, savefn='output.ply'):\n    \"\"\"\n    Definition to generate a PLY file from given points.\n\n    Parameters\n    ----------\n    points      : ndarray\n                  List of points with the size of MxNx3.\n    savefn      : string\n                  Filename for a PLY file.\n\n    \"\"\"\n    if np.__name__ != 'numpy':\n        import numpy as np_ply\n    else:\n        np_ply = np\n    # Generate equation\n    samples = [points.shape[0], points.shape[1]]\n    # Generate vertices.\n    pnts = []\n    tris = []\n    for idx in range(0, samples[0]):\n        for idy in range(0, samples[1]):\n            pnt = (points[idx, idy, 0],\n                   points[idx, idy, 1], points[idx, idy, 2])\n            pnts.append(pnt)\n    color = [255, 255, 255]\n    for idx in range(0, samples[0]-1):\n        for idy in range(0, samples[1]-1):\n            tris.append(([idy+(idx+1)*samples[0], idy+idx*samples[0],\n                        idy+1+idx*samples[0]], color[0], color[1], color[2]))\n            tris.append(([idy+(idx+1)*samples[0], idy+1+idx*samples[0],\n                        idy+1+(idx+1)*samples[0]], color[0], color[1], color[2]))\n    tris = np_ply.asarray(tris, dtype=[(\n        'vertex_indices', 'i4', (3,)), ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')])\n    pnts = np_ply.asarray(pnts, dtype=[('x', 'f4'), ('y', 'f4'), ('z', 'f4')])\n    # Save mesh.\n    el1 = PlyElement.describe(pnts, 'vertex', comments=['Vertex data'])\n    el2 = PlyElement.describe(tris, 'face', comments=['Face data'])\n    PlyData([el1, el2], text=\"True\").write(savefn)\n</code></pre>"},{"location":"odak/tools/#odak.tools.conversions.convert_to_numpy","title":"<code>convert_to_numpy(a)</code>","text":"<p>A definition to convert Torch to Numpy.</p> <p>Parameters:</p> <ul> <li> <code>a</code>           \u2013            <pre><code>     Input Torch array.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>b</code> (              <code>ndarray</code> )          \u2013            <p>Converted array.</p> </li> </ul> Source code in <code>odak/tools/conversions.py</code> <pre><code>def convert_to_numpy(a):\n    \"\"\"\n    A definition to convert Torch to Numpy.\n\n    Parameters\n    ----------\n    a          : torch.Tensor\n                 Input Torch array.\n\n    Returns\n    ----------\n    b          : numpy.ndarray\n                 Converted array.\n    \"\"\"\n    b = a.to('cpu').detach().numpy()\n    return b\n</code></pre>"},{"location":"odak/tools/#odak.tools.conversions.convert_to_torch","title":"<code>convert_to_torch(a, grad=True)</code>","text":"<p>A definition to convert Numpy arrays to Torch.</p> <p>Parameters:</p> <ul> <li> <code>a</code>           \u2013            <pre><code>     Input Numpy array.\n</code></pre> </li> <li> <code>grad</code>           \u2013            <pre><code>     Set if the converted array requires gradient.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>c</code> (              <code>Tensor</code> )          \u2013            <p>Converted array.</p> </li> </ul> Source code in <code>odak/tools/conversions.py</code> <pre><code>def convert_to_torch(a, grad=True):\n    \"\"\"\n    A definition to convert Numpy arrays to Torch.\n\n    Parameters\n    ----------\n    a          : ndarray\n                 Input Numpy array.\n    grad       : bool\n                 Set if the converted array requires gradient.\n\n    Returns\n    ----------\n    c          : torch.Tensor\n                 Converted array.\n    \"\"\"\n    b = np.copy(a)\n    c = torch.from_numpy(b)\n    c.requires_grad_(grad)\n    return c\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.check_directory","title":"<code>check_directory(directory)</code>","text":"<p>Definition to check if a directory exist. If it doesn't exist, this definition will create one.</p> <p>Parameters:</p> <ul> <li> <code>directory</code>           \u2013            <pre><code>        Full directory path.\n</code></pre> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def check_directory(directory):\n    \"\"\"\n    Definition to check if a directory exist. If it doesn't exist, this definition will create one.\n\n\n    Parameters\n    ----------\n    directory     : str\n                    Full directory path.\n    \"\"\"\n    if not os.path.exists(expanduser(directory)):\n        os.makedirs(expanduser(directory))\n        return False\n    return True\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.convert_bytes","title":"<code>convert_bytes(num)</code>","text":"<p>A definition to convert bytes to semantic scheme (MB,GB or alike). Inspired from https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python#2104083.</p> <p>Parameters:</p> <ul> <li> <code>num</code>           \u2013            <pre><code>     Size in bytes\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>num</code> (              <code>float</code> )          \u2013            <p>Size in new unit.</p> </li> <li> <code>x</code> (              <code>str</code> )          \u2013            <p>New unit bytes, KB, MB, GB or TB.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def convert_bytes(num):\n    \"\"\"\n    A definition to convert bytes to semantic scheme (MB,GB or alike). Inspired from https://stackoverflow.com/questions/2104080/how-can-i-check-file-size-in-python#2104083.\n\n\n    Parameters\n    ----------\n    num        : float\n                 Size in bytes\n\n\n    Returns\n    ----------\n    num        : float\n                 Size in new unit.\n    x          : str\n                 New unit bytes, KB, MB, GB or TB.\n    \"\"\"\n    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n        if num &lt; 1024.0:\n            return num, x\n        num /= 1024.0\n    return None, None\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.copy_file","title":"<code>copy_file(source, destination, follow_symlinks=True)</code>","text":"<p>Definition to copy a file from one location to another.</p> <p>Parameters:</p> <ul> <li> <code>source</code>           \u2013            <pre><code>          Source filename.\n</code></pre> </li> <li> <code>destination</code>           \u2013            <pre><code>          Destination filename.\n</code></pre> </li> <li> <code>follow_symlinks</code>               (<code>bool</code>, default:                   <code>True</code> )           \u2013            <pre><code>          Set to True to follow the source of symbolic links.\n</code></pre> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def copy_file(source, destination, follow_symlinks = True):\n    \"\"\"\n    Definition to copy a file from one location to another.\n\n\n\n    Parameters\n    ----------\n    source          : str\n                      Source filename.\n    destination     : str\n                      Destination filename.\n    follow_symlinks : bool\n                      Set to True to follow the source of symbolic links.\n    \"\"\"\n    return shutil.copyfile(\n                           expanduser(source),\n                           expanduser(source),\n                           follow_symlinks = follow_symlinks\n                          )\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.expanduser","title":"<code>expanduser(filename)</code>","text":"<p>Definition to decode filename using namespaces and shortcuts.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>        Filename.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_filename</code> (              <code>str</code> )          \u2013            <p>Filename.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def expanduser(filename):\n    \"\"\"\n    Definition to decode filename using namespaces and shortcuts.\n\n\n    Parameters\n    ----------\n    filename      : str\n                    Filename.\n\n\n    Returns\n    -------\n    new_filename  : str\n                    Filename.\n    \"\"\"\n    new_filename = os.path.expanduser(filename)\n    return new_filename\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.get_base_filename","title":"<code>get_base_filename(filename)</code>","text":"<p>Definition to retrieve the base filename and extension type.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>         Input filename.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>basename</code> (              <code>str</code> )          \u2013            <p>Basename extracted from the filename.</p> </li> <li> <code>extension</code> (              <code>str</code> )          \u2013            <p>Extension extracted from the filename.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def get_base_filename(filename):\n    \"\"\"\n    Definition to retrieve the base filename and extension type.\n\n\n    Parameters\n    ----------\n    filename       : str\n                     Input filename.\n\n\n    Returns\n    -------\n    basename       : str\n                     Basename extracted from the filename.\n    extension      : str\n                     Extension extracted from the filename.\n    \"\"\"\n    cache = os.path.basename(filename)\n    basename = os.path.splitext(cache)[0]\n    extension = os.path.splitext(cache)[1]\n    return basename, extension\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.list_files","title":"<code>list_files(path, key='*.*', recursive=True)</code>","text":"<p>Definition to list files in a given path with a given key.</p> <p>Parameters:</p> <ul> <li> <code>path</code>           \u2013            <pre><code>      Path to a folder.\n</code></pre> </li> <li> <code>key</code>           \u2013            <pre><code>      Key used for scanning a path.\n</code></pre> </li> <li> <code>recursive</code>           \u2013            <pre><code>      If set True, scan the path recursively.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>files_list</code> (              <code>ndarray</code> )          \u2013            <p>list of files found in a given path.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def list_files(path, key = '*.*', recursive = True):\n    \"\"\"\n    Definition to list files in a given path with a given key.\n\n\n    Parameters\n    ----------\n    path        : str\n                  Path to a folder.\n    key         : str\n                  Key used for scanning a path.\n    recursive   : bool\n                  If set True, scan the path recursively.\n\n\n    Returns\n    ----------\n    files_list  : ndarray\n                  list of files found in a given path.\n    \"\"\"\n    if recursive == True:\n        search_result = pathlib.Path(expanduser(path)).rglob(key)\n    elif recursive == False:\n        search_result = pathlib.Path(expanduser(path)).glob(key)\n    files_list = []\n    for item in search_result:\n        files_list.append(str(item))\n    files_list = sorted(files_list)\n    return files_list\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.load_dictionary","title":"<code>load_dictionary(filename)</code>","text":"<p>Definition to load a dictionary (JSON) file.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>        Filename.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>settings</code> (              <code>dict</code> )          \u2013            <p>Dictionary read from the file.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def load_dictionary(filename):\n    \"\"\"\n    Definition to load a dictionary (JSON) file.\n\n\n    Parameters\n    ----------\n    filename      : str\n                    Filename.\n\n\n    Returns\n    ----------\n    settings      : dict\n                    Dictionary read from the file.\n\n    \"\"\"\n    settings = json.load(open(expanduser(filename)))\n    return settings\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.load_image","title":"<code>load_image(fn, normalizeby=0.0, torch_style=False)</code>","text":"<p>Definition to load an image from a given location as a Numpy array.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>normalizeby</code>           \u2013            <pre><code>       Value to to normalize images with. Default value of zero will lead to no normalization.\n</code></pre> </li> <li> <code>torch_style</code>           \u2013            <pre><code>       If set True, it will load an image mxnx3 as 3xmxn.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>image</code> (              <code>ndarray</code> )          \u2013            <p>Image loaded as a Numpy array.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def load_image(fn, normalizeby = 0., torch_style = False):\n    \"\"\" \n    Definition to load an image from a given location as a Numpy array.\n\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    normalizeby  : float\n                   Value to to normalize images with. Default value of zero will lead to no normalization.\n    torch_style  : bool\n                   If set True, it will load an image mxnx3 as 3xmxn.\n\n\n    Returns\n    ----------\n    image        :  ndarray\n                    Image loaded as a Numpy array.\n\n    \"\"\"\n    image = cv2.imread(expanduser(fn), cv2.IMREAD_UNCHANGED)\n    if isinstance(image, type(None)):\n         logging.warning('Image not properly loaded. Check filename or image type.')    \n         sys.exit()\n    if len(image.shape) &gt; 2:\n        new_image = np.copy(image)\n        new_image[:, :, 0] = image[:, :, 2]\n        new_image[:, :, 2] = image[:, :, 0]\n        image = new_image\n    if normalizeby != 0.:\n        image = image * 1. / normalizeby\n    if torch_style == True and len(image.shape) &gt; 2:\n        image = np.moveaxis(image, -1, 0)\n    return image.astype(float)\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.read_text_file","title":"<code>read_text_file(filename)</code>","text":"<p>Definition to read a given text file and convert it into a Pythonic list.</p> <p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>          Source filename (i.e. test.txt).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>content</code> (              <code>list</code> )          \u2013            <p>Pythonic string list containing the text from the file provided.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def read_text_file(filename):\n    \"\"\"\n    Definition to read a given text file and convert it into a Pythonic list.\n\n\n    Parameters\n    ----------\n    filename        : str\n                      Source filename (i.e. test.txt).\n\n\n    Returns\n    -------\n    content         : list\n                      Pythonic string list containing the text from the file provided.\n    \"\"\"\n    content = []\n    loaded_file = open(expanduser(filename))\n    while line := loaded_file.readline():\n        content.append(line.rstrip())\n    return content\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.resize_image","title":"<code>resize_image(img, target_size)</code>","text":"<p>Definition to resize a given image to a target shape.</p> <p>Parameters:</p> <ul> <li> <code>img</code>           \u2013            <pre><code>        MxN image to be resized.\n        Image must be normalized (0-1).\n</code></pre> </li> <li> <code>target_size</code>           \u2013            <pre><code>        Target shape.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>img</code> (              <code>ndarray</code> )          \u2013            <p>Resized image.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def resize_image(img, target_size):\n    \"\"\"\n    Definition to resize a given image to a target shape.\n\n\n    Parameters\n    ----------\n    img           : ndarray\n                    MxN image to be resized.\n                    Image must be normalized (0-1).\n    target_size   : list\n                    Target shape.\n\n\n    Returns\n    ----------\n    img           : ndarray\n                    Resized image.\n\n    \"\"\"\n    img = cv2.resize(img, dsize=(target_size[0], target_size[1]), interpolation=cv2.INTER_AREA)\n    return img\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.save_dictionary","title":"<code>save_dictionary(settings, filename)</code>","text":"<p>Definition to load a dictionary (JSON) file.</p> <p>Parameters:</p> <ul> <li> <code>settings</code>           \u2013            <pre><code>        Dictionary read from the file.\n</code></pre> </li> <li> <code>filename</code>           \u2013            <pre><code>        Filename.\n</code></pre> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def save_dictionary(settings, filename):\n    \"\"\"\n    Definition to load a dictionary (JSON) file.\n\n\n    Parameters\n    ----------\n    settings      : dict\n                    Dictionary read from the file.\n    filename      : str\n                    Filename.\n    \"\"\"\n    with open(expanduser(filename), 'w', encoding='utf-8') as f:\n        json.dump(settings, f, ensure_ascii=False, indent=4)\n    return settings\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.save_image","title":"<code>save_image(fn, img, cmin=0, cmax=255, color_depth=8)</code>","text":"<p>Definition to save a Numpy array as an image.</p> <p>Parameters:</p> <ul> <li> <code>fn</code>           \u2013            <pre><code>       Filename.\n</code></pre> </li> <li> <code>img</code>           \u2013            <pre><code>       A numpy array with NxMx3 or NxMx1 shapes.\n</code></pre> </li> <li> <code>cmin</code>           \u2013            <pre><code>       Minimum value that will be interpreted as 0 level in the final image.\n</code></pre> </li> <li> <code>cmax</code>           \u2013            <pre><code>       Maximum value that will be interpreted as 255 level in the final image.\n</code></pre> </li> <li> <code>color_depth</code>           \u2013            <pre><code>       Pixel color depth in bits, default is eight bits.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>bool</code> (              <code>bool</code> )          \u2013            <p>True if successful.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def save_image(fn, img, cmin = 0, cmax = 255, color_depth = 8):\n    \"\"\"\n    Definition to save a Numpy array as an image.\n\n\n    Parameters\n    ----------\n    fn           : str\n                   Filename.\n    img          : ndarray\n                   A numpy array with NxMx3 or NxMx1 shapes.\n    cmin         : int\n                   Minimum value that will be interpreted as 0 level in the final image.\n    cmax         : int\n                   Maximum value that will be interpreted as 255 level in the final image.\n    color_depth  : int\n                   Pixel color depth in bits, default is eight bits.\n\n\n    Returns\n    ----------\n    bool         :  bool\n                    True if successful.\n\n    \"\"\"\n    input_img = np.copy(img).astype(np.float32)\n    cmin = float(cmin)\n    cmax = float(cmax)\n    input_img[input_img &lt; cmin] = cmin\n    input_img[input_img &gt; cmax] = cmax\n    input_img /= cmax\n    input_img = input_img * 1. * (2 ** color_depth - 1)\n    if color_depth == 8:\n        input_img = input_img.astype(np.uint8)\n    elif color_depth == 16:\n        input_img = input_img.astype(np.uint16)\n    if len(input_img.shape) &gt; 2:\n        if input_img.shape[2] &gt; 1:\n            cache_img = np.copy(input_img)\n            cache_img[:, :, 0] = input_img[:, :, 2]\n            cache_img[:, :, 2] = input_img[:, :, 0]\n            input_img = cache_img\n    cv2.imwrite(expanduser(fn), input_img)\n    return True\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.shell_command","title":"<code>shell_command(cmd, cwd='.', timeout=None, check=True)</code>","text":"<p>Definition to initiate shell commands.</p> <p>Parameters:</p> <ul> <li> <code>cmd</code>           \u2013            <pre><code>       Command to be executed.\n</code></pre> </li> <li> <code>cwd</code>           \u2013            <pre><code>       Working directory.\n</code></pre> </li> <li> <code>timeout</code>           \u2013            <pre><code>       Timeout if the process isn't complete in the given number of seconds.\n</code></pre> </li> <li> <code>check</code>           \u2013            <pre><code>       Set it to True to return the results and to enable timeout.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>proc</code> (              <code>Popen</code> )          \u2013            <p>Generated process.</p> </li> <li> <code>outs</code> (              <code>str</code> )          \u2013            <p>Outputs of the executed command, returns None when check is set to False.</p> </li> <li> <code>errs</code> (              <code>str</code> )          \u2013            <p>Errors of the executed command, returns None when check is set to False.</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def shell_command(cmd, cwd = '.', timeout = None, check = True):\n    \"\"\"\n    Definition to initiate shell commands.\n\n\n    Parameters\n    ----------\n    cmd          : list\n                   Command to be executed. \n    cwd          : str\n                   Working directory.\n    timeout      : int\n                   Timeout if the process isn't complete in the given number of seconds.\n    check        : bool\n                   Set it to True to return the results and to enable timeout.\n\n\n    Returns\n    ----------\n    proc         : subprocess.Popen\n                   Generated process.\n    outs         : str\n                   Outputs of the executed command, returns None when check is set to False.\n    errs         : str\n                   Errors of the executed command, returns None when check is set to False.\n\n    \"\"\"\n    for item_id in range(len(cmd)):\n        cmd[item_id] = expanduser(cmd[item_id])\n    proc = subprocess.Popen(\n                            cmd,\n                            cwd = cwd,\n                            stdout = subprocess.PIPE\n                           )\n    if check == False:\n        return proc, None, None\n    try:\n        outs, errs = proc.communicate(timeout = timeout)\n    except subprocess.TimeoutExpired:\n        proc.kill()\n        outs, errs = proc.communicate()\n    return proc, outs, errs\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.size_of_a_file","title":"<code>size_of_a_file(file_path)</code>","text":"<p>A definition to get size of a file with a relevant unit.</p> <p>Parameters:</p> <ul> <li> <code>file_path</code>           \u2013            <pre><code>     Path of the file.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>a</code> (              <code>float</code> )          \u2013            <p>Size of the file.</p> </li> <li> <code>b</code> (              <code>str</code> )          \u2013            <p>Unit of the size (bytes, KB, MB, GB or TB).</p> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def size_of_a_file(file_path):\n    \"\"\"\n    A definition to get size of a file with a relevant unit.\n\n\n    Parameters\n    ----------\n    file_path  : float\n                 Path of the file.\n\n\n    Returns\n    ----------\n    a          : float\n                 Size of the file.\n    b          : str\n                 Unit of the size (bytes, KB, MB, GB or TB).\n    \"\"\"\n    if os.path.isfile(file_path):\n        file_info = os.stat(file_path)\n        a, b = convert_bytes(file_info.st_size)\n        return a, b\n    return None, None\n</code></pre>"},{"location":"odak/tools/#odak.tools.file.write_to_text_file","title":"<code>write_to_text_file(content, filename, write_flag='w')</code>","text":"<p>Defininition to write a Pythonic list to a text file.</p> <p>Parameters:</p> <ul> <li> <code>content</code>           \u2013            <pre><code>          Pythonic string list to be written to a file.\n</code></pre> </li> <li> <code>filename</code>           \u2013            <pre><code>          Destination filename (i.e. test.txt).\n</code></pre> </li> <li> <code>write_flag</code>           \u2013            <pre><code>          Defines the interaction with the file. \n          The default is \"w\" (overwrite any existing content).\n          For more see: https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files\n</code></pre> </li> </ul> Source code in <code>odak/tools/file.py</code> <pre><code>def write_to_text_file(content, filename, write_flag = 'w'):\n    \"\"\"\n    Defininition to write a Pythonic list to a text file.\n\n\n    Parameters\n    ----------\n    content         : list\n                      Pythonic string list to be written to a file.\n    filename        : str\n                      Destination filename (i.e. test.txt).\n    write_flag      : str\n                      Defines the interaction with the file. \n                      The default is \"w\" (overwrite any existing content).\n                      For more see: https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files\n    \"\"\"\n    with open(expanduser(filename), write_flag) as f:\n        for line in content:\n            f.write('{}\\n'.format(line))\n    return True\n</code></pre>"},{"location":"odak/tools/#odak.tools.latex.latex","title":"<code>latex</code>","text":"<p>A class to work with latex documents.</p> Source code in <code>odak/tools/latex.py</code> <pre><code>class latex():\n    \"\"\"\n    A class to work with latex documents.\n    \"\"\"\n    def __init__(\n                 self,\n                 filename\n                ):\n        \"\"\"\n        Parameters\n        ----------\n        filename     : str\n                       Source filename (i.e. sample.tex).\n        \"\"\"\n        self.filename = filename\n        self.content = read_text_file(self.filename)\n        self.content_type = []\n        self.latex_dictionary = [\n                                 '\\\\documentclass',\n                                 '\\\\if',\n                                 '\\\\pdf',\n                                 '\\\\else',\n                                 '\\\\fi',\n                                 '\\\\vgtc',\n                                 '\\\\teaser',\n                                 '\\\\abstract',\n                                 '\\\\CCS',\n                                 '\\\\usepackage',\n                                 '\\\\PassOptionsToPackage',\n                                 '\\\\definecolor',\n                                 '\\\\AtBeginDocument',\n                                 '\\\\providecommand',\n                                 '\\\\setcopyright',\n                                 '\\\\copyrightyear',\n                                 '\\\\acmYear',\n                                 '\\\\citestyle',\n                                 '\\\\newcommand',\n                                 '\\\\acmDOI',\n                                 '\\\\newabbreviation',\n                                 '\\\\global',\n                                 '\\\\begin{document}',\n                                 '\\\\author',\n                                 '\\\\affiliation',\n                                 '\\\\email',\n                                 '\\\\institution',\n                                 '\\\\streetaddress',\n                                 '\\\\city',\n                                 '\\\\country',\n                                 '\\\\postcode',\n                                 '\\\\ccsdesc',\n                                 '\\\\received',\n                                 '\\\\includegraphics',\n                                 '\\\\caption',\n                                 '\\\\centering',\n                                 '\\\\label',\n                                 '\\\\maketitle',\n                                 '\\\\toprule',\n                                 '\\\\multirow',\n                                 '\\\\multicolumn',\n                                 '\\\\cmidrule',\n                                 '\\\\addlinespace',\n                                 '\\\\midrule',\n                                 '\\\\cellcolor',\n                                 '\\\\bibliography',\n                                 '}',\n                                 '\\\\title',\n                                 '&lt;/ccs2012&gt;',\n                                 '\\\\bottomrule',\n                                 '&lt;concept&gt;',\n                                 '&lt;concept',\n                                 '&lt;ccs',\n                                 '\\\\item',\n                                 '&lt;/concept',\n                                 '\\\\begin{abstract}',\n                                 '\\\\end{abstract}',\n                                 '\\\\endinput',\n                                 '\\\\\\\\'\n                                ]\n        self.latex_begin_dictionary = [\n                                       '\\\\begin{figure}',\n                                       '\\\\begin{figure*}',\n                                       '\\\\begin{equation}',\n                                       '\\\\begin{CCSXML}',\n                                       '\\\\begin{teaserfigure}',\n                                       '\\\\begin{table*}',\n                                       '\\\\begin{table}',\n                                       '\\\\begin{gather}',\n                                       '\\\\begin{align}',\n                                      ]\n        self.latex_end_dictionary = [\n                                     '\\\\end{figure}',\n                                     '\\\\end{figure*}',\n                                     '\\\\end{equation}',\n                                     '\\\\end{CCSXML}',\n                                     '\\\\end{teaserfigure}',\n                                     '\\\\end{table*}',\n                                     '\\\\end{table}',\n                                     '\\\\end{gather}',\n                                     '\\\\end{align}',\n                                    ]\n        self._label_lines()\n\n\n    def set_latex_dictonaries(self, begin_dictionary, end_dictionary, syntax_dictionary):\n        \"\"\"\n        Set document specific dictionaries so that the lines could be labelled in accordance.\n\n\n        Parameters\n        ----------\n        begin_dictionary     : list\n                               Pythonic list containing latex syntax for begin commands (i.e. \\\\begin{align}).\n        end_dictionary       : list\n                               Pythonic list containing latex syntax for end commands (i.e. \\\\end{table}).\n        syntax_dictionary    : list\n                               Pythonic list containing latex syntax (i.e. \\\\item).\n\n        \"\"\"\n        self.latex_begin_dictionary = begin_dictionary\n        self.latex_end_dictionary = end_dictionary\n        self.latex_dictionary = syntax_dictionary\n        self._label_lines\n\n\n    def _label_lines(self):\n        \"\"\"\n        Internal function for labelling lines.\n        \"\"\"\n        content_type_flag = False\n        for line_id, line in enumerate(self.content):\n            while len(line) &gt; 0 and line[0] == ' ':\n                 line = line[1::]\n            self.content[line_id] = line\n            if len(line) == 0:\n                content_type = 'empty'\n            elif line[0] == '%':\n                content_type = 'comment'\n            else:\n                content_type = 'text'\n            for syntax in self.latex_begin_dictionary:\n                if line.find(syntax) != -1:\n                    content_type_flag = True\n                    content_type = 'latex'\n            for syntax in self.latex_dictionary:\n                if line.find(syntax) != -1:\n                    content_type = 'latex'\n            if content_type_flag == True:\n                content_type = 'latex'\n                for syntax in self.latex_end_dictionary:\n                    if line.find(syntax) != -1:\n                         content_type_flag = False\n            self.content_type.append(content_type)\n\n\n    def get_line_count(self):\n        \"\"\"\n        Definition to get the line count.\n\n\n        Returns\n        -------\n        line_count     : int\n                         Number of lines in the loaded latex document.\n        \"\"\"\n        self.line_count = len(self.content)\n        return self.line_count\n\n\n    def get_line(self, line_id = 0):\n        \"\"\"\n        Definition to get a specific line by inputting a line nunber.\n\n\n        Returns\n        ----------\n        line           : str\n                         Requested line.\n        content_type   : str\n                         Line's content type (e.g., latex, comment, text).\n        \"\"\"\n        line = self.content[line_id]\n        content_type = self.content_type[line_id]\n        return line, content_type\n</code></pre>"},{"location":"odak/tools/#odak.tools.latex.latex.__init__","title":"<code>__init__(filename)</code>","text":"<p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>       Source filename (i.e. sample.tex).\n</code></pre> </li> </ul> Source code in <code>odak/tools/latex.py</code> <pre><code>def __init__(\n             self,\n             filename\n            ):\n    \"\"\"\n    Parameters\n    ----------\n    filename     : str\n                   Source filename (i.e. sample.tex).\n    \"\"\"\n    self.filename = filename\n    self.content = read_text_file(self.filename)\n    self.content_type = []\n    self.latex_dictionary = [\n                             '\\\\documentclass',\n                             '\\\\if',\n                             '\\\\pdf',\n                             '\\\\else',\n                             '\\\\fi',\n                             '\\\\vgtc',\n                             '\\\\teaser',\n                             '\\\\abstract',\n                             '\\\\CCS',\n                             '\\\\usepackage',\n                             '\\\\PassOptionsToPackage',\n                             '\\\\definecolor',\n                             '\\\\AtBeginDocument',\n                             '\\\\providecommand',\n                             '\\\\setcopyright',\n                             '\\\\copyrightyear',\n                             '\\\\acmYear',\n                             '\\\\citestyle',\n                             '\\\\newcommand',\n                             '\\\\acmDOI',\n                             '\\\\newabbreviation',\n                             '\\\\global',\n                             '\\\\begin{document}',\n                             '\\\\author',\n                             '\\\\affiliation',\n                             '\\\\email',\n                             '\\\\institution',\n                             '\\\\streetaddress',\n                             '\\\\city',\n                             '\\\\country',\n                             '\\\\postcode',\n                             '\\\\ccsdesc',\n                             '\\\\received',\n                             '\\\\includegraphics',\n                             '\\\\caption',\n                             '\\\\centering',\n                             '\\\\label',\n                             '\\\\maketitle',\n                             '\\\\toprule',\n                             '\\\\multirow',\n                             '\\\\multicolumn',\n                             '\\\\cmidrule',\n                             '\\\\addlinespace',\n                             '\\\\midrule',\n                             '\\\\cellcolor',\n                             '\\\\bibliography',\n                             '}',\n                             '\\\\title',\n                             '&lt;/ccs2012&gt;',\n                             '\\\\bottomrule',\n                             '&lt;concept&gt;',\n                             '&lt;concept',\n                             '&lt;ccs',\n                             '\\\\item',\n                             '&lt;/concept',\n                             '\\\\begin{abstract}',\n                             '\\\\end{abstract}',\n                             '\\\\endinput',\n                             '\\\\\\\\'\n                            ]\n    self.latex_begin_dictionary = [\n                                   '\\\\begin{figure}',\n                                   '\\\\begin{figure*}',\n                                   '\\\\begin{equation}',\n                                   '\\\\begin{CCSXML}',\n                                   '\\\\begin{teaserfigure}',\n                                   '\\\\begin{table*}',\n                                   '\\\\begin{table}',\n                                   '\\\\begin{gather}',\n                                   '\\\\begin{align}',\n                                  ]\n    self.latex_end_dictionary = [\n                                 '\\\\end{figure}',\n                                 '\\\\end{figure*}',\n                                 '\\\\end{equation}',\n                                 '\\\\end{CCSXML}',\n                                 '\\\\end{teaserfigure}',\n                                 '\\\\end{table*}',\n                                 '\\\\end{table}',\n                                 '\\\\end{gather}',\n                                 '\\\\end{align}',\n                                ]\n    self._label_lines()\n</code></pre>"},{"location":"odak/tools/#odak.tools.latex.latex.get_line","title":"<code>get_line(line_id=0)</code>","text":"<p>Definition to get a specific line by inputting a line nunber.</p> <p>Returns:</p> <ul> <li> <code>line</code> (              <code>str</code> )          \u2013            <p>Requested line.</p> </li> <li> <code>content_type</code> (              <code>str</code> )          \u2013            <p>Line's content type (e.g., latex, comment, text).</p> </li> </ul> Source code in <code>odak/tools/latex.py</code> <pre><code>def get_line(self, line_id = 0):\n    \"\"\"\n    Definition to get a specific line by inputting a line nunber.\n\n\n    Returns\n    ----------\n    line           : str\n                     Requested line.\n    content_type   : str\n                     Line's content type (e.g., latex, comment, text).\n    \"\"\"\n    line = self.content[line_id]\n    content_type = self.content_type[line_id]\n    return line, content_type\n</code></pre>"},{"location":"odak/tools/#odak.tools.latex.latex.get_line_count","title":"<code>get_line_count()</code>","text":"<p>Definition to get the line count.</p> <p>Returns:</p> <ul> <li> <code>line_count</code> (              <code>int</code> )          \u2013            <p>Number of lines in the loaded latex document.</p> </li> </ul> Source code in <code>odak/tools/latex.py</code> <pre><code>def get_line_count(self):\n    \"\"\"\n    Definition to get the line count.\n\n\n    Returns\n    -------\n    line_count     : int\n                     Number of lines in the loaded latex document.\n    \"\"\"\n    self.line_count = len(self.content)\n    return self.line_count\n</code></pre>"},{"location":"odak/tools/#odak.tools.latex.latex.set_latex_dictonaries","title":"<code>set_latex_dictonaries(begin_dictionary, end_dictionary, syntax_dictionary)</code>","text":"<p>Set document specific dictionaries so that the lines could be labelled in accordance.</p> <p>Parameters:</p> <ul> <li> <code>begin_dictionary</code>           \u2013            <pre><code>               Pythonic list containing latex syntax for begin commands (i.e. \\begin{align}).\n</code></pre> </li> <li> <code>end_dictionary</code>           \u2013            <pre><code>               Pythonic list containing latex syntax for end commands (i.e. \\end{table}).\n</code></pre> </li> <li> <code>syntax_dictionary</code>           \u2013            <pre><code>               Pythonic list containing latex syntax (i.e. \\item).\n</code></pre> </li> </ul> Source code in <code>odak/tools/latex.py</code> <pre><code>def set_latex_dictonaries(self, begin_dictionary, end_dictionary, syntax_dictionary):\n    \"\"\"\n    Set document specific dictionaries so that the lines could be labelled in accordance.\n\n\n    Parameters\n    ----------\n    begin_dictionary     : list\n                           Pythonic list containing latex syntax for begin commands (i.e. \\\\begin{align}).\n    end_dictionary       : list\n                           Pythonic list containing latex syntax for end commands (i.e. \\\\end{table}).\n    syntax_dictionary    : list\n                           Pythonic list containing latex syntax (i.e. \\\\item).\n\n    \"\"\"\n    self.latex_begin_dictionary = begin_dictionary\n    self.latex_end_dictionary = end_dictionary\n    self.latex_dictionary = syntax_dictionary\n    self._label_lines\n</code></pre>"},{"location":"odak/tools/#odak.tools.matrix.blur_gaussian","title":"<code>blur_gaussian(field, kernel_length=[21, 21], nsigma=[3, 3])</code>","text":"<p>A definition to blur a field using a Gaussian kernel.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>        MxN field.\n</code></pre> </li> <li> <code>kernel_length</code>               (<code>list</code>, default:                   <code>[21, 21]</code> )           \u2013            <pre><code>        Length of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>nsigma</code>           \u2013            <pre><code>        Sigma of the Gaussian kernel along X and Y axes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>blurred_field</code> (              <code>ndarray</code> )          \u2013            <p>Blurred field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def blur_gaussian(field, kernel_length=[21, 21], nsigma=[3, 3]):\n    \"\"\"\n    A definition to blur a field using a Gaussian kernel.\n\n    Parameters\n    ----------\n    field         : ndarray\n                    MxN field.\n    kernel_length : list\n                    Length of the Gaussian kernel along X and Y axes.\n    nsigma        : list\n                    Sigma of the Gaussian kernel along X and Y axes.\n\n    Returns\n    ----------\n    blurred_field : ndarray\n                    Blurred field.\n    \"\"\"\n    kernel = generate_2d_gaussian(kernel_length, nsigma)\n    kernel = zero_pad(kernel, field.shape)\n    blurred_field = convolve2d(field, kernel)\n    blurred_field = blurred_field/np.amax(blurred_field)\n    return blurred_field\n</code></pre>"},{"location":"odak/tools/#odak.tools.matrix.convolve2d","title":"<code>convolve2d(field, kernel)</code>","text":"<p>Definition to convolve a field with a kernel by multiplying in frequency space.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field with MxN shape.\n</code></pre> </li> <li> <code>kernel</code>           \u2013            <pre><code>      Input kernel with MxN shape.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>ndarray</code> )          \u2013            <p>Convolved field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def convolve2d(field, kernel):\n    \"\"\"\n    Definition to convolve a field with a kernel by multiplying in frequency space.\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field with MxN shape.\n    kernel      : ndarray\n                  Input kernel with MxN shape.\n\n    Returns\n    ----------\n    new_field   : ndarray\n                  Convolved field.\n    \"\"\"\n    fr = np.fft.fft2(field)\n    fr2 = np.fft.fft2(np.flipud(np.fliplr(kernel)))\n    m, n = fr.shape\n    new_field = np.real(np.fft.ifft2(fr*fr2))\n    new_field = np.roll(new_field, int(-m/2+1), axis=0)\n    new_field = np.roll(new_field, int(-n/2+1), axis=1)\n    return new_field\n</code></pre>"},{"location":"odak/tools/#odak.tools.matrix.create_empty_list","title":"<code>create_empty_list(dimensions=[1, 1])</code>","text":"<p>A definition to create an empty Pythonic list.</p> <p>Parameters:</p> <ul> <li> <code>dimensions</code>           \u2013            <pre><code>       Dimensions of the list to be created.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_list</code> (              <code>list</code> )          \u2013            <p>New empty list.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def create_empty_list(dimensions = [1, 1]):\n    \"\"\"\n    A definition to create an empty Pythonic list.\n\n    Parameters\n    ----------\n    dimensions   : list\n                   Dimensions of the list to be created.\n\n    Returns\n    -------\n    new_list     : list\n                   New empty list.\n    \"\"\"\n    new_list = 0\n    for n in reversed(dimensions):\n        new_list = [new_list] * n\n    return new_list\n</code></pre>"},{"location":"odak/tools/#odak.tools.matrix.crop_center","title":"<code>crop_center(field, size=None)</code>","text":"<p>Definition to crop the center of a field with 2Mx2N size. The outcome is a MxN array.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field 2Mx2N array.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>cropped</code> (              <code>ndarray</code> )          \u2013            <p>Cropped version of the input field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def crop_center(field, size=None):\n    \"\"\"\n    Definition to crop the center of a field with 2Mx2N size. The outcome is a MxN array.\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field 2Mx2N array.\n\n    Returns\n    ----------\n    cropped     : ndarray\n                  Cropped version of the input field.\n    \"\"\"\n    if type(size) == type(None):\n        qx = int(np.ceil(field.shape[0])/4)\n        qy = int(np.ceil(field.shape[1])/4)\n        cropped = np.copy(field[qx:3*qx, qy:3*qy])\n    else:\n        cx = int(np.ceil(field.shape[0]/2))\n        cy = int(np.ceil(field.shape[1]/2))\n        hx = int(np.ceil(size[0]/2))\n        hy = int(np.ceil(size[1]/2))\n        cropped = np.copy(field[cx-hx:cx+hx, cy-hy:cy+hy])\n    return cropped\n</code></pre>"},{"location":"odak/tools/#odak.tools.matrix.generate_2d_gaussian","title":"<code>generate_2d_gaussian(kernel_length=[21, 21], nsigma=[3, 3])</code>","text":"<p>Generate 2D Gaussian kernel. Inspired from https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy</p> <p>Parameters:</p> <ul> <li> <code>kernel_length</code>               (<code>list</code>, default:                   <code>[21, 21]</code> )           \u2013            <pre><code>        Length of the Gaussian kernel along X and Y axes.\n</code></pre> </li> <li> <code>nsigma</code>           \u2013            <pre><code>        Sigma of the Gaussian kernel along X and Y axes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>kernel_2d</code> (              <code>ndarray</code> )          \u2013            <p>Generated Gaussian kernel.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def generate_2d_gaussian(kernel_length=[21, 21], nsigma=[3, 3]):\n    \"\"\"\n    Generate 2D Gaussian kernel. Inspired from https://stackoverflow.com/questions/29731726/how-to-calculate-a-gaussian-kernel-matrix-efficiently-in-numpy\n\n    Parameters\n    ----------\n    kernel_length : list\n                    Length of the Gaussian kernel along X and Y axes.\n    nsigma        : list\n                    Sigma of the Gaussian kernel along X and Y axes.\n\n    Returns\n    ----------\n    kernel_2d     : ndarray\n                    Generated Gaussian kernel.\n    \"\"\"\n    x = np.linspace(-nsigma[0], nsigma[0], kernel_length[0]+1)\n    y = np.linspace(-nsigma[1], nsigma[1], kernel_length[1]+1)\n    xx, yy = np.meshgrid(x, y)\n    kernel_2d = np.exp(-0.5*(np.square(xx) /\n                       np.square(nsigma[0]) + np.square(yy)/np.square(nsigma[1])))\n    kernel_2d = kernel_2d/kernel_2d.sum()\n    return kernel_2d\n</code></pre>"},{"location":"odak/tools/#odak.tools.matrix.generate_bandlimits","title":"<code>generate_bandlimits(size=[512, 512], levels=9)</code>","text":"<p>A definition to calculate octaves used in bandlimiting frequencies in the frequency domain.</p> <p>Parameters:</p> <ul> <li> <code>size</code>           \u2013            <pre><code>     Size of each mask in octaves.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>masks</code> (              <code>ndarray</code> )          \u2013            <p>Masks (Octaves).</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def generate_bandlimits(size=[512, 512], levels=9):\n    \"\"\"\n    A definition to calculate octaves used in bandlimiting frequencies in the frequency domain.\n\n    Parameters\n    ----------\n    size       : list\n                 Size of each mask in octaves.\n\n    Returns\n    ----------\n    masks      : ndarray\n                 Masks (Octaves).\n    \"\"\"\n    masks = np.zeros((levels, size[0], size[1]))\n    cx = int(size[0]/2)\n    cy = int(size[1]/2)\n    for i in range(0, masks.shape[0]):\n        deltax = int((size[0])/(2**(i+1)))\n        deltay = int((size[1])/(2**(i+1)))\n        masks[\n            i,\n            cx-deltax:cx+deltax,\n            cy-deltay:cy+deltay\n        ] = 1.\n        masks[\n            i,\n            int(cx-deltax/2.):int(cx+deltax/2.),\n            int(cy-deltay/2.):int(cy+deltay/2.)\n        ] = 0.\n    masks = np.asarray(masks)\n    return masks\n</code></pre>"},{"location":"odak/tools/#odak.tools.matrix.nufft2","title":"<code>nufft2(field, fx, fy, size=None, sign=1, eps=10 ** -12)</code>","text":"<p>A definition to take 2D Non-Uniform Fast Fourier Transform (NUFFT).</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field.\n</code></pre> </li> <li> <code>fx</code>           \u2013            <pre><code>      Frequencies along x axis.\n</code></pre> </li> <li> <code>fy</code>           \u2013            <pre><code>      Frequencies along y axis.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Size.\n</code></pre> </li> <li> <code>sign</code>           \u2013            <pre><code>      Sign of the exponential used in NUFFT kernel.\n</code></pre> </li> <li> <code>eps</code>           \u2013            <pre><code>      Accuracy of NUFFT.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>Inverse NUFFT of the input field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def nufft2(field, fx, fy, size=None, sign=1, eps=10**(-12)):\n    \"\"\"\n    A definition to take 2D Non-Uniform Fast Fourier Transform (NUFFT).\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field.\n    fx          : ndarray\n                  Frequencies along x axis.\n    fy          : ndarray\n                  Frequencies along y axis.\n    size        : list\n                  Size.\n    sign        : float\n                  Sign of the exponential used in NUFFT kernel.\n    eps         : float\n                  Accuracy of NUFFT.\n\n    Returns\n    ----------\n    result      : ndarray\n                  Inverse NUFFT of the input field.\n    \"\"\"\n    try:\n        import finufft\n    except:\n        print('odak.tools.nufft2 requires finufft to be installed: pip install finufft')\n    image = np.copy(field).astype(np.complex128)\n    result = finufft.nufft2d2(\n        fx.flatten(), fy.flatten(), image, eps=eps, isign=sign)\n    if type(size) == type(None):\n        result = result.reshape(field.shape)\n    else:\n        result = result.reshape(size)\n    return result\n</code></pre>"},{"location":"odak/tools/#odak.tools.matrix.nuifft2","title":"<code>nuifft2(field, fx, fy, size=None, sign=1, eps=10 ** -12)</code>","text":"<p>A definition to take 2D Adjoint Non-Uniform Fast Fourier Transform (NUFFT).</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field.\n</code></pre> </li> <li> <code>fx</code>           \u2013            <pre><code>      Frequencies along x axis.\n</code></pre> </li> <li> <code>fy</code>           \u2013            <pre><code>      Frequencies along y axis.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Shape of the NUFFT calculated for an input field.\n</code></pre> </li> <li> <code>sign</code>           \u2013            <pre><code>      Sign of the exponential used in NUFFT kernel.\n</code></pre> </li> <li> <code>eps</code>           \u2013            <pre><code>      Accuracy of NUFFT.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>NUFFT of the input field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def nuifft2(field, fx, fy, size=None, sign=1, eps=10**(-12)):\n    \"\"\"\n    A definition to take 2D Adjoint Non-Uniform Fast Fourier Transform (NUFFT).\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field.\n    fx          : ndarray\n                  Frequencies along x axis.\n    fy          : ndarray\n                  Frequencies along y axis.\n    size        : list or ndarray\n                  Shape of the NUFFT calculated for an input field.\n    sign        : float\n                  Sign of the exponential used in NUFFT kernel.\n    eps         : float\n                  Accuracy of NUFFT.\n\n    Returns\n    ----------\n    result      : ndarray\n                  NUFFT of the input field.\n    \"\"\"\n    try:\n        import finufft\n    except:\n        print('odak.tools.nuifft2 requires finufft to be installed: pip install finufft')\n    image = np.copy(field).astype(np.complex128)\n    if type(size) == type(None):\n        result = finufft.nufft2d1(\n            fx.flatten(),\n            fy.flatten(),\n            image.flatten(),\n            image.shape,\n            eps=eps,\n            isign=sign\n        )\n    else:\n        result = finufft.nufft2d1(\n            fx.flatten(),\n            fy.flatten(),\n            image.flatten(),\n            (size[0], size[1]),\n            eps=eps,\n            isign=sign\n        )\n    result = np.asarray(result)\n    return result\n</code></pre>"},{"location":"odak/tools/#odak.tools.matrix.quantize","title":"<code>quantize(image_field, bits=4)</code>","text":"<p>Definitio to quantize a image field (0-255, 8 bit) to a certain bits level.</p> <p>Parameters:</p> <ul> <li> <code>image_field</code>               (<code>ndarray</code>)           \u2013            <pre><code>      Input image field.\n</code></pre> </li> <li> <code>bits</code>           \u2013            <pre><code>      A value in between 0 to 8. Can not be zero.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>ndarray</code> )          \u2013            <p>Quantized image field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def quantize(image_field, bits=4):\n    \"\"\"\n    Definitio to quantize a image field (0-255, 8 bit) to a certain bits level.\n\n    Parameters\n    ----------\n    image_field : ndarray\n                  Input image field.\n    bits        : int\n                  A value in between 0 to 8. Can not be zero.\n\n    Returns\n    ----------\n    new_field   : ndarray\n                  Quantized image field.\n    \"\"\"\n    divider = 2**(8-bits)\n    new_field = image_field/divider\n    new_field = new_field.astype(np.int64)\n    return new_field\n</code></pre>"},{"location":"odak/tools/#odak.tools.matrix.zero_pad","title":"<code>zero_pad(field, size=None, method='center')</code>","text":"<p>Definition to zero pad a MxN array to 2Mx2N array.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>            Input field MxN array.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>            Size to be zeropadded.\n</code></pre> </li> <li> <code>method</code>           \u2013            <pre><code>            Zeropad either by placing the content to center or to the left.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field_zero_padded</code> (              <code>ndarray</code> )          \u2013            <p>Zeropadded version of the input field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def zero_pad(field, size=None, method='center'):\n    \"\"\"\n    Definition to zero pad a MxN array to 2Mx2N array.\n\n    Parameters\n    ----------\n    field             : ndarray\n                        Input field MxN array.\n    size              : list\n                        Size to be zeropadded.\n    method            : str\n                        Zeropad either by placing the content to center or to the left.\n\n    Returns\n    ----------\n    field_zero_padded : ndarray\n                        Zeropadded version of the input field.\n    \"\"\"\n    if type(size) == type(None):\n        hx = int(np.ceil(field.shape[0])/2)\n        hy = int(np.ceil(field.shape[1])/2)\n    else:\n        hx = int(np.ceil((size[0]-field.shape[0])/2))\n        hy = int(np.ceil((size[1]-field.shape[1])/2))\n    if method == 'center':\n        field_zero_padded = np.pad(\n            field, ([hx, hx], [hy, hy]), constant_values=(0, 0))\n    elif method == 'left aligned':\n        field_zero_padded = np.pad(\n            field, ([0, 2*hx], [0, 2*hy]), constant_values=(0, 0))\n    if type(size) != type(None):\n        field_zero_padded = field_zero_padded[0:size[0], 0:size[1]]\n    return field_zero_padded\n</code></pre>"},{"location":"odak/tools/#odak.tools.markdown.markdown","title":"<code>markdown</code>","text":"<p>A class to work with markdown documents.</p> Source code in <code>odak/tools/markdown.py</code> <pre><code>class markdown():\n    \"\"\"\n    A class to work with markdown documents.\n    \"\"\"\n    def __init__(\n                 self,\n                 filename\n                ):\n        \"\"\"\n        Parameters\n        ----------\n        filename     : str\n                       Source filename (i.e. sample.md).\n        \"\"\"\n        self.filename = filename\n        self.content = read_text_file(self.filename)\n        self.content_type = []\n        self.markdown_dictionary = [\n                                     '#',\n                                   ]\n        self.markdown_begin_dictionary = [\n                                          '```bash',\n                                          '```python',\n                                          '```',\n                                         ]\n        self.markdown_end_dictionary = [\n                                        '```',\n                                       ]\n        self._label_lines()\n\n\n    def set_dictonaries(self, begin_dictionary, end_dictionary, syntax_dictionary):\n        \"\"\"\n        Set document specific dictionaries so that the lines could be labelled in accordance.\n\n\n        Parameters\n        ----------\n        begin_dictionary     : list\n                               Pythonic list containing markdown syntax for beginning of blocks (e.g., code, html).\n        end_dictionary       : list\n                               Pythonic list containing markdown syntax for end of blocks (e.g., code, html).\n        syntax_dictionary    : list\n                               Pythonic list containing markdown syntax (i.e. \\\\item).\n\n        \"\"\"\n        self.markdown_begin_dictionary = begin_dictionary\n        self.markdown_end_dictionary = end_dictionary\n        self.markdown_dictionary = syntax_dictionary\n        self._label_lines\n\n\n    def _label_lines(self):\n        \"\"\"\n        Internal function for labelling lines.\n        \"\"\"\n        content_type_flag = False\n        for line_id, line in enumerate(self.content):\n            while len(line) &gt; 0 and line[0] == ' ':\n                 line = line[1::]\n            self.content[line_id] = line\n            if len(line) == 0:\n                content_type = 'empty'\n            elif line[0] == '%':\n                content_type = 'comment'\n            else:\n                content_type = 'text'\n            for syntax in self.markdown_begin_dictionary:\n                if line.find(syntax) != -1:\n                    content_type_flag = True\n                    content_type = 'markdown'\n            for syntax in self.markdown_dictionary:\n                if line.find(syntax) != -1:\n                    content_type = 'markdown'\n            if content_type_flag == True:\n                content_type = 'markdown'\n                for syntax in self.markdown_end_dictionary:\n                    if line.find(syntax) != -1:\n                         content_type_flag = False\n            self.content_type.append(content_type)\n\n\n    def get_line_count(self):\n        \"\"\"\n        Definition to get the line count.\n\n\n        Returns\n        -------\n        line_count     : int\n                         Number of lines in the loaded markdown document.\n        \"\"\"\n        self.line_count = len(self.content)\n        return self.line_count\n\n\n    def get_line(self, line_id = 0):\n        \"\"\"\n        Definition to get a specific line by inputting a line nunber.\n\n\n        Returns\n        ----------\n        line           : str\n                         Requested line.\n        content_type   : str\n                         Line's content type (e.g., markdown, comment, text).\n        \"\"\"\n        line = self.content[line_id]\n        content_type = self.content_type[line_id]\n        return line, content_type\n</code></pre>"},{"location":"odak/tools/#odak.tools.markdown.markdown.__init__","title":"<code>__init__(filename)</code>","text":"<p>Parameters:</p> <ul> <li> <code>filename</code>           \u2013            <pre><code>       Source filename (i.e. sample.md).\n</code></pre> </li> </ul> Source code in <code>odak/tools/markdown.py</code> <pre><code>def __init__(\n             self,\n             filename\n            ):\n    \"\"\"\n    Parameters\n    ----------\n    filename     : str\n                   Source filename (i.e. sample.md).\n    \"\"\"\n    self.filename = filename\n    self.content = read_text_file(self.filename)\n    self.content_type = []\n    self.markdown_dictionary = [\n                                 '#',\n                               ]\n    self.markdown_begin_dictionary = [\n                                      '```bash',\n                                      '```python',\n                                      '```',\n                                     ]\n    self.markdown_end_dictionary = [\n                                    '```',\n                                   ]\n    self._label_lines()\n</code></pre>"},{"location":"odak/tools/#odak.tools.markdown.markdown.get_line","title":"<code>get_line(line_id=0)</code>","text":"<p>Definition to get a specific line by inputting a line nunber.</p> <p>Returns:</p> <ul> <li> <code>line</code> (              <code>str</code> )          \u2013            <p>Requested line.</p> </li> <li> <code>content_type</code> (              <code>str</code> )          \u2013            <p>Line's content type (e.g., markdown, comment, text).</p> </li> </ul> Source code in <code>odak/tools/markdown.py</code> <pre><code>def get_line(self, line_id = 0):\n    \"\"\"\n    Definition to get a specific line by inputting a line nunber.\n\n\n    Returns\n    ----------\n    line           : str\n                     Requested line.\n    content_type   : str\n                     Line's content type (e.g., markdown, comment, text).\n    \"\"\"\n    line = self.content[line_id]\n    content_type = self.content_type[line_id]\n    return line, content_type\n</code></pre>"},{"location":"odak/tools/#odak.tools.markdown.markdown.get_line_count","title":"<code>get_line_count()</code>","text":"<p>Definition to get the line count.</p> <p>Returns:</p> <ul> <li> <code>line_count</code> (              <code>int</code> )          \u2013            <p>Number of lines in the loaded markdown document.</p> </li> </ul> Source code in <code>odak/tools/markdown.py</code> <pre><code>def get_line_count(self):\n    \"\"\"\n    Definition to get the line count.\n\n\n    Returns\n    -------\n    line_count     : int\n                     Number of lines in the loaded markdown document.\n    \"\"\"\n    self.line_count = len(self.content)\n    return self.line_count\n</code></pre>"},{"location":"odak/tools/#odak.tools.markdown.markdown.set_dictonaries","title":"<code>set_dictonaries(begin_dictionary, end_dictionary, syntax_dictionary)</code>","text":"<p>Set document specific dictionaries so that the lines could be labelled in accordance.</p> <p>Parameters:</p> <ul> <li> <code>begin_dictionary</code>           \u2013            <pre><code>               Pythonic list containing markdown syntax for beginning of blocks (e.g., code, html).\n</code></pre> </li> <li> <code>end_dictionary</code>           \u2013            <pre><code>               Pythonic list containing markdown syntax for end of blocks (e.g., code, html).\n</code></pre> </li> <li> <code>syntax_dictionary</code>           \u2013            <pre><code>               Pythonic list containing markdown syntax (i.e. \\item).\n</code></pre> </li> </ul> Source code in <code>odak/tools/markdown.py</code> <pre><code>def set_dictonaries(self, begin_dictionary, end_dictionary, syntax_dictionary):\n    \"\"\"\n    Set document specific dictionaries so that the lines could be labelled in accordance.\n\n\n    Parameters\n    ----------\n    begin_dictionary     : list\n                           Pythonic list containing markdown syntax for beginning of blocks (e.g., code, html).\n    end_dictionary       : list\n                           Pythonic list containing markdown syntax for end of blocks (e.g., code, html).\n    syntax_dictionary    : list\n                           Pythonic list containing markdown syntax (i.e. \\\\item).\n\n    \"\"\"\n    self.markdown_begin_dictionary = begin_dictionary\n    self.markdown_end_dictionary = end_dictionary\n    self.markdown_dictionary = syntax_dictionary\n    self._label_lines\n</code></pre>"},{"location":"odak/tools/#odak.tools.sample.batch_of_rays","title":"<code>batch_of_rays(entry, exit)</code>","text":"<p>Definition to generate a batch of rays with given entry point(s) and exit point(s). Note that the mapping is one to one, meaning nth item in your entry points list will exit from nth item in your exit list and generate that particular ray. Note that you can have a combination like nx3 points for entry or exit and 1 point for entry or exit. But if you have multiple points both for entry and exit, the number of points have to be same both for entry and exit.</p> <p>Parameters:</p> <ul> <li> <code>entry</code>           \u2013            <pre><code>     Either a single point with size of 3 or multiple points with the size of nx3.\n</code></pre> </li> <li> <code>exit</code>           \u2013            <pre><code>     Either a single point with size of 3 or multiple points with the size of nx3.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rays</code> (              <code>ndarray</code> )          \u2013            <p>Generated batch of rays.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def batch_of_rays(entry, exit):\n    \"\"\"\n    Definition to generate a batch of rays with given entry point(s) and exit point(s). Note that the mapping is one to one, meaning nth item in your entry points list will exit from nth item in your exit list and generate that particular ray. Note that you can have a combination like nx3 points for entry or exit and 1 point for entry or exit. But if you have multiple points both for entry and exit, the number of points have to be same both for entry and exit.\n\n    Parameters\n    ----------\n    entry      : ndarray\n                 Either a single point with size of 3 or multiple points with the size of nx3.\n    exit       : ndarray\n                 Either a single point with size of 3 or multiple points with the size of nx3.\n\n    Returns\n    ----------\n    rays       : ndarray\n                 Generated batch of rays.\n    \"\"\"\n    norays = np.array([0, 0])\n    if len(entry.shape) == 1:\n        entry = entry.reshape((1, 3))\n    if len(exit.shape) == 1:\n        exit = exit.reshape((1, 3))\n    norays = np.amax(np.asarray([entry.shape[0], exit.shape[0]]))\n    if norays &gt; exit.shape[0]:\n        exit = np.repeat(exit, norays, axis=0)\n    elif norays &gt; entry.shape[0]:\n        entry = np.repeat(entry, norays, axis=0)\n    rays = []\n    norays = int(norays)\n    for i in range(norays):\n        rays.append(\n            create_ray_from_two_points(\n                entry[i],\n                exit[i]\n            )\n        )\n    rays = np.asarray(rays)\n    return rays\n</code></pre>"},{"location":"odak/tools/#odak.tools.sample.box_volume_sample","title":"<code>box_volume_sample(no=[10, 10, 10], size=[100.0, 100.0, 100.0], center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate samples in a box volume.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Physical size of the volume.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the volume.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the volume.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def box_volume_sample(no=[10, 10, 10], size=[100., 100., 100.], center=[0., 0., 0.], angles=[0., 0., 0.]):\n    \"\"\"\n    Definition to generate samples in a box volume.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    size        : list\n                  Physical size of the volume.\n    center      : list\n                  Center location of the volume.\n    angles      : list\n                  Tilt of the volume.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.zeros((no[0], no[1], no[2], 3))\n    x, y, z = np.mgrid[0:no[0], 0:no[1], 0:no[2]]\n    step = [\n        size[0]/no[0],\n        size[1]/no[1],\n        size[2]/no[2]\n    ]\n    samples[:, :, :, 0] = x*step[0]+step[0]/2.-size[0]/2.\n    samples[:, :, :, 1] = y*step[1]+step[1]/2.-size[1]/2.\n    samples[:, :, :, 2] = z*step[2]+step[2]/2.-size[2]/2.\n    samples = samples.reshape(\n        (samples.shape[0]*samples.shape[1]*samples.shape[2], samples.shape[3]))\n    samples = rotate_points(samples, angles=angles, offset=center)\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.sample.circular_sample","title":"<code>circular_sample(no=[10, 10], radius=10.0, center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate samples inside a circle over a surface.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of the circle.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def circular_sample(no=[10, 10], radius=10., center=[0., 0., 0.], angles=[0., 0., 0.]):\n    \"\"\"\n    Definition to generate samples inside a circle over a surface.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    radius      : float\n                  Radius of the circle.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.zeros((no[0]+1, no[1]+1, 3))\n    r_angles, r = np.mgrid[0:no[0]+1, 0:no[1]+1]\n    r = r/np.amax(r)*radius\n    r_angles = r_angles/np.amax(r_angles)*np.pi*2\n    samples[:, :, 0] = r*np.cos(r_angles)\n    samples[:, :, 1] = r*np.sin(r_angles)\n    samples = samples[1:no[0]+1, 1:no[1]+1, :]\n    samples = samples.reshape(\n        (samples.shape[0]*samples.shape[1], samples.shape[2]))\n    samples = rotate_points(samples, angles=angles, offset=center)\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.sample.circular_uniform_random_sample","title":"<code>circular_uniform_random_sample(no=[10, 50], radius=10.0, center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate sample inside a circle uniformly but randomly.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of the circle.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def circular_uniform_random_sample(no=[10, 50], radius=10., center=[0., 0., 0.], angles=[0., 0., 0.]):\n    \"\"\" \n    Definition to generate sample inside a circle uniformly but randomly.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    radius      : float\n                  Radius of the circle.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.empty((0, 3))\n    rs = np.sqrt(np.random.uniform(0, 1, no[0]))\n    angs = np.random.uniform(0, 2*np.pi, no[1])\n    for i in rs:\n        for angle in angs:\n            r = radius*i\n            point = np.array(\n                [float(r*np.cos(angle)), float(r*np.sin(angle)), 0])\n            samples = np.vstack((samples, point))\n    samples = rotate_points(samples, angles=angles, offset=center)\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.sample.circular_uniform_sample","title":"<code>circular_uniform_sample(no=[10, 50], radius=10.0, center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate sample inside a circle uniformly.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of the circle.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def circular_uniform_sample(no=[10, 50], radius=10., center=[0., 0., 0.], angles=[0., 0., 0.]):\n    \"\"\"\n    Definition to generate sample inside a circle uniformly.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    radius      : float\n                  Radius of the circle.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.empty((0, 3))\n    for i in range(0, no[0]):\n        r = i/no[0]*radius\n        ang_no = no[1]*i/no[0]\n        for j in range(0, int(no[1]*i/no[0])):\n            angle = j/ang_no*2*np.pi\n            point = np.array(\n                [float(r*np.cos(angle)), float(r*np.sin(angle)), 0])\n            samples = np.vstack((samples, point))\n    samples = rotate_points(samples, angles=angles, offset=center)\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.sample.grid_sample","title":"<code>grid_sample(no=[10, 10], size=[100.0, 100.0], center=[0.0, 0.0, 0.0], angles=[0.0, 0.0, 0.0])</code>","text":"<p>Definition to generate samples over a surface.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Physical size of the surface.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center location of the surface.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>      Tilt of the surface.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def grid_sample(no=[10, 10], size=[100., 100.], center=[0., 0., 0.], angles=[0., 0., 0.]):\n    \"\"\"\n    Definition to generate samples over a surface.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    size        : list\n                  Physical size of the surface.\n    center      : list\n                  Center location of the surface.\n    angles      : list\n                  Tilt of the surface.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.zeros((no[0], no[1], 3))\n    step = [\n        size[0]/(no[0]-1),\n        size[1]/(no[1]-1)\n    ]\n    x, y = np.mgrid[0:no[0], 0:no[1]]\n    samples[:, :, 0] = x*step[0]-size[0]/2.\n    samples[:, :, 1] = y*step[1]-size[1]/2.\n    samples = samples.reshape(\n        (samples.shape[0]*samples.shape[1], samples.shape[2]))\n    samples = rotate_points(samples, angles=angles, offset=center)\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.sample.random_sample_point_cloud","title":"<code>random_sample_point_cloud(point_cloud, no, p=None)</code>","text":"<p>Definition to pull a subset of points from a point cloud with a given probability.</p> <p>Parameters:</p> <ul> <li> <code>point_cloud</code>           \u2013            <pre><code>       Point cloud array.\n</code></pre> </li> <li> <code>no</code>           \u2013            <pre><code>       Number of samples.\n</code></pre> </li> <li> <code>p</code>           \u2013            <pre><code>       Probability list in the same size as no.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>subset</code> (              <code>ndarray</code> )          \u2013            <p>Subset of the given point cloud.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def random_sample_point_cloud(point_cloud, no, p=None):\n    \"\"\"\n    Definition to pull a subset of points from a point cloud with a given probability.\n\n    Parameters\n    ----------\n    point_cloud  : ndarray\n                   Point cloud array.\n    no           : list\n                   Number of samples.\n    p            : list\n                   Probability list in the same size as no.\n\n    Returns\n    ----------\n    subset       : ndarray\n                   Subset of the given point cloud.\n    \"\"\"\n    choice = np.random.choice(point_cloud.shape[0], no, p)\n    subset = point_cloud[choice, :]\n    return subset\n</code></pre>"},{"location":"odak/tools/#odak.tools.sample.sphere_sample","title":"<code>sphere_sample(no=[10, 10], radius=1.0, center=[0.0, 0.0, 0.0], k=[1, 2])</code>","text":"<p>Definition to generate a regular sample set on the surface of a sphere using polar coordinates.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of a sphere.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center of a sphere.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>      Multipliers for gathering samples. If you set k=[1,2] it will draw samples from a perfect sphere.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def sphere_sample(no=[10, 10], radius=1., center=[0., 0., 0.], k=[1, 2]):\n    \"\"\"\n    Definition to generate a regular sample set on the surface of a sphere using polar coordinates.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    radius      : float\n                  Radius of a sphere.\n    center      : list\n                  Center of a sphere.\n    k           : list\n                  Multipliers for gathering samples. If you set k=[1,2] it will draw samples from a perfect sphere.\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n    \"\"\"\n    samples = np.zeros((no[0], no[1], 3))\n    psi, teta = np.mgrid[0:no[0], 0:no[1]]\n    psi = k[0]*np.pi/no[0]*psi\n    teta = k[1]*np.pi/no[1]*teta\n    samples[:, :, 0] = center[0]+radius*np.sin(psi)*np.cos(teta)\n    samples[:, :, 1] = center[0]+radius*np.sin(psi)*np.sin(teta)\n    samples[:, :, 2] = center[0]+radius*np.cos(psi)\n    samples = samples.reshape((no[0]*no[1], 3))\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.sample.sphere_sample_uniform","title":"<code>sphere_sample_uniform(no=[10, 10], radius=1.0, center=[0.0, 0.0, 0.0], k=[1, 2])</code>","text":"<p>Definition to generate an uniform sample set on the surface of a sphere using polar coordinates.</p> <p>Parameters:</p> <ul> <li> <code>no</code>           \u2013            <pre><code>      Number of samples.\n</code></pre> </li> <li> <code>radius</code>           \u2013            <pre><code>      Radius of a sphere.\n</code></pre> </li> <li> <code>center</code>           \u2013            <pre><code>      Center of a sphere.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>      Multipliers for gathering samples. If you set k=[1,2] it will draw samples from a perfect sphere.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>samples</code> (              <code>ndarray</code> )          \u2013            <p>Samples generated.</p> </li> </ul> Source code in <code>odak/tools/sample.py</code> <pre><code>def sphere_sample_uniform(no=[10, 10], radius=1., center=[0., 0., 0.], k=[1, 2]):\n    \"\"\"\n    Definition to generate an uniform sample set on the surface of a sphere using polar coordinates.\n\n    Parameters\n    ----------\n    no          : list\n                  Number of samples.\n    radius      : float\n                  Radius of a sphere.\n    center      : list\n                  Center of a sphere.\n    k           : list\n                  Multipliers for gathering samples. If you set k=[1,2] it will draw samples from a perfect sphere.\n\n\n    Returns\n    ----------\n    samples     : ndarray\n                  Samples generated.\n\n    \"\"\"\n    samples = np.zeros((no[0], no[1], 3))\n    row = np.arange(0, no[0])\n    psi, teta = np.mgrid[0:no[0], 0:no[1]]\n    for psi_id in range(0, no[0]):\n        psi[psi_id] = np.roll(row, psi_id, axis=0)\n        teta[psi_id] = np.roll(row, -psi_id, axis=0)\n    psi = k[0]*np.pi/no[0]*psi\n    teta = k[1]*np.pi/no[1]*teta\n    samples[:, :, 0] = center[0]+radius*np.sin(psi)*np.cos(teta)\n    samples[:, :, 1] = center[1]+radius*np.sin(psi)*np.sin(teta)\n    samples[:, :, 2] = center[2]+radius*np.cos(psi)\n    samples = samples.reshape((no[0]*no[1], 3))\n    return samples\n</code></pre>"},{"location":"odak/tools/#odak.tools.vector.closest_point_to_a_ray","title":"<code>closest_point_to_a_ray(point, ray)</code>","text":"<p>Definition to calculate the point on a ray that is closest to given point.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>        Given point in X,Y,Z.\n</code></pre> </li> <li> <code>ray</code>           \u2013            <pre><code>        Given ray.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>closest_point</code> (              <code>ndarray</code> )          \u2013            <p>Calculated closest point.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def closest_point_to_a_ray(point, ray):\n    \"\"\"\n    Definition to calculate the point on a ray that is closest to given point.\n\n    Parameters\n    ----------\n    point         : list\n                    Given point in X,Y,Z.\n    ray           : ndarray\n                    Given ray.\n\n    Returns\n    ---------\n    closest_point : ndarray\n                    Calculated closest point.\n    \"\"\"\n    from odak.raytracing import propagate_a_ray\n    if len(ray.shape) == 2:\n        ray = ray.reshape((1, 2, 3))\n    p0 = ray[:, 0]\n    p1 = propagate_a_ray(ray, 1.)\n    if len(p1.shape) == 2:\n        p1 = p1.reshape((1, 2, 3))\n    p1 = p1[:, 0]\n    p1 = p1.reshape(3)\n    p0 = p0.reshape(3)\n    point = point.reshape(3)\n    closest_distance = -np.dot((p0-point), (p1-p0))/np.sum((p1-p0)**2)\n    closest_point = propagate_a_ray(ray, closest_distance)[0]\n    return closest_point\n</code></pre>"},{"location":"odak/tools/#odak.tools.vector.cross_product","title":"<code>cross_product(vector1, vector2)</code>","text":"<p>Definition to cross product two vectors and return the resultant vector. Used method described under: http://en.wikipedia.org/wiki/Cross_product</p> <p>Parameters:</p> <ul> <li> <code>vector1</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> <li> <code>vector2</code>           \u2013            <pre><code>       A vector/ray.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>ray</code> (              <code>ndarray</code> )          \u2013            <p>Array that contains starting points and cosines of a created ray.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def cross_product(vector1, vector2):\n    \"\"\"\n    Definition to cross product two vectors and return the resultant vector. Used method described under: http://en.wikipedia.org/wiki/Cross_product\n\n    Parameters\n    ----------\n    vector1      : ndarray\n                   A vector/ray.\n    vector2      : ndarray\n                   A vector/ray.\n\n    Returns\n    ----------\n    ray          : ndarray\n                   Array that contains starting points and cosines of a created ray.\n    \"\"\"\n    angle = np.cross(vector1[1].T, vector2[1].T)\n    angle = np.asarray(angle)\n    ray = np.array([vector1[0], angle], dtype=np.float32)\n    return ray\n</code></pre>"},{"location":"odak/tools/#odak.tools.vector.distance_between_point_clouds","title":"<code>distance_between_point_clouds(points0, points1)</code>","text":"<p>A definition to find distance between every point in one cloud to other points in the other point cloud.</p> <p>Parameters:</p> <ul> <li> <code>points0</code>           \u2013            <pre><code>      Mx3 points.\n</code></pre> </li> <li> <code>points1</code>           \u2013            <pre><code>      Nx3 points.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distances</code> (              <code>ndarray</code> )          \u2013            <p>MxN distances.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def distance_between_point_clouds(points0, points1):\n    \"\"\"\n    A definition to find distance between every point in one cloud to other points in the other point cloud.\n    Parameters\n    ----------\n    points0     : ndarray\n                  Mx3 points.\n    points1     : ndarray\n                  Nx3 points.\n\n    Returns\n    ----------\n    distances   : ndarray\n                  MxN distances.\n    \"\"\"\n    c = points1.reshape((1, points1.shape[0], points1.shape[1]))\n    a = np.repeat(c, points0.shape[0], axis=0)\n    b = points0.reshape((points0.shape[0], 1, points0.shape[1]))\n    b = np.repeat(b, a.shape[1], axis=1)\n    distances = np.sqrt(np.sum((a-b)**2, axis=2))\n    return distances\n</code></pre>"},{"location":"odak/tools/#odak.tools.vector.distance_between_two_points","title":"<code>distance_between_two_points(point1, point2)</code>","text":"<p>Definition to calculate distance between two given points.</p> <p>Parameters:</p> <ul> <li> <code>point1</code>           \u2013            <pre><code>      First point in X,Y,Z.\n</code></pre> </li> <li> <code>point2</code>           \u2013            <pre><code>      Second point in X,Y,Z.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between given two points.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def distance_between_two_points(point1, point2):\n    \"\"\"\n    Definition to calculate distance between two given points.\n\n    Parameters\n    ----------\n    point1      : list\n                  First point in X,Y,Z.\n    point2      : list\n                  Second point in X,Y,Z.\n\n    Returns\n    ----------\n    distance    : float\n                  Distance in between given two points.\n    \"\"\"\n    point1 = np.asarray(point1)\n    point2 = np.asarray(point2)\n    if len(point1.shape) == 1 and len(point2.shape) == 1:\n        distance = np.sqrt(np.sum((point1-point2)**2))\n    elif len(point1.shape) == 2 or len(point2.shape) == 2:\n        distance = np.sqrt(np.sum((point1-point2)**2, axis=1))\n    return distance\n</code></pre>"},{"location":"odak/tools/#odak.tools.vector.point_to_ray_distance","title":"<code>point_to_ray_distance(point, ray_point_0, ray_point_1)</code>","text":"<p>Definition to find point's closest distance to a line represented with two points.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>      Point to be tested.\n</code></pre> </li> <li> <code>ray_point_0</code>               (<code>ndarray</code>)           \u2013            <pre><code>      First point to represent a line.\n</code></pre> </li> <li> <code>ray_point_1</code>               (<code>ndarray</code>)           \u2013            <pre><code>      Second point to represent a line.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Calculated distance.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def point_to_ray_distance(point, ray_point_0, ray_point_1):\n    \"\"\"\n    Definition to find point's closest distance to a line represented with two points.\n\n    Parameters\n    ----------\n    point       : ndarray\n                  Point to be tested.\n    ray_point_0 : ndarray\n                  First point to represent a line.\n    ray_point_1 : ndarray\n                  Second point to represent a line.\n\n    Returns\n    ----------\n    distance    : float\n                  Calculated distance.\n    \"\"\"\n    distance = np.sum(np.cross((point-ray_point_0), (point-ray_point_1))\n                      ** 2)/np.sum((ray_point_1-ray_point_0)**2)\n    return distance\n</code></pre>"},{"location":"odak/tools/#odak.tools.vector.same_side","title":"<code>same_side(p1, p2, a, b)</code>","text":"<p>Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.</p> <p>Parameters:</p> <ul> <li> <code>p1</code>           \u2013            <pre><code>      Point(s) to check.\n</code></pre> </li> <li> <code>p2</code>           \u2013            <pre><code>      This is the point check against.\n</code></pre> </li> <li> <code>a</code>           \u2013            <pre><code>      First point that forms the line.\n</code></pre> </li> <li> <code>b</code>           \u2013            <pre><code>      Second point that forms the line.\n</code></pre> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def same_side(p1, p2, a, b):\n    \"\"\"\n    Definition to figure which side a point is on with respect to a line and a point. See http://www.blackpawn.com/texts/pointinpoly/ for more. If p1 and p2 are on the sameside, this definition returns True.\n\n    Parameters\n    ----------\n    p1          : list\n                  Point(s) to check.\n    p2          : list\n                  This is the point check against.\n    a           : list\n                  First point that forms the line.\n    b           : list\n                  Second point that forms the line.\n    \"\"\"\n    ba = np.subtract(b, a)\n    p1a = np.subtract(p1, a)\n    p2a = np.subtract(p2, a)\n    cp1 = np.cross(ba, p1a)\n    cp2 = np.cross(ba, p2a)\n    test = np.dot(cp1, cp2)\n    if len(p1.shape) &gt; 1:\n        return test &gt;= 0\n    if test &gt;= 0:\n        return True\n    return False\n</code></pre>"},{"location":"odak/tools/#odak.tools.transformation.rotate_point","title":"<code>rotate_point(point, angles=[0, 0, 0], mode='XYZ', origin=[0, 0, 0], offset=[0, 0, 0])</code>","text":"<p>Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.</p> <p>Parameters:</p> <ul> <li> <code>point</code>           \u2013            <pre><code>       A point.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>       Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n</code></pre> </li> <li> <code>origin</code>           \u2013            <pre><code>       Reference point for a rotation.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>       Shift with the given offset.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>Result of the rotation</p> </li> <li> <code>rotx</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along X axis.</p> </li> <li> <code>roty</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along Y axis.</p> </li> <li> <code>rotz</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along Z axis.</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotate_point(point, angles = [0, 0, 0], mode = 'XYZ', origin = [0, 0, 0], offset = [0, 0, 0]):\n    \"\"\"\n    Definition to rotate a given point. Note that rotation is always with respect to 0,0,0.\n\n    Parameters\n    ----------\n    point        : ndarray\n                   A point.\n    angles       : list\n                   Rotation angles in degrees. \n    mode         : str\n                   Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n    origin       : list\n                   Reference point for a rotation.\n    offset       : list\n                   Shift with the given offset.\n\n    Returns\n    ----------\n    result       : ndarray\n                   Result of the rotation\n    rotx         : ndarray\n                   Rotation matrix along X axis.\n    roty         : ndarray\n                   Rotation matrix along Y axis.\n    rotz         : ndarray\n                   Rotation matrix along Z axis.\n    \"\"\"\n    point = np.asarray(point)\n    point -= np.asarray(origin)\n    rotx = rotmatx(angles[0])\n    roty = rotmaty(angles[1])\n    rotz = rotmatz(angles[2])\n    if mode == 'XYZ':\n        result = np.dot(rotz, np.dot(roty, np.dot(rotx, point)))\n    elif mode == 'XZY':\n        result = np.dot(roty, np.dot(rotz, np.dot(rotx, point)))\n    elif mode == 'YXZ':\n        result = np.dot(rotz, np.dot(rotx, np.dot(roty, point)))\n    elif mode == 'ZXY':\n        result = np.dot(roty, np.dot(rotx, np.dot(rotz, point)))\n    elif mode == 'ZYX':\n        result = np.dot(rotx, np.dot(roty, np.dot(rotz, point)))\n    result += np.asarray(origin)\n    result += np.asarray(offset)\n    return result, rotx, roty, rotz\n</code></pre>"},{"location":"odak/tools/#odak.tools.transformation.rotate_points","title":"<code>rotate_points(points, angles=[0, 0, 0], mode='XYZ', origin=[0, 0, 0], offset=[0, 0, 0])</code>","text":"<p>Definition to rotate points.</p> <p>Parameters:</p> <ul> <li> <code>points</code>           \u2013            <pre><code>       Points.\n</code></pre> </li> <li> <code>angles</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> <li> <code>mode</code>           \u2013            <pre><code>       Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n</code></pre> </li> <li> <code>origin</code>           \u2013            <pre><code>       Reference point for a rotation.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>       Shift with the given offset.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>Result of the rotation</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotate_points(points, angles = [0, 0, 0], mode = 'XYZ', origin = [0, 0, 0], offset = [0, 0, 0]):\n    \"\"\"\n    Definition to rotate points.\n\n    Parameters\n    ----------\n    points       : ndarray\n                   Points.\n    angles       : list\n                   Rotation angles in degrees. \n    mode         : str\n                   Rotation mode determines ordering of the rotations at each axis. There are XYZ,YXZ,ZXY and ZYX modes.\n    origin       : list\n                   Reference point for a rotation.\n    offset       : list\n                   Shift with the given offset.\n\n    Returns\n    ----------\n    result       : ndarray\n                   Result of the rotation   \n    \"\"\"\n    points = np.asarray(points)\n    if angles[0] == 0 and angles[1] == 0 and angles[2] == 0:\n        result = np.array(offset) + points\n        return result\n    points -= np.array(origin)\n    rotx = rotmatx(angles[0])\n    roty = rotmaty(angles[1])\n    rotz = rotmatz(angles[2])\n    if mode == 'XYZ':\n        result = np.dot(rotz, np.dot(roty, np.dot(rotx, points.T))).T\n    elif mode == 'XZY':\n        result = np.dot(roty, np.dot(rotz, np.dot(rotx, points.T))).T\n    elif mode == 'YXZ':\n        result = np.dot(rotz, np.dot(rotx, np.dot(roty, points.T))).T\n    elif mode == 'ZXY':\n        result = np.dot(roty, np.dot(rotx, np.dot(rotz, points.T))).T\n    elif mode == 'ZYX':\n        result = np.dot(rotx, np.dot(roty, np.dot(rotz, points.T))).T\n    result += np.array(origin)\n    result += np.array(offset)\n    return result\n</code></pre>"},{"location":"odak/tools/#odak.tools.transformation.rotmatx","title":"<code>rotmatx(angle)</code>","text":"<p>Definition to generate a rotation matrix along X axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rotx</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along X axis.</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotmatx(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along X axis.\n\n    Parameters\n    ----------\n    angle        : list\n                   Rotation angles in degrees.\n\n    Returns\n    -------\n    rotx         : ndarray\n                   Rotation matrix along X axis.\n    \"\"\"\n    angle = np.float64(angle)\n    angle = np.radians(angle)\n    rotx = np.array([\n        [1.,               0.,               0.],\n        [0.,  math.cos(angle), -math.sin(angle)],\n        [0.,  math.sin(angle),  math.cos(angle)]\n    ], dtype=np.float64)\n    return rotx\n</code></pre>"},{"location":"odak/tools/#odak.tools.transformation.rotmaty","title":"<code>rotmaty(angle)</code>","text":"<p>Definition to generate a rotation matrix along Y axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>roty</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along Y axis.</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotmaty(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along Y axis.\n\n    Parameters\n    ----------\n    angle        : list\n                   Rotation angles in degrees.\n\n    Returns\n    -------\n    roty         : ndarray\n                   Rotation matrix along Y axis.\n    \"\"\"\n    angle = np.radians(angle)\n    roty = np.array([\n        [math.cos(angle),  0., math.sin(angle)],\n        [0.,               1.,              0.],\n        [-math.sin(angle), 0., math.cos(angle)]\n    ], dtype=np.float64)\n    return roty\n</code></pre>"},{"location":"odak/tools/#odak.tools.transformation.rotmatz","title":"<code>rotmatz(angle)</code>","text":"<p>Definition to generate a rotation matrix along Z axis.</p> <p>Parameters:</p> <ul> <li> <code>angle</code>           \u2013            <pre><code>       Rotation angles in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>rotz</code> (              <code>ndarray</code> )          \u2013            <p>Rotation matrix along Z axis.</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def rotmatz(angle):\n    \"\"\"\n    Definition to generate a rotation matrix along Z axis.\n\n    Parameters\n    ----------\n    angle        : list\n                   Rotation angles in degrees.\n\n    Returns\n    -------\n    rotz         : ndarray\n                   Rotation matrix along Z axis.\n    \"\"\"\n    angle = np.radians(angle)\n    rotz = np.array([\n        [math.cos(angle), -math.sin(angle), 0.],\n        [math.sin(angle),  math.cos(angle), 0.],\n        [0.,               0., 1.]\n    ], dtype=np.float64)\n\n    return rotz\n</code></pre>"},{"location":"odak/tools/#odak.tools.transformation.tilt_towards","title":"<code>tilt_towards(location, lookat)</code>","text":"<p>Definition to tilt surface normal of a plane towards a point.</p> <p>Parameters:</p> <ul> <li> <code>location</code>           \u2013            <pre><code>       Center of the plane to be tilted.\n</code></pre> </li> <li> <code>lookat</code>           \u2013            <pre><code>       Tilt towards this point.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>angles</code> (              <code>list</code> )          \u2013            <p>Rotation angles in degrees.</p> </li> </ul> Source code in <code>odak/tools/transformation.py</code> <pre><code>def tilt_towards(location, lookat):\n    \"\"\"\n    Definition to tilt surface normal of a plane towards a point.\n\n    Parameters\n    ----------\n    location     : list\n                   Center of the plane to be tilted.\n    lookat       : list\n                   Tilt towards this point.\n\n    Returns\n    ----------\n    angles       : list\n                   Rotation angles in degrees.\n    \"\"\"\n    dx = location[0]-lookat[0]\n    dy = location[1]-lookat[1]\n    dz = location[2]-lookat[2]\n    dist = np.sqrt(dx**2+dy**2+dz**2)\n    phi = np.arctan2(dy, dx)\n    theta = np.arccos(dz/dist)\n    angles = [\n        0,\n        np.degrees(theta).tolist(),\n        np.degrees(phi).tolist()\n    ]\n    return angles\n</code></pre>"},{"location":"odak/wave/","title":"odak.wave","text":"<p><code>odak.wave</code></p> <p>Provides necessary definitions for merging geometric optics with wave theory and classical approaches in the wave theory as well. See \"Introduction to Fourier Optcs\" from Joseph Goodman for the theoratical explanation.</p>"},{"location":"odak/wave/#odak.wave.adaptive_sampling_angular_spectrum","title":"<code>adaptive_sampling_angular_spectrum(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate adaptive sampling angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def adaptive_sampling_angular_spectrum(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate adaptive sampling angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    iflag = -1\n    eps = 10**(-12)\n    nv, nu = field.shape\n    l = nu*dx\n    x = np.linspace(-l/2, l/2, nu)\n    y = np.linspace(-l/2, l/2, nv)\n    X, Y = np.meshgrid(x, y)\n    fx = np.linspace(-1./2./dx, 1./2./dx, nu)\n    fy = np.linspace(-1./2./dx, 1./2./dx, nv)\n    FX, FY = np.meshgrid(fx, fy)\n    forig = 1./2./dx\n    fc2 = 1./2*(nu/wavelength/np.abs(distance))**0.5\n    ss = np.abs(fc2)/forig\n    zc = nu*dx**2/wavelength\n    K = nu/2/np.amax(np.abs(fx))\n    m = 2\n    nnu2 = m*nu\n    nnv2 = m*nv\n    fxn = np.linspace(-1./2./dx, 1./2./dx, nnu2)\n    fyn = np.linspace(-1./2./dx, 1./2./dx, nnv2)\n    if np.abs(distance) &gt; zc*2:\n        fxn = fxn*ss\n        fyn = fyn*ss\n    FXN, FYN = np.meshgrid(fxn, fyn)\n    Hn = np.exp(1j*k*distance*(1-(FXN*wavelength)**2-(FYN*wavelength)**2)**0.5)\n    FX = FXN/np.amax(FXN)*np.pi\n    FY = FYN/np.amax(FYN)*np.pi\n    t_2 = nufft2(field, FX*ss, FY*ss, size=[nnv2, nnu2], sign=iflag, eps=eps)\n    FX = FX/np.amax(FX)*np.pi\n    FY = FY/np.amax(FY)*np.pi\n    result = nuifft2(Hn*t_2, FX*ss, FY*ss, size=[nv, nu], sign=-iflag, eps=eps)\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.add_phase","title":"<code>add_phase(field, new_phase)</code>","text":"<p>Definition for adding a phase to a given complex field.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>       Complex field.\n</code></pre> </li> <li> <code>new_phase</code>           \u2013            <pre><code>       Complex phase.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>complex64</code> )          \u2013            <p>Complex field.</p> </li> </ul> Source code in <code>odak/wave/__init__.py</code> <pre><code>def add_phase(field, new_phase):\n    \"\"\"\n    Definition for adding a phase to a given complex field.\n\n    Parameters\n    ----------\n    field        : np.complex64\n                   Complex field.\n    new_phase    : np.complex64\n                   Complex phase.\n\n    Returns\n    -------\n    new_field    : np.complex64\n                   Complex field.\n    \"\"\"\n    phase = calculate_phase(field)\n    amplitude = calculate_amplitude(field)\n    new_field = amplitude*np.cos(phase+new_phase) + \\\n        1j*amplitude*np.sin(phase+new_phase)\n    return new_field\n</code></pre>"},{"location":"odak/wave/#odak.wave.add_random_phase","title":"<code>add_random_phase(field)</code>","text":"<p>Definition for adding a random phase to a given complex field.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>       Complex field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>complex64</code> )          \u2013            <p>Complex field.</p> </li> </ul> Source code in <code>odak/wave/__init__.py</code> <pre><code>def add_random_phase(field):\n    \"\"\"\n    Definition for adding a random phase to a given complex field.\n\n    Parameters\n    ----------\n    field        : np.complex64\n                   Complex field.\n\n    Returns\n    -------\n    new_field    : np.complex64\n                   Complex field.\n    \"\"\"\n    random_phase = np.pi*np.random.random(field.shape)\n    new_field = add_phase(field, random_phase)\n    return new_field\n</code></pre>"},{"location":"odak/wave/#odak.wave.adjust_phase_only_slm_range","title":"<code>adjust_phase_only_slm_range(native_range, working_wavelength, native_wavelength)</code>","text":"<p>Definition for calculating the phase range of the Spatial Light Modulator (SLM) for a given wavelength. Here you prove maximum angle as the lower bound is typically zero. If the lower bound isn't zero in angles, you can use this very same definition for calculating lower angular bound as well.</p> <p>Parameters:</p> <ul> <li> <code>native_range</code>           \u2013            <pre><code>             Native range of the phase only SLM in radians (i.e. two pi).\n</code></pre> </li> <li> <code>working_wavelength</code>               (<code>float</code>)           \u2013            <pre><code>             Wavelength of the illumination source or some working wavelength.\n</code></pre> </li> <li> <code>native_wavelength</code>           \u2013            <pre><code>             Wavelength which the SLM is designed for.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_range</code> (              <code>float</code> )          \u2013            <p>Calculated phase range in radians.</p> </li> </ul> Source code in <code>odak/wave/__init__.py</code> <pre><code>def adjust_phase_only_slm_range(native_range, working_wavelength, native_wavelength):\n    \"\"\"\n    Definition for calculating the phase range of the Spatial Light Modulator (SLM) for a given wavelength. Here you prove maximum angle as the lower bound is typically zero. If the lower bound isn't zero in angles, you can use this very same definition for calculating lower angular bound as well.\n\n    Parameters\n    ----------\n    native_range       : float\n                         Native range of the phase only SLM in radians (i.e. two pi).\n    working_wavelength : float\n                         Wavelength of the illumination source or some working wavelength.\n    native_wavelength  : float\n                         Wavelength which the SLM is designed for.\n\n    Returns\n    -------\n    new_range          : float\n                         Calculated phase range in radians.\n    \"\"\"\n    new_range = native_range/working_wavelength*native_wavelength\n    return new_range\n</code></pre>"},{"location":"odak/wave/#odak.wave.angular_spectrum","title":"<code>angular_spectrum(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate angular spectrum based beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def angular_spectrum(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate angular spectrum based beam propagation.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    nv, nu = field.shape\n    fx = np.linspace(-1. / 2. /dx, 1. /2. /dx, nu)\n    fy = np.linspace(-1. / 2. /dx, 1. /2. /dx, nv)\n    FX, FY = np.meshgrid(fx, fy)\n    H = np.exp(1j * k * distance * (1 - (FX * wavelength) ** 2 - (FY * wavelength) ** 2) ** 0.5)\n    U1 = np.fft.fftshift(np.fft.fft2(field))\n    U2 = H * U1\n    result = np.fft.ifft2(np.fft.ifftshift(U2))\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.band_extended_angular_spectrum","title":"<code>band_extended_angular_spectrum(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate bandextended angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics Letters 45.6 (2020): 1543-1546.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def band_extended_angular_spectrum(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate bandextended angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics Letters 45.6 (2020): 1543-1546.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    iflag = -1\n    eps = 10**(-12)\n    nv, nu = field.shape\n    l = nu*dx\n    x = np.linspace(-l/2, l/2, nu)\n    y = np.linspace(-l/2, l/2, nv)\n    X, Y = np.meshgrid(x, y)\n    Z = X**2+Y**2\n    fx = np.linspace(-1./2./dx, 1./2./dx, nu)\n    fy = np.linspace(-1./2./dx, 1./2./dx, nv)\n    FX, FY = np.meshgrid(fx, fy)\n    K = nu/2/np.amax(fx)\n    fcn = 1./2*(nu/wavelength/np.abs(distance))**0.5\n    ss = np.abs(fcn)/np.amax(np.abs(fx))\n    zc = nu*dx**2/wavelength\n    if np.abs(distance) &lt; zc:\n        fxn = fx\n        fyn = fy\n    else:\n        fxn = fx*ss\n        fyn = fy*ss\n    FXN, FYN = np.meshgrid(fxn, fyn)\n    Hn = np.exp(1j*k*distance*(1-(FXN*wavelength)**2-(FYN*wavelength)**2)**0.5)\n    X = X/np.amax(X)*np.pi\n    Y = Y/np.amax(Y)*np.pi\n    t_asmNUFT = nufft2(field, X*ss, Y*ss, sign=iflag, eps=eps)\n    result = nuifft2(Hn*t_asmNUFT, X*ss, Y*ss, sign=-iflag, eps=eps)\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.band_limited_angular_spectrum","title":"<code>band_limited_angular_spectrum(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate bandlimited angular spectrum based beam propagation. For more Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def band_limited_angular_spectrum(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate bandlimited angular spectrum based beam propagation. For more Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    nv, nu = field.shape\n    fx = np.linspace(-1. / 2. /dx, 1. /2. /dx, nu)\n    fy = np.linspace(-1. / 2. /dx, 1. /2. /dx, nv)\n    FX, FY = np.meshgrid(fx, fy)\n    H_exp = np.exp(1j * k * distance * (1 - (FX * wavelength) ** 2 - (FY * wavelength) ** 2) ** 0.5)\n\n    x = dx * float(nu)\n    y = dx * float(nv)\n    fx_max = 1 / np.sqrt((2 * distance * (1 / x))**2 + 1) / wavelength\n    fy_max = 1 / np.sqrt((2 * distance * (1 / y))**2 + 1) / wavelength\n    H_filter = ((np.abs(FX) &lt; fx_max) &amp; (np.abs(FY) &lt; fy_max))\n    H = H_filter * H_exp\n\n    U1 = np.fft.fftshift(np.fft.fft2(field))\n    U2 = H * U1\n    result = np.fft.ifft2(np.fft.ifftshift(U2))\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.calculate_intensity","title":"<code>calculate_intensity(field)</code>","text":"<p>Definition to calculate intensity of a single or multiple given electric field(s).</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>       Electric fields or an electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>intensity</code> (              <code>float</code> )          \u2013            <p>Intensity or intensities of electric field(s).</p> </li> </ul> Source code in <code>odak/wave/__init__.py</code> <pre><code>def calculate_intensity(field):\n    \"\"\"\n    Definition to calculate intensity of a single or multiple given electric field(s).\n\n    Parameters\n    ----------\n    field        : ndarray.complex or complex\n                   Electric fields or an electric field.\n\n    Returns\n    -------\n    intensity    : float\n                   Intensity or intensities of electric field(s).\n    \"\"\"\n    intensity = np.abs(field)**2\n    return intensity\n</code></pre>"},{"location":"odak/wave/#odak.wave.distance_between_two_points","title":"<code>distance_between_two_points(point1, point2)</code>","text":"<p>Definition to calculate distance between two given points.</p> <p>Parameters:</p> <ul> <li> <code>point1</code>           \u2013            <pre><code>      First point in X,Y,Z.\n</code></pre> </li> <li> <code>point2</code>           \u2013            <pre><code>      Second point in X,Y,Z.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>distance</code> (              <code>float</code> )          \u2013            <p>Distance in between given two points.</p> </li> </ul> Source code in <code>odak/tools/vector.py</code> <pre><code>def distance_between_two_points(point1, point2):\n    \"\"\"\n    Definition to calculate distance between two given points.\n\n    Parameters\n    ----------\n    point1      : list\n                  First point in X,Y,Z.\n    point2      : list\n                  Second point in X,Y,Z.\n\n    Returns\n    ----------\n    distance    : float\n                  Distance in between given two points.\n    \"\"\"\n    point1 = np.asarray(point1)\n    point2 = np.asarray(point2)\n    if len(point1.shape) == 1 and len(point2.shape) == 1:\n        distance = np.sqrt(np.sum((point1-point2)**2))\n    elif len(point1.shape) == 2 or len(point2.shape) == 2:\n        distance = np.sqrt(np.sum((point1-point2)**2, axis=1))\n    return distance\n</code></pre>"},{"location":"odak/wave/#odak.wave.double_convergence","title":"<code>double_convergence(nx, ny, k, r, dx)</code>","text":"<p>A definition to generate initial phase for a Gerchberg-Saxton method. For more details consult Sun, Peng, et al. \"Holographic near-eye display system based on double-convergence light Gerchberg-Saxton algorithm.\" Optics express 26.8 (2018): 10140-10151.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>     Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>     Size of the output along Y.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>     See odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>r</code>           \u2013            <pre><code>     The distance between location of a light source and an image plane.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>     Pixel pitch.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>function</code> (              <code>ndarray</code> )          \u2013            <p>Generated phase pattern for a Gerchberg-Saxton method.</p> </li> </ul> Source code in <code>odak/wave/lens.py</code> <pre><code>def double_convergence(nx, ny, k, r, dx):\n    \"\"\"\n    A definition to generate initial phase for a Gerchberg-Saxton method. For more details consult Sun, Peng, et al. \"Holographic near-eye display system based on double-convergence light Gerchberg-Saxton algorithm.\" Optics express 26.8 (2018): 10140-10151.\n\n    Parameters\n    ----------\n    nx         : int\n                 Size of the output along X.\n    ny         : int\n                 Size of the output along Y.\n    k          : odak.wave.wavenumber\n                 See odak.wave.wavenumber for more.\n    r          : float\n                 The distance between location of a light source and an image plane.\n    dx         : float\n                 Pixel pitch.\n\n    Returns\n    -------\n    function   : ndarray\n                 Generated phase pattern for a Gerchberg-Saxton method.\n    \"\"\"\n    size = [ny, nx]\n    x = np.linspace(-size[0]*dx/2, size[0]*dx/2, size[0])\n    y = np.linspace(-size[1]*dx/2, size[1]*dx/2, size[1])\n    X, Y = np.meshgrid(x, y)\n    Z = X**2+Y**2\n    w = np.exp(1j*k*Z/r)\n    return w\n</code></pre>"},{"location":"odak/wave/#odak.wave.electric_field_per_plane_wave","title":"<code>electric_field_per_plane_wave(amplitude, opd, k, phase=0, w=0, t=0)</code>","text":"<p>Definition to return state of a plane wave at a particular distance and time.</p> <p>Parameters:</p> <ul> <li> <code>amplitude</code>           \u2013            <pre><code>       Amplitude of a wave.\n</code></pre> </li> <li> <code>opd</code>           \u2013            <pre><code>       Optical path difference in mm.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>       Wave number of a wave, see odak.wave.parameters.wavenumber for more.\n</code></pre> </li> <li> <code>phase</code>           \u2013            <pre><code>       Initial phase of a wave.\n</code></pre> </li> <li> <code>w</code>           \u2013            <pre><code>       Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more.\n</code></pre> </li> <li> <code>t</code>           \u2013            <pre><code>       Time in seconds.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field</code> (              <code>complex</code> )          \u2013            <p>A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)).</p> </li> </ul> Source code in <code>odak/wave/vector.py</code> <pre><code>def electric_field_per_plane_wave(amplitude, opd, k, phase=0, w=0, t=0):\n    \"\"\"\n    Definition to return state of a plane wave at a particular distance and time.\n\n    Parameters\n    ----------\n    amplitude    : float\n                   Amplitude of a wave.\n    opd          : float\n                   Optical path difference in mm.\n    k            : float\n                   Wave number of a wave, see odak.wave.parameters.wavenumber for more.\n    phase        : float\n                   Initial phase of a wave.\n    w            : float\n                   Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more.\n    t            : float\n                   Time in seconds.\n\n    Returns\n    -------\n    field        : complex\n                   A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)).\n    \"\"\"\n    field = amplitude*np.exp(1j*(-w*t+opd*k+phase))/opd**2\n    return field\n</code></pre>"},{"location":"odak/wave/#odak.wave.fraunhofer","title":"<code>fraunhofer(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate Fraunhofer based beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def fraunhofer(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate Fraunhofer based beam propagation.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    nv, nu = field.shape\n    l = nu*dx\n    l2 = wavelength*distance/dx\n    dx2 = wavelength*distance/l\n    fx = np.linspace(-l2/2., l2/2., nu)\n    fy = np.linspace(-l2/2., l2/2., nv)\n    FX, FY = np.meshgrid(fx, fy)\n    FZ = FX**2+FY**2\n    c = np.exp(1j*k*distance)/(1j*wavelength*distance) * \\\n        np.exp(1j*k/(2*distance)*FZ)\n    result = c*np.fft.ifftshift(np.fft.fft2(np.fft.fftshift(field)))*dx**2\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.fraunhofer_equal_size_adjust","title":"<code>fraunhofer_equal_size_adjust(field, distance, dx, wavelength)</code>","text":"<p>A definition to match the physical size of the original field with the propagated field.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def fraunhofer_equal_size_adjust(field, distance, dx, wavelength):\n    \"\"\"\n    A definition to match the physical size of the original field with the propagated field.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    new_field        : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    nv, nu = field.shape\n    l1 = nu*dx\n    l2 = wavelength*distance/dx\n    m = l1/l2\n    px = int(m*nu)\n    py = int(m*nv)\n    nx = int(field.shape[0]/2-px/2)\n    ny = int(field.shape[1]/2-py/2)\n    new_field = np.copy(field[nx:nx+px, ny:ny+py])\n    return new_field\n</code></pre>"},{"location":"odak/wave/#odak.wave.fraunhofer_inverse","title":"<code>fraunhofer_inverse(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate Inverse Fraunhofer based beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def fraunhofer_inverse(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate Inverse Fraunhofer based beam propagation.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    distance = np.abs(distance)\n    nv, nu = field.shape\n    l = nu*dx\n    l2 = wavelength*distance/dx\n    dx2 = wavelength*distance/l\n    fx = np.linspace(-l2/2., l2/2., nu)\n    fy = np.linspace(-l2/2., l2/2., nv)\n    FX, FY = np.meshgrid(fx, fy)\n    FZ = FX**2+FY**2\n    c = np.exp(1j*k*distance)/(1j*wavelength*distance) * \\\n        np.exp(1j*k/(2*distance)*FZ)\n    result = np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(field/dx**2/c)))\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.generate_complex_field","title":"<code>generate_complex_field(amplitude, phase)</code>","text":"<p>Definition to generate a complex field with a given amplitude and phase.</p> <p>Parameters:</p> <ul> <li> <code>amplitude</code>           \u2013            <pre><code>            Amplitude of the field.\n</code></pre> </li> <li> <code>phase</code>           \u2013            <pre><code>            Phase of the field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field</code> (              <code>ndarray</code> )          \u2013            <p>Complex field.</p> </li> </ul> Source code in <code>odak/wave/__init__.py</code> <pre><code>def generate_complex_field(amplitude, phase):\n    \"\"\"\n    Definition to generate a complex field with a given amplitude and phase.\n\n    Parameters\n    ----------\n    amplitude         : ndarray\n                        Amplitude of the field.\n    phase             : ndarray\n                        Phase of the field.\n\n    Returns\n    -------\n    field             : ndarray\n                        Complex field.\n    \"\"\"\n    field = amplitude*np.cos(phase)+1j*amplitude*np.sin(phase)\n    return field\n</code></pre>"},{"location":"odak/wave/#odak.wave.gerchberg_saxton","title":"<code>gerchberg_saxton(field, n_iterations, distance, dx, wavelength, slm_range=6.28, propagation_type='IR Fresnel', initial_phase=None)</code>","text":"<p>Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>slm_range</code>           \u2013            <pre><code>           Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more.\n</code></pre> </li> <li> <code>propagation_type</code>               (<code>str</code>, default:                   <code>'IR Fresnel'</code> )           \u2013            <pre><code>           Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer).\n</code></pre> </li> <li> <code>initial_phase</code>           \u2013            <pre><code>           Phase to be added to the initial value.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>hologram</code> (              <code>complex</code> )          \u2013            <p>Calculated complex hologram.</p> </li> <li> <code>reconstruction</code> (              <code>complex</code> )          \u2013            <p>Calculated reconstruction using calculated hologram.</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def gerchberg_saxton(field, n_iterations, distance, dx, wavelength, slm_range=6.28, propagation_type='IR Fresnel', initial_phase=None):\n    \"\"\"\n    Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246.\n\n    Parameters\n    ----------\n    field            : np.complex64\n                       Complex field (MxN).\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    slm_range        : float\n                       Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more.\n    propagation_type : str\n                       Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer).\n    initial_phase    : np.complex64\n                       Phase to be added to the initial value.\n\n    Returns\n    -------\n    hologram         : np.complex\n                       Calculated complex hologram.\n    reconstruction   : np.complex\n                       Calculated reconstruction using calculated hologram. \n    \"\"\"\n    k = wavenumber(wavelength)\n    target = calculate_amplitude(field)\n    hologram = generate_complex_field(np.ones(field.shape), 0)\n    hologram = zero_pad(hologram)\n    if type(initial_phase) == type(None):\n        hologram = add_random_phase(hologram)\n    else:\n        initial_phase = zero_pad(initial_phase)\n        hologram = add_phase(hologram, initial_phase)\n    center = [int(hologram.shape[0]/2.), int(hologram.shape[1]/2.)]\n    orig_shape = [int(field.shape[0]/2.), int(field.shape[1]/2.)]\n    for i in tqdm(range(n_iterations), leave=False):\n        reconstruction = propagate_beam(\n            hologram, k, distance, dx, wavelength, propagation_type)\n        new_target = calculate_amplitude(reconstruction)\n        new_target[\n            center[0]-orig_shape[0]:center[0]+orig_shape[0],\n            center[1]-orig_shape[1]:center[1]+orig_shape[1]\n        ] = target\n        reconstruction = generate_complex_field(\n            new_target, calculate_phase(reconstruction))\n        hologram = propagate_beam(\n            reconstruction, k, -distance, dx, wavelength, propagation_type)\n        hologram = generate_complex_field(1, calculate_phase(hologram))\n        hologram = hologram[\n            center[0]-orig_shape[0]:center[0]+orig_shape[0],\n            center[1]-orig_shape[1]:center[1]+orig_shape[1],\n        ]\n        hologram = zero_pad(hologram)\n    reconstruction = propagate_beam(\n        hologram, k, distance, dx, wavelength, propagation_type)\n    hologram = hologram[\n        center[0]-orig_shape[0]:center[0]+orig_shape[0],\n        center[1]-orig_shape[1]:center[1]+orig_shape[1]\n    ]\n    reconstruction = reconstruction[\n        center[0]-orig_shape[0]:center[0]+orig_shape[0],\n        center[1]-orig_shape[1]:center[1]+orig_shape[1]\n    ]\n    return hologram, reconstruction\n</code></pre>"},{"location":"odak/wave/#odak.wave.gerchberg_saxton_3d","title":"<code>gerchberg_saxton_3d(fields, n_iterations, distances, dx, wavelength, slm_range=6.28, propagation_type='IR Fresnel', initial_phase=None, target_type='no constraint', coefficients=None)</code>","text":"<p>Definition to compute a multi plane hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Zhou, Pengcheng, et al. \"30.4: Multi\u2010plane holographic display with a uniform 3D Gerchberg\u2010Saxton algorithm.\" SID Symposium Digest of Technical Papers. Vol. 46. No. 1. 2015.</p> <p>Parameters:</p> <ul> <li> <code>fields</code>           \u2013            <pre><code>           Complex fields (MxN).\n</code></pre> </li> <li> <code>distances</code>           \u2013            <pre><code>           Propagation distances.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>slm_range</code>           \u2013            <pre><code>           Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more.\n</code></pre> </li> <li> <code>propagation_type</code>               (<code>str</code>, default:                   <code>'IR Fresnel'</code> )           \u2013            <pre><code>           Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer).\n</code></pre> </li> <li> <code>initial_phase</code>           \u2013            <pre><code>           Phase to be added to the initial value.\n</code></pre> </li> <li> <code>target_type</code>           \u2013            <pre><code>           Target type. `No constraint` targets the input target as is. `Double constraint` follows the idea in this paper, which claims to suppress speckle: Chang, Chenliang, et al. \"Speckle-suppressed phase-only holographic three-dimensional display based on double-constraint Gerchberg\u2013Saxton algorithm.\" Applied optics 54.23 (2015): 6994-7001.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>hologram</code> (              <code>complex</code> )          \u2013            <p>Calculated complex hologram.</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def gerchberg_saxton_3d(fields, n_iterations, distances, dx, wavelength, slm_range=6.28, propagation_type='IR Fresnel', initial_phase=None, target_type='no constraint', coefficients=None):\n    \"\"\"\n    Definition to compute a multi plane hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Zhou, Pengcheng, et al. \"30.4: Multi\u2010plane holographic display with a uniform 3D Gerchberg\u2010Saxton algorithm.\" SID Symposium Digest of Technical Papers. Vol. 46. No. 1. 2015.\n\n    Parameters\n    ----------\n    fields           : np.complex64\n                       Complex fields (MxN).\n    distances        : list\n                       Propagation distances.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    slm_range        : float\n                       Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more.\n    propagation_type : str\n                       Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer).\n    initial_phase    : np.complex64\n                       Phase to be added to the initial value.\n    target_type      : str\n                       Target type. `No constraint` targets the input target as is. `Double constraint` follows the idea in this paper, which claims to suppress speckle: Chang, Chenliang, et al. \"Speckle-suppressed phase-only holographic three-dimensional display based on double-constraint Gerchberg\u2013Saxton algorithm.\" Applied optics 54.23 (2015): 6994-7001. \n\n    Returns\n    -------\n    hologram         : np.complex\n                       Calculated complex hologram.\n    \"\"\"\n    k = wavenumber(wavelength)\n    targets = calculate_amplitude(np.asarray(fields)).astype(np.float64)\n    hologram = generate_complex_field(np.ones(targets[0].shape), 0)\n    hologram = zero_pad(hologram)\n    if type(initial_phase) == type(None):\n        hologram = add_random_phase(hologram)\n    else:\n        initial_phase = zero_pad(initial_phase)\n        hologram = add_phase(hologram, initial_phase)\n    center = [int(hologram.shape[0]/2.), int(hologram.shape[1]/2.)]\n    orig_shape = [int(fields[0].shape[0]/2.), int(fields[0].shape[1]/2.)]\n    holograms = np.zeros(\n        (len(distances), hologram.shape[0], hologram.shape[1]), dtype=np.complex64)\n    for i in tqdm(range(n_iterations), leave=False):\n        for distance_id in tqdm(range(len(distances)), leave=False):\n            distance = distances[distance_id]\n            reconstruction = propagate_beam(\n                hologram, k, distance, dx, wavelength, propagation_type)\n            if target_type == 'double constraint':\n                if type(coefficients) == type(None):\n                    raise Exception(\n                        \"Provide coeeficients of alpha,beta and gamma for double constraint.\")\n                alpha = coefficients[0]\n                beta = coefficients[1]\n                gamma = coefficients[2]\n                target_current = 2*alpha * \\\n                    np.copy(targets[distance_id])-beta * \\\n                    calculate_amplitude(reconstruction)\n                target_current[target_current == 0] = gamma * \\\n                    np.abs(reconstruction[target_current == 0])\n            elif target_type == 'no constraint':\n                target_current = np.abs(targets[distance_id])\n            new_target = calculate_amplitude(reconstruction)\n            new_target[\n                center[0]-orig_shape[0]:center[0]+orig_shape[0],\n                center[1]-orig_shape[1]:center[1]+orig_shape[1]\n            ] = target_current\n            reconstruction = generate_complex_field(\n                new_target, calculate_phase(reconstruction))\n            hologram_layer = propagate_beam(\n                reconstruction, k, -distance, dx, wavelength, propagation_type)\n            hologram_layer = generate_complex_field(\n                1., calculate_phase(hologram_layer))\n            hologram_layer = hologram_layer[\n                center[0]-orig_shape[0]:center[0]+orig_shape[0],\n                center[1]-orig_shape[1]:center[1]+orig_shape[1]\n            ]\n            hologram_layer = zero_pad(hologram_layer)\n            holograms[distance_id] = hologram_layer\n        hologram = np.sum(holograms, axis=0)\n    hologram = hologram[\n        center[0]-orig_shape[0]:center[0]+orig_shape[0],\n        center[1]-orig_shape[1]:center[1]+orig_shape[1]\n    ]\n    return hologram\n</code></pre>"},{"location":"odak/wave/#odak.wave.impulse_response_fresnel","title":"<code>impulse_response_fresnel(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate impulse response based Fresnel approximation for beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def impulse_response_fresnel(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate impulse response based Fresnel approximation for beam propagation.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n\n    \"\"\"\n    nv, nu = field.shape\n    x = np.linspace(-nu / 2 * dx, nu / 2 * dx, nu)\n    y = np.linspace(-nv / 2 * dx, nv / 2 * dx, nv)\n    X, Y = np.meshgrid(x, y)\n    h = 1. / (1j * wavelength * distance) * np.exp(1j * k / (2 * distance) * (X ** 2 + Y ** 2))\n    H = np.fft.fft2(np.fft.fftshift(h))*dx**2\n    U1 = np.fft.fft2(np.fft.fftshift(field))\n    U2 = H * U1\n    result = np.fft.ifftshift(np.fft.ifft2(U2)) /(dx**2)\n\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.linear_grating","title":"<code>linear_grating(nx, ny, every=2, add=3.14, axis='x')</code>","text":"<p>A definition to generate a linear grating.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>     Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>     Size of the output along Y.\n</code></pre> </li> <li> <code>every</code>           \u2013            <pre><code>     Add the add value at every given number.\n</code></pre> </li> <li> <code>add</code>           \u2013            <pre><code>     Angle to be added.\n</code></pre> </li> <li> <code>axis</code>           \u2013            <pre><code>     Axis eiter X,Y or both.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field</code> (              <code>ndarray</code> )          \u2013            <p>Linear grating term.</p> </li> </ul> Source code in <code>odak/wave/lens.py</code> <pre><code>def linear_grating(nx, ny, every=2, add=3.14, axis='x'):\n    \"\"\"\n    A definition to generate a linear grating.\n\n    Parameters\n    ----------\n    nx         : int\n                 Size of the output along X.\n    ny         : int\n                 Size of the output along Y.\n    every      : int\n                 Add the add value at every given number.\n    add        : float\n                 Angle to be added.\n    axis       : string\n                 Axis eiter X,Y or both.\n\n    Returns\n    -------\n    field      : ndarray\n                 Linear grating term.\n    \"\"\"\n    grating = np.zeros((nx, ny), dtype=np.complex64)\n    if axis == 'x':\n        grating[::every, :] = np.exp(1j*add)\n    if axis == 'y':\n        grating[:, ::every] = np.exp(1j*add)\n    if axis == 'xy':\n        checker = np.indices((nx, ny)).sum(axis=0) % every\n        checker += 1\n        checker = checker % 2\n        grating = np.exp(1j*checker*add)\n    return grating\n</code></pre>"},{"location":"odak/wave/#odak.wave.nufft2","title":"<code>nufft2(field, fx, fy, size=None, sign=1, eps=10 ** -12)</code>","text":"<p>A definition to take 2D Non-Uniform Fast Fourier Transform (NUFFT).</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field.\n</code></pre> </li> <li> <code>fx</code>           \u2013            <pre><code>      Frequencies along x axis.\n</code></pre> </li> <li> <code>fy</code>           \u2013            <pre><code>      Frequencies along y axis.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Size.\n</code></pre> </li> <li> <code>sign</code>           \u2013            <pre><code>      Sign of the exponential used in NUFFT kernel.\n</code></pre> </li> <li> <code>eps</code>           \u2013            <pre><code>      Accuracy of NUFFT.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>Inverse NUFFT of the input field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def nufft2(field, fx, fy, size=None, sign=1, eps=10**(-12)):\n    \"\"\"\n    A definition to take 2D Non-Uniform Fast Fourier Transform (NUFFT).\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field.\n    fx          : ndarray\n                  Frequencies along x axis.\n    fy          : ndarray\n                  Frequencies along y axis.\n    size        : list\n                  Size.\n    sign        : float\n                  Sign of the exponential used in NUFFT kernel.\n    eps         : float\n                  Accuracy of NUFFT.\n\n    Returns\n    ----------\n    result      : ndarray\n                  Inverse NUFFT of the input field.\n    \"\"\"\n    try:\n        import finufft\n    except:\n        print('odak.tools.nufft2 requires finufft to be installed: pip install finufft')\n    image = np.copy(field).astype(np.complex128)\n    result = finufft.nufft2d2(\n        fx.flatten(), fy.flatten(), image, eps=eps, isign=sign)\n    if type(size) == type(None):\n        result = result.reshape(field.shape)\n    else:\n        result = result.reshape(size)\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.nuifft2","title":"<code>nuifft2(field, fx, fy, size=None, sign=1, eps=10 ** -12)</code>","text":"<p>A definition to take 2D Adjoint Non-Uniform Fast Fourier Transform (NUFFT).</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>      Input field.\n</code></pre> </li> <li> <code>fx</code>           \u2013            <pre><code>      Frequencies along x axis.\n</code></pre> </li> <li> <code>fy</code>           \u2013            <pre><code>      Frequencies along y axis.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>      Shape of the NUFFT calculated for an input field.\n</code></pre> </li> <li> <code>sign</code>           \u2013            <pre><code>      Sign of the exponential used in NUFFT kernel.\n</code></pre> </li> <li> <code>eps</code>           \u2013            <pre><code>      Accuracy of NUFFT.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>ndarray</code> )          \u2013            <p>NUFFT of the input field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def nuifft2(field, fx, fy, size=None, sign=1, eps=10**(-12)):\n    \"\"\"\n    A definition to take 2D Adjoint Non-Uniform Fast Fourier Transform (NUFFT).\n\n    Parameters\n    ----------\n    field       : ndarray\n                  Input field.\n    fx          : ndarray\n                  Frequencies along x axis.\n    fy          : ndarray\n                  Frequencies along y axis.\n    size        : list or ndarray\n                  Shape of the NUFFT calculated for an input field.\n    sign        : float\n                  Sign of the exponential used in NUFFT kernel.\n    eps         : float\n                  Accuracy of NUFFT.\n\n    Returns\n    ----------\n    result      : ndarray\n                  NUFFT of the input field.\n    \"\"\"\n    try:\n        import finufft\n    except:\n        print('odak.tools.nuifft2 requires finufft to be installed: pip install finufft')\n    image = np.copy(field).astype(np.complex128)\n    if type(size) == type(None):\n        result = finufft.nufft2d1(\n            fx.flatten(),\n            fy.flatten(),\n            image.flatten(),\n            image.shape,\n            eps=eps,\n            isign=sign\n        )\n    else:\n        result = finufft.nufft2d1(\n            fx.flatten(),\n            fy.flatten(),\n            image.flatten(),\n            (size[0], size[1]),\n            eps=eps,\n            isign=sign\n        )\n    result = np.asarray(result)\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.prism_phase_function","title":"<code>prism_phase_function(nx, ny, k, angle, dx=0.001, axis='x')</code>","text":"<p>A definition to generate 2D phase function that represents a prism. See Goodman's Introduction to Fourier Optics book for more.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>     Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>     Size of the output along Y.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>     See odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>angle</code>           \u2013            <pre><code>     Tilt angle of the prism in degrees.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>     Pixel pitch.\n</code></pre> </li> <li> <code>axis</code>           \u2013            <pre><code>     Axis of the prism.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>prism</code> (              <code>ndarray</code> )          \u2013            <p>Generated phase function for a prism.</p> </li> </ul> Source code in <code>odak/wave/lens.py</code> <pre><code>def prism_phase_function(nx, ny, k, angle, dx=0.001, axis='x'):\n    \"\"\"\n    A definition to generate 2D phase function that represents a prism. See Goodman's Introduction to Fourier Optics book for more.\n\n    Parameters\n    ----------\n    nx         : int\n                 Size of the output along X.\n    ny         : int\n                 Size of the output along Y.\n    k          : odak.wave.wavenumber\n                 See odak.wave.wavenumber for more.\n    angle      : float\n                 Tilt angle of the prism in degrees.\n    dx         : float\n                 Pixel pitch.\n    axis       : str\n                 Axis of the prism.\n\n    Returns\n    -------\n    prism      : ndarray\n                 Generated phase function for a prism.\n    \"\"\"\n    angle = np.radians(angle)\n    size = [ny, nx]\n    x = np.linspace(-size[0]*dx/2, size[0]*dx/2, size[0])\n    y = np.linspace(-size[1]*dx/2, size[1]*dx/2, size[1])\n    X, Y = np.meshgrid(x, y)\n    if axis == 'y':\n        prism = np.exp(-1j*k*np.sin(angle)*Y)\n    elif axis == 'x':\n        prism = np.exp(-1j*k*np.sin(angle)*X)\n    return prism\n</code></pre>"},{"location":"odak/wave/#odak.wave.produce_phase_only_slm_pattern","title":"<code>produce_phase_only_slm_pattern(hologram, slm_range, filename=None, bits=8, default_range=6.28, illumination=None)</code>","text":"<p>Definition for producing a pattern for a phase only Spatial Light Modulator (SLM) using a given field.</p> <p>Parameters:</p> <ul> <li> <code>hologram</code>           \u2013            <pre><code>             Input holographic field.\n</code></pre> </li> <li> <code>slm_range</code>           \u2013            <pre><code>             Range of the phase only SLM in radians for a working wavelength (i.e. two pi). See odak.wave.adjust_phase_only_slm_range() for more.\n</code></pre> </li> <li> <code>filename</code>           \u2013            <pre><code>             Optional variable, if provided the patterns will be save to given location.\n</code></pre> </li> <li> <code>bits</code>           \u2013            <pre><code>             Quantization bits.\n</code></pre> </li> <li> <code>default_range</code>           \u2013            <pre><code>             Default range of phase only SLM.\n</code></pre> </li> <li> <code>illumination</code>           \u2013            <pre><code>             Spatial illumination distribution.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>pattern</code> (              <code>complex64</code> )          \u2013            <p>Adjusted phase only pattern.</p> </li> <li> <code>hologram_digital</code> (              <code>int</code> )          \u2013            <p>Digital representation of the hologram.</p> </li> </ul> Source code in <code>odak/wave/__init__.py</code> <pre><code>def produce_phase_only_slm_pattern(hologram, slm_range, filename=None, bits=8, default_range=6.28, illumination=None):\n    \"\"\"\n    Definition for producing a pattern for a phase only Spatial Light Modulator (SLM) using a given field.\n\n    Parameters\n    ----------\n    hologram           : np.complex64\n                         Input holographic field.\n    slm_range          : float\n                         Range of the phase only SLM in radians for a working wavelength (i.e. two pi). See odak.wave.adjust_phase_only_slm_range() for more.\n    filename           : str\n                         Optional variable, if provided the patterns will be save to given location.\n    bits               : int\n                         Quantization bits.\n    default_range      : float \n                         Default range of phase only SLM.\n    illumination       : np.ndarray\n                         Spatial illumination distribution.\n\n    Returns\n    -------\n    pattern            : np.complex64\n                         Adjusted phase only pattern.\n    hologram_digital   : np.int\n                         Digital representation of the hologram.\n    \"\"\"\n    #hologram_phase   = calculate_phase(hologram) % default_range\n    hologram_phase = calculate_phase(hologram)\n    hologram_phase = hologram_phase % slm_range\n    hologram_phase /= slm_range\n    hologram_phase *= 2**bits\n    hologram_phase = hologram_phase.astype(np.int32)\n    hologram_digital = np.copy(hologram_phase)\n    if type(filename) != type(None):\n        save_image(\n            filename,\n            hologram_phase,\n            cmin=0,\n            cmax=2**bits\n        )\n    hologram_phase = hologram_phase.astype(np.float64)\n    hologram_phase *= slm_range/2**bits\n    if type(illumination) == type(None):\n        A = 1.\n    else:\n        A = illumination\n    return A*np.cos(hologram_phase)+A*1j*np.sin(hologram_phase), hologram_digital\n</code></pre>"},{"location":"odak/wave/#odak.wave.propagate_beam","title":"<code>propagate_beam(field, k, distance, dx, wavelength, propagation_type='IR Fresnel')</code>","text":"<p>Definitions for Fresnel Impulse Response (IR), Angular Spectrum (AS), Bandlimited Angular Spectrum (BAS), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. For more on Bandlimited Fresnel impulse response also known as Bandlimited Angular Spectrum method see \"Band-limited Angular Spectrum Method for Numerical Simulation of Free-Space Propagation in Far and Near Fields\".</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>propagation_type</code>               (<code>str</code>, default:                   <code>'IR Fresnel'</code> )           \u2013            <pre><code>           Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def propagate_beam(field, k, distance, dx, wavelength, propagation_type='IR Fresnel'):\n    \"\"\"\n    Definitions for Fresnel Impulse Response (IR), Angular Spectrum (AS), Bandlimited Angular Spectrum (BAS), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. For more on Bandlimited Fresnel impulse response also known as Bandlimited Angular Spectrum method see \"Band-limited Angular Spectrum Method for Numerical Simulation of Free-Space Propagation in Far and Near Fields\".\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    propagation_type : str\n                       Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer).\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    if propagation_type == 'Rayleigh-Sommerfeld':\n        result = rayleigh_sommerfeld(field, k, distance, dx, wavelength)\n    elif propagation_type == 'Angular Spectrum':\n        result = angular_spectrum(field, k, distance, dx, wavelength)\n    elif propagation_type == 'Impulse Response Fresnel':\n        result = impulse_response_fresnel(field, k, distance, dx, wavelength)\n    elif propagation_type == 'Bandlimited Angular Spectrum':\n        result = band_limited_angular_spectrum(\n            field, k, distance, dx, wavelength)\n    elif propagation_type == 'Bandextended Angular Spectrum':\n        result = band_extended_angular_spectrum(\n            field, k, distance, dx, wavelength)\n    elif propagation_type == 'Adaptive Sampling Angular Spectrum':\n        result = adaptive_sampling_angular_spectrum(\n            field, k, distance, dx, wavelength)\n    elif propagation_type == 'Transfer Function Fresnel':\n        result = transfer_function_fresnel(field, k, distance, dx, wavelength)\n    elif propagation_type == 'Fraunhofer':\n        result = fraunhofer(field, k, distance, dx, wavelength)\n    elif propagation_type == 'Fraunhofer Inverse':\n        result = fraunhofer_inverse(field, k, distance, dx, wavelength)\n    else:\n        raise Exception(\"Unknown propagation type selected.\")\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.propagate_field","title":"<code>propagate_field(points0, points1, field0, wave_number, direction=1)</code>","text":"<p>Definition to propagate a field from points to an another points in space: propagate a given array of spherical sources to given set of points in space.</p> <p>Parameters:</p> <ul> <li> <code>points0</code>           \u2013            <pre><code>        Start points (i.e. odak.tools.grid_sample).\n</code></pre> </li> <li> <code>points1</code>           \u2013            <pre><code>        End points (ie. odak.tools.grid_sample).\n</code></pre> </li> <li> <code>field0</code>           \u2013            <pre><code>        Field for given starting points.\n</code></pre> </li> <li> <code>wave_number</code>           \u2013            <pre><code>        Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>direction</code>           \u2013            <pre><code>        For propagating in forward direction set as 1, otherwise -1.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field1</code> (              <code>ndarray</code> )          \u2013            <p>Field for given end points.</p> </li> </ul> Source code in <code>odak/wave/vector.py</code> <pre><code>def propagate_field(points0, points1, field0, wave_number, direction=1):\n    \"\"\"\n    Definition to propagate a field from points to an another points in space: propagate a given array of spherical sources to given set of points in space.\n\n    Parameters\n    ----------\n    points0       : ndarray\n                    Start points (i.e. odak.tools.grid_sample).\n    points1       : ndarray\n                    End points (ie. odak.tools.grid_sample).\n    field0        : ndarray\n                    Field for given starting points.\n    wave_number   : float\n                    Wave number of a wave, see odak.wave.wavenumber for more.\n    direction     : float\n                    For propagating in forward direction set as 1, otherwise -1.\n\n    Returns\n    -------\n    field1        : ndarray\n                    Field for given end points.\n    \"\"\"\n    field1 = np.zeros(points1.shape[0], dtype=np.complex64)\n    for point_id in range(points0.shape[0]):\n        point = points0[point_id]\n        distances = distance_between_two_points(\n            point,\n            points1\n        )\n        field1 += electric_field_per_plane_wave(\n            calculate_amplitude(field0[point_id]),\n            distances*direction,\n            wave_number,\n            phase=calculate_phase(field0[point_id])\n        )\n    return field1\n</code></pre>"},{"location":"odak/wave/#odak.wave.propagate_plane_waves","title":"<code>propagate_plane_waves(field, opd, k, w=0, t=0)</code>","text":"<p>Definition to propagate a field representing a plane wave at a particular distance and time.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>       Complex field.\n</code></pre> </li> <li> <code>opd</code>           \u2013            <pre><code>       Optical path difference in mm.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>       Wave number of a wave, see odak.wave.parameters.wavenumber for more.\n</code></pre> </li> <li> <code>w</code>           \u2013            <pre><code>       Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more.\n</code></pre> </li> <li> <code>t</code>           \u2013            <pre><code>       Time in seconds.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>complex</code> )          \u2013            <p>A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)).</p> </li> </ul> Source code in <code>odak/wave/vector.py</code> <pre><code>def propagate_plane_waves(field, opd, k, w=0, t=0):\n    \"\"\"\n    Definition to propagate a field representing a plane wave at a particular distance and time.\n\n    Parameters\n    ----------\n    field        : complex\n                   Complex field.\n    opd          : float\n                   Optical path difference in mm.\n    k            : float\n                   Wave number of a wave, see odak.wave.parameters.wavenumber for more.\n    w            : float\n                   Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more.\n    t            : float\n                   Time in seconds.\n\n    Returns\n    -------\n    new_field     : complex\n                    A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)).\n    \"\"\"\n    new_field = field*np.exp(1j*(-w*t+opd*k))/opd**2\n    return new_field\n</code></pre>"},{"location":"odak/wave/#odak.wave.quadratic_phase_function","title":"<code>quadratic_phase_function(nx, ny, k, focal=0.4, dx=0.001, offset=[0, 0])</code>","text":"<p>A definition to generate 2D quadratic phase function, which is typically use to represent lenses.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>     Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>     Size of the output along Y.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>     See odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>focal</code>           \u2013            <pre><code>     Focal length of the quadratic phase function.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>     Pixel pitch.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>     Deviation from the center along X and Y axes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>function</code> (              <code>ndarray</code> )          \u2013            <p>Generated quadratic phase function.</p> </li> </ul> Source code in <code>odak/wave/lens.py</code> <pre><code>def quadratic_phase_function(nx, ny, k, focal=0.4, dx=0.001, offset=[0, 0]):\n    \"\"\" \n    A definition to generate 2D quadratic phase function, which is typically use to represent lenses.\n\n    Parameters\n    ----------\n    nx         : int\n                 Size of the output along X.\n    ny         : int\n                 Size of the output along Y.\n    k          : odak.wave.wavenumber\n                 See odak.wave.wavenumber for more.\n    focal      : float\n                 Focal length of the quadratic phase function.\n    dx         : float\n                 Pixel pitch.\n    offset     : list\n                 Deviation from the center along X and Y axes.\n\n    Returns\n    -------\n    function   : ndarray\n                 Generated quadratic phase function.\n    \"\"\"\n    size = [nx, ny]\n    x = np.linspace(-size[0]*dx/2, size[0]*dx/2, size[0])-offset[1]*dx\n    y = np.linspace(-size[1]*dx/2, size[1]*dx/2, size[1])-offset[0]*dx\n    X, Y = np.meshgrid(x, y)\n    Z = X**2+Y**2\n    qwf = np.exp(1j*k*0.5*np.sin(Z/focal))\n    return qwf\n</code></pre>"},{"location":"odak/wave/#odak.wave.rayleigh_resolution","title":"<code>rayleigh_resolution(diameter, focal=None, wavelength=0.0005)</code>","text":"<p>Definition to calculate rayleigh resolution limit of a lens with a certain focal length and an aperture. Lens is assumed to be focusing a plane wave at a focal distance.</p> Parameter <p>diameter    : float               Diameter of a lens. focal       : float               Focal length of a lens, when focal length is provided, spatial resolution is provided at the focal plane. When focal length isn't provided angular resolution is provided. wavelength  : float               Wavelength of light.</p> <p>Returns:</p> <ul> <li> <code>resolution</code> (              <code>float</code> )          \u2013            <p>Resolvable angular or spatial spot size, see focal in parameters to know what to expect.</p> </li> </ul> Source code in <code>odak/wave/__init__.py</code> <pre><code>def rayleigh_resolution(diameter, focal=None, wavelength=0.0005):\n    \"\"\"\n    Definition to calculate rayleigh resolution limit of a lens with a certain focal length and an aperture. Lens is assumed to be focusing a plane wave at a focal distance.\n\n    Parameter\n    ---------\n    diameter    : float\n                  Diameter of a lens.\n    focal       : float\n                  Focal length of a lens, when focal length is provided, spatial resolution is provided at the focal plane. When focal length isn't provided angular resolution is provided.\n    wavelength  : float\n                  Wavelength of light.\n\n    Returns\n    --------\n    resolution  : float\n                  Resolvable angular or spatial spot size, see focal in parameters to know what to expect.\n\n    \"\"\"\n    resolution = 1.22*wavelength/diameter\n    if type(focal) != type(None):\n        resolution *= focal\n    return resolution\n</code></pre>"},{"location":"odak/wave/#odak.wave.rayleigh_sommerfeld","title":"<code>rayleigh_sommerfeld(field, k, distance, dx, wavelength)</code>","text":"<p>Definition to compute beam propagation using Rayleigh-Sommerfeld's diffraction formula (Huygens-Fresnel Principle). For more see Section 3.5.2 in Goodman, Joseph W. Introduction to Fourier optics. Roberts and Company Publishers, 2005.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def rayleigh_sommerfeld(field, k, distance, dx, wavelength):\n    \"\"\"\n    Definition to compute beam propagation using Rayleigh-Sommerfeld's diffraction formula (Huygens-Fresnel Principle). For more see Section 3.5.2 in Goodman, Joseph W. Introduction to Fourier optics. Roberts and Company Publishers, 2005.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    nv, nu = field.shape\n    x = np.linspace(-nv * dx / 2, nv * dx / 2, nv)\n    y = np.linspace(-nu * dx / 2, nu * dx / 2, nu)\n    X, Y = np.meshgrid(x, y)\n    Z = X ** 2 + Y ** 2\n    result = np.zeros(field.shape, dtype=np.complex64)\n    direction = int(distance/np.abs(distance))\n    for i in range(nu):\n        for j in range(nv):\n            if field[i, j] != 0:\n                r01 = np.sqrt(distance ** 2 + (X - X[i, j]) ** 2 + (Y - Y[i, j]) ** 2) * direction\n                cosnr01 = distance / r01\n                result += field[i, j] * np.exp(1j * k * r01) / r01 * cosnr01\n    result *= 1. / (1j * wavelength)\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.rotationspeed","title":"<code>rotationspeed(wavelength, c=3 * 10 ** 11)</code>","text":"<p>Definition for calculating rotation speed of a wave (w in A*e^(j(wt+phi))).</p> <p>Parameters:</p> <ul> <li> <code>wavelength</code>           \u2013            <pre><code>       Wavelength of a wave in mm.\n</code></pre> </li> <li> <code>c</code>           \u2013            <pre><code>       Speed of wave in mm/seconds. Default is the speed of light in the void!\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>w</code> (              <code>float</code> )          \u2013            <p>Rotation speed.</p> </li> </ul> Source code in <code>odak/wave/__init__.py</code> <pre><code>def rotationspeed(wavelength, c=3*10**11):\n    \"\"\"\n    Definition for calculating rotation speed of a wave (w in A*e^(j(wt+phi))).\n\n    Parameters\n    ----------\n    wavelength   : float\n                   Wavelength of a wave in mm.\n    c            : float\n                   Speed of wave in mm/seconds. Default is the speed of light in the void!\n\n    Returns\n    -------\n    w            : float\n                   Rotation speed.\n\n    \"\"\"\n    f = c*wavelength\n    w = 2*np.pi*f\n    return w\n</code></pre>"},{"location":"odak/wave/#odak.wave.set_amplitude","title":"<code>set_amplitude(field, amplitude)</code>","text":"<p>Definition to keep phase as is and change the amplitude of a given field.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>       Complex field.\n</code></pre> </li> <li> <code>amplitude</code>           \u2013            <pre><code>       Amplitudes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>complex64</code> )          \u2013            <p>Complex field.</p> </li> </ul> Source code in <code>odak/wave/__init__.py</code> <pre><code>def set_amplitude(field, amplitude):\n    \"\"\"\n    Definition to keep phase as is and change the amplitude of a given field.\n\n    Parameters\n    ----------\n    field        : np.complex64\n                   Complex field.\n    amplitude    : np.array or np.complex64\n                   Amplitudes.\n\n    Returns\n    -------\n    new_field    : np.complex64\n                   Complex field.\n    \"\"\"\n    amplitude = calculate_amplitude(amplitude)\n    phase = calculate_phase(field)\n    new_field = amplitude*np.cos(phase)+1j*amplitude*np.sin(phase)\n    return new_field\n</code></pre>"},{"location":"odak/wave/#odak.wave.transfer_function_fresnel","title":"<code>transfer_function_fresnel(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate convolution based Fresnel approximation for beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def transfer_function_fresnel(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate convolution based Fresnel approximation for beam propagation.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n\n    \"\"\"\n    nv, nu = field.shape\n    L = nu*dx\n    fx = np.linspace(-1. / 2. /dx, 1. /2. /dx, nu)\n    fy = np.linspace(-1. / 2. /dx, 1. /2. /dx, nv)\n    FX, FY = np.meshgrid(fx, fy)\n    H = np.exp(-1j * distance * (k - np.pi * wavelength * (FX**2 + FY**2) ))\n    U1 = np.fft.fft2(np.fft.fftshift(field)) * ((1/L)**2)\n    U2 = np.fft.fftshift(H)*U1\n    result = np.fft.ifftshift(np.fft.ifft2(U2)) / ((1/L)**2)\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.wavenumber","title":"<code>wavenumber(wavelength)</code>","text":"<p>Definition for calculating the wavenumber of a plane wave.</p> <p>Parameters:</p> <ul> <li> <code>wavelength</code>           \u2013            <pre><code>       Wavelength of a wave in mm.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>k</code> (              <code>float</code> )          \u2013            <p>Wave number for a given wavelength.</p> </li> </ul> Source code in <code>odak/wave/__init__.py</code> <pre><code>def wavenumber(wavelength):\n    \"\"\"\n    Definition for calculating the wavenumber of a plane wave.\n\n    Parameters\n    ----------\n    wavelength   : float\n                   Wavelength of a wave in mm.\n\n    Returns\n    -------\n    k            : float\n                   Wave number for a given wavelength.\n    \"\"\"\n    k = 2*np.pi/wavelength\n    return k\n</code></pre>"},{"location":"odak/wave/#odak.wave.zero_pad","title":"<code>zero_pad(field, size=None, method='center')</code>","text":"<p>Definition to zero pad a MxN array to 2Mx2N array.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>            Input field MxN array.\n</code></pre> </li> <li> <code>size</code>           \u2013            <pre><code>            Size to be zeropadded.\n</code></pre> </li> <li> <code>method</code>           \u2013            <pre><code>            Zeropad either by placing the content to center or to the left.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field_zero_padded</code> (              <code>ndarray</code> )          \u2013            <p>Zeropadded version of the input field.</p> </li> </ul> Source code in <code>odak/tools/matrix.py</code> <pre><code>def zero_pad(field, size=None, method='center'):\n    \"\"\"\n    Definition to zero pad a MxN array to 2Mx2N array.\n\n    Parameters\n    ----------\n    field             : ndarray\n                        Input field MxN array.\n    size              : list\n                        Size to be zeropadded.\n    method            : str\n                        Zeropad either by placing the content to center or to the left.\n\n    Returns\n    ----------\n    field_zero_padded : ndarray\n                        Zeropadded version of the input field.\n    \"\"\"\n    if type(size) == type(None):\n        hx = int(np.ceil(field.shape[0])/2)\n        hy = int(np.ceil(field.shape[1])/2)\n    else:\n        hx = int(np.ceil((size[0]-field.shape[0])/2))\n        hy = int(np.ceil((size[1]-field.shape[1])/2))\n    if method == 'center':\n        field_zero_padded = np.pad(\n            field, ([hx, hx], [hy, hy]), constant_values=(0, 0))\n    elif method == 'left aligned':\n        field_zero_padded = np.pad(\n            field, ([0, 2*hx], [0, 2*hy]), constant_values=(0, 0))\n    if type(size) != type(None):\n        field_zero_padded = field_zero_padded[0:size[0], 0:size[1]]\n    return field_zero_padded\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.adaptive_sampling_angular_spectrum","title":"<code>adaptive_sampling_angular_spectrum(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate adaptive sampling angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def adaptive_sampling_angular_spectrum(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate adaptive sampling angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Adaptive-sampling angular spectrum method with full utilization of space-bandwidth product.\" Optics Letters 45.16 (2020): 4416-4419.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    iflag = -1\n    eps = 10**(-12)\n    nv, nu = field.shape\n    l = nu*dx\n    x = np.linspace(-l/2, l/2, nu)\n    y = np.linspace(-l/2, l/2, nv)\n    X, Y = np.meshgrid(x, y)\n    fx = np.linspace(-1./2./dx, 1./2./dx, nu)\n    fy = np.linspace(-1./2./dx, 1./2./dx, nv)\n    FX, FY = np.meshgrid(fx, fy)\n    forig = 1./2./dx\n    fc2 = 1./2*(nu/wavelength/np.abs(distance))**0.5\n    ss = np.abs(fc2)/forig\n    zc = nu*dx**2/wavelength\n    K = nu/2/np.amax(np.abs(fx))\n    m = 2\n    nnu2 = m*nu\n    nnv2 = m*nv\n    fxn = np.linspace(-1./2./dx, 1./2./dx, nnu2)\n    fyn = np.linspace(-1./2./dx, 1./2./dx, nnv2)\n    if np.abs(distance) &gt; zc*2:\n        fxn = fxn*ss\n        fyn = fyn*ss\n    FXN, FYN = np.meshgrid(fxn, fyn)\n    Hn = np.exp(1j*k*distance*(1-(FXN*wavelength)**2-(FYN*wavelength)**2)**0.5)\n    FX = FXN/np.amax(FXN)*np.pi\n    FY = FYN/np.amax(FYN)*np.pi\n    t_2 = nufft2(field, FX*ss, FY*ss, size=[nnv2, nnu2], sign=iflag, eps=eps)\n    FX = FX/np.amax(FX)*np.pi\n    FY = FY/np.amax(FY)*np.pi\n    result = nuifft2(Hn*t_2, FX*ss, FY*ss, size=[nv, nu], sign=-iflag, eps=eps)\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.angular_spectrum","title":"<code>angular_spectrum(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate angular spectrum based beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def angular_spectrum(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate angular spectrum based beam propagation.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    nv, nu = field.shape\n    fx = np.linspace(-1. / 2. /dx, 1. /2. /dx, nu)\n    fy = np.linspace(-1. / 2. /dx, 1. /2. /dx, nv)\n    FX, FY = np.meshgrid(fx, fy)\n    H = np.exp(1j * k * distance * (1 - (FX * wavelength) ** 2 - (FY * wavelength) ** 2) ** 0.5)\n    U1 = np.fft.fftshift(np.fft.fft2(field))\n    U2 = H * U1\n    result = np.fft.ifft2(np.fft.ifftshift(U2))\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.band_extended_angular_spectrum","title":"<code>band_extended_angular_spectrum(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate bandextended angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics Letters 45.6 (2020): 1543-1546.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def band_extended_angular_spectrum(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate bandextended angular spectrum based beam propagation. For more Zhang, Wenhui, Hao Zhang, and Guofan Jin. \"Band-extended angular spectrum method for accurate diffraction calculation in a wide propagation range.\" Optics Letters 45.6 (2020): 1543-1546.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    iflag = -1\n    eps = 10**(-12)\n    nv, nu = field.shape\n    l = nu*dx\n    x = np.linspace(-l/2, l/2, nu)\n    y = np.linspace(-l/2, l/2, nv)\n    X, Y = np.meshgrid(x, y)\n    Z = X**2+Y**2\n    fx = np.linspace(-1./2./dx, 1./2./dx, nu)\n    fy = np.linspace(-1./2./dx, 1./2./dx, nv)\n    FX, FY = np.meshgrid(fx, fy)\n    K = nu/2/np.amax(fx)\n    fcn = 1./2*(nu/wavelength/np.abs(distance))**0.5\n    ss = np.abs(fcn)/np.amax(np.abs(fx))\n    zc = nu*dx**2/wavelength\n    if np.abs(distance) &lt; zc:\n        fxn = fx\n        fyn = fy\n    else:\n        fxn = fx*ss\n        fyn = fy*ss\n    FXN, FYN = np.meshgrid(fxn, fyn)\n    Hn = np.exp(1j*k*distance*(1-(FXN*wavelength)**2-(FYN*wavelength)**2)**0.5)\n    X = X/np.amax(X)*np.pi\n    Y = Y/np.amax(Y)*np.pi\n    t_asmNUFT = nufft2(field, X*ss, Y*ss, sign=iflag, eps=eps)\n    result = nuifft2(Hn*t_asmNUFT, X*ss, Y*ss, sign=-iflag, eps=eps)\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.band_limited_angular_spectrum","title":"<code>band_limited_angular_spectrum(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate bandlimited angular spectrum based beam propagation. For more Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def band_limited_angular_spectrum(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate bandlimited angular spectrum based beam propagation. For more Matsushima, Kyoji, and Tomoyoshi Shimobaba. \"Band-limited angular spectrum method for numerical simulation of free-space propagation in far and near fields.\" Optics express 17.22 (2009): 19662-19673.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    nv, nu = field.shape\n    fx = np.linspace(-1. / 2. /dx, 1. /2. /dx, nu)\n    fy = np.linspace(-1. / 2. /dx, 1. /2. /dx, nv)\n    FX, FY = np.meshgrid(fx, fy)\n    H_exp = np.exp(1j * k * distance * (1 - (FX * wavelength) ** 2 - (FY * wavelength) ** 2) ** 0.5)\n\n    x = dx * float(nu)\n    y = dx * float(nv)\n    fx_max = 1 / np.sqrt((2 * distance * (1 / x))**2 + 1) / wavelength\n    fy_max = 1 / np.sqrt((2 * distance * (1 / y))**2 + 1) / wavelength\n    H_filter = ((np.abs(FX) &lt; fx_max) &amp; (np.abs(FY) &lt; fy_max))\n    H = H_filter * H_exp\n\n    U1 = np.fft.fftshift(np.fft.fft2(field))\n    U2 = H * U1\n    result = np.fft.ifft2(np.fft.ifftshift(U2))\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.fraunhofer","title":"<code>fraunhofer(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate Fraunhofer based beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def fraunhofer(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate Fraunhofer based beam propagation.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    nv, nu = field.shape\n    l = nu*dx\n    l2 = wavelength*distance/dx\n    dx2 = wavelength*distance/l\n    fx = np.linspace(-l2/2., l2/2., nu)\n    fy = np.linspace(-l2/2., l2/2., nv)\n    FX, FY = np.meshgrid(fx, fy)\n    FZ = FX**2+FY**2\n    c = np.exp(1j*k*distance)/(1j*wavelength*distance) * \\\n        np.exp(1j*k/(2*distance)*FZ)\n    result = c*np.fft.ifftshift(np.fft.fft2(np.fft.fftshift(field)))*dx**2\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.fraunhofer_equal_size_adjust","title":"<code>fraunhofer_equal_size_adjust(field, distance, dx, wavelength)</code>","text":"<p>A definition to match the physical size of the original field with the propagated field.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def fraunhofer_equal_size_adjust(field, distance, dx, wavelength):\n    \"\"\"\n    A definition to match the physical size of the original field with the propagated field.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    new_field        : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    nv, nu = field.shape\n    l1 = nu*dx\n    l2 = wavelength*distance/dx\n    m = l1/l2\n    px = int(m*nu)\n    py = int(m*nv)\n    nx = int(field.shape[0]/2-px/2)\n    ny = int(field.shape[1]/2-py/2)\n    new_field = np.copy(field[nx:nx+px, ny:ny+py])\n    return new_field\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.fraunhofer_inverse","title":"<code>fraunhofer_inverse(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate Inverse Fraunhofer based beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def fraunhofer_inverse(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate Inverse Fraunhofer based beam propagation.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    distance = np.abs(distance)\n    nv, nu = field.shape\n    l = nu*dx\n    l2 = wavelength*distance/dx\n    dx2 = wavelength*distance/l\n    fx = np.linspace(-l2/2., l2/2., nu)\n    fy = np.linspace(-l2/2., l2/2., nv)\n    FX, FY = np.meshgrid(fx, fy)\n    FZ = FX**2+FY**2\n    c = np.exp(1j*k*distance)/(1j*wavelength*distance) * \\\n        np.exp(1j*k/(2*distance)*FZ)\n    result = np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(field/dx**2/c)))\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.gerchberg_saxton","title":"<code>gerchberg_saxton(field, n_iterations, distance, dx, wavelength, slm_range=6.28, propagation_type='IR Fresnel', initial_phase=None)</code>","text":"<p>Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>slm_range</code>           \u2013            <pre><code>           Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more.\n</code></pre> </li> <li> <code>propagation_type</code>               (<code>str</code>, default:                   <code>'IR Fresnel'</code> )           \u2013            <pre><code>           Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer).\n</code></pre> </li> <li> <code>initial_phase</code>           \u2013            <pre><code>           Phase to be added to the initial value.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>hologram</code> (              <code>complex</code> )          \u2013            <p>Calculated complex hologram.</p> </li> <li> <code>reconstruction</code> (              <code>complex</code> )          \u2013            <p>Calculated reconstruction using calculated hologram.</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def gerchberg_saxton(field, n_iterations, distance, dx, wavelength, slm_range=6.28, propagation_type='IR Fresnel', initial_phase=None):\n    \"\"\"\n    Definition to compute a hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Gerchberg, Ralph W. \"A practical algorithm for the determination of phase from image and diffraction plane pictures.\" Optik 35 (1972): 237-246.\n\n    Parameters\n    ----------\n    field            : np.complex64\n                       Complex field (MxN).\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    slm_range        : float\n                       Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more.\n    propagation_type : str\n                       Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer).\n    initial_phase    : np.complex64\n                       Phase to be added to the initial value.\n\n    Returns\n    -------\n    hologram         : np.complex\n                       Calculated complex hologram.\n    reconstruction   : np.complex\n                       Calculated reconstruction using calculated hologram. \n    \"\"\"\n    k = wavenumber(wavelength)\n    target = calculate_amplitude(field)\n    hologram = generate_complex_field(np.ones(field.shape), 0)\n    hologram = zero_pad(hologram)\n    if type(initial_phase) == type(None):\n        hologram = add_random_phase(hologram)\n    else:\n        initial_phase = zero_pad(initial_phase)\n        hologram = add_phase(hologram, initial_phase)\n    center = [int(hologram.shape[0]/2.), int(hologram.shape[1]/2.)]\n    orig_shape = [int(field.shape[0]/2.), int(field.shape[1]/2.)]\n    for i in tqdm(range(n_iterations), leave=False):\n        reconstruction = propagate_beam(\n            hologram, k, distance, dx, wavelength, propagation_type)\n        new_target = calculate_amplitude(reconstruction)\n        new_target[\n            center[0]-orig_shape[0]:center[0]+orig_shape[0],\n            center[1]-orig_shape[1]:center[1]+orig_shape[1]\n        ] = target\n        reconstruction = generate_complex_field(\n            new_target, calculate_phase(reconstruction))\n        hologram = propagate_beam(\n            reconstruction, k, -distance, dx, wavelength, propagation_type)\n        hologram = generate_complex_field(1, calculate_phase(hologram))\n        hologram = hologram[\n            center[0]-orig_shape[0]:center[0]+orig_shape[0],\n            center[1]-orig_shape[1]:center[1]+orig_shape[1],\n        ]\n        hologram = zero_pad(hologram)\n    reconstruction = propagate_beam(\n        hologram, k, distance, dx, wavelength, propagation_type)\n    hologram = hologram[\n        center[0]-orig_shape[0]:center[0]+orig_shape[0],\n        center[1]-orig_shape[1]:center[1]+orig_shape[1]\n    ]\n    reconstruction = reconstruction[\n        center[0]-orig_shape[0]:center[0]+orig_shape[0],\n        center[1]-orig_shape[1]:center[1]+orig_shape[1]\n    ]\n    return hologram, reconstruction\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.gerchberg_saxton_3d","title":"<code>gerchberg_saxton_3d(fields, n_iterations, distances, dx, wavelength, slm_range=6.28, propagation_type='IR Fresnel', initial_phase=None, target_type='no constraint', coefficients=None)</code>","text":"<p>Definition to compute a multi plane hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Zhou, Pengcheng, et al. \"30.4: Multi\u2010plane holographic display with a uniform 3D Gerchberg\u2010Saxton algorithm.\" SID Symposium Digest of Technical Papers. Vol. 46. No. 1. 2015.</p> <p>Parameters:</p> <ul> <li> <code>fields</code>           \u2013            <pre><code>           Complex fields (MxN).\n</code></pre> </li> <li> <code>distances</code>           \u2013            <pre><code>           Propagation distances.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>slm_range</code>           \u2013            <pre><code>           Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more.\n</code></pre> </li> <li> <code>propagation_type</code>               (<code>str</code>, default:                   <code>'IR Fresnel'</code> )           \u2013            <pre><code>           Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer).\n</code></pre> </li> <li> <code>initial_phase</code>           \u2013            <pre><code>           Phase to be added to the initial value.\n</code></pre> </li> <li> <code>target_type</code>           \u2013            <pre><code>           Target type. `No constraint` targets the input target as is. `Double constraint` follows the idea in this paper, which claims to suppress speckle: Chang, Chenliang, et al. \"Speckle-suppressed phase-only holographic three-dimensional display based on double-constraint Gerchberg\u2013Saxton algorithm.\" Applied optics 54.23 (2015): 6994-7001.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>hologram</code> (              <code>complex</code> )          \u2013            <p>Calculated complex hologram.</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def gerchberg_saxton_3d(fields, n_iterations, distances, dx, wavelength, slm_range=6.28, propagation_type='IR Fresnel', initial_phase=None, target_type='no constraint', coefficients=None):\n    \"\"\"\n    Definition to compute a multi plane hologram using an iterative method called Gerchberg-Saxton phase retrieval algorithm. For more on the method, see: Zhou, Pengcheng, et al. \"30.4: Multi\u2010plane holographic display with a uniform 3D Gerchberg\u2010Saxton algorithm.\" SID Symposium Digest of Technical Papers. Vol. 46. No. 1. 2015.\n\n    Parameters\n    ----------\n    fields           : np.complex64\n                       Complex fields (MxN).\n    distances        : list\n                       Propagation distances.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    slm_range        : float\n                       Typically this is equal to two pi. See odak.wave.adjust_phase_only_slm_range() for more.\n    propagation_type : str\n                       Type of the propagation (IR Fresnel, TR Fresnel, Fraunhofer).\n    initial_phase    : np.complex64\n                       Phase to be added to the initial value.\n    target_type      : str\n                       Target type. `No constraint` targets the input target as is. `Double constraint` follows the idea in this paper, which claims to suppress speckle: Chang, Chenliang, et al. \"Speckle-suppressed phase-only holographic three-dimensional display based on double-constraint Gerchberg\u2013Saxton algorithm.\" Applied optics 54.23 (2015): 6994-7001. \n\n    Returns\n    -------\n    hologram         : np.complex\n                       Calculated complex hologram.\n    \"\"\"\n    k = wavenumber(wavelength)\n    targets = calculate_amplitude(np.asarray(fields)).astype(np.float64)\n    hologram = generate_complex_field(np.ones(targets[0].shape), 0)\n    hologram = zero_pad(hologram)\n    if type(initial_phase) == type(None):\n        hologram = add_random_phase(hologram)\n    else:\n        initial_phase = zero_pad(initial_phase)\n        hologram = add_phase(hologram, initial_phase)\n    center = [int(hologram.shape[0]/2.), int(hologram.shape[1]/2.)]\n    orig_shape = [int(fields[0].shape[0]/2.), int(fields[0].shape[1]/2.)]\n    holograms = np.zeros(\n        (len(distances), hologram.shape[0], hologram.shape[1]), dtype=np.complex64)\n    for i in tqdm(range(n_iterations), leave=False):\n        for distance_id in tqdm(range(len(distances)), leave=False):\n            distance = distances[distance_id]\n            reconstruction = propagate_beam(\n                hologram, k, distance, dx, wavelength, propagation_type)\n            if target_type == 'double constraint':\n                if type(coefficients) == type(None):\n                    raise Exception(\n                        \"Provide coeeficients of alpha,beta and gamma for double constraint.\")\n                alpha = coefficients[0]\n                beta = coefficients[1]\n                gamma = coefficients[2]\n                target_current = 2*alpha * \\\n                    np.copy(targets[distance_id])-beta * \\\n                    calculate_amplitude(reconstruction)\n                target_current[target_current == 0] = gamma * \\\n                    np.abs(reconstruction[target_current == 0])\n            elif target_type == 'no constraint':\n                target_current = np.abs(targets[distance_id])\n            new_target = calculate_amplitude(reconstruction)\n            new_target[\n                center[0]-orig_shape[0]:center[0]+orig_shape[0],\n                center[1]-orig_shape[1]:center[1]+orig_shape[1]\n            ] = target_current\n            reconstruction = generate_complex_field(\n                new_target, calculate_phase(reconstruction))\n            hologram_layer = propagate_beam(\n                reconstruction, k, -distance, dx, wavelength, propagation_type)\n            hologram_layer = generate_complex_field(\n                1., calculate_phase(hologram_layer))\n            hologram_layer = hologram_layer[\n                center[0]-orig_shape[0]:center[0]+orig_shape[0],\n                center[1]-orig_shape[1]:center[1]+orig_shape[1]\n            ]\n            hologram_layer = zero_pad(hologram_layer)\n            holograms[distance_id] = hologram_layer\n        hologram = np.sum(holograms, axis=0)\n    hologram = hologram[\n        center[0]-orig_shape[0]:center[0]+orig_shape[0],\n        center[1]-orig_shape[1]:center[1]+orig_shape[1]\n    ]\n    return hologram\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.impulse_response_fresnel","title":"<code>impulse_response_fresnel(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate impulse response based Fresnel approximation for beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def impulse_response_fresnel(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate impulse response based Fresnel approximation for beam propagation.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n\n    \"\"\"\n    nv, nu = field.shape\n    x = np.linspace(-nu / 2 * dx, nu / 2 * dx, nu)\n    y = np.linspace(-nv / 2 * dx, nv / 2 * dx, nv)\n    X, Y = np.meshgrid(x, y)\n    h = 1. / (1j * wavelength * distance) * np.exp(1j * k / (2 * distance) * (X ** 2 + Y ** 2))\n    H = np.fft.fft2(np.fft.fftshift(h))*dx**2\n    U1 = np.fft.fft2(np.fft.fftshift(field))\n    U2 = H * U1\n    result = np.fft.ifftshift(np.fft.ifft2(U2)) /(dx**2)\n\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.propagate_beam","title":"<code>propagate_beam(field, k, distance, dx, wavelength, propagation_type='IR Fresnel')</code>","text":"<p>Definitions for Fresnel Impulse Response (IR), Angular Spectrum (AS), Bandlimited Angular Spectrum (BAS), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. For more on Bandlimited Fresnel impulse response also known as Bandlimited Angular Spectrum method see \"Band-limited Angular Spectrum Method for Numerical Simulation of Free-Space Propagation in Far and Near Fields\".</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> <li> <code>propagation_type</code>               (<code>str</code>, default:                   <code>'IR Fresnel'</code> )           \u2013            <pre><code>           Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer).\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def propagate_beam(field, k, distance, dx, wavelength, propagation_type='IR Fresnel'):\n    \"\"\"\n    Definitions for Fresnel Impulse Response (IR), Angular Spectrum (AS), Bandlimited Angular Spectrum (BAS), Fresnel Transfer Function (TF), Fraunhofer diffraction in accordence with \"Computational Fourier Optics\" by David Vuelz. For more on Bandlimited Fresnel impulse response also known as Bandlimited Angular Spectrum method see \"Band-limited Angular Spectrum Method for Numerical Simulation of Free-Space Propagation in Far and Near Fields\".\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n    propagation_type : str\n                       Type of the propagation (IR Fresnel, Angular Spectrum, Bandlimited Angular Spectrum, TR Fresnel, Fraunhofer).\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    if propagation_type == 'Rayleigh-Sommerfeld':\n        result = rayleigh_sommerfeld(field, k, distance, dx, wavelength)\n    elif propagation_type == 'Angular Spectrum':\n        result = angular_spectrum(field, k, distance, dx, wavelength)\n    elif propagation_type == 'Impulse Response Fresnel':\n        result = impulse_response_fresnel(field, k, distance, dx, wavelength)\n    elif propagation_type == 'Bandlimited Angular Spectrum':\n        result = band_limited_angular_spectrum(\n            field, k, distance, dx, wavelength)\n    elif propagation_type == 'Bandextended Angular Spectrum':\n        result = band_extended_angular_spectrum(\n            field, k, distance, dx, wavelength)\n    elif propagation_type == 'Adaptive Sampling Angular Spectrum':\n        result = adaptive_sampling_angular_spectrum(\n            field, k, distance, dx, wavelength)\n    elif propagation_type == 'Transfer Function Fresnel':\n        result = transfer_function_fresnel(field, k, distance, dx, wavelength)\n    elif propagation_type == 'Fraunhofer':\n        result = fraunhofer(field, k, distance, dx, wavelength)\n    elif propagation_type == 'Fraunhofer Inverse':\n        result = fraunhofer_inverse(field, k, distance, dx, wavelength)\n    else:\n        raise Exception(\"Unknown propagation type selected.\")\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.rayleigh_sommerfeld","title":"<code>rayleigh_sommerfeld(field, k, distance, dx, wavelength)</code>","text":"<p>Definition to compute beam propagation using Rayleigh-Sommerfeld's diffraction formula (Huygens-Fresnel Principle). For more see Section 3.5.2 in Goodman, Joseph W. Introduction to Fourier optics. Roberts and Company Publishers, 2005.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def rayleigh_sommerfeld(field, k, distance, dx, wavelength):\n    \"\"\"\n    Definition to compute beam propagation using Rayleigh-Sommerfeld's diffraction formula (Huygens-Fresnel Principle). For more see Section 3.5.2 in Goodman, Joseph W. Introduction to Fourier optics. Roberts and Company Publishers, 2005.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n    \"\"\"\n    nv, nu = field.shape\n    x = np.linspace(-nv * dx / 2, nv * dx / 2, nv)\n    y = np.linspace(-nu * dx / 2, nu * dx / 2, nu)\n    X, Y = np.meshgrid(x, y)\n    Z = X ** 2 + Y ** 2\n    result = np.zeros(field.shape, dtype=np.complex64)\n    direction = int(distance/np.abs(distance))\n    for i in range(nu):\n        for j in range(nv):\n            if field[i, j] != 0:\n                r01 = np.sqrt(distance ** 2 + (X - X[i, j]) ** 2 + (Y - Y[i, j]) ** 2) * direction\n                cosnr01 = distance / r01\n                result += field[i, j] * np.exp(1j * k * r01) / r01 * cosnr01\n    result *= 1. / (1j * wavelength)\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.classical.transfer_function_fresnel","title":"<code>transfer_function_fresnel(field, k, distance, dx, wavelength)</code>","text":"<p>A definition to calculate convolution based Fresnel approximation for beam propagation.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>           Complex field (MxN).\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>           Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>distance</code>           \u2013            <pre><code>           Propagation distance.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>           Size of one single pixel in the field grid (in meters).\n</code></pre> </li> <li> <code>wavelength</code>           \u2013            <pre><code>           Wavelength of the electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>result</code> (              <code>complex</code> )          \u2013            <p>Final complex field (MxN).</p> </li> </ul> Source code in <code>odak/wave/classical.py</code> <pre><code>def transfer_function_fresnel(field, k, distance, dx, wavelength):\n    \"\"\"\n    A definition to calculate convolution based Fresnel approximation for beam propagation.\n\n    Parameters\n    ----------\n    field            : np.complex\n                       Complex field (MxN).\n    k                : odak.wave.wavenumber\n                       Wave number of a wave, see odak.wave.wavenumber for more.\n    distance         : float\n                       Propagation distance.\n    dx               : float\n                       Size of one single pixel in the field grid (in meters).\n    wavelength       : float\n                       Wavelength of the electric field.\n\n    Returns\n    -------\n    result           : np.complex\n                       Final complex field (MxN).\n\n    \"\"\"\n    nv, nu = field.shape\n    L = nu*dx\n    fx = np.linspace(-1. / 2. /dx, 1. /2. /dx, nu)\n    fy = np.linspace(-1. / 2. /dx, 1. /2. /dx, nv)\n    FX, FY = np.meshgrid(fx, fy)\n    H = np.exp(-1j * distance * (k - np.pi * wavelength * (FX**2 + FY**2) ))\n    U1 = np.fft.fft2(np.fft.fftshift(field)) * ((1/L)**2)\n    U2 = np.fft.fftshift(H)*U1\n    result = np.fft.ifftshift(np.fft.ifft2(U2)) / ((1/L)**2)\n    return result\n</code></pre>"},{"location":"odak/wave/#odak.wave.lens.double_convergence","title":"<code>double_convergence(nx, ny, k, r, dx)</code>","text":"<p>A definition to generate initial phase for a Gerchberg-Saxton method. For more details consult Sun, Peng, et al. \"Holographic near-eye display system based on double-convergence light Gerchberg-Saxton algorithm.\" Optics express 26.8 (2018): 10140-10151.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>     Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>     Size of the output along Y.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>     See odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>r</code>           \u2013            <pre><code>     The distance between location of a light source and an image plane.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>     Pixel pitch.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>function</code> (              <code>ndarray</code> )          \u2013            <p>Generated phase pattern for a Gerchberg-Saxton method.</p> </li> </ul> Source code in <code>odak/wave/lens.py</code> <pre><code>def double_convergence(nx, ny, k, r, dx):\n    \"\"\"\n    A definition to generate initial phase for a Gerchberg-Saxton method. For more details consult Sun, Peng, et al. \"Holographic near-eye display system based on double-convergence light Gerchberg-Saxton algorithm.\" Optics express 26.8 (2018): 10140-10151.\n\n    Parameters\n    ----------\n    nx         : int\n                 Size of the output along X.\n    ny         : int\n                 Size of the output along Y.\n    k          : odak.wave.wavenumber\n                 See odak.wave.wavenumber for more.\n    r          : float\n                 The distance between location of a light source and an image plane.\n    dx         : float\n                 Pixel pitch.\n\n    Returns\n    -------\n    function   : ndarray\n                 Generated phase pattern for a Gerchberg-Saxton method.\n    \"\"\"\n    size = [ny, nx]\n    x = np.linspace(-size[0]*dx/2, size[0]*dx/2, size[0])\n    y = np.linspace(-size[1]*dx/2, size[1]*dx/2, size[1])\n    X, Y = np.meshgrid(x, y)\n    Z = X**2+Y**2\n    w = np.exp(1j*k*Z/r)\n    return w\n</code></pre>"},{"location":"odak/wave/#odak.wave.lens.linear_grating","title":"<code>linear_grating(nx, ny, every=2, add=3.14, axis='x')</code>","text":"<p>A definition to generate a linear grating.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>     Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>     Size of the output along Y.\n</code></pre> </li> <li> <code>every</code>           \u2013            <pre><code>     Add the add value at every given number.\n</code></pre> </li> <li> <code>add</code>           \u2013            <pre><code>     Angle to be added.\n</code></pre> </li> <li> <code>axis</code>           \u2013            <pre><code>     Axis eiter X,Y or both.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field</code> (              <code>ndarray</code> )          \u2013            <p>Linear grating term.</p> </li> </ul> Source code in <code>odak/wave/lens.py</code> <pre><code>def linear_grating(nx, ny, every=2, add=3.14, axis='x'):\n    \"\"\"\n    A definition to generate a linear grating.\n\n    Parameters\n    ----------\n    nx         : int\n                 Size of the output along X.\n    ny         : int\n                 Size of the output along Y.\n    every      : int\n                 Add the add value at every given number.\n    add        : float\n                 Angle to be added.\n    axis       : string\n                 Axis eiter X,Y or both.\n\n    Returns\n    -------\n    field      : ndarray\n                 Linear grating term.\n    \"\"\"\n    grating = np.zeros((nx, ny), dtype=np.complex64)\n    if axis == 'x':\n        grating[::every, :] = np.exp(1j*add)\n    if axis == 'y':\n        grating[:, ::every] = np.exp(1j*add)\n    if axis == 'xy':\n        checker = np.indices((nx, ny)).sum(axis=0) % every\n        checker += 1\n        checker = checker % 2\n        grating = np.exp(1j*checker*add)\n    return grating\n</code></pre>"},{"location":"odak/wave/#odak.wave.lens.prism_phase_function","title":"<code>prism_phase_function(nx, ny, k, angle, dx=0.001, axis='x')</code>","text":"<p>A definition to generate 2D phase function that represents a prism. See Goodman's Introduction to Fourier Optics book for more.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>     Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>     Size of the output along Y.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>     See odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>angle</code>           \u2013            <pre><code>     Tilt angle of the prism in degrees.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>     Pixel pitch.\n</code></pre> </li> <li> <code>axis</code>           \u2013            <pre><code>     Axis of the prism.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>prism</code> (              <code>ndarray</code> )          \u2013            <p>Generated phase function for a prism.</p> </li> </ul> Source code in <code>odak/wave/lens.py</code> <pre><code>def prism_phase_function(nx, ny, k, angle, dx=0.001, axis='x'):\n    \"\"\"\n    A definition to generate 2D phase function that represents a prism. See Goodman's Introduction to Fourier Optics book for more.\n\n    Parameters\n    ----------\n    nx         : int\n                 Size of the output along X.\n    ny         : int\n                 Size of the output along Y.\n    k          : odak.wave.wavenumber\n                 See odak.wave.wavenumber for more.\n    angle      : float\n                 Tilt angle of the prism in degrees.\n    dx         : float\n                 Pixel pitch.\n    axis       : str\n                 Axis of the prism.\n\n    Returns\n    -------\n    prism      : ndarray\n                 Generated phase function for a prism.\n    \"\"\"\n    angle = np.radians(angle)\n    size = [ny, nx]\n    x = np.linspace(-size[0]*dx/2, size[0]*dx/2, size[0])\n    y = np.linspace(-size[1]*dx/2, size[1]*dx/2, size[1])\n    X, Y = np.meshgrid(x, y)\n    if axis == 'y':\n        prism = np.exp(-1j*k*np.sin(angle)*Y)\n    elif axis == 'x':\n        prism = np.exp(-1j*k*np.sin(angle)*X)\n    return prism\n</code></pre>"},{"location":"odak/wave/#odak.wave.lens.quadratic_phase_function","title":"<code>quadratic_phase_function(nx, ny, k, focal=0.4, dx=0.001, offset=[0, 0])</code>","text":"<p>A definition to generate 2D quadratic phase function, which is typically use to represent lenses.</p> <p>Parameters:</p> <ul> <li> <code>nx</code>           \u2013            <pre><code>     Size of the output along X.\n</code></pre> </li> <li> <code>ny</code>           \u2013            <pre><code>     Size of the output along Y.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>     See odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>focal</code>           \u2013            <pre><code>     Focal length of the quadratic phase function.\n</code></pre> </li> <li> <code>dx</code>           \u2013            <pre><code>     Pixel pitch.\n</code></pre> </li> <li> <code>offset</code>           \u2013            <pre><code>     Deviation from the center along X and Y axes.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>function</code> (              <code>ndarray</code> )          \u2013            <p>Generated quadratic phase function.</p> </li> </ul> Source code in <code>odak/wave/lens.py</code> <pre><code>def quadratic_phase_function(nx, ny, k, focal=0.4, dx=0.001, offset=[0, 0]):\n    \"\"\" \n    A definition to generate 2D quadratic phase function, which is typically use to represent lenses.\n\n    Parameters\n    ----------\n    nx         : int\n                 Size of the output along X.\n    ny         : int\n                 Size of the output along Y.\n    k          : odak.wave.wavenumber\n                 See odak.wave.wavenumber for more.\n    focal      : float\n                 Focal length of the quadratic phase function.\n    dx         : float\n                 Pixel pitch.\n    offset     : list\n                 Deviation from the center along X and Y axes.\n\n    Returns\n    -------\n    function   : ndarray\n                 Generated quadratic phase function.\n    \"\"\"\n    size = [nx, ny]\n    x = np.linspace(-size[0]*dx/2, size[0]*dx/2, size[0])-offset[1]*dx\n    y = np.linspace(-size[1]*dx/2, size[1]*dx/2, size[1])-offset[0]*dx\n    X, Y = np.meshgrid(x, y)\n    Z = X**2+Y**2\n    qwf = np.exp(1j*k*0.5*np.sin(Z/focal))\n    return qwf\n</code></pre>"},{"location":"odak/wave/#odak.wave.utils.calculate_amplitude","title":"<code>calculate_amplitude(field)</code>","text":"<p>Definition to calculate amplitude of a single or multiple given electric field(s).</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>       Electric fields or an electric field.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>amplitude</code> (              <code>float</code> )          \u2013            <p>Amplitude or amplitudes of electric field(s).</p> </li> </ul> Source code in <code>odak/wave/utils.py</code> <pre><code>def calculate_amplitude(field):\n    \"\"\" \n    Definition to calculate amplitude of a single or multiple given electric field(s).\n\n    Parameters\n    ----------\n    field        : ndarray.complex or complex\n                   Electric fields or an electric field.\n\n    Returns\n    -------\n    amplitude    : float\n                   Amplitude or amplitudes of electric field(s).\n    \"\"\"\n    amplitude = np.abs(field)\n    return amplitude\n</code></pre>"},{"location":"odak/wave/#odak.wave.utils.calculate_phase","title":"<code>calculate_phase(field, deg=False)</code>","text":"<p>Definition to calculate phase of a single or multiple given electric field(s).</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>       Electric fields or an electric field.\n</code></pre> </li> <li> <code>deg</code>           \u2013            <pre><code>       If set True, the angles will be returned in degrees.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>phase</code> (              <code>float</code> )          \u2013            <p>Phase or phases of electric field(s) in radians.</p> </li> </ul> Source code in <code>odak/wave/utils.py</code> <pre><code>def calculate_phase(field, deg=False):\n    \"\"\" \n    Definition to calculate phase of a single or multiple given electric field(s).\n\n    Parameters\n    ----------\n    field        : ndarray.complex or complex\n                   Electric fields or an electric field.\n    deg          : bool\n                   If set True, the angles will be returned in degrees.\n\n    Returns\n    -------\n    phase        : float\n                   Phase or phases of electric field(s) in radians.\n    \"\"\"\n    phase = np.angle(field)\n    if deg == True:\n        phase *= 180./np.pi\n    return phase\n</code></pre>"},{"location":"odak/wave/#odak.wave.vector.electric_field_per_plane_wave","title":"<code>electric_field_per_plane_wave(amplitude, opd, k, phase=0, w=0, t=0)</code>","text":"<p>Definition to return state of a plane wave at a particular distance and time.</p> <p>Parameters:</p> <ul> <li> <code>amplitude</code>           \u2013            <pre><code>       Amplitude of a wave.\n</code></pre> </li> <li> <code>opd</code>           \u2013            <pre><code>       Optical path difference in mm.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>       Wave number of a wave, see odak.wave.parameters.wavenumber for more.\n</code></pre> </li> <li> <code>phase</code>           \u2013            <pre><code>       Initial phase of a wave.\n</code></pre> </li> <li> <code>w</code>           \u2013            <pre><code>       Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more.\n</code></pre> </li> <li> <code>t</code>           \u2013            <pre><code>       Time in seconds.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field</code> (              <code>complex</code> )          \u2013            <p>A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)).</p> </li> </ul> Source code in <code>odak/wave/vector.py</code> <pre><code>def electric_field_per_plane_wave(amplitude, opd, k, phase=0, w=0, t=0):\n    \"\"\"\n    Definition to return state of a plane wave at a particular distance and time.\n\n    Parameters\n    ----------\n    amplitude    : float\n                   Amplitude of a wave.\n    opd          : float\n                   Optical path difference in mm.\n    k            : float\n                   Wave number of a wave, see odak.wave.parameters.wavenumber for more.\n    phase        : float\n                   Initial phase of a wave.\n    w            : float\n                   Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more.\n    t            : float\n                   Time in seconds.\n\n    Returns\n    -------\n    field        : complex\n                   A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)).\n    \"\"\"\n    field = amplitude*np.exp(1j*(-w*t+opd*k+phase))/opd**2\n    return field\n</code></pre>"},{"location":"odak/wave/#odak.wave.vector.propagate_field","title":"<code>propagate_field(points0, points1, field0, wave_number, direction=1)</code>","text":"<p>Definition to propagate a field from points to an another points in space: propagate a given array of spherical sources to given set of points in space.</p> <p>Parameters:</p> <ul> <li> <code>points0</code>           \u2013            <pre><code>        Start points (i.e. odak.tools.grid_sample).\n</code></pre> </li> <li> <code>points1</code>           \u2013            <pre><code>        End points (ie. odak.tools.grid_sample).\n</code></pre> </li> <li> <code>field0</code>           \u2013            <pre><code>        Field for given starting points.\n</code></pre> </li> <li> <code>wave_number</code>           \u2013            <pre><code>        Wave number of a wave, see odak.wave.wavenumber for more.\n</code></pre> </li> <li> <code>direction</code>           \u2013            <pre><code>        For propagating in forward direction set as 1, otherwise -1.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>field1</code> (              <code>ndarray</code> )          \u2013            <p>Field for given end points.</p> </li> </ul> Source code in <code>odak/wave/vector.py</code> <pre><code>def propagate_field(points0, points1, field0, wave_number, direction=1):\n    \"\"\"\n    Definition to propagate a field from points to an another points in space: propagate a given array of spherical sources to given set of points in space.\n\n    Parameters\n    ----------\n    points0       : ndarray\n                    Start points (i.e. odak.tools.grid_sample).\n    points1       : ndarray\n                    End points (ie. odak.tools.grid_sample).\n    field0        : ndarray\n                    Field for given starting points.\n    wave_number   : float\n                    Wave number of a wave, see odak.wave.wavenumber for more.\n    direction     : float\n                    For propagating in forward direction set as 1, otherwise -1.\n\n    Returns\n    -------\n    field1        : ndarray\n                    Field for given end points.\n    \"\"\"\n    field1 = np.zeros(points1.shape[0], dtype=np.complex64)\n    for point_id in range(points0.shape[0]):\n        point = points0[point_id]\n        distances = distance_between_two_points(\n            point,\n            points1\n        )\n        field1 += electric_field_per_plane_wave(\n            calculate_amplitude(field0[point_id]),\n            distances*direction,\n            wave_number,\n            phase=calculate_phase(field0[point_id])\n        )\n    return field1\n</code></pre>"},{"location":"odak/wave/#odak.wave.vector.propagate_plane_waves","title":"<code>propagate_plane_waves(field, opd, k, w=0, t=0)</code>","text":"<p>Definition to propagate a field representing a plane wave at a particular distance and time.</p> <p>Parameters:</p> <ul> <li> <code>field</code>           \u2013            <pre><code>       Complex field.\n</code></pre> </li> <li> <code>opd</code>           \u2013            <pre><code>       Optical path difference in mm.\n</code></pre> </li> <li> <code>k</code>           \u2013            <pre><code>       Wave number of a wave, see odak.wave.parameters.wavenumber for more.\n</code></pre> </li> <li> <code>w</code>           \u2013            <pre><code>       Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more.\n</code></pre> </li> <li> <code>t</code>           \u2013            <pre><code>       Time in seconds.\n</code></pre> </li> </ul> <p>Returns:</p> <ul> <li> <code>new_field</code> (              <code>complex</code> )          \u2013            <p>A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)).</p> </li> </ul> Source code in <code>odak/wave/vector.py</code> <pre><code>def propagate_plane_waves(field, opd, k, w=0, t=0):\n    \"\"\"\n    Definition to propagate a field representing a plane wave at a particular distance and time.\n\n    Parameters\n    ----------\n    field        : complex\n                   Complex field.\n    opd          : float\n                   Optical path difference in mm.\n    k            : float\n                   Wave number of a wave, see odak.wave.parameters.wavenumber for more.\n    w            : float\n                   Rotation speed of a wave, see odak.wave.parameters.rotationspeed for more.\n    t            : float\n                   Time in seconds.\n\n    Returns\n    -------\n    new_field     : complex\n                    A complex number that provides the resultant field in the complex form A*e^(j(wt+phi)).\n    \"\"\"\n    new_field = field*np.exp(1j*(-w*t+opd*k))/opd**2\n    return new_field\n</code></pre>"}]}